<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discretized Streams: An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tathagata</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discretized Streams: An Efficient and Fault-Tolerant Model for Stream Processing on Large Clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many important &quot;big data&quot; applications need to process data arriving in real time. However, current programming models for distributed stream processing are relatively low-level, often leaving the user to worry about consistency of state across the system and fault recovery. Furthermore, the models that provide fault recovery do so in an expensive manner, requiring either hot repli-cation or long recovery times. We propose a new programming model, discretized streams (D-Streams), that offers a high-level functional programming API, strong consistency, and efficient fault recovery. D-Streams support a new recovery mechanism that improves efficiency over the traditional replication and upstream backup solutions in streaming databases: parallel recovery of lost state across the cluster. We have prototyped D-Streams in an extension to the Spark cluster computing framework called Spark Streaming, which lets users seamlessly in-termix streaming, batch and interactive queries.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much of "big data" is received in real time, and is most valuable at its time of arrival. For example, a social network may want to identify trending conversation topics within minutes, an ad provider may want to train a model of which users click a new ad, and a service operator may want to mine log files to detect failures within seconds.</p><p>To handle the volumes of data and computation they involve, these applications need to be distributed over clusters. However, despite substantial work on cluster programming models for batch computation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">22]</ref>, there are few similarly high-level tools for stream processing. Most current distributed stream processing systems, including Yahoo!'s S4 <ref type="bibr" target="#b18">[19]</ref>, Twitter's Storm <ref type="bibr">[21]</ref>, and streaming databases <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, are based on a recordat-a-time processing model, where nodes receive each record, update internal state, and send out new records in response. This model raises several challenges in a large-scale cloud environment:</p><p>• Fault tolerance: Record-at-a-time systems provide recovery through either replication, where there are two copies of each processing node, or upstream backup, where nodes buffer sent messages and replay them to a second copy of a failed downstream node. Neither approach is attractive in large clusters: replication needs 2× the hardware and may not work if two nodes fail, while upstream backup takes a long time to recover, as the entire system must wait for the standby node to recover the failed node's state.</p><p>• Consistency: Depending on the system, it can be hard to reason about the global state, because different nodes may be processing data that arrived at different times. For example, suppose that a system counts page views from male users on one node and from females on another. If one of these nodes is backlogged, the ratio of their counters will be wrong.</p><p>• Unification with batch processing: Because the interface of streaming systems is event-driven, it is quite different from the APIs of batch systems, so users have to write two versions of each analytics task. In addition, it is difficult to combine streaming data with historical data, e.g., join a stream of events against historical data to make a decision. In this work, we present a new programming model, discretized streams (D-Streams), that overcomes these challenges. The key idea behind D-Streams is to treat a streaming computation as a series of deterministic batch computations on small time intervals. For example, we might place the data received each second into a new interval, and run a MapReduce operation on each interval to compute a count. Similarly, we can perform a running count over several intervals by adding the new counts from each interval to the old result. Two immediate advantages of the D-Stream model are that consistency is well-defined (each record is processed atomically with the interval in which it arrives), and that the processing model is easy to unify with batch systems. In addition, as we shall show, we can use similar recovery mechanisms to batch systems, albeit at a much smaller timescale, to mitigate failures more efficiently than existing streaming systems, i.e., recover data faster at a lower cost.</p><p>There are two key challenges in realizing the DStream model. The first is making the latency (interval granularity) low. Traditional batch systems like Hadoop and Dryad fall short here because they keep state on disk between jobs and take tens of seconds to run each job. Instead, to meet a target latency of several seconds, we keep intermediate state in memory. However, simply putting the state into a general-purpose in-memory storage system, such as a key-value store <ref type="bibr" target="#b16">[17]</ref>, would be expensive due to the cost of data replication. Instead, we build on Resilient Distributed Datasets (RDDs) <ref type="bibr" target="#b21">[23]</ref>, a storage abstraction that can rebuild lost data without replication by tracking the operations needed to recompute it. Along with a fast execution engine (Spark <ref type="bibr" target="#b21">[23]</ref>) that supports tasks as small as 100 ms, we show that we can achieve latencies as low as a second. We believe that this is sufficient for many real-world big data applications, where the timescale of events monitored (e.g., trends in a social network) is much higher.</p><p>The second challenge is recovering quickly from failures. Here, we use the deterministic nature of the batch operations in each interval to provide a new recovery mechanism that has not been present in previous streaming systems: parallel recovery of a lost node's state. Each node in the cluster works to recompute part of the lost node's RDDs, resulting in faster recovery than upstream backup without the cost of replication. Parallel recovery was hard to implement in record-at-a-time systems due to the complex state maintenance protocols needed even for basic replication (e.g., <ref type="bibr">Flux [20]</ref>), 1 but is simple with the deterministic model of D-Streams.</p><p>We have prototyped D-Streams in Spark Streaming, an extension to the Spark cluster computing engine <ref type="bibr" target="#b21">[23]</ref>. In addition to enabling low-latency stream processing, Spark Streaming interoperates cleanly with Spark's batch and interactive processing features, letting users run adhoc queries on arriving streams or mix streaming and historical data from the same high-level API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Discretized Streams (D-Streams)</head><p>The key idea behind our model is to treat streaming computations as a series of deterministic batch computations on small time intervals. The input data received during each interval is stored reliably across the cluster to form an input dataset for that interval. Once the time interval completes, this dataset is processed via deterministic parallel operations, such as map, reduce and groupBy, to produce new datasets representing program outputs or intermediate state. We store these results in resilient distributed datasets (RDDs) <ref type="bibr" target="#b21">[23]</ref>, an efficient storage abstraction that avoids replication by using lineage for fault recovery, as we shall explain later.</p><p>A discretized stream or D-Stream groups together a series of RDDs and lets the user manipulate them to through various operators. D-Streams provide both stateless operators, such as map, which act independently on each time interval, and stateful operators, such as aggre- <ref type="bibr" target="#b0">1</ref> The one parallel recovery algorithm we are aware of, by Hwang et al. <ref type="bibr" target="#b10">[11]</ref>, only tolerates one node failure and cannot mitigate stragglers.  Finally, to recover from failures, both D-Streams and RDDs track their lineage, that is, the set of deterministic operations used to build them. We track this information as a dependency graph, similar to <ref type="figure" target="#fig_1">Figure 1</ref>. When a node fails, we recompute the RDD partitions that were on it by rerunning the map, reduce, etc. operations used to build them on the data still in the cluster. The system also periodically checkpoints state RDDs (e.g., by replicating every fifth RDD) to prevent infinite recomputation, but this does not need to happen for all data, because recovery is often fast: the lost partitions can be recomputed in parallel on separate nodes, as we shall discuss in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D-Stream Operators D-Streams provide two types of operators to let users build streaming programs:</head><p>• Transformation operators, which produce a new DStream from one or more parent streams. These can be either stateless (i.e., act independently on each interval) or stateful (share data across intervals).</p><p>• Output operators, which let the program write data to external systems (e.g., save each RDD to HDFS). D-Streams support the same stateless transformations available in typical batch frameworks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b20">22]</ref>, including map, reduce, groupBy, and join. We reused all of the op- In addition, D-Streams introduce new stateful operators that work over multiple intervals. These include:</p><p>• Windowing: The window operator groups all of the records from a range of past time intervals into a single RDD. For example, in our earlier code, calling pairs.window("5s").reduceByKey <ref type="formula">(</ref> This computes a per-interval count for each time interval only once, but has to add the counts for the past five seconds repeatedly, as in <ref type="figure" target="#fig_2">Figure 2a</ref>. A more efficient version for invertible aggregation functions also takes a function for "subtracting" values and updates state incrementally <ref type="figure" target="#fig_2">(Figure 2b</ref>).</p><p>• Time-skewed joins: Users can join a stream against its own RDDs from some time in the past to compute trends-for example, how current page view counts compare to page views five minutes ago.</p><p>Finally, the user calls output operators to transfer results out of D-Streams into external systems (e.g., for display on a dashboard). We provide two such operators: save, which writes each RDD in a D-Stream to a storage system, 2 and foreach, which runs a user code snippet (any Spark code) on each RDD in a stream. For example, a user can print the counts computed above with:</p><formula xml:id="formula_0">counts.foreach(rdd =&gt; println(rdd.collect()))</formula><p>Unification with Batch and Interactive Processing Because D-Streams follow the same processing model as batch systems, the two can naturally be combined. Spark Streaming provides several powerful features to unify streaming and batch processing.</p><p>First, D-Streams can be combined with static RDDs computed, for example, by loading a file. For example, one might join a stream of incoming tweets against a precomputed spam filter, or compare it with historical data.</p><p>Second, users can run a D-Stream program as a batch job on previous historical data. This makes it easy compute a new streaming report on past data as well.</p><p>Third, users can attach a Scala console to a Spark Streaming program to run ad-hoc queries on D-Streams interactively, using Spark's existing fast interactive query capability <ref type="bibr" target="#b21">[23]</ref>. For example, the user could query the most popular words in a time range by typing:</p><p>counts.slice("21:00", "21:05").topK <ref type="formula">(10)</ref> The ability to quickly query any state in the system is invaluable for users troubleshooting issues in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fault Recovery</head><p>Classical streaming systems update mutable state on a per-record basis and use either replication or upstream backup for fault recovery <ref type="bibr" target="#b11">[12]</ref>.</p><p>The replication approach creates two or more copies of each process in the data flow graph <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref>. Supporting one node failure therefore doubles the hardware cost. Furthermore, if two nodes in the same replica fail, the system is not recoverable. For these reasons, replication is not cost-effective in our large-scale cloud setting.</p><p>In upstream backup <ref type="bibr" target="#b11">[12]</ref>, each upstream node buffers the data sent to downstream nodes locally until it gets an acknowledgement that all related computations are done. When a node fails, the upstream node retransmits all unacknowledged data to a standby node, which takes over the role of the failed node and reprocesses the data. The disadvantage of this approach is long recovery times, as the system must wait for the standby node to catch up.</p><p>To address these issues, D-Streams employ a new approach: parallel recovery. The system periodically checkpoints some of the state RDDs, by asynchronously replicating them to other nodes. For example, in a view count program computing hourly windows, the system could checkpoint results every minute. When a node fails, the system detects the missing RDD partitions and launches tasks to recover them from latest checkpoint. Many fine-grained tasks can be launched at the same time to compute different RDD partitions on different nodes. Thus, parallel recovery finishes faster than upstream backup, at a much lower cost than replication.</p><p>To show the benefit of this approach, we present results from a simple analytical model in <ref type="figure" target="#fig_4">Figure 3</ref>. The model assumes that the system is recovering from a minute-old checkpoint and that the bottleneck resource in the recovery process is CPU. In the upstream backup line, a single idle machine performs all of the recovery and then starts processing new records. It takes a long time to catch up at high system loads because new records for it continue to accumulate while it is rebuilding old state. <ref type="bibr" target="#b2">3</ref> In the other lines, all of the machines partake in recovery, while also processing new records. With more nodes, parallel recovery catches up with the arriving stream much faster than upstream backup. <ref type="bibr" target="#b3">4</ref> One reason why parallel recovery was hard to perform in previous streaming systems is that they process data on a per-record basis, which requires complex and costly bookkeeping protocols (e.g., <ref type="bibr">Flux [20]</ref>) even for basic replication. In contrast, D-Streams apply deterministic transformations at the much coarser granularity of RDD partitions, which leads to far lighter bookkeeping and simple recovery similar to batch data flow systems <ref type="bibr" target="#b5">[6]</ref>.</p><p>Finally, beside node failures, another important concern in large clusters is stragglers <ref type="bibr" target="#b5">[6]</ref>. Fortunately, DStreams can also recover from stragglers in the same way as batch frameworks like MapReduce, by executing speculative backup copies of slow tasks. This type of speculation would again be difficult in a record-at-a-time system, but becomes simple with deterministic tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We implemented a prototype of Spark Streaming that extends the existing Spark runtime and can receive streams of data either from the network or from files periodically uploaded to HDFS. We briefly evaluated its scalability and fault recovery using experiments on Amazon EC2. We used nodes with 4 cores and 15 GB RAM.</p><p>Scalability We evaluated the scalability of the system through two applications: Grep, which counts input records matching a pattern, and WordCount, which performs a sliding window word count over 10 second windows. For both applications, we measured the maximum throughput achievable on different-sized clusters with an end-to-end latency target of either 1 or 2 seconds. By end-to-end latency, we mean the total time between when a record enters the system and when it appears in a result, including the time it waits for its batch to start. We used a batching interval of 0.5s and 100-byte records. <ref type="figure" target="#fig_5">Figure 4</ref> plots the results. We see that the system can process roughly 40 MB/second/node (400K records/s/node) for Grep and 20 MB/s/node (200K records/s/node) for WordCount at sub-second latency, as well as slightly more data if we allow 2s of latency. The system also scales nearly linearly to 50 nodes. The scaling is not perfect because there are more stragglers with more nodes.</p><p>Parallel Recovery We evaluated parallel fault recovery using two applications, both of which received 10 MB/s of data per node on 10 nodes, and used 2-second batching intervals. The first application, MaxCount, performed a word count in each 2-second interval, and computed the maximum count for each word over the past 30 seconds using a sliding window. Because max is not an invertible operation, we used the na¨ıvena¨ıve reduceByWindow that recomputes every 2s. We ran this application both without any checkpointing (except for replication of the input data). Each interval took 1.66s to process before the failure, whereas the average processing time of the interval during which a failure happened was 2.64s (std.dev. 0.19s). Even though results dating back 30 seconds had to be recomputed, this was done in parallel, costing only one extra second of latency.</p><p>The second application performed a sliding word count with a 30s window using the the incremental reduceByWindow operator, and checkpointed data every 30s. Here, a failure-free interval took 1.47s, while an interval with a failure took on average 2.31s (std.dev. 0.43s). Recovery was faster than with MaxCount because each interval's output only depends on three previous RDDs (the total count for the previous interval, the local count for the current interval, and the local count for 30 seconds ago). However, one interesting effect was that any interval within the next 30s after a failure could exhibit a slowdown, because it might discover that part of the local counts for the interval 30s before it were lost. <ref type="figure" target="#fig_6">Figure 5</ref> shows an example of this, where the interval at 30s, when the failure occurs, takes 2.26s to recover, but the intervals at times 32, 34, 46 and 48 also take slightly longer. We plan to eagerly recompute lost RDDs from the past to mitigate this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The seminal academic work on stream processing was in streaming databases such as Aurora, Borealis, Telegraph, and STREAM <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b0">1]</ref>. These systems provide a SQL interface and achieve fault recovery through either replication (an active or passive standby for each node <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>) or upstream backup <ref type="bibr" target="#b11">[12]</ref>. We make two contributions over these systems. First, we provide a general programming interface, similar to DryadLINQ <ref type="bibr" target="#b20">[22]</ref>, instead of just SQL. Second, we provide a more efficient recovery mechanism: parallel recomputation of lost state. Parallel recovery is feasible due to the deterministic nature of DStreams, which lets us recompute lost partitions on other nodes. In contrast, streaming DBs update mutable state for each incoming record, and thus require complex protocols for both replication (e.g., <ref type="bibr">Flux [20]</ref>) and upstream backup <ref type="bibr" target="#b11">[12]</ref>. The only parallel recovery protocol we are aware of, by Hwang et al <ref type="bibr" target="#b10">[11]</ref>, only tolerates one node failure, and cannot handle stragglers.</p><p>In industry, most stream processing frameworks use a lower-level message passing interface, where users write stateful code to process records in a queue. Examples include S4, Storm, and Flume <ref type="bibr" target="#b18">[19,</ref><ref type="bibr">21,</ref><ref type="bibr" target="#b6">7]</ref>. These systems generally guarantee at-least-once message delivery, but unlike D-Streams, they require the user to manually handle state recovery on failures (e.g., by keeping all state in a replicated database) and consistency across nodes.</p><p>Several recent research systems have looked at online processing in clusters. MapReduce Online <ref type="bibr" target="#b4">[5]</ref> is a streaming Hadoop runtime, but cannot compose multiple MapReduce steps into a query or recover stateful reduce tasks. iMR <ref type="bibr" target="#b14">[15]</ref> is an in-situ MapReduce engine for log processing, but does not support more general computation graphs and can lose data on failure. CBP <ref type="bibr" target="#b13">[14]</ref> and Comet <ref type="bibr" target="#b9">[10]</ref> provide "bulk incremental processing" by running MapReduce jobs on new data every few minutes to update state in a distributed file system; however, they incur the high overhead of replicated on-disk storage. In contrast, D-Streams can keep state in memory without costly replication, and achieve order of magnitude lower latencies. Naiad <ref type="bibr" target="#b15">[16]</ref> runs computations incrementally, but does not yet have a cluster implementation or a discussion of fault tolerance. Percolator <ref type="bibr" target="#b17">[18]</ref> performs incremental computations using triggers, but does not offer consistency guarantees across nodes or high-level operators like map and join.</p><p>Finally, our parallel recovery mechanism is conceptually similar to recovery techniques in MapReduce, GFS, and RAMCloud <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17]</ref>, which all leverage repartitioning. Our contribution is to show that this mechanism can be applied on small enough timescales for stream processing. In addition, unlike GFS and RAMCloud, we recompute lost data instead of having to replicate all data, avoiding the network and storage cost of replication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented discretized streams (D-Streams), a stream programming model for large clusters that provides consistency, efficient fault recovery, and powerful integration with batch systems. The key idea is to treat streaming as a series of short batch jobs, and bring down the latency of these jobs as much as possible. This brings many of the benefits of batch processing models to stream processing, including clear consistency semantics and a new parallel recovery technique that we believe is the first truly cost-efficient recovery technique for stream processing in large clusters. Our implementation, Spark Streaming, lets users seamlessly intermix streaming, batch and interactive queries.</p><p>In future work, we plan to use Spark Streaming to build integrated systems that combine these types of processing <ref type="bibr" target="#b7">[8]</ref>, and to further explore the limits of DStreams. In particular, we are interested in pushing the latency even lower (to about 100 ms) and in recovering from failures faster by providing approximate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lineage graph for the RDDs in the view count program. Each oblong shows an RDD, whose partitions are drawn as circles. Lineage is tracked at the granularity of partitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Comparing the na¨ıvena¨ıve variant of reduceByWindow (a) with the incremental variant for invertible functions (b). Both versions compute a per-interval count only once, but the second avoids re-summing each window. Boxes denote RDDs, while arrows show the operations used to compute window [t, t + 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>_+_) yields a D-Stream of word counts on intervals [0, 5), [1, 6), [2, 7), etc. Window is the most general stateful oper- ator, but it is also often inefficient, as it repeats work. • Incremental aggregation: For the common use case of computing an aggregate value, such as a count or sum, over a sliding window, D-Streams have several variants of a reduceByWindow operator. The simplest one only takes an associative "merge" operation for combining values. For example, one might write: pairs.reduceByWindow("5s", (a, b) =&gt; a + b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recovery time for parallel recovery vs. upstream backup on N nodes, as a function of the load before a failure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of Grep and sliding WordCount on different cluster sizes. We show the maximum throughput attainable under an end-to-end latency below 1 second or 2 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Processing time of intervals during one run of the WordCount job. After a failure, the jobs for the next 30s (shown in red) can take slightly longer than average because they may need to recompute the local counts from 30s ago.</figDesc></figure>

			<note place="foot" n="2"> We can use any storage system supported by Hadoop, e.g., HDFS or HBase, by calling into Hadoop&apos;s I/O interfaces to these systems.</note>

			<note place="foot" n="3"> For example, when the load is 0.5, the standby node first has to spend 0.5 minutes to recompute the 1 minute of state lost since the checkpoint, then 0.25 minutes to process the data that arrived in those 0.5 minutes, then 0.125 minutes to process the data in this time, etc. 4 Note that when the system&apos;s load before failure is high enough, it can never recover because the load exceeds the available resources.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">STREAM: The Stanford stream data management system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nishizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rosenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fault-tolerance in the Borealis distributed stream processing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TelegraphCQ: Continuous dataflow processing for an uncertain world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scalable distributed stream processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cetintemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Condie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alvaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<title level="m">MapReduce online. NSDI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Flume</surname></persName>
		</author>
		<ptr target="http://incubator.apache.org/flume/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Continuous analytics: Rethinking query processing in a network-effect world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thombre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CIDR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Google File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP &apos;03</title>
		<meeting>SOSP &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comet: batched stream processing for data intensive distributed computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCC &apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A cooperative, self-configuring high-availability solution for stream processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">High-availability algorithms for distributed stream processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cetintemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Continuous analytics over discontinuous streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Farina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golovko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thombre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stateful bulk processing for incremental analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yocum</surname></persName>
		</author>
		<editor>SoCC</editor>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">In-situ MapReduce for log processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trezzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yocum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Naiad: The animating spirit of rivers and streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP Poster Session</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast crash recovery in RAMCloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale incremental processing using distributed transactions and notifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S4</forename><surname>Apache</surname></persName>
		</author>
		<ptr target="http://incubator.apache.org/s4/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Highly available, fault-tolerant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brewer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>parallel dataflows. SIGMOD</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">´</forename><forename type="middle">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;08</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
