<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Supporting Dynamic GPU Computing Result Reuse in the Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Husheng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangchun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Supporting Dynamic GPU Computing Result Reuse in the Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Graphics processing units (GPUs) have been adopted by major cloud vendors, as GPUs provide orders-of-magnitude speedup for computation-intensive data-parallel applications. In the cloud, efficiently sharing GPU resources among multiple virtual machines (VMs) is not so straightforward. Recent research has been conducted to develop GPU virtualization technologies, making it feasible for VMs to share GPU resources in a reliable manner. This paper seeks to improve the efficiency of sharing GPU resources in the cloud for accelerating general-purpose workloads. Our key observation is that redundant GPU computation requests are being seen in many GPU-accelerated workloads in the cloud, such as cloud gaming where multiple clients playing the same game call GPUs to perform physics simulation. We have measured this redundancy using a gaming case study, and found that more than 24% (47%) of the GPU computation requests called by the same VM (multiple VMs) are identical. To exploit this redundancy, we present GRU (GPU Result re-Use), a GPU sharing, result memoization and reuse ecosystem in a cloud environment. GRU transparently enables VMs in the cloud to share a single GPU efficiently, and memoizes GPU computation results for reuse. It leverages the GPU full-virtualization technology , which enables GPU result memoization and reuse without modification of existing device drivers and operating systems. We have implemented GRU on top of the Xen hypervisor. Preliminary experiments show that GRU is able to achieve a significant speedup of up to 18 times compared to the state-of-the-art GPU virtualization framework, while adding a rather small amount of run-time overheads.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphics processing units (GPUs) are powerful for accelerating computation-intensive data-parallel applications, due to their highly multithreaded architecture and high-bandwidth memory. Along with the support of various programming models and runtime environments such as the compute unified device architecture (CUDA) <ref type="bibr" target="#b20">[21]</ref> from NVIDIA and OpenCL <ref type="bibr" target="#b14">[15]</ref> from Apple, GPUs can be easily used for general-purpose computing (a.k.a., GPGPU) in addition to dedicated graphics applications. GPUs have already been widely used to accelerate general-purpose workloads in many application domains such as supercomputing systems. Unfortunately, enterprise and cloud computing domains have inefficient access to GPU virtualization technology because the required resource isolation of multiple virtual machines (VMs) accessing the same set of GPU resources cannot be efficiently guaranteed.</p><p>Efficiently utilizing GPU resources in the cloud environment is not so straightforward. Current commercial cloud providers (e.g., Amazon Elastic Compute Cloud-EC2) dedicate an individual physical instance of GPUs to a VM, which causes resource under-utilization. A feasible solution to make GPU a truly shared and schedulable resource in the cloud is through GPU virtualization, which allows multiple VMs to access the same set of GPUs simultaneously. Recent research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> implements several GPU virtualization technologies for graphics rendering applications as well as GPGPU computing workloads.</p><p>With GPU virtualization technologies and the corresponding operating systems (OS) support, it is feasible for multiple VMs to share underlying GPUs in a relatively reliable manner. In this paper, we seek to further improve the efficiency of sharing GPU resources in the cloud. Our key observation is that redundancy has been seen in many cloud applications, such as scientific computing and cloud gaming where multiple clients playing the same game call GPUs to perform physics simulation for the same scenes. For example, the popular game Call of Duty: <ref type="bibr">Ghosts [28]</ref> uses GPGPU to enhance smoke particles. When two VM instances are running the same game, the same game scenes and cutscenes will generate redundant GPGPU computations. Moreover, most GPGPU computation-related commercial software and applications prefer to use libraries provided or maintained by NVIDIA, since such libraries are more stable and highly-optimized. This fact increases the probability of seeing redundant GPGPU computing requests. For example, scientific applications use cuBLAS library for matrix operation, cuFFTW library for fast fourier transform (FFT). The standardization of GPGPU interface unifies the kernel code of GPU computation (a piece of GPU-accelerated code which is often relatively simple), making input data be the only difference among GPGPU computations.</p><p>Motivated by this, we explore the possibility of correctly and efficiently reusing GPU computing results among multiple VMs as well as for each individual VM. CPU instruction reuse and memoization have been proposed to make program execution on CPUs faster and more predictable <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. Compared to CPU instruction reuse, reusing results processed on GPUs may be more viable, due to the nature of GPU processing. Employing GPU for general-purpose computing is usually a triple-step procedure: copying input data to GPU, launching a kernel, and acquiring output. This triple-step procedure makes GPGPU a "blackbox"-given input data and computation code, GPU will output the resultwhich provides a more straightforward processing environment compared to computing using CPUs. On GPUs, computation requests with the same input data and kernel will generate the same results. Similar to the dynamic CPU instruction reuse technology, it is possible to dynamically eliminate redundant GPGPU computations by buffering reusable results, so that future requests with the same input (data and kernel) can directly reuse the results without incurring redundant computations. We call this GPU computing result reuse, which has great potential of improving the throughput, response, and energy efficiency of using cloud resources. This paper presents GRU -a GPU sharing, result memoization and reuse ecosystem in the cloud. GRU extends GPUvm <ref type="bibr" target="#b25">[26]</ref> which is an open-source GPU virtualization tool built on top of Xen <ref type="bibr" target="#b3">[4]</ref>, exposing physical GPU model to guest VMs and bridging the semantic gap between domU (i.e., domain U) and dom0 (i.e., domain 0). Leveraging the GPU full-virtualization technology implemented in GPUvm <ref type="bibr" target="#b25">[26]</ref>, GRU transparently allows GPU result reuse and memoization without modification of existing device drivers and operating systems. GRU is composed of three main components: (i) Qemu-Xen which collects the MMIO (memory mapped I/O) operations and forwards GPU commands, (ii) the aggregator which interprets the parameters of GPU requests and manages the GPU memory, and (iii) the reuse engine which uses LUT (look up table) to identify cached results and stores computation results in a "shadow result memory". Once GPU computational requests with the same input data and kernel code are received and identified, GRU directs the results back from shadow result memory without incurring redundant computations. Preliminary experiments show that our prototype can improve the throughput of GPUvm by up to 18 times, while adding a rather small amount of runtime overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Case Study</head><p>We conducted a measurements-based case study to motivate the necessity of GPU computing result reuse. We choose a popular game named "Mirror's Edge" for the  <ref type="bibr" target="#b21">[22]</ref>. PhysX is a physics simulation engine build on top of CUDA, which utilizes GPU to simulate debris, glass shard, spark effects, dynamic fog, etc.</p><p>We used NVIDIA's command line profiling tool nvprof <ref type="bibr" target="#b22">[23]</ref> to monitor the game execution and collect the GPU computation trace. We also wrote an inspection tool employing dll injection and API hook to intercept the parameters of each GPU-related operation. We executed the game twice, each for ten minutes from the prologue chapter. For each execution, we performed random actions for the game play. The statistics of tracing these two executions are shown in <ref type="table" target="#tab_0">Table 1</ref>. During the first execution, we collected 173, 766 CUDA kernel launches that originate from 24 unique kernels. The total execution time of GPGPU computing due to these kernel launches adds up to 22.31 seconds, which implies that for the eight-minute gaming time plus the two-minute logo displaying and opening animation time, nearly 5% is spent on GPGPU computing. During the second execution, 123, 175 CUDA kernel launches that originate from 19 unique kernels are collected. The total GPGPU computing time is 17.1 seconds.</p><p>We use a hash table to store the combination of kernel and its parameters, which include the input data and kernel configuration. For each individual execution, if a kernel launch can find a tuple with the same input (kernel and parameters) in the hash table, then it is considered as a redundant launch; otherwise this launch is stored in the hash table as a new tuple. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the percentage of redundant launches for the two executions reaches up to 24.71% and 25.63%, respectively. Moreover, the identical launch percentage shown in <ref type="table" target="#tab_0">Table 1</ref> indicates the percentage of kernel launches in both executions with the same kernel and data. The number of identical launches is 81, 879, accounting for 47.12% and 66.47% of the kernel launches in the first and second execution, respectively.</p><p>Through this case study of running real-world games, we observe a large percentage of redundant kernel launches, either within a VM or among multiple VMs running the same game. This observation can be exploited in the cloud gaming scenario, where thousands of VM instances run the same game program. We thus seek to eliminate redundant GPU computations by reusing previous results, which leads to faster execution, higher throughput and energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prototype Implementation</head><p>In this section, we present the design and implementation details of the GRU ecosystem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GRU Ecosystem Overview</head><p>GRU is built on top of GPUvm <ref type="bibr" target="#b25">[26]</ref>. GPUvm is an open source Xen-based GPU virtualization software providing virtualization approaches by exposing host GPU device model to guest device drivers. The ecosystem of GRU is described in <ref type="figure" target="#fig_0">Fig. 1</ref>, which is composed of an extended Qemu-Xen, an aggregator, and a reuse engine. We extend the Qemu-Xen emulator and GPUvm's aggregator to intercept GPU memory-copy and computation operations, i.e., kernel code and the corresponding input data.</p><p>To enable result reuse, we implement a reuse engine in dom0, which closely interacts with the aggregator. The extended Qemu-Xen component emulates GPU device models and exposes them to domU, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Currently, discrete GPUs are treated as I/O devices by the operating system. Therefore, all communications between GPU and operating system are via MMIO. All write-and read-related operations on virtual MMIO are intercepted and forwarded to the aggregator through inter process communication (IPC). Similar to the original aggregator implementation presented in GPUvm, GRU's extended aggregator is still in charge of GPU scheduling and memory management. The difference is that our aggregator also needs to identify and interpret high-level behaviors (data transfer or kernel launch) and related parameters (input memory address, size, kernel configuration, etc.) from received GPU operations. The interpreted information is then sent to the reuse engine which lies in dom0 bridging the aggregator and GPU. The reuse engine maintains a LUT to index previous GPU computation results. It also selectively caches results in a "shadow result memory", acting as an in-memory database. After receiving aggregated requests from the aggregator and querying cached results from its LUT, the reuse engine decides whether to issue  <ref type="figure">Figure 2</ref>: Workflow of memoization and reuse system. requests to GPU; or if it is a redundant request, the reuse engine directly returns the results that are stored in the shadow result memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bridging Semantic Gap</head><p>GPU is controlled by the CPU using GPU commands that are architecture-specific. A single GPU command is an atomicity operation, which is essentially a value written by CPU and fetched by GPU at a specific address. A series of such commands combine together to perform a high-level GPU operation, such as host-to-device memory-copy or kernel launch. However, interpreting high-level operations from GPU commands is very challenging, particularly given that current NVIDIA GPU drivers and hardware architectures are close-source. This is the semantic gap we need to bridge at the hypervisor level when implementing GRU.</p><p>According to the full-virtualization implementation of GPUvm, the device driver in domU considers the emulated GPU model as an ordinary physical GPU instance and write GPU commands to it. Qemu-Xen can intercept all the commands related to I/O and forward them to the aggregator. GPUvm has implemented data structures in its aggregator to parse GPU commands. Unfortunately, this is not enough for implementing our reuse framework. Different from GPUvm, our reuse framework not only parses and forwards the GPU commands, but also need to interpret what operations these commands will perform. For example, when we capture a series of memory write operations, we need to identify which combination of operations indicates a host-to-device data transfer operation, and trace the addresses of source and destination together with its size. Each of these steps is quite challenging because NVDIA makes its device details closed to public. Thanks to the reverse engineering work on GPU device and drivers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>, we borrowed and implemented their data structures at GRU's aggregator to trace all the needed information (e.g., input data address and size). After the aggregator gets the information, it sends the interpreted requests to the reuse engine in dom0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Result Memoization and Reuse</head><p>The reuse engine is a user process in dom0, which receives requests from the aggregator, checks reuse possibility, and then returns the reusable computation results to the aggregator. <ref type="figure">Fig. 2</ref> depicts a detailed block diagram of the various components implemented in the reuse engine and how they operate in general. Specifically, an input request is first parsed by the command parser to check whether it is a computation-related request. If it is a host-to-device memory transfer request, the reuse engine delays issuing it to the physical GPU device till the reusability of the corresponding kernel launch request has been checked. The kernel code together with the input data of a kernel launch request are hashed by the hashing function. The LUT uses the corresponding hash value to identify a redundant computation request. Each entry of the LUT contains the address of a reusable result in shadow result memory. We employ pseudo LRU <ref type="bibr" target="#b0">[1]</ref> as the replacement policy.</p><p>A hit occurs in the LUT probing when we have a memoized result, in which case we skip the actual computation on GPU and generate a faked interrupt to the emulated GPU model to notify the completion of computation. Address of the cached result in shadow result memory is returned to the result manager, which locates the cached result and prepares the data transfer to domU. When the result manager receives the followed GPU request of transferring back the result, it performs the actual result transfer. The GPU status parser is in charge of all the faking work for the status change in the emulated GPU model to notify the completion of computations.</p><p>On the contrary, a miss in LUT indicates that result of the current computation request is not available. Such computation requests are then issued to the physical GPU. A new entry is reserved for this computation in LUT, replacing a previous entry according to pseudo-LRU if there is not enough space. After the actual computation on GPU completes, the result will be stored in the shadow result memory and the corresponding tuple in LUT will be updated accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Preliminary Evaluation</head><p>We have conducted a preliminary set of experiments to evaluate: 1) the overhead incurred by GRU; 2) the speedup in terms of throughput under different scenarios. These experiments are conducted on a desktop machine with i7-4790K 4.0 GHz processor, 16 GB memory, and NVIDIA Quadro 6000 GPU. The tool stack of Xen is extended from GPUvm for better supporting address translation and GPU command interpretation. In both dom0 and domU, Linux 3.6.5 is running as the operating system with Nouveau (an open-source device driver for NVIDIA GPUs) as the GPU device driver,  and Gdev <ref type="bibr" target="#b16">[17]</ref>, which is an open-source GPGPU runtime and driver software, is running as the CUDA engine. GPU full-virtualization with optimizations provided by GPUvm is employed as the GPU sharing framework. In the experiments, we start two domU instances, assigning 1GB of memory to each domU and 8GB to dom0. The benchmarks used in this evaluation include Rodinia benchmarks <ref type="bibr" target="#b4">[5]</ref> and one synthesized CUDA game, as listed in <ref type="table" target="#tab_2">Table 2</ref>. First we evaluated the runtime overhead incurred by GRU, compared to the original GPUvm. The overhead mainly comes from three sources: (i) data movement between domU and dom0, (ii) hashing and probing in LUT, and (iii) storing results in the shadow result memory. <ref type="figure" target="#fig_1">Fig. 3</ref> depicts the normalized execution time under GPUvm and GRU when executing each benchmark once. The x-axis of <ref type="figure" target="#fig_1">Fig. 3</ref> represents the benchmark with a specific configuration. For example, madd 1024 means the input data size of the matrix addition kernel is 1024 × 1024. We observe that the overheads incurred under GRU cause a less than 16% increase in execution times for all benchmarks. For a majority of the considered benchmarks (9 out of 11), the overheads cause a less than 7% increase in execution times. As seen in <ref type="figure" target="#fig_1">Fig. 3</ref>, the madd 2048 benchmark yields the highest overhead (16%), which is because the relatively large input data of the madd benchmark cause more overheads in LUT hashing and data movement.</p><p>In the second set of experiments, we evaluated the throughput performance under GRU compared to GPUvm. We synthesize each benchmark to execute (both memory-copy and kernel launch operations) re- We configure the benchmarks to execute in four scenarios with two (five, eight, and ten, respectively) times of LUT hit, which implies 20% (50%, 80%, and 100%, respectively) of the executions will benefit from reusing the cached results. For the chess game, we simulated game play against a chess AI for ten moves, where 20% (50%, 80%, 100%, respectively) of the AI computations are previously cached. We define throughput as the reciprocal of execution time. <ref type="figure" target="#fig_2">Fig. 4</ref> depicts the normalized throughput of benchmarks in four configurations, where GPUvm is used as the baseline. We observe that the throughput of two benchmarks (madd and backprop) are lower than GPUvm when the hit rate is 20%. This is because the overheads GRU introduces negate the benefits of result reuse. When the hit rate is greater than 20%, GRU outperforms GPUvm for all benchmarks, sometimes by a wide margin. For example, when the hit rate equals 80% (100%), GRU is able to achieve a speedup of 4.5 (18.01) and 3.6 (12.15) for mmul 2048 and lud 4096, respectively. The performance improvement for these two benchmarks is the most significant due to their computation-intensive nature. That is, they both have relatively small input data size but rather heavy computation workloads.</p><p>The above-discussed experimental evaluation, although in its preliminary form, show that GRU is promising in achieving significant performance improvement, particularly when the redundancy seen in GPU computation requests increases, while adding a rather small amount of runtime overheads to the current GPU virtualization framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Using GPUs in the cloud. Current approaches to utilize GPUs in the cloud are classified into I/O passthrough <ref type="bibr" target="#b1">[2]</ref>, API remoting <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>, paravirtualization <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref> and full-virtualization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. I/O pass-through, which dedicates an individual physical instance of GPUs to an VM, does not allow GPU sharing among VMs. API remoting, in which GPU calls are forwarded from the guest VMs to the host, needs to change the software stack in both guest and host. The para-virtualization approach provides multiplexing access to GPU through an ideal device model, with the drawback of modifying the device drivers in guest VMs. The full-virtualization approach is able to support GPU sharing among VMs without modification of existing device drivers and operating systems. However, all these aforementioned work does not exploit the idea of GPU computing result reuse. Our framework leverages the GPU full-virtualization approach and implements GPU computing result reuse to further improve the efficiency of sharing GPU resources in the cloud. Computing result reuse. Result reuse for CPU computation has been proposed for decades. <ref type="bibr">Sodani et al. [25]</ref> proposed dynamic instruction reuse to eliminate redundant execution of instruction groups with same inputs. Connors et al. proposed a compiler-directed computation reuse approach <ref type="bibr" target="#b5">[6]</ref> and explored the hardware support model <ref type="bibr" target="#b6">[7]</ref>. <ref type="bibr">Fu et al. proposed VMST [12]</ref> that reuses large blob of binary code to achieve virtual machine introspection. The similarity of computation between CPU and GPU (instructions and inputs) makes result reuse applicable to GPU. Recently, Arnau et al. <ref type="bibr" target="#b2">[3]</ref> presented a hardware memoization approach to eliminate redundant fragment shader executions on a mobile GPU. Different from these work, we focus on realizing GPU computing result reuse in a cloud computing environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Redundant GPU computation requests exist in many GPU-accelerated workloads in the cloud. This observation motivates us to explore the possibility of correctly and efficiently reusing GPU computing results among multiple VMs as well as for an individual VM. We propose GRU, a GPU sharing, result memoization and reuse ecosystem for cloud computing. Preliminary experiments show that GRU is able to improve the throughput of eleven GPGPU benchmarks, often by a wide margin, compared to a state-of-art GPU virtualization framework.</p><p>The GRU system framework is still at its initial stage. In the near future, we plan to complete and optimize the implementation of GRU by performing the following tasks: (i) implementing the framework in a more stabilized and generalized manner, which can reliably support more complicated kernels (e.g., those with data dependencies) and applications (e.g., cloud gaming). (ii) introducing reuse-measuring metrics or compiler-assisted technique to identify partially reusable results. (iii) for result-resilient computation or approximate computation where results do not need to be exactly accurate, exploring methods to enlarge the reusability or introducing signature-based match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Topics</head><p>We hope to receive the reviewers' comments on the idea, design, and implementation of the GRU ecosystem presented in this paper. Any suggestions in particular on improving the efficiency and reliability of our current design and implementation would be greatly appreciated. The controversial point of this paper mainly lies in the efficiency of the current GPU full-virtualization system framework on top of which our work builds upon. Since current GPU virtualization techniques are not yet mature enough, the overhead and latency caused by virtualizing GPUs could be large under certain circumstances.</p><p>This paper may generate interesting discussions about how to utilize and share GPUs in a cloud environment in a more feasible and efficient manner. Researchers from related domains, including GPU architecture, GPGPU computing, operating systems, data/computing reuse, high performance computing, cloud gaming, and cloud computing in general, may be interested in and inspired by this work.</p><p>We would like to point out to the reviewers that our system currently only supports the reuse of computations with the exact same kernel codes and input data. We are currently working on supporting partial result reuse. Also, our current GRU implementation does not support graphics-related rendering and multiplexing applications. As seen in the preliminary experiments, for scenarios where little redundancy is seen in GPU computation requests, the benefits brought by GRU could be trivial (even if the GRU-incurred overheads are reasonably small). Thus, it is important to explore potential techniques that can improve the result reusability under different scenarios. Finally, if the efficiency (w.r.t. overheads) and reliability of current GPU virtualization technologies do not improve to reach a mature stage, the whole ideal of GPU computing result reuse might fall apart.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Ecosystem of sharing and reusing framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Normalized execution time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Normalized throughput peatedly for 10 times after CUDA context initialization. We configure the benchmarks to execute in four scenarios with two (five, eight, and ten, respectively) times of LUT hit, which implies 20% (50%, 80%, and 100%, respectively) of the executions will benefit from reusing the cached results. For the chess game, we simulated game play against a chess AI for ten moves, where 20% (50%, 80%, 100%, respectively) of the AI computations are previously cached. We define throughput as the reciprocal of execution time. Fig. 4 depicts the normalized throughput of benchmarks in four configurations, where GPUvm is used as the baseline. We observe that the throughput of two benchmarks (madd and backprop) are lower than GPUvm when the hit rate is 20%. This is because the overheads GRU introduces negate the benefits of result reuse. When the hit rate is greater than 20%, GRU outperforms GPUvm for all benchmarks, sometimes by a wide margin. For example, when the hit rate equals 80% (100%), GRU is able to achieve a speedup of 4.5 (18.01) and 3.6 (12.15) for mmul 2048 and lud 4096, respectively. The performance improvement for these two benchmarks is the most significant due to their computation-intensive nature. That is, they both have relatively small input data size but rather heavy computation workloads. The above-discussed experimental evaluation, although in its preliminary form, show that GRU is promising in achieving significant performance improvement, particularly when the redundancy seen in GPU computation requests increases, while adding a rather small amount of runtime overheads to the current GPU virtualization framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : The statistics of executed kernels on GPU, from two execution traces of Mirror's Edge</head><label>1</label><figDesc></figDesc><table>#Launch Uniq 
Exe 
Redundant Identical 
R1 
173766 
24 
22.31s 
24.71% 
47.12% 
R2 
123175 
19 
17.10s 
25.63% 
66.47% 

case study, as it not only uses GPU to display but uti-
lizes GPU for general-purpose computing as well. Mir-
ror's Edge is an action-adventure 3-D video game pub-
lished by Electronic Arts. It is powered by the Un-
real Engine 3 [10] which features support for NVIDIA's 
PhysX </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>1</head><label>1</label><figDesc></figDesc><table>Tag 
LRU Kernel Inputs Results 

GPU cmd parser 
&amp; Hash Function 

Input requests 

GPU status parser 
&amp; Result Management 

GPU 

Miss! Issue requests 
to physical GPU 

Output response 

Hit! Return results 
&amp; skip computation 
Probe LUT 

Update LUT 

Raw GPU 
status trans 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Benchmarks used in evaluation</head><label>2</label><figDesc></figDesc><table>NAME 
Description 
chess 
Chinese Chess game with naive AI 
madd 
Matrix addition 
mmul 
Matrix multiplication 
srad 
Speckle reducing anisotropic diffusion 
srad2 
Another version of srad with pseudo-inputs 
backprop Back propagation 
hotspot 
Physics simulation 
lud 
LU Decomposition 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance evaluation of cache replacement policies for the spec cpu2000 benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Zoubi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milenkovic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM-SE</title>
		<imprint>
			<biblScope unit="page" from="267" to="272" />
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amazon elastic compute cloud (amazon ec2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amazon</forename><surname>Com</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eliminating redundant fragment shader executions on a mobile gpu via hardware memoization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Parcerisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xekalakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="529" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barham</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warfield</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rodinia: A benchmark suite for heterogeneous computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skadron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IISWC</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compiler-directed dynamic computation reuse: rationale and initial results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conners</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="158" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hardware support for dynamic activation of compilerdirected computation reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connors</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="222" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gpu virtualization on vmware&apos;s hosted i/o architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dowty</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugerman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the number of gpubased accelerators in high performance clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duato</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quintana-Ort´iort´i</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Rcuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="224" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Epic Games</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Engine</surname></persName>
		</author>
		<ptr target="https://www.unrealengine.com" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<ptr target="http://nouveau.freedesktop.org" />
	</analytic>
	<monogr>
		<title level="j">FREEDESKTOP. Nouveau Open-Source Driver</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Space traveling across vm: Automatically bridging the semantic gap in virtual machine introspection via online kernel data redirection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="586" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A gpgpu transparent virtualization component for high performance computing clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giunta</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coviello</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Par</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="379" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Logv: Low-overhead gpgpu virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gottschlag</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hillenbrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bellosa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FHC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">OpenCL-The open standard for parallel programming of heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Group</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O W</forename></persName>
		</author>
		<ptr target="https://www.khronos.org/opencl" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gpuaccelerated virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kharche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gvim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCVirt</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Gdev: First-Class GPU Resource Management in the Operating System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kato</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcthrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maltzahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX Annual Technical Conference</title>
		<meeting>of USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koscielnicki</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Envytools</surname></persName>
		</author>
		<ptr target="git://0x04.net/envytools.git" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Vmm-independent graphics acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lagar-Cavilla</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And De Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosix</forename><surname>Virtualcl Cluster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platform</surname></persName>
		</author>
		<ptr target="http://www.mosix.org/txt_vcl.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Compute unified device architecture programming guide</title>
		<imprint/>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Physx</surname></persName>
		</author>
		<ptr target="http://www.geforce.com/hardware/technology/physx" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Cuda Toolkit Documentation</surname></persName>
		</author>
		<ptr target="http://docs.nvidia.com/cuda/profiler-users-guide" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gpuaccelerated high-performance computing in virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vcuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="804" to="816" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamic instruction reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sodani</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sohi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="194" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzuki</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kono</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gpuvm</surname></persName>
		</author>
		<title level="m">Why not virtualizing gpus at the hypervisor? In ATC (2014), USENIX</title>
		<imprint>
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A full gpu virtualization solution with mediated pass-through</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cowperthwaite</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
	<note>ATC</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ward</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Call Of Duty</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/Call_of_Duty:_Ghosts" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
