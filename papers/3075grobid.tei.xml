<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Model-Based Namespace Metadata Benchmark for HDFS A Model-Based Namespace Metadata Benchmark for HDFS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 18-20. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">L</forename><surname>Abad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><forename type="middle">L</forename><surname>Abad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Roberts</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep2">Escuela Superior Politécnica del Litoral</orgName>
								<orgName type="department" key="dep3">Inc</orgName>
								<orgName type="institution" key="instit1">University of Illinois at Urbana-Champaign; Nathan Roberts</orgName>
								<orgName type="institution" key="instit2">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
									<region>Inc</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Model-Based Namespace Metadata Benchmark for HDFS A Model-Based Namespace Metadata Benchmark for HDFS</title>
					</analytic>
					<monogr>
						<title level="m">• Philadelphia, PA USENIX Association 11th International Conference on Autonomic Computing</title>
						<imprint>
							<biblScope unit="page">113</biblScope>
							<date type="published">June 18-20. 2014</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 11th International Conference on Autonomic Computing (ICAC &apos;14) is sponsored by USENIX. https://www.usenix.org/conference/icac14/technical-sessions/presentation/abad</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Efficient namespace metadata management is increasingly important as next-generation storage systems are designed for peta and exascales. New schemes have been proposed; however, their evaluation has been insufficient due to a lack of an appropriate namespace metadata benchmark. We describe MimesisBench, a novel names-pace metadata benchmark for next-generation storage systems, and demonstrate its usefulness through a study of the scalability and performance of the Hadoop Distributed File System (HDFS).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There are no metadata-intensive benchmarks with realistic workloads for next-generation storage systems <ref type="bibr">[1,</ref><ref type="bibr" target="#b7">13]</ref>. A few existing tools <ref type="bibr" target="#b1">[7,</ref><ref type="bibr" target="#b3">9]</ref> are useful as microbenchmarks, but do not reproduce realistic workloads.</p><p>The storage community has long-acknowledged the need for benchmarks based on realistic workloads, and programs like SPECsfs2008 <ref type="bibr" target="#b6">[12]</ref> and Filebench <ref type="bibr" target="#b0">[6]</ref> are extensively used for this purpose in traditional storage systems. However, emerging Big Data workloads like MapReduce <ref type="bibr">[2]</ref> have not been properly synthesized yet.</p><p>We present MimesisBench, a novel metadata-intensive storage benchmark suitable for Big Data workloads. MimesisBench consists of a workload modeling tool, a workload generator, and a workload profile from a large cluster at Yahoo. More workloads will be added in the future.</p><p>The model on which MimesisBench is based <ref type="bibr">[3]</ref> allows it to generate type-aware workloads, in which specific types of file behavior can be isolated or modified for 'what-if' and sensitivity analysis. These types of files are modeled autonomically, using unsupervised statistical clustering. The model also supports multidimensional workload scaling.</p><p>MimesisBench's Hadoop-based implementation allows it to be used in any storage system that is compatible with Hadoop (e.g., HDFS, Ceph, CassandraFS, Lustre). We have released the benchmark and workloads as open source so that other researchers can benefit from it <ref type="bibr">1</ref> .</p><p>This paper makes two contributions. First, we extend a model for temporal locality and popularity in object request streams <ref type="bibr">[3]</ref> to: (1) include other operations in addition to regular accesses to objects (opens), (2) support pre-existing files (created before the benchmark), and (3) support a realistic hierarchical namespace. Second, we use this model to implement a metadata-intensive storage benchmark to issue realistic workloads on distributed storage systems. This benchmark can be used to evaluate the performance of storage systems without having to deploy a large cluster and its applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We extend a model we proposed for generating accesses to objects (e.g., opens to files) <ref type="bibr">[3]</ref>, which is able to reproduce temporal correlations in object request streams that arise from the long-term popularity of the objects and their short-term temporal correlations. This model is suitable for Big Data workloads because it supports highly dynamic populations, it is fast and scalable to millions of objects, and it is workload-agnostic so it can be used to model emerging workloads.</p><p>Objects or files in a stationary segment of a request stream are modeled as a set of delayed renewal processes <ref type="bibr" target="#b5">[11]</ref> (one per object). Each object in the stream is characterized by its time of first access, its access interarrival distribution, and its active span (time during which an object is accessed). With this approach, the system-wide popularity distribution asymptotically emerges through explicit reproduction of the per-object request arrivals and active span <ref type="bibr">[3]</ref>. However, this model is unscalable, as it models each object independently.</p><p>To reduce the model size, a lightweight version uses unsupervised statistical clustering (k-means) to identify groups of objects with similar behavior and significantly reduce the model space by modeling "types of objects" instead of individual objects. As a result, the clustered model is suitable for synthetic workload generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Extensions to the model</head><p>The model described above cannot be used directly to test a storage system since it only reproduces accesses (e.g., opens) to an object (i.e., file) and not other operations that are also critical in a storage system like creates and deletes. We propose the following extensions to the original model to make it suitable for namespace metadata benchmarking: (1) additional storage system operations, (2) pre-existing files, and (3) realistic namespaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Extension 1: Additional operations</head><p>We first extend our model to include file creations, deletions and list operations in addition to regular opens. We focus on these operations because together they constitute more than 95% of the namespace metadata operations in MapReduce clusters <ref type="bibr">[2]</ref>, thus accounting for the vast majority of the workload.</p><p>The list operations are like opens and only read or access the namespace. Thus, we model both list and opens together as accesses to files. A parameter keeps track of the percentage of read operations that constitute opens and those that are list.</p><p>On the other hand, creates and deletes write or modify the namespace, and are characterized independently.</p><p>(a) Original model (b) Extended model <ref type="figure">Figure 1</ref>: Operations on a single file, with the original and the extended model. The file's activation time is given by its creation time plus the delay to its first access; the deactivation time is given by its activation time plus its active span. <ref type="figure">Figure 1</ref> shows how the original and extended models generate operations on a file of a particular type (cluster). The extended model requires the following additional per-cluster statistical information: distribution of create interarrivals, distribution of delay to deletion (relative to the object's last access in the stream, which is given by the time of first access + active span), and percentage of accesses that are open operations (list are the remaining percentage). In addition, the activation time is relative to the creation stamp of a file (or to the beginning of the test, for files that already exist at t 0 ). <ref type="table" target="#tab_0">Table 1</ref> lists the parameters required by the extended model to generate events to files.</p><p>This extension does not affect the access patterns preserved by the original model, which are characterized by the per-cluster access interarrival distribution and the per-cluster distribution of active spans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Extension 2: Pre-existing files</head><p>When benchmarking storage systems, we must consider that their performance depends on the state of the underlying file system <ref type="bibr">[1,</ref><ref type="bibr">4]</ref>, namely the pre-existing files and the structure of the hierarchical namespace (the latter is discussed in the next subsection).</p><p>We extend our model to keep track of the number of Array with percentage of files at each depth in hierarchy files (within each file type) that were created sometime before the beginning of the modeled trace.</p><p>We could infer if a file exists prior to the captured trace of namespace events by making the assumption that any file accessed in the trace, but not created during it, is a pre-existing file. However, this approach would lead to an inaccurate model if the trace contains many operations on files that do not exist (e.g., due to users incorrectly entering the name of a file).</p><p>To avoid this problem we can use a namespace metadata trace that contains a snapshot of the namespace (file and directory hierarchy), in addition to the set of events that operate atop that namespace (e.g., open a file, list directory contents) <ref type="bibr">[1]</ref>. The traces we analyzed consist of access logs obtained by parsing the name node audit logs, plus namespace snapshots obtained with Hadoop's Offline Image Viewer tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Extension 3: Realistic namespaces</head><p>Earlier, we proposed [1] a statistical model for generating realistic namespace hierarchies, and implemented a namespace generation module (NGM) based on it. To the best of our knowledge, this is the only available tool that can generate large realistic namespaces 2 .</p><p>Prior to issuing the workload, the NGM is used to generate a realistic directory structure, which preserves the following statistical properties of the original: number of directories, distribution of directories at each depth, and distribution of subdirectories per directory.</p><p>To integrate the files to this directory structure, we add a parameter to the model, the percentage of files at each depth of the hierarchy, and proportionally assign files to each depth according to this parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Assumptions and limitations</head><p>Our model can be used to generate realistic synthetic workloads to evaluate the namespace metadata management subsystems. Dimensions related to data input/output behavior-like a correlation between file size and popularity or the length of data read/write operations-are out of the scope of our model.</p><p>We model stationary segments of object request streams. Workloads consisting of a few stationary segments can be divided using the approach in <ref type="bibr" target="#b9">[15]</ref>. However, in practice, benchmark runs tend to issue the workload of a period of one hour or less, so the need to consider non-stationary segments is not critical.</p><p>We represent arrivals with a sequence of interarrival times X = (X 1 , X 2 , ...), where X is a sequence of independent, identically distributed random variables. In practice, there could be autocorrelations in the arrival process, leading to bursty behavior at different timescales.</p><p>In <ref type="bibr">[3]</ref> we briefly discuss how ON/OFF processes can be used to capture burstiness.</p><p>We assume that each arrival process of the accesses to a file in the storage system is ergodic, so its interarrival distribution can be deduced from a single realization of the process (i.e., the trace of events representing the segment being modeled). In addition, instead of forcing a fit to a particular interarrival distribution, we use the empirical distribution of the interarrivals inferred from the arrivals observed in the trace being modeled.</p><p>Finally, there may be unknown, but important, behavior in the original workloads that our model does not capture. Some of these dimensions, like spatial locality, may be added to our model in the future. However, there is a trade-off of increased complexity due to adding these dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design</head><p>Similar to Hadoop's DFSIO <ref type="bibr" target="#b10">[16]</ref> and S-live <ref type="bibr" target="#b3">[9]</ref>, MimesisBench is a MapReduce job in which a set of mappers simultaneously issue requests to the storage layer. Each mapper is in charge of issuing the operations on files of a particular type (as encapsulated by the model). The cluster used to run MimesisBench must have at least k nodes available to run a mapper task each, so that the full workload can be issued simultaneously and the mappers do not interfere with each other during their operation.</p><p>A run of MimesisBench has two phases: First, the preexisting files are created; next, the workload is issued.</p><p>In each phase, a job coordinator parses the parameters and generates configuration files for each worker. In addition, the job coordinator of the first phase creates the hierarchical namespace based on an input parameter file that has been pre-generated with the Namespace Generation Module (NGM).</p><p>In the first phase, the workers create the target number of files for each type. A parameter tells the workers to use a flat namespace (create all files in a single, configurable path) or a hierarchical one. Files of each type are created at different levels of the namespace hierarchy, proportionally to the configuration parameter of files at each depth (which indicates what percentage of files are located at each depth). The subdirectories at each depth are assigned to a file type, proportionally to the weight of the cluster (w j ) to which the file belongs.</p><p>In the second phase, the workers issue the load. Each load generator worker reads the configuration for the specific file type that it has been assigned and waits in a time-based barrier 3 to start issuing the load corresponding to the files that belong to the type it is in charge of. Two data structures are used to keep track of the files and events: a PriorityBlockingQueue of files (sorted by the timestamp of the next event-create, open, etc.-of that file) and a FIFO BlockingQueue of events to be issued <ref type="bibr">4</ref> . Three threads coordinate access to these data structures: a file introduction thread adds files to the priority queue (using the cluster's create interarrival distribution) <ref type="bibr">5</ref> , another thread continuously polls the priority queue and adds details of the next event to be issued to the back of the FIFO queue. Finally, a consumer thread pulls the information of the next event to be issued from the FIFO queue and schedules it to be issued at the proper time, using Java's ScheduledExecutorService.</p><p>All file create operations create files of size zero. This allows us to ignore the effect of writing bytes to a file, handled by the data nodes, and concentrate on evaluating the namespace metadata server (name node in HDFS) performance.</p><p>A configurable maximum allowed drift is used to abort a run of the benchmark if the events are falling behind from their original schedule. In that case, more mappers would be needed to issue the workload.</p><p>A collector reducer task gathers the stats from the workers and generates aggregate results (see <ref type="table" target="#tab_1">Table 2</ref>). </p><formula xml:id="formula_0">n new = n + w 1 × n (1) w 1 new × n new = 2(w 1 × n) (2) w j new × n new = w j × n, j ∈ {2, ..., k}<label>(3)</label></formula><p>Time. Interarrivals can be accelerated by multiplying the random variable by a scaling factor between 0 and 1. A scaling factor of 1 reproduces the original workload, while a scaling factor of 0 provides maximum stress on the system by issuing all the operations as fast as possible (0 millisecond wait between operations). Interarrivals can also be slowed down by multiplying the random variable by a constant greater than 1.</p><p>Active span. The active span random variable can also be multiplied by a user-defined constant. Modifying the active span has the effect of modifying the number of accesses of the files, thus modifying their popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Isolating workloads</head><p>The user can choose to isolate the workload of a particular file type or cluster (i.e., turn-off the other types of files). This can be used to analyze how a particular type of file affects the performance of the system.</p><p>Summary <ref type="table" target="#tab_2">Table 3</ref> shows a summary of the features of MimesisBench and other related tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluating a Big Data storage system</head><p>We demonstrate the usefulness of MimesisBench by using it to evaluate the performance and scalability of the HDFS name node across several dimensions.</p><p>We modeled a 1-day (12/1/2011) namespace metadata trace from a Hadoop cluster at Yahoo. The trace came from a 4100-node production cluster <ref type="bibr" target="#b4">[10]</ref>. It contains 60.9 million opens events that target 4.3 million distinct files, and 4 PB of used storage space. For a detailed workload characterization of this cluster see <ref type="bibr">[2]</ref>. We modeled the trace using 30 file types or clusters (i.e., k = 30 for the k-means clustering algorithm). We chose this value of k because it was the smallest for which we could obtain a close approximation of the file popularity distribution (Kolmogorov-Smirnov distance between the real and synthetic CDFs &lt; 4%).</p><p>Our performance testbed had 32 nodes, each with 2 Xeon 2.4GHz quad-core processors and 24GB RAM. <ref type="figure" target="#fig_0">Figure 2</ref> shows the effect on the operation latency as interarrivals are accelerated. In general, read operations are faster than write operations because acquiring an exclusive lock is not necessary to read the namespace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Latency of opens, creates and deletes</head><p>The latency of the operations is not initially affected by issuing them faster. This shows, that the performance of the name node is not degraded as more operations are issued per second. However, when the clients issue more than 16,800 operations per second (c ≥ 0.2), the name node's performance starts degrading rapidly. This information can be used to determine whether the name node can properly support an increased workload in the future. <ref type="figure" target="#fig_1">Figure 3</ref> shows the impact a hierarchical namespace (versus a flat one) has on the name node. The performance degrades significantly faster on a flat namespace than on a hierarchical one. The hierarchical namespace can serve up to 19,696 ops/sec versus 10,284 ops/sec for the flat namespace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Flat versus hierarchical namespaces</head><p>These results show that using benchmarks that create files in a flat namespace (see <ref type="table" target="#tab_2">Table 3</ref>) is not desirable as they place a heavier and unrealistic burden on the locking mechanisms of the metadata server. In this case, the problem is with the locking used to log namespace write operations used for auditing purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Isolating workloads</head><p>We isolated the workload of two file types with different access patterns and observe the effects on latency and throughput (see <ref type="table">Table 4</ref>). We chose these two clusters because they represent two extreme, read-mostly (cluster 29) and write-heavy (cluster 17), yet realistic, workloads.</p><p>We ran several tests with events issued at normal speed * Creates hierarchies with a given depth and width. Characteristics like subdirectories per directory and directories per depth, are not supported. (interarrival scaling factor = 1) and at full speed (interarrival scaling factor = 0). To ensure a fair comparison, we adjusted the number of mappers issuing the workload so that the total number of operations issued in both tests was roughly the same. At 30 mappers for cluster 17, and 12 mappers for cluster 29, the number of operations issued in the tests was 3 559 101 ± 0.9%. <ref type="figure" target="#fig_2">Figure 4</ref> shows the latency of open events. The latency of the open events degrades significantly more in a write-heavy workload: When operations are issued at full speed, the latency of opens in the write-heavy workload increases 3.9x in cluster 17 versus 1.4x in cluster 29. In addition, at maximum issuing speed (interarrival scaling = 0) the name node can serve 8 times more operations when the workload constitutes only reads: 53,233 vs. 6,453 ops/sec for clusters 29 and 17, respectively. <ref type="table">Table 4</ref>: Characteristics of the two clusters whose workload was isolated. We chose these two clusters because they represent two extreme, read-mostly and write-heavy, workloads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluating a proposed enhancement</head><p>A common use for benchmarks is to evaluate a new design against an old design. In this subsection, we show the results of one example of this type of evaluation.</p><p>The HDFS-5239 <ref type="bibr" target="#b2">[8]</ref> Jira allows the namespace lock fairness to be changed from fair (default) to unfair. This  <ref type="table">Table 4</ref>). Standard deviation &lt; 0.5 msecs.</p><p>change may be desirable in large clusters, as an unfair lock has a higher throughput than a fair lock.  <ref type="table" target="#tab_4">Table 5</ref> shows the name node throughput improvement for the following scenarios: full workload on a hierarchical namespace, full workload on a flat namespace, a realistic read-mostly workload (cluster 29), and a realistic write-heavy workload (cluster 17). The improvement when the full workload is issued is around 10%. However, for a read-mostly workload, the improvement is as high as 94%. This is a result of the bottleneck that exists in the logging mechanism used for write operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Stability of the results</head><p>Benchmarks are often used to evaluate new designs against an old design, or against another competing new design. An improvement of 10% in performance may be desirable to push in a large system, as long as we can trust the results of the benchmark. For this reason, it is important to have a small variability in the results produced by a benchmark. For example, it is not reasonable to trust a 10% performance improvement if the coefficient of variation, c v , of the results is 8%.</p><p>Our experiments had a very small standard deviation and corresponding coefficient of variation (or the ratio of the standard deviation to the mean). For all the sets of experiments we ran, the greatest observed coefficient of variation was 2.38%, with an average coefficient of variation of 1.08%. These results suggest that MimesisBench is suitable as a tool to evaluate expected performance improvements of new designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Some prior tools provide a subset of the features of MimesisBench, as summarized in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Mdtest issues metadata intensive workloads. However, it does not support realistic workloads or namespaces. In addition, mdtest interfaces with the storage system via system calls and has not been ported to the interfaces of next-generation storage systems, nor does it allow distributed workload generation.</p><p>Filebench <ref type="bibr" target="#b0">[6]</ref> uses a POSIX-compliant interface and includes traditional workloads like web server, file server, and database server. Emerging Big Data workloads have not been included as pre-defined workloads.</p><p>SPECsfs2008 <ref type="bibr" target="#b6">[12]</ref> is used as a standard to enable comparison of file server throughput and response times across different vendors and configurations. It supports NFSv3 and CIFS APIs and comes with a pre-defined workload based on enterprise NFS and CIFS workloads.</p><p>For HDFS, the Hadoop developer community has designed two benchmarks that can test I/O operations and do simple stress-tests on storage layer: DFSIO <ref type="bibr" target="#b10">[16]</ref> and S-live <ref type="bibr" target="#b3">[9]</ref>. However, these benchmarks do not reproduce realistic workloads and can only be used as microbenchmarks. Another Apache tool, NNBench <ref type="bibr" target="#b1">[7]</ref> was created to benchmark the HDFS name node; however, it can only issue one type of operation at a time, and does not work atop a realistic namespace.</p><p>Tarasov et al. <ref type="bibr" target="#b8">[14]</ref> found that traditional workloads, like those provided with Filebench, are a poor replacement for virtual machine workloads. We consider another emerging workload: MapReduce clusters.</p><p>Impressions <ref type="bibr">[4]</ref> generates realistic file system images; however, it is not readily coupled with a workload generator to easily reproduce workloads that operate on the generated namespace. Furthermore, the generative model used by Impressions to create the file system hierarchy is not able to reproduce the distributions observed in our analysis, nor is it able scale to the large hierarchies observed in the Big Data systems we have studied.</p><p>Chen et al. <ref type="bibr">[5]</ref> proposed the use of multi-dimensional statistical correlation (k-means) to obtain storage system access patterns and design insights in user, application, file, and directory levels. However, the clustering was not leveraged for workload generation or benchmarking.</p><p>In earlier work, we developed Mimesis <ref type="bibr">[1]</ref>, a synthetic trace generator for namespace metadata traces. However, it was too CPU-intensive to issue operations at real-time and as such is inapplicable for benchmarking real systems (though its synthetic traces could be used in tracebased evaluations). In addition, its model does not reproduce file popularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented MimesisBench, a metadata-intensive storage benchmark suitable for Big Data workloads. MimesisBench consists of a workload-generating software and a workload from a Yahoo Big Data cluster.</p><p>MimesisBench is extensible and more workloads can be added in the future. It is based on a novel model that allows it to generate type-aware workloads, in which specific type of file behavior can be isolated or modified for 'what-if' and sensitivity analysis. In addition, it supports multi-dimensional workload scaling.</p><p>MimesisBench is implemented on top of the Apache Hadoop framework, which allows it to be used in any storage system that is compatible with Hadoop. We have released the benchmark and workload as open source.</p><p>A study of the performance and scalability of HDFS was presented to show the usefulness of metadataintensive benchmarking using MimesisBench.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Left: Operation latency for varying interarrival speeds, av- eraged across five runs. In each run, the interarrival random variable, X, was multiplied by a constant, c, shown in the x-axis. When c = 0, the operations are issued as fast as possible. When c = 1 the opera- tion interarrival mimics the interarrivals of the original trace. For all data points, the standard deviation is very small (&lt; 0.7 msecs). Right: Throughput vs. operation (open) latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Throughput vs. open latency in a hierarchical and a flat namespace. Five runs; standard deviation &lt; 1.5 msecs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Latency of open events, with two interarrival scaling factors (0 or operations at full speed, and 1 or operations issued at their original speed), averaged across five runs, for two clusters with different read/write ratios (see Table 4). Standard deviation &lt; 0.5 msecs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Parameters used to generate events on files; O i is the object or file i, i ∈ {1, ..., n}; K j is the cluster or file type j, j ∈ {1, ..., k}. Symbol Description n number of files in the trace or request stream k number of clusters or types of files F j interarrival distribution of accesses to an object in K0 ); C j + G j = Activation j H j active span distribution ∀O i ∈ K j ; Activation j + H j = Deactivation j D j distribution of delay to delete event of objects in K j ; Deactivation j + D j = Deletion j p percentage of accesses that are opens (1 − p: list) w j percentage of objects in K j ;</head><label>1</label><figDesc></figDesc><table>j 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :files. A workload profile comes with a con- figured number of files, n, as observed in the original trace on which the model is based. One can increase or decrease the number of files to emulate a larger load.unchanged, we can obtain the values of n new and {w 1 new , w 2 new , ..., w k new } by solving:</head><label>2</label><figDesc></figDesc><table>Statistics reported by MimesisBench. 
Description 
Unit 

Throughput 
ops/sec 
Active time (workers) 
msecs 
Operations issued 
ops 
Successful creates/deletes/opens/list 
ops 
Average latency: create/delete/open/list 
msecs 

3.1 Scaling workloads 

A workload can be scaled across several dimensions: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Features of MimesisBench and other related tools. See Section 5 for a description of these tools.</head><label>3</label><figDesc></figDesc><table>Feature 
NNBench 
DFSIO 
S-live 
Filebench 
SPECsfs 
mdtest 
MimesisBench 

For next-generation storage 
񮽙 
񮽙 
񮽙 
񮽙 
Metadata-intensive 
񮽙 
񮽙 
񮽙 
Realistic workloads 
񮽙 
񮽙 
񮽙 
Type-aware workload 
񮽙 
񮽙 
Autonomic type-awareness 
񮽙 
Hierarchical namespace 
Semi-flat + 
Semi-flat + 
Limited  *  
Fixed 
񮽙 
Issues I/O load 





+ Only multiple directories at depth 1 are supported. 
 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Throughput improvement obtained when using an unfair namespace lock (HDFS-5239), for four different workloads.</figDesc><table>Fair lock 
Unfair lock 
Improvement 

Hierarchical 
19 696 ops/sec 
21 899 ops/sec 
11% 
Flat 
10 255 ops/sec 
11 299 ops/sec 
10% 
Read-mostly 
53 324 ops/sec 
103 331 ops/sec 
94% 
Write-heavy 
8 694 ops/sec 
8 701 ops/sec 
1% 

</table></figure>

			<note place="foot" n="1"> Available: http://sites.google.com/site/cristinaabad</note>

			<note place="foot" n="2"> We tested the only other alternative system, the Impressions framework [4], and were not able to generate the large namespaces observed in Big Data storage deployments since it was designed to model smaller (more traditional) namespaces. Furthermore, at the time of this writing, the Impressions framework is no longer available for download.</note>

			<note place="foot" n="3"> The clocks of the nodes in a Hadoop cluster are typically synchronized to support Kerberos authentication. 4 The size of these queues in the current implementation is set to be the number of files of that particular type for the former, and 1 500 000 for the latter. 5 Pre-existing files of a type are added to the queue upon loading.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our anonymous reviewers for their many insightful comments and suggestions. This work was partially completed during C. Abad's PhD studies at UIUC and during her internship at Yahoo. R. Campbell and C. Abad were supported in part by AFRL grant FA8750-11-2-0084. Y. Lu is partially supported by NSF grant CNS-1150080. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">net/projects/ filebench</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filebench</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
	<note>Last accessed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Benchmarking and stress testing an Hadoop cluster with TeraSort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noll</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename></persName>
		</author>
		<ptr target="TestDFSIO&amp;co.www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench" />
		<imprint>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
	<note>Last accessed</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Allow FSNamesystem lock fairness to be configurable. issues.apache.org/jira/ browse/HDFS-5239</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sharp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-03-26" />
		</imprint>
	</monogr>
	<note>Last accessed</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A stress-test tool for HDFS (Apache JIRA HDFS-708). issues.apache. org/jira/browse/HDFS-708</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shvachko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010-12-30" />
		</imprint>
	</monogr>
<note type="report_type">Last accessed</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Hadoop Distributed File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shvachko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chansler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MSST</title>
		<meeting>MSST</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chapter 14: Renewal processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegrist</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Virtual Laboratories in Probability and Statistics. 2013. Last accessed</title>
		<imprint>
			<date type="published" when="2013-03-23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specsfs2008</surname></persName>
		</author>
		<ptr target="http://www.spec.org/sfs2008/" />
		<imprint>
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
<note type="report_type">Last accessed</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benchmarking file system benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarasov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhanage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seltzer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Virtual machine workloads: The case for new benchmarks for NAS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarasov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zadok</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Usenix FAST</title>
		<meeting>Usenix FAST</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Extracting flexible, replayable models from large block traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tarasov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hilde-Brand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zadok</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Usenix FAST</title>
		<meeting>Usenix FAST</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Next: Benchmarking performance. hortonworks.com/blog/delivering-onhadoop-next-benchmarking-performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vavilapalli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Delivering On Hadoop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-12-30" />
		</imprint>
	</monogr>
	<note>Last accessed</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
