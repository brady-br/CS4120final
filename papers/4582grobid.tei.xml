<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">USENIX Association 7th USENIX Conference on File and Storage Technologies 211 Smoke and Mirrors: Reflecting Files at a Geographically Remote Location Without Loss of Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakim</forename><surname>Weatherspoon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><surname>Ganesh</surname></persName>
							<email>lakshmi@cs.cornell.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Marian</surname></persName>
							<email>tudorm@cs.cornell.edu</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Microsoft Research, Silicon Valley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Balakrishnan</surname></persName>
							<email>maheshba@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Birman</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14853</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">USENIX Association 7th USENIX Conference on File and Storage Technologies 211 Smoke and Mirrors: Reflecting Files at a Geographically Remote Location Without Loss of Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The Smoke and Mirrors File System (SMFS) mirrors files at geographically remote datacenter locations with negligible impact on file system performance at the primary site, and minimal degradation as a function of link latency. It accomplishes this goal using wide-area links that run at extremely high speeds, but have long round-trip-time latencies-a combination of properties that poses problems for traditional mirroring solutions. In addition to its raw speed, SMFS maintains good synchronization: should the primary site become completely unavailable, the system minimizes loss of work, even for applications that simultaneously update groups of files. We present the SMFS design, then evaluate the system on Emulab and the Cornell National Lambda Rail (NLR) Ring testbed. Intended applications include wide-area file sharing and remote backup for disaster recovery.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Securing data from large-scale disasters is important, especially for critical enterprises such as major banks, brokerages, and other service providers. Data loss can be catastrophic for any company -Gartner estimates that 40% of enterprises that experience a disaster (e.g. loss of a site) go out of business within five years <ref type="bibr" target="#b41">[41]</ref>. Data loss failure in a large bank can have much greater consequences with potentially global implications.</p><p>Accordingly, many organizations are looking at dedicated high-speed optical links as a disaster tolerance option: they hope to continuously mirror vital data at remote locations, ensuring safety from geographically localized failures such as those caused by natural disasters or other calamities. However, taking advantage of this new capability in the wide-area has been a challenge; existing mirroring solutions are highly latency sensitive <ref type="bibr" target="#b18">[19]</ref>. As a result, many critical enterprises operate at risk of catastrophic data loss <ref type="bibr" target="#b21">[22]</ref>.</p><p>The central trade-off involves balancing safety against performance. So-called synchronous mirroring solutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> block applications until data is safely mirrored at the remote location: the primary site waits for an acknowledgment from the remote site before allowing the application to continue executing. These are very safe, but extremely sensitive to link latency. Semisynchronous mirroring solutions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b42">42]</ref> allow the application to continue executing once data has been written to a local disk; the updates are transmitted as soon as possible, but data can still be lost if disaster strikes. The end of the spectrum is fully asynchronous: not only does the application resume as soon as the data is written locally, but updates are also batched and may be transmitted periodically, for instance every thirty minutes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">31]</ref>. These solutions perform best, but have the weakest safety guarantees. Today, most enterprises primarily use asynchronous or semi-synchronous remote mirroring solutions over the wide-area, despite the significant risks posed by such a stance. Their applications simply cannot tolerate the performance degradation of synchronous solutions <ref type="bibr" target="#b21">[22]</ref>. The US Treasury Department and the Finance Sector Technology Consortium have identified the creation of new options as a top priority for the community <ref type="bibr" target="#b30">[30]</ref>.</p><p>In this paper, we explore a new mirroring option called network-sync, which potentially offers stronger guarantees on data reliability than semi-synchronous and asynchronous solutions while retaining their performance. It is designed around two principles. First, it proactively adds redundancy at the network level to transmitted data. Second, it exposes the level of in-network redundancy added for any sent data via feedback notifications. Proactive redundancy allows for reliable transmission with latency and jitter independent of the length of the link, a property critical for long-distance mirroring. Feedback makes it possible for a file system (or other applications) to respond to clients as soon as enough recovery data has been transmitted to ensure that the desired safety level has been reached. (1) Synchronous mirroring provides a remote-sync guarantee: data is not lost in the event of disaster, but performance is extremely sensitive to the distance between sites. (2) Asynchronous and semi-synchronous mirroring give a local-sync guarantee: performance is independent of distance between mirrors, but can suffer significant data loss when disaster strikes. (3) A new network-sync mirroring option with performance similar to local-sync protocols, but with improved reliability.</p><p>Of course, data can still be lost; network-sync is not as safe as a synchronous solution. If the primary site fails and the wide-area network simultaneously partitions, data will still be lost. Such scenarios are uncommon, however. Network-sync offers the developer a valuable new option for trading data reliability against performance.</p><p>Although this paper focuses on the Smoke and Mirrors File System (SMFS), we believe that many kinds of applications could benefit from a network-sync option. These include other kinds of storage systems where remote mirroring is performed by a disk array (e.g. <ref type="bibr" target="#b11">[12]</ref>), a storage area network (e.g. <ref type="bibr" target="#b18">[19]</ref>), or a more traditional file server (e.g. <ref type="bibr" target="#b31">[31]</ref>). Network-sync might also be valuable in transactional databases that stream update logs from a primary site to a backup, or to other kinds of faulttolerant services.</p><p>Beyond its use of the network-sync option, SMFS has a second interesting property. Many applications update files in groups, and in such cases, if even one of the files in a group is out of date, the whole group may be useless <ref type="bibr">(Seneca [19]</ref> calls this atomic, in-order asynchronous batched commits; SnapMirror <ref type="bibr" target="#b31">[31]</ref> offers a similar capability). SMFS addresses the need in two ways. First, if an application updates multiple files in a short period of time, the updates will reach the remote site with minimal temporal skew. Second, SMFS maintains groupmirroring consistency, in which files in the same file system can be updated as a group in a single operation where the group of updates will all be reflected by the remote mirror site atomically, either all or none.</p><p>In summary, our paper makes the following contributions:</p><p>â€¢ We propose a new remote mirroring option called network-sync in which error-correction packets are proactively transmitted, and link-state is exposed through a callback interface.</p><p>â€¢ We describe the implementation and evaluation of SMFS, a new mirroring file system that supports both capabilities, using an emulated wide-area network (Emulab <ref type="bibr" target="#b40">[40]</ref>) and the Cornell National Lambda Rail (NLR) Ring testbed <ref type="bibr" target="#b0">[1]</ref>. This evaluation shows that SMFS:</p><p>-Can be tuned to lose little or no data in the event of a rolling disaster.</p><p>-Supports high update throughput, masking wide-area latency between the primary site and the mirror.</p><p>-Minimizes jitter when files are updated in short periods of time.</p><p>â€¢ We show that SMFS has good group-update performance and suggest that this represents a benefit to using a log-structured file architecture in remote mirroring.</p><p>The rest of this paper is structured as follows. We discuss our fault model in Section 2. In Section 3, we describe the network-sync option. We describe the SMFS protocols that interact with the network-sync option in Section 4. In Section 5, we evaluate the design and implementation. Finally, Section 6 describes related work and Section 7 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">What's the Worst that Could</head><p>Happen?</p><p>We argue that our work responds to a serious imperative confronted by the financial community (as well as by other critical infrastructure providers). As noted above, today many enterprises opt to use asynchronous or semisynchronous remote mirroring solutions despite the risks they pose, because synchronous solutions are perceived as prohibitively expensive in terms of performance <ref type="bibr" target="#b21">[22]</ref>. In effect, these enterprises have concluded that there simply is no way to maintain a backup at geographically re- mote distances at the update rates seen within their datacenters. Faced with this apparent impossibility, they literally risk disaster. It is not feasible to simply legislate a solution, because today's technical options are inadequate. Financial systems are under huge competitive pressure to support enormous transaction rates, and as the clearing time for transactions continues to diminish towards immediate settlement, the amounts of money at risk from even a small loss of data will continue to rise <ref type="bibr" target="#b19">[20]</ref>. Asking a bank to operate in slow-motion so as to continuously and synchronously maintain a remote mirrored backup is just not practical: the institution would fail for reasons of non-competitiveness.</p><p>Our work cannot completely eliminate this problem: for the largest transactions, synchronous mirroring (or some other means of guaranteeing that data will survive any possible outage) will remain necessary. Nonetheless, we believe that there may be a very large class of applications with intermediary data stability needs. If we can reduce the window of vulnerability significantly, our hypothesis is that even in a true disaster that takes the primary site offline and simultaneously disrupts the network, the challenges of restarting using the backup will be reduced. Institutions betting on network-sync would still be making a bet, but we believe the bet is a much less extreme one, and much easier to justify.</p><p>Failure Model and Assumptions: We assume that failures can occur at any level -including storage devices, storage area network, network links, switches, hubs, wide-area network, and/or an entire site. Further, we assume that they can fail simultaneously or even in sequence: a rolling disaster. However, we assume that the storage system at each site is capable of tolerating and recovering from all but the most extreme local failures. Also, sites may have redundant network paths connecting them. This allows us to focus on the tolerance of failures that disable an entire site, and on combinations of failures such as the loss of both an entire site and the network connecting it to the backup (what we call a rolling disaster). <ref type="figure">Figure 2</ref> illustrates some points of failure.</p><p>With respect to wide-area optical links, we assume that even though industry standards essentially preclude data loss on the links themselves, wide-area connections include layers of electronics: routers, gateways, firewalls, etc. These components can and do drop packets, and at very high data rates, so can the operating system on the destination machine to which data is being sent. Accordingly, our model assumes wide-area networks with high data rates (10 to 40 Gbits) but sporadic packet loss, potentially bursty. The packet loss model used in our experiments is based on actual observations of TeraGrid, a scientific data network that links scientific supercomputing centers and has precisely these characteristics. In particular, Balakrishnan et al. <ref type="bibr" target="#b9">[10]</ref> cite loss rates over 0.1% at times on uncongested optical-link paths between supercomputing centers. As a result, we emulate disaster with up to 1% loss rates in our evaluation of Section 5.</p><p>Of course, reliable transmission protocols such as TCP are typically used to communicate updates and acknowledgments between sites. Nonetheless, under our assumptions, a lost packet may prevent later received packets from being delivered to the mirrored storage system. The problem is that once the primary site has failed, there may be no way to recover a lost packet, and because TCP is sequenced, all data sent after the lost packet will be discarded in such situations -the gap prevents their delivery.</p><p>Data Loss Model: We consider data to be lost if an update has been acknowledged to the client, but the corresponding data no longer exists in the system. Today's remote mirroring regimes all experience data loss, but the degree of disaster needed to trigger loss varies:</p><p>â€¢ Synchronous mirroring only sends acknowledgments to the client after receiving a response from the mirror. Data cannot be lost unless both primary and mirror sites fail.</p><p>â€¢ Semi-synchronous mirroring sends acknowledgments to the client after data written is locally stored at the primary site and an update is sent to the mirror. This scheme does not lose data unless the primary site fails and sent packets do not make it to the mirror. For example, packets may be lost while resident in local buffers and before being sent on the wire, the network may experience packet loss, partition, or components may fail at the mirror.</p><p>â€¢ Asynchronous mirroring sends acknowledgments to the client immediately after data is written locally. Data loss can occur even if just the primary site fails. Many products form snapshots periodically, for example, every twenty minutes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b31">31]</ref>. Twenty minutes of data could thus be lost if a failure disrupts snapshot transmission.</p><p>Goals: Our work can be understood as an enhancement of the semi-synchronous style of mirroring. The basic idea is to ensure that once a packet has been sent, the likelihood that it will be lost is as low as possible. We do this by sending error recovery data along with the packet and informing the sending application when error recovery has been sent. Further, by exposing link state, an error correcting coding scheme can be tuned to better match the characteristics observed in existing high-speed wide-area networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network-Sync Remote Mirroring</head><p>Network-sync strikes a balance between performance and reliability, offering similar performance as semisynchronous solutions, but with increased reliability. We use a forward-error correction protocol to increase the reliability of high-quality optical links. For example, a link that drops one out of every 1 trillion bits or 125 million 1 KB packets (this is the maximum error threshold beyond which current carrier-grade optical equipment shuts down) can be pushed into losing less than 1 out of every 10 16 packets by the simple expedient of sending each packet twice -a figure that begins to approach disk reliability levels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>. By adding a callback when error recovery data has been sent, we can permit the application to resume execution once these encoded packets are sent, in effect treating the wide-area link as a kind of network disk. In this case, data is temporarily "stored" in the network while being shipped across the wide-area to the remote mirror. <ref type="figure">Figure 1</ref> illustrates this capability.</p><p>One can imagine many ways of implementing this behavior (e.g. datacenter gateway routers). In general, implementations of network-sync remote mirroring must satisfy two requirements. First, they should proactively enhance the reliability of the network, sending recovery data without waiting for any form of negative acknowledgment (e.g. TCP fast retransmit) or timeouts keyed to the round-trip-time (RTT) to the remote site. Second, they must expose the status of outgoing data, so that the sender can resume activity as soon as a desired level of in-flight redundancy has been achieved for pending updates. Section 3.1 discusses the network-sync option, Section 3.2 discusses an implementation of it, and Section 3.3 discusses its tolerance to disaster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network-Sync Option</head><p>Assuming that an external client interacts with a primary site and the primary site implements some higher level remote mirroring protocol, network-sync enhances that remote mirroring protocol as follows. First, a host located at the primary site submits a write request to a local storage system such as a disk array (e.g. <ref type="bibr" target="#b11">[12]</ref>), storage area network (e.g. <ref type="bibr" target="#b18">[19]</ref>), or file server (e.g. <ref type="bibr" target="#b31">[31]</ref>). The local storage system simultaneously applies the requested operation to its local storage image and uses a reliable transport protocol such as TCP to forward the request to a storage system located at the remote mirror. To implement the network-sync option, an egress router located at the primary site forwards the IP packets associated with the request, sends additional error correcting packets to an ingress router located at the remote site, and then performs a callback, notifying the local storage system which of the pending updates are now safely in transit 1 . The local storage system then replies to the requesting host, which can advance to any subsequent dependent operations. We assume that ingress and egress routers are under the control of site operators, thus can be modified to implement network-sync functionality.</p><p>Later, perhaps 50ms or so may elapse before the remote mirror storage system receives the mirrored request-possibly after the network-sync layer has reconstructed one or more lost packets using the combination of data received and error-recovery packets received. It applies the request to its local storage image, generates a storage level acknowledgment, and sends a response. Finally, when the primary storage system receives the response, perhaps 100ms later, it knows with certainty that the request has been mirrored and can garbage collect any remaining state (e.g. <ref type="bibr" target="#b18">[19]</ref>). Notice that if a client requires the stronger assurances of a true remote-sync, the possibility exists of offering that guarantee selectively, on a per-operation basis. <ref type="figure">Figure 3</ref> illustrates the networksync mirroring option and <ref type="table">Table 1</ref> contrasts it to existing solutions.</p><p>1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>storage-level data proactive redundancy redundancy feedback storage-level ack 6. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Primary</head><p>Remote Mirror</p><p>Figure 3: Network-Sync Remote Mirroring Option. <ref type="formula">(1)</ref> A primary-site storage system simultaneously applies a request locally and forwards it to the remote mirror. After the network-sync layer (2) routes the request and sends additional error correcting packets, it (3) sends an acknowledgment to the local storage system -at this point, the storage system and application can safely move to the next operation. Later, (4) a remote mirror storage system receives the mirrored request-possibly after the network-sync layer recovered some lost packets. It applies the request to its local storage image, generates a storage level acknowledgment, and (5) sends a response. Finally, <ref type="formula">(7)</ref> when the primary storage system receives the response, it knows with certainty that the request has been mirrored and can gargage collect. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mirror</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Maelstrom: Network-sync Implementation</head><p>The network-sync implementation used in our work is based on Forward Error Correction (FEC). FEC is a generic term for a broad collection of techniques aimed at proactively recovering from packet loss or corruption. FEC implementations for data generated in real-time are typically parameterized by a rate (r, c): for every r data packets, c error correction packets are introduced into the stream. Of importance here is the fact that FEC performance is independent of link length (except to the extent that loss rates may be length-dependent).</p><p>The specific FEC protocol we worked with is called Maelstrom <ref type="bibr" target="#b9">[10]</ref>, and is designed to match the observed loss properties of multi-hop wide-area networks such as TeraGrid. Maelstrom is a symmetric network appliance that resides between the datacenter and the widearea link, much like a NAT box. The solution is completely transparent to applications using it, and employs a mixture of technologies: routing tricks to conceal itself from the endpoints, a link-layer reliability protocol (currently TCP), and a novel FEC encoding called layered interleaving, designed for data transfer over long-haul links with potentially bursty loss patterns. To minimize the rate-sensitivity of traditional FEC solutions, Maelstrom aggregates all data flowing between the primary and backup sites an operates on the resulting high-speed stream. See Balakrishnan et al. <ref type="bibr" target="#b9">[10]</ref> for a detailed description of layered interleaving and analysis of its performance tolerance to random and bursty loss.</p><p>Maelstrom also adds feedback notification callbacks. Every time Maelstrom transmits a FEC packet, it also issues a callback. The local storage system then employs a redundancy model to infer the level of safety associated with in-flight data packets. For this purpose, a local storage system needs to know the underlying network's properties -loss rate, burst length, etc. It uses these to model the behavior of Maelstrom mathematically <ref type="bibr" target="#b9">[10]</ref>, and then makes worst-case assumptions about network loss to arrive at the needed estimate of the risk of data loss. We expect system operators monitor network behavior and periodically adjust Maelstrom parameters to adapt to any changes in the network characteristics.</p><p>There are cases in which the Maelstrom FEC protocol is unable to repair the loss (this can only occur if several packets are lost, and in specific patterns that prevent us from using FEC packets for recovery). To address such loss patterns, we run our mirroring solution over TCP, which in turn runs over Maelstrom: if Maelstrom fails to recover a lost packet, the end-to-end TCP protocol will recover it from the sender.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussion</head><p>The key metric for any disaster-tolerant remote mirroring technology is the distance by which datacenters can be separated. Today, a disturbing number of New York City banks maintain backups in New Jersey or Brooklyn, because they simply cannot tolerate higher latencies.</p><p>The underlying problem is that these systems typically operate over TCP/IP. Obviously, the operators tune the system to match the properties of the network. For example, TCP can be configured to use massive sender buffers and unusually large segments; also, an application can be modified to employ multiple side-by-side streams (e.g. GridFTP). Yet even with such steps, the protocol remains purely reactive-recovery packets are sent only in response to actual indications of failure, in the form of negative acknowledgments (i.e. fast retransmit) or timeouts keyed to the round-trip-time (RTT). Consequently, their recovery time is tightly linked to the distance between communicating end-points. TCP/IP, for example, requires a minimum of around 1.5 RTTs to recover lost data, which translates into substantial fractions of a second if the mirrors are on different continents. No matter how large we make the TCP buffers, the remote data stream will experience an RTT hiccup each time loss occurs: to deliver data in order, the receiver must await the missing data before subsequent packets can be delivered.</p><p>Network-sync evades this RTT issue, but does not protect the application against every possible rolling disaster scenario. Packets can still be queued in the local-area when disaster strikes. Further, the network can partitioned in the split second(s) before a primary site fails. Neither proactive redundancy or network-level callbacks will prevent loss in these cases. Accordingly, we envision that applications will need a mixture of remotesync and network-sync, with the former reserved for particularly sensitive scenarios, and the latter used in most cases.</p><p>Another issue is failover and recovery. Since the network-sync option enhances remote mirroring protocols, we assume that a complete remote mirroring protocol will itself handle failover and recovery directly <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b19">20]</ref>. As a result, in this work, we focus on evaluating the fault tolerant capabilities of a network-sync option and do not discuss failover and recovery protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mirroring Consistency via SMFS</head><p>We will say that a mirror image is inconsistent if out of order updates are applied to the mirror, or the application updates a group of files, and a period ensues during which some of the mirrored copies reflect the updates but others are stale. Inconsistency is a well-known problem when using networks to access file systems, and the issue can be exacerbated when mirroring. For example, suppose that one were to mirror an NFS server, using the standard but unreliable UDP transport protocol. Primary and remote file systems can easily become inconsistent, since UDP packets can be reordered on the wire, particularly if a packet is dropped and the NFS protocol is forced to resend it. Even if a reliable transport protocol is used, in cases where the file system is spread over multiple storage servers, or applications update groups of files, skew in update behavior between the different mirrored servers may be perceived as an inconsistency by applications.</p><p>To address this issue, SMFS implements a file system that preserves the order of operations in the structure of the file system itself, a distributed log-structured file system (distributed-LFS) <ref type="bibr" target="#b1">2</ref> , where a particular log is distributed over multiple disks. Similar to LFS <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b26">27]</ref>, it embeds a UNIX tree-structured file system into an append only log format <ref type="figure" target="#fig_1">(Figure 4</ref>). It breaks a particular log into multiple segments that each have a finite maximum size and are the units of storage allocation and cleaning.</p><p>Although log-structured file systems may be unpopular in general settings (due to worries about high cleaning costs if the file system fills up), a log structure turns out to be nearly ideal for file mirroring. First, it is well known that an append-only log-structure is optimized for write performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b35">35]</ref>. Second, by combining data and order of operations into one structure -the log -identical structures can be managed naturally at remote locations. Finally, log operations can be pipelined, increasing system throughput. Of course, none of this eliminates worries about segment cleaning costs. Our assumption is that because SMFS would be used only for files that need to be mirrored, such as backups and checkpoints, it can be configured with ample capacity-far from the tipping point at which these overheads become problematic.</p><p>In Sections 4.1 and 4.2, we describe the storage systems architecture and API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">SMFS Architecture</head><p>The SMFS architecture is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. It works as follows. Clients access file system data by communi- The metadata server allocates space for the newly created log on storage servers that it selects. The file server then interacts directly with the storage server for append(), read(), and free() operations.</p><p>cating with a file server (e.g. using the NFS protocol). File servers handle writes in a similar fashion to LFS. The log is updated by traversing a file system in a depthfirst manner, first appending modified data blocks to the log, storing the log address in an inode, then appending the modified inode to the log, and so on <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">29]</ref>. Reads are handled as in any conventional file system; starting with the root inode (stored in memory or a known location on disk) pointers are traversed to the desired file inode and data blocks. Although file servers provide a file system abstraction to clients, they are merely hosts in the storage system and stable storage resides with separate storage servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SMFS API</head><p>File servers interact with storage servers through a thin log interface-create(), append(), read(), and free(). create() communicates with a metadata server to allocate storage resources for a new log; it assigns responsibility for the new log to a storage server. After a log has been created, a file server uses the append() operation to add data to the log. The file server communicates directly with a log's storage server to append data. The storage server assigns the order of each append-assigns the address in the log to a particular append-and atomically commits the operation. SMFS maintains group-mirroring consistency, in which a single append() can contain updates to many different files where the group of updates will all be reflected by the storage system atomically, either all or none. read() returns the data associated with a log address. Finally, free() takes a log address and marks the address for later cleaning. In particular, after a block has been modified or file removed, the file system calls free() on all blocks that are no longer referenced. The create(), append(), and free() operations are mirrored between the primary site and remote mirror.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we evaluate the network-sync remote mirroring option, running our SMFS prototype on Emulab <ref type="bibr" target="#b40">[40]</ref> and the Cornell National Lambda Rail (NLR) Rings testbed <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Environment</head><p>The implementation of SMFS that we worked was implemented as a user-mode application coded in Java. SMFS borrows heavily from our earlier file system, Antiquity <ref type="bibr" target="#b39">[39]</ref>; however, the log address was modified to be a segment identifier and offset into the segment. A hash of the block can optionally be computed, but it is used as a checksum instead of as part of the block address in the log. We focus our evaluation on the append() operation since that is by far the dominant operation mirrored between two sites. SMFS uses the Maelstrom network appliance <ref type="bibr" target="#b9">[10]</ref> as the implementation of the network-sync option. Maelstrom can run as a user-mode module, but for the experiments reported here, it was dropped into the operating system, where it runs as a Linux 2.6.20 kernel module with hooks into the kernel packet filter <ref type="bibr" target="#b1">[2]</ref>. Packets destined for the opposite site are routed through a pair of Maelstrom appliances located at each site. More importantly, situating a network appliance at the egress and ingress router for each site creates a virtual link between the two sites, which presents many opportunities for increasing mirroring reliability and performance.</p><p>The Maelstrom egress router captures packets, which it processes to create redundant packets. The original IP packets are forwarded unaltered; the redundant packets are then sent to the ingress router using a UDP channel. The ingress router captures and stores a window consisting of the last K IP packets that it has seen. Upon receiving a redundant packet it checks it against the last K IP packets. If there is an opportunity to recover any lost IP packet it does so, and forwards the newly recovered IP packet through a raw socket to the intended destination. Note that each appliance works in both egress and ingress mode since we handle duplex traffic.</p><p>To implement network-sync redundancy feedback, the Maelstrom kernel module tracks each TCP flow and sends an acknowledgment to the sender. Each acknowledgment includes a byte offset from the beginning of the stream up to the most recent byte that was included in an error correcting packet that was sent to the ingress router.</p><p>We used the TCP Reno congenstion control algorithm to communicate between mirrored storage systems for all experiments. We experimented with other congestion control algorithms such as cubic; however, the results were nearly identical since we were measuring packets lost after a primary site failure due to a disaster.</p><p>We tested the setup on Emulab <ref type="bibr" target="#b40">[40]</ref>; our topology emulates two clusters of eight machines each, separated by a wide-area high capacity link with 50 to 200 ms RTT and 1 Gbps. Each machine has one 3.0 GHz Pentium 64-bit Xeon processor with 2.0 GB of memory and a 146 GB disks. Nodes are connected locally via a gigabit Ethernet switch. We apply load to these deployments using up to 64 testers located on the same cluster as the primary. A single tester is an individual application that has only one outstanding request at a time. <ref type="figure">Figure 3</ref> shows the topology of our Emulab experimental setup (with the difference that we used eight nodes per cluster, and not four). Throughout all subsequent experiments, link loss is random, independent and identically distributed. See Balakrishnan et al <ref type="bibr" target="#b9">[10]</ref> for an analysis with bursty link loss. Finally, all experiments show the average and standard deviation over five runs.</p><p>The overall SMFS prototype is fast enough to saturate a gigabit wide-area link, hence our decision to work with a user-mode Java implementation has little bearing on the experiments we now report: even if SMFS was implemented in the kernel in C, the network would still be the bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Metrics</head><p>We identify the following three metrics to evaluate the efficacy of SMFS:</p><p>â€¢ Data Loss: What happens in the event of a disaster at the primary? For varying loss rates on the wide-area link, how much does the mirror site diverge from the primary? We want our system to minimize this divergence.</p><p>â€¢ Latency: Latency can be used to measure both performance and reliability. Application-perceived latency measures (perceived) performance. Mirroring latency, on the other hand, measures reliability. In particular, the lower the latency, and the smaller the spread of its distribution, the better the fidelity of the mirror to the primary.</p><p>â€¢ Throughput: Throughput is a good measure of performance. The property we desire from our system is that throughput should degrade gracefully with increasing link loss and latency. Also, for mirroring solutions that use forward error correcting (FEC) codes, there is a fundamental tradeoff between data reliability and goodput (i.e. application level throughput); proactive redundancy via FEC increases tolerance to link loss and latency, but reduces the maximum goodput due to the overhead of FEC codes. We focus on goodput.  For effective comparison, we define the following five configurations; all configurations use TCP to communicate between each pair of storage servers.</p><p>â€¢ Local-sync: This is the canonical state-of-the-art solution. It is a semi-synchronous solution. As soon as the request has been applied to the local storage image and the local kernel buffers a request to send a message to the remote mirror, the local storage server responds to the application; it does not wait for feedback from remote mirror, or even for the packet to be placed on the wire.</p><p>â€¢ Remote-sync: This is the other end of the spectrum. It is a synchronous solution. The local storage server waits for a storage-level acknowledgment from the remote mirror before responding to the application.</p><p>â€¢ Network-sync: This is SMFS running with a network-sync option, implemented by Maelstrom in the manner outlined in Section 3 (e.g. with TCP over FEC). The network-sync layer provides feedback after proactively injecting redundancy into the network. SMFS responds to the application after receiving these redundancy notification.</p><p>â€¢ Local-sync+FEC: As a comparison point, this scheme is the local-sync mechanism, with Maelstrom running on the wide-area link, but without network-level callbacks to report when FEC packets are placed on the wire (i.e. storage servers are unaware of the proactive redundancy). The local server permits the application to resume execution as soon as data has been written to the local storage system.</p><p>â€¢ Remote-sync+FEC: As a second comparison point, this scheme is the remote-sync mechanism, again using Maelstrom on the wide-area link but without upcalls when FEC packets are sent. The local server waits for the remote storage system to acknowledge updates.</p><p>These five SMFS configurations are evaluated on each of the above metrics, and their comparative performance is presented. The Network-sync, Local-sync+FEC, and  Remote-sync+FEC configurations all use the Maelstrom layered interleaving forward error correction codes with parameters (r, c) = (8, 3), which increases the tolerance to network transmission errors, but reduces the goodput by as much as 8/11 of the maximum throughput without any proactive redundancy. <ref type="table" target="#tab_3">Table 2</ref> lists the configuration parameters used in the experiments described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reliability During Disaster</head><p>We measure reliability in two ways:</p><p>â€¢ In the event of a disaster at the primary site, how much data loss results?</p><p>â€¢ How much are the primary and mirror sites allowed to diverge?</p><p>These questions are highly related; we distinguish between them as follows: The maximum amount by which the primary and mirror sites can diverge is the extent of the bandwidth-delay product of the link between them; however, the amount of data lost in the event of failure depends on how much of this data has been acknowledged to the application. In other words, how often can we be caught in a lie? For instance, with a remote-sync solution (synchronous mirroring), though bandwidth-delay product -and hence primary-to-mirror divergence -may be high, data loss is zero. This, of course, is at severe cost to performance. With a localsync solution (async-or semi-synchronous mirroring), on the other hand, data loss is equal to divergence. The following experiments show that the network-sync solution with SMFS achieves a desirable mean between these two extremes.</p><p>Disaster Test <ref type="figure" target="#fig_4">Figure 6</ref> shows the amount of data loss in the event of a disaster for the local-sync, localsync+FEC, and network-sync solutions; we do not test  the remote-sync and remote-sync+FEC solutions in this experiment since these solutions do not lose data.</p><p>The rolling disaster, failure of the wide-area link and crash of all primary site processes, occurred two minutes into the experiment. The wide-area link operated at 0% loss until immediately before the disaster occurred, when loss rate was increased for 0.5 seconds, thereafter the link was killed (See Section 2 for a description of rolling disasters). The x-axis shows the wide-area link loss rate immediately before the link is killed; link losses are random, independent and identically distributed. The y-axis shows both the total number of messages sent and total number of messages lost-lost messages were perceived as durable by the application but were not received by the remote mirror. Messages were of size 4 kB.</p><p>The total number of messages sent is similar for all configurations since the link loss rate was 0% for most of the experiment. However, local-sync lost a significant number of messages that had been reported to the application as durable under the policy discussed in Section 3.1. These unrecoverable messages were ones buffered in the kernel, but still in transit on the wide area link; when the sending datacenter crashed and the link (independently) dropped the original copy of the message, TCP recovery was unable to overcome the loss.</p><p>Local-sync+FEC lost packets as well: it lost packets still buffered in the kernel, but not packets that had already been transmitted -in the latter case, the proactive redundancy mechanism was adequate to overcome the loss. The best outcome is visible in the right-most histogram at 0.1%, 0.5%, and 1% link loss: here we see that although the network-sync solution experienced the same level of link-induced message loss, all the lost packets that had been reported as durable to the sender application were in fact recovered on the receiver side of the link. This supports the premise that a networksync solution can tolerate disaster while minimizing loss. Combined with results from Section 5.4, we demonstrate that the network-sync solution actually achieves the best balance between reliability and performance. <ref type="figure" target="#fig_6">Figure 7</ref> quantifies the advantage of network-sync over local-sync+FEC. In this experiment, we run the same disaster scenario as above, but with 1% link loss during disaster and we vary the FEC parameter c (i.e. the number of recovery packets). At c = 0, there are no recovery packets for either local-sync+FEC or networksync-if a data packet is lost during disaster, it cannot be recovered and TCP cannot deliver any subsequent data to the remote mirror process. Similarly, at c = 1, the number of lost packets is relatively high for both localsync+FEC and network-sync since one recovery packet is not sufficient to mask 1% link loss. With c &gt; 1, the number of recovery packets is often sufficient to mask loss on the wide-area link; however, local-sync+FEC loses data packets that did not transit outside the localarea before disaster, whereas with network-sync, primary storage servers respond to the client only after receiving a callback from the egress gateway. As a result, networksync can potentially reduce data loss in a disaster.</p><p>Latency <ref type="figure" target="#fig_7">Figure 8</ref> shows how latency is distributed across all requests for local-sync, local-sync+FEC, and network-sync solutions. Latency is the time between a local storage server sending a request and a remote storage server receiving the request. We see that these solutions show similar latency for zero link loss, but localsync+FEC and network-sync show considerably better latency than local-sync for a lossy link. Furthermore, the latency spread of local-sync+FEC and network-sync solutions is considerably less than the spread of the localsync solution -particularly as loss increases; proactive redundancy helps to reduce latency jitter on lossy links. Smaller variance in this latency distribution helps to ensure that updates submitted as a group will arrive at the remote site with minimum temporal skew, enabling the entire group to be written instead of not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance</head><p>System Throughput <ref type="figure" target="#fig_9">Figure 9</ref> compares the performance of the five different mirroring solutions. The xaxis shows loss probability on the wide-area link being  increased from 0% to 1%, while the y-axis shows the throughput achieved by each of these mirroring solutions. All mirroring solutions use 64 testers over eight storage servers.</p><p>At 0% loss we see that the local-sync and remotesync solutions achieve the highest throughput because they do not use proactive redundancy, thus the goodput of the wide-area link is not reduced by the overhead of any forward error correcting packets. On the other hand, local-sync+FEC, remote-sync+FEC, and networksync achieve lower throughput because the forward error correcting packets reduce the goodput in these cases. The forward error correction overhead is tunable; increasing FEC overhead often increases transmission reliability but reduces throughput. There is a slight degradation of performance for network-sync since SMFS waits for feedback from the egress router instead of responding immediately after the local kernel buffers the send request. Finally, the remote-sync and remote-sync+FEC achieve comparable performance to all the other configurations since there is no loss on the wide-area link and the storage servers can saturate the link with overlapping mirroring requests.</p><p>At higher loss rates, 0.1%, 0.5%, and 1%, we see that any solution that uses proactive redundancy (local-sync+FEC, remote-sync+FEC, and network-sync) achieves more than an order of magnitude higher  throughput over any solution that does not. This illustrates the power of proactive redundancy, which makes it possible for these solutions to recover from lost packets at the remote mirror using locally-available data. Further, we observe that these proactive redundancy solutions perform comparably in both asynchronous and synchronous modes: in these experiments, the wide-area network is the bottleneck since overlapping operations can saturate the wide-area link. <ref type="figure" target="#fig_10">Figure 10</ref> shows the system throughput of the network-sync solution as the wide-area one-way link latency increases from 25 ms to 100 ms. It demonstrates that the network-sync solution (or any solution that uses proactive redundancy) can effectively mask latency and loss of a wide-area link.</p><p>Application Throughput The previous set of experiments studied system-level throughput, using a large number of testers. An interesting related study is presented here, of individual-application throughput in each SMFS configuration. <ref type="figure" target="#fig_0">Figure 11</ref> shows the effect of increasing loss probability on the throughput of a application, with only one outstanding request at a time.  We see now that local-sync(+FEC) and networksync solutions perform better than remote-sync(+FEC). The reason for this difference is that with asynchrony, network-sync can return an acknowledgment to the application as soon as a request is on the wide-area link, providing an opportunity to pipeline requests. This is in contrast to conventional asynchrony, where the application would receive an acknowledgment as soon as a request is buffered. The advantage with the former is that it provides performance gain without hurting reliability. The disadvantage is that pure buffering is a local system call operation, which can return to the application sooner and can achieve higher throughput as seen by the local-sync(+FEC) solutions. However, this increase in throughput is at a sacrifice of reliability; any buffered data may be lost in the event of a crash before it is sent (See <ref type="figure" target="#fig_4">Figure 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Cornell National Lambda Rail Rings</head><p>In addition to our emulated setup and results, we are beginning to physically study systems that operate on dedicated lambda networks that might be seen in cutting edge financial, military, and educational settings. To study these "personal" lambda networks, we have created a new testbed consisting of optical network paths of varying physical length that start and end at Cornell, the Cornell National Lambda Rail (NLR) Rings testbed.</p><p>The Cornell NLR-Rings testbed consists of three rings: a short ring that goes from Cornell to New York City and back, a medium ring that goes to Chicago down to Atlanta and back, and a long ring that goes to Seattle down to Los Angeles and back. The one-way latency is 7.9 ms, 37 ms, and 94 ms, for the short, medium, and long rings, respectively. The underlying optical networking technology is state-of-the-art: a 10 Gbps wide-area network running on dedicated fiber optics (separate from the public Internet) and created as a scientific research infrastructure by the NLR consortium <ref type="bibr" target="#b2">[3]</ref>. Each ring includes multiple segments of optical fiber, linked by routers and repeaters. More importantly, for the medium and long ring, each network packet traverses a unique path without going along the same segment. See NLR <ref type="bibr" target="#b2">[3]</ref> for a map.</p><p>Though all rings in the testbed are capable of 10 Gbps end-to-end, we are only able to operate at hundreds of megabits per second at this time due to network construction. Nonetheless, we are able to study the effects of disaster on dedicated wide-area lambda networks and hope to be able to use increasingly more bandwidth in the future.</p><p>To study the effects of disaster in this wide-area testbed, we conduct the same disaster experiment described in Section 5.3. We induced loss on the wide-area link 0.5 second before the primary site fails via a router that we control. Later, when the primary site fails, the wide-area link and all processes are killed. <ref type="figure" target="#fig_13">Figure 12</ref> shows data loss during this disaster for the medium path on the Cornell NLR-Rings testbed. The x-axis shows the loss induced on the wide-area link (link losses are random, independent and identically distributed) and the y-axis shows the number of messages sent and the number of unrecoverable messages. There are two interesting results illustrated. First, local-sync lost messages even when no loss was induced on the wide-area link. This may be because our wide-area testbed may drop packets, which prevents local-sync protocols from delivering to the mirroring application. Local-sync+FEC and network-sync, on the other hand, did not lose messages because both can mask wide-area link loss. Second, due to the relatively low bandwidth, packets were able to transit outside of the local-area, preventing loss from occurring in the local-area and enabling both localsync+FEC and network-sync to mask wide-area link loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Mirroring modes</head><p>Synchronous mirroring, like IBM's Peer-to-Peer Remote Copy (PPRC) <ref type="bibr" target="#b5">[6]</ref> and EMC's Symmetrix Remote Data Facility (SRDF) <ref type="bibr" target="#b11">[12]</ref> is a technique often used in disaster tolerance solutions. It guarantees that local copies of data are consistent with copies at a remote site, and also guarantees that the mirror sites are as up-to-date as possible. Naturally, the drawback is that of added I/O latency to every write operation; furthermore, long distance links make this technique prohibitively expensive.</p><p>An alternate solution is to use asynchronous remote mirroring <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">31]</ref>. For example, SnapMirror <ref type="bibr" target="#b31">[31]</ref> provides asynchronous mirroring of file systems by periodically transferring self-consistent data snapshots from a source volume to a destination volume. Users are provided with a knob for setting the frequency of updatesif set to a high value, the mirror would be nearly current with the source, while setting to a low value reduces the network bandwidth consumption at the risk of increased data loss. Seneca <ref type="bibr" target="#b18">[19]</ref> is a storage area network mirroring solution and similarly attempts to reduce the amount of traffic sent over the wide-area network.</p><p>SnapMirror works at the block level, using the WAFL <ref type="bibr" target="#b16">[17]</ref> file system active block map to identify changed blocks and avoid sending deleted blocks. Moreover, since it operates at this level, it is able to optimize data reads and writes. The authors showed that for update intervals as short as one minute, data transfers were reduced by 30% to 80%.</p><p>Similar to SnapMirror, Seneca <ref type="bibr" target="#b18">[19]</ref> is another asynchronous mirroring solution that attempts to reduce the traffic sent over the wide-area network, but also increases the risk of data loss. Seneca operates at the level of a storage area network (SAN) instead of the file system level.</p><p>Semi-synchronous mirroring is yet another mode of operation, closely related to both synchronous and asynchronous mirroring. In such a mode, writes are sent to both the local and the remote storage sites at the same time, the I/O operation returning when the local write is completed. However subsequent write I/O is delayed until the completion of the preceding remote write command. In <ref type="bibr" target="#b42">[42]</ref> the authors show that by leveraging a log policy for the active remote write commands the system is able to allow a limited number of write I/O operations to proceed before waiting for acknowledgment from the remote site, thereby reducing the latency significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Error correcting codes</head><p>Packet level forward error correcting (FEC) schemes typically transmit c repair packets for every r data packets, using coding schemes with which all data packets can be reconstructed if at least r out of r + c data and repair packets are received <ref type="bibr" target="#b17">[18]</ref>. In contrast, convolution codes work on bit or symbol streams of arbitrary length, and are most often decoded with the Viterbi algorithm <ref type="bibr" target="#b38">[38]</ref>. Our work favors FEC: FEC schemes have the benefit of being highly tunable -trading off overhead and timeliness, and are very stable under stress -provided that the recovery does not result in high levels of traffic.</p><p>FEC techniques are increasingly popular. Recent applications include FEC for multicasting data to large groups <ref type="bibr" target="#b34">[34]</ref>, where FEC can be employed either by receivers <ref type="bibr" target="#b8">[9]</ref> or senders <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28]</ref>. In general, fast, efficient encodings like Tornado codes <ref type="bibr" target="#b10">[11]</ref> make sender-based FEC schemes very attractive in scenarios where dedicated senders distribute bulk data to a large number of receivers.</p><p>Likewise, FEC can be used when connections experience long transmission delays, in which case the use of redundancy helps bound the delivery delays within some acceptable limits, even in the presence of errors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">33]</ref>. For example, deep space satellite communications <ref type="bibr" target="#b43">[43]</ref> have been using error correcting codes for decades both for achieving maximal information transfer over a restricted bandwidth communication link and in the presence of data corruption.</p><p>SMFS is not the first system to propose exposing network state to higher level storage systems <ref type="bibr" target="#b32">[32]</ref>. The difference, however, is that network-sync can be implemented with gateway routers under the control of site operators and does not require change to wide-area Internet routers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Reliable Storage &amp; Recovery</head><p>Recent studies have shown that failures plague storage and other components of large computing datacenters <ref type="bibr" target="#b36">[36]</ref>. As a result, many systems replicate data to reduce risk of data loss <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">37]</ref>. However, replication alone is not complete without recovery.</p><p>Recovery in the face of disaster has been a problem that has received a lot of attention <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. In <ref type="bibr" target="#b19">[20]</ref>, for example, the authors propose a reactive way to solve the data recovery scheduling problem once the disaster has occurred. Potential recovery processes are first mapped onto recovery graphs -the recovery graphs capture alternative approaches for recovering workloads, precedence relationships, timing constraints, etc. The recovery scheduling problem is encoded as an optimization problem with the end goal of finding the schedule that minimizes some measure of penalty; several methods for finding optimal and near-optimal solutions are given.</p><p>Aguilera et. al. <ref type="bibr" target="#b3">[4]</ref> explore the tradeoff between the ability to recover and the cost of recovery in enterprise storage systems. They propose a multi-tier file system called TierFS that employs a "recoverability log" used to increase the recoverability of lower tiers by using the highest tier.</p><p>Both LOCKSS <ref type="bibr" target="#b25">[26]</ref> and Deep Store <ref type="bibr" target="#b44">[44]</ref> address the problem of reliably preserving large volumes of data for virtually indefinite periods of time, dealing with threats like format obsolescence and "bit-rot." LOCKSS consists of a set of low-cost, independent, persistent cooperating caches that use a voting scheme to detect and repair damaged content. Deep Store eliminates redundancy both within and across files; it distributes data for scalability and provides variable levels of replication based on the importance or the degree of dependency of each chunk of stored data.</p><p>Baker et. al. <ref type="bibr" target="#b7">[8]</ref> consider the problem of recovery from failure of long-term storage of digital information. They propose a "reliability model" encompassing latent and correlated faults, and the detection time of such latent faults. They show that a simple combination of auditing (to detect latent faults) as soon as possible, automatic recovery and independence of replicas yields the most benefit with respect to the cost of each technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The conundrum facing many disaster tolerance and recovery designs is the tradeoff between loss of performance and the potential loss of data. On the one hand, it may not be desirable to slow application response time until it is assured that data will not be lost in the event of disaster. On the other hand, the prospect of data loss can be catastrophic for many companies and organizations. Unfortunately, there is not much of a middle ground in the design space and designers must choose one or the other.</p><p>The network-sync remote mirroring option potentially offers an improvement, providing performance of enterprise-level semi-synchronous remote mirroring solutions while increasing their data reliability guarantees. Like native semi-synchronous protocols, network-sync protocols simultaneously send each update to the remote mirror as the primary handles the update locally. Rather than waiting for an acknowledgment from the remote mirror, it delays only until it receives feedback from an underlying communication layer, acknowledging that data and repair packets have been placed on the external wide-area network. This minimizes the loss of data in the event of disaster. Applications requiring strong remote-sync guarantees can still wait for a remote acknowledgment, but for most purposes, network-sync represents an appealing new option. Our experiments show that SMFS, a remote mirroring solution that uses the network-sync option, exhibits performance that is independent of link-latency, in marked contrast to most existing technologies. routers receive packets from the wide-area network and forward packets to local datacenter networks. Generally, egress routers also function as ingress routers and visa versa since they handle duplex traffic. <ref type="bibr" target="#b1">2</ref> A distributed log-structured file system can expose an NFS interface to hosts; however, it stores data in a distributed log-structured file system instead of a local UNIX file system (UFS).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 illustratesFigure 1 :</head><label>11</label><figDesc>Figure 1: Remote Mirroring Options. (1) Synchronous mirroring provides a remote-sync guarantee: data is not lost in the event of disaster, but performance is extremely sensitive to the distance between sites. (2) Asynchronous and semi-synchronous mirroring give a local-sync guarantee: performance is independent of distance between mirrors, but can suffer significant data loss when disaster strikes. (3) A new network-sync mirroring option with performance similar to local-sync protocols, but with improved reliability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Format of a log after writing a file system with two sub directories /dir1/file1 and /dir2/file2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: File System Architecture: Applications communicate with the file server through (possibly) a NFS interface. The file server in turn communicates with the metadata server through the create() function call. The metadata server allocates space for the newly created log on storage servers that it selects. The file server then interacts directly with the storage server for append(), read(), and free() operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on link Local-sync (canonical soln) total msgs sent Local-sync+FEC total msgs sent Network-sync total msgs sent Unrecoverable lost msgs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Data loss as a result of disaster and wide-area link failure, varying link loss (50ms one-way latency and FEC params (r, c) = (8, 3)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Data loss as a result of disaster and wide-area link failure, varying FEC param c (50ms one-way latency, 1% link loss).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Latency distribution as a function of wide-area link loss (50ms one-way latency).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Effect of varying wide-area one-way link loss on Aggregate Throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Effect of varying wide-area link latency on Aggregate Throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Effect of varying wide-area link loss on PerClient Throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>on link Local-sync (canonical soln) total msgs sent Local-sync+FEC total msgs sent Network-sync total msgs sent Unrecoverable lost msgs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Data loss as a result of disaster and wide-area link failure (Cornell NLR-Rings, 37 ms one-way delay).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>47 ms = Network oneâˆ’way latency</head><label>47</label><figDesc></figDesc><table>Gateway 
Routers 

Datacenter 

Primary 
Mirror 

16 ms = Speed of Light oneâˆ’way latency 

Partition 
Network 
Outage 
Power 
Failure 
Site 
Loss 
Packet 

Figure 2: Example Failure Events. A single failure event may not result in loss of data. However, multiple nearly-
simultaneous failure events (i.e. rolling disaster) may result in data loss for asynchronous and semi-synchronous 
remote mirroring. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Experimental Configuration Parameters</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="7">th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="1"> Egress and ingress routers operate as gateway routers between datacenter and wide-area networks, where egress routers send packets from local datacenter networks to the wide-area network and ingress</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our shepherd James Plank, the anonymous reviewers, and Robbert van Renesse for their comments that shaped the final version of this paper. Also, we would like to thank all who contributed to setting up the Cornell NLR-Rings testbed: Dan Freedman, Cornell Facilities Support Scott Yoest and Larry Parmelee, CIT-NCS networking engineering Eric Cronise and Dan Eckstrom, and NLR network engineering Greg Boles, Brent Sweeny, and Joe Lappa. Finally, we would like to thank Intel and Cisco for providing necessary routing and computing equipment, and NSF TRUST and AFRL Castor grant for funding support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.cs.cornell.edu/~hweather/nlr" />
		<title level="m">Cornell national lambda rail (nlr) rings testbed</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nat</forename><surname>Firewalling</surname></persName>
		</author>
		<ptr target="http://www.netfilter.org.Lastaccessed" />
		<imprint>
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="http://www.nlr.net" />
	</analytic>
	<monogr>
		<title level="j">National lambda rail</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving recoverability in multitier storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aguilera</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muniswamy-Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uysal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DSN</title>
		<meeting>of DSN</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Serverless Network File Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neefe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="1995-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Advanced functions for storage subsystems: Supporting continuous availability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azagury</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Factor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An IBM SYSTEM Journal</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An analysis of latent sector errors in disk drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairavasundaram</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schindler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMETRICS</title>
		<meeting>of ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fresh look at the reliability of long-term digital storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S H</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roussopou-Los</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Giuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bungale</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ricochet: Lateral error correction for time-critical multicast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pleisch</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maelstrom: Transparent error correction for lambda networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Weather-Spoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vollset</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A digital fountain approach to reliable distribution of bulk data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Byers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rege</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGCOMM Comput. Commun. Rev</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Symmetrix remote data facility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corp</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<ptr target="http://www.emc.com/products/family/srdf-family.htm.Lastaccessed" />
		<imprint>
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The backup book: disaster recovery from desktop to data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cougias</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heiberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koop</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>SchaserVartan Books</publisher>
			<pubPlace>Lecanto, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2003-10" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving file system reliability with i/o shepherding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunawi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The zebra striped network file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousterhout</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>ACM TOCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">File system design for an NFS file server appliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hitz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Technical Conference</title>
		<meeting>the USENIX Technical Conference</meeting>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The case for packet level FEC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huitema</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IFIP Workshop on Protocols for High-Speed Networks</title>
		<meeting>of the IFIP Workshop on Protocols for High-Speed Networks</meeting>
		<imprint>
			<date type="published" when="1996-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Remote mirroring done write</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilkes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seneca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the road to recovery: Restoring data after disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>San-Tos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM EuroSys</title>
		<meeting>of ACM EuroSys</meeting>
		<imprint>
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A framework for evaluating storage system dependability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DSN</title>
		<meeting>of DSN<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">877</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing for disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keeton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilkes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="59" to="62" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Petal: Distributed virtual disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thekkath</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM ASPLOS</title>
		<meeting>of ACM ASPLOS</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Myriad: Cost-effective disaster tolerance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Maccormick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Replication in the harp file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liskov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shrira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGOPS</title>
		<meeting>of ACM SIGOPS</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The LOCKSS peer-to-peer digital preservation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maniatis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Roussopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Giuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Rosen-Thal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>ACM TOCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving the performance of log-structured file systems with adaptive methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthews</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Parity-based loss recovery for reliable multicast transmission</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nonnenmacher</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biersack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towsley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="349" to="361" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Berkeley db java edition architecture. An Oracle White Paper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oracle</forename></persName>
		</author>
		<ptr target="http://www.oracle.com/database/berkeley-db/je/index.html" />
		<imprint>
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Closing the gap: A research and development agenda to improve the resiliency of the banking and finance sector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parsons</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Peretti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grochow</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005-03" />
		</imprint>
		<respStmt>
			<orgName>U.S. Department of the Treasury Study</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Snapmirror: File system based asynchronous mirroring for disaster recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patterson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Manley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Federwisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owara</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Managing data storage in the network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plank</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Swany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="50" to="58" />
			<date type="published" when="2001-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective erasure codes for reliable computer communication protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizzo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Communication Review</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">RMDP: An fec-based reliable multicast protocol for wireless environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizzo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicisano</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM Mobile Computer and Communication Review</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosenblum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousterhout</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disk failures in the real world: what does an mttf of 1,000,000 hours mean to you?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schroeder</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gibson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Frangipani: A scalable distributed file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thekkath</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimal decoding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viterbi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IT</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Antiquity: Exploiting a secure log for wide-area distributed storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weatherspoon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubiatow-Icz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM EuroSys</title>
		<meeting>of ACM EuroSys</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">An integrated experimental environment for distributed systems and networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">White</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lepreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gu-Ruprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joglekar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX OSDI</title>
		<meeting>of USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Disaster recovery plans and systems are essential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Witty</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gartner Research Gartner FirstTake</title>
		<imprint>
			<date type="published" when="2001-09-12" />
			<biblScope unit="page" from="14" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An implementation of semi-synchronous remote mirroring system for sans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Workshop of Grid and Cooperative Computing (GCC)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Distributed source coding for satellite communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1111" to="1120" />
			<date type="published" when="1999-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep store: an archival storage system architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>Pollack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st International Conference on Data Engineering (ICDE)</title>
		<meeting>21st International Conference on Data Engineering (ICDE)</meeting>
		<imprint>
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
