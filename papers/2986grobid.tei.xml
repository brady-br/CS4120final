<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Utilitarian Performance Isolation in Shared SSDs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Utilitarian Performance Isolation in Shared SSDs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper proposes a utilitarian performance isolation (UPI) scheme for shared SSD settings. UPI exploits SSD&apos;s abundant parallelism to maximize the utility of all tenants while providing performance isolation. Our approach is in contrast to static resource partitioning techniques that bind parallelism, isolation, and capacity altogether. We demonstrate that our proposed scheme reduces the 99th percentile response time by 38.5% for a latency-critical workload, and the average response time by 16.1% for a high-throughput workload compared to the static approaches.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thanks to its low latency, collectively massively parallelism, and high density, SSDs are now critical components in today's large storage systems from cloud services to high-performance computing environments. However, SSDs exhibit substantial performance instabilities and variations due to background management tasks <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b14">13,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b22">21]</ref>. These problems are further exacerbated in shared storages, where multiple tenants interfere with each other <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b21">20]</ref>, causing a so-called noisy neighbor effect.</p><p>To curtail the effects of inter-tenant I/O interferences, the current trend is to configure the SSD into multiple isolated regions <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b21">20]</ref>. By partitioning it internally and assigning each tenant a separate region, either at the channel-level or chip-level, each would be physically isolated and the workload of one tenant would not degrade the performance of another. However, statically partitioning internal resources can be detrimental across multiple dimensions. First, only a fraction of the physical storage space can be used for each tenant. Second, the average performance suffers by limiting the overall parallelism. Lastly, the quality of service (QoS) degrades on highly contended flash resources as the load cannot be  effectively distributed. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the effect of parallelism on the average read performance across seven workloads from Microsoft production servers <ref type="bibr" target="#b15">[14]</ref>. Each runs alone, but by reducing the number of flash memory channels and chips available in the SSD from 4(channels)x4(chips per channel) to 1x2, workloads such as DAP-PS, LM-TBE, and RAD-BE experience over a two-fold increase in the average read response time.</p><p>In this paper, we make an argument for allocating resources dynamically by considering the utility of each tenant to exploit the abundant parallelism in the SSD and to mitigate the effects of I/O interferences. Our intuition is that SSDs can leverage its unique characteristics to make data placements that satisfy both performance isolation and improved efficiency without the overheads associated with load balancing and data relocation. For SSDs, the load can be balanced by controlling where the data will be written, and data relocation is natural with the use of internal management tasks such as garbage collection, wear leveling, and read scrubbing.</p><p>To that end, we propose a utilitarian performance isolation (UPI) scheme where tenant writes are striped across disjoint sets of flash memory chips to reduce I/O interference, reads are serviced from wherever the data is located, and data is relocated among one set to another when necessary. Our preliminary results show that UPI reduces the 99% QoS by 38.5% for a latencycritical tenant, and the average response time by 16.1% for a throughput-oriented one compared to the static approaches.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>• Mitigating the effects of garbage collection in SSDs.</p><p>Resource contention between host request handling and background management tasks (such as garbage collection) has been pointed out as the main source of performance degradation in SSDs. ttFlash <ref type="bibr" target="#b22">[21]</ref> uses a RAID-like parity scheme to reduce garbage collection (GC)-induced slowdowns; QoSFC <ref type="bibr" target="#b16">[15]</ref> dynamically manages SSD tasks to maintain a stable performance state; and RL-assisted GC <ref type="bibr" target="#b14">[13]</ref> schedules GC by predicting host's idle time with reinforcement learning. These techniques reduce the effects of background tasks in SSDs and complement our proposed scheme for reducing inter-tenant interferences.</p><p>• Static resource partitioning in SSDs. By dedicating separate flash memory channels or chips to each tenant, statically partitioned SSDs aim to provide deterministic I/O. This approach was first proposed in vFlash <ref type="bibr" target="#b21">[20]</ref> and more recently has culminated to the proposal of NVMe sets <ref type="bibr" target="#b3">[3]</ref>. <ref type="bibr">FlashBlox [10]</ref> manages asymmetric wears across isolated regions by swapping the entire data between channels in coarse time granularities. However, partitioning resources for isolation is a double-edged sword as it also slashes the inherent parallelism in the device, and each isolated region may become underutilized or overcrowded.</p><p>• Data lifetime tagging in SSDs. Grouping data of similar lifetime reduces the number of valid page copies during GC, and thus improves its overall efficiency <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b10">9]</ref>. Multi-stream <ref type="bibr" target="#b13">[12]</ref> first proposed an interface for the host to tag data with similar lifetimes, and this has been included as directives in the NVM Express specification <ref type="bibr" target="#b2">[2]</ref>. The effectiveness of multistreaming has been demonstrated across multiple domains such as key-value store <ref type="bibr" target="#b13">[12]</ref>, virtualized storage <ref type="bibr" target="#b17">[16]</ref>, high-performance computing <ref type="bibr" target="#b9">[8]</ref>, and file system <ref type="bibr" target="#b19">[18]</ref>. However, its performance improvements stem from the increased overall efficiency of GC, and it does not directly address performance isolation in shared SSD settings.</p><p>• Data placement in storage arrays. Several studies investigate optimal data placement in large storage arrays so that each tenant receives its fair share of I/O resources <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b23">22]</ref>. While these techniques consider data relocation and load balancing as performance overheads, SSDs present unique opportunities-outof-place update and existing internal need for data relocation-that make it intuitive to handle data placement and relocation within the device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Utilitarian Performance Isolation</head><p>The proposed utilitarian performance isolation (UPI) scheme decouples resources to perform write (allocated chips) and resources to store data (allocated blocks). We follow the multi-stream model <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b17">16]</ref> and allow data from only one tenant to be written to each flash memory block. Thus, at any given point in time, each block is associated with only one tenant, or is unused (holds no valid data). Each tenant is allocated a set of flash memory chips for writing data. We call this the allocation set S t of a tenant t. Allocation sets are mutually exclusive and collectively exhaustive. A tenant writes data to chips of its allocation set, but may not write to a chip in another tenant's set, even if there is an unused block. Allocation sets dynamically change over time, reacting to the workload demands and state of the SSD. Reads, on the other hand, are serviced from wherever the data is located, regardless of sets. SSD internal management tasks such as garbage collection (GC) may read a tenant's data located anywhere and write them across its own set. <ref type="figure" target="#fig_4">Figure 2</ref> illustrates the static partitioning and our proposed UPI scheme. In this scenario, the SSD is shared among three tenants (blue , red , and green ). The blue tenant requires large capacity but infrequently accesses the device, the red is a write-intensive application, and the green's read requests are QoS-sensitive. In <ref type="figure" target="#fig_4">Fig- ure 2a</ref>, the flash memory chips are divided among the three tenants according to capacity. This results in underutilized chips for the region assigned to the blue tenant, high GC overhead for the red, and limited parallelism for the green. On the other hand, in <ref type="figure" target="#fig_4">Figure 2b</ref>, each tenant's allocation is based on its dynamic workload characteristics. Blue altruistically shares its parallelism with other tenants, the red benefits from the increased write throughput and reduced GC overhead, and the green enjoys the increased degree of parallelism and isolated performance for its read accesses. Allocation sets thus must be carefully assigned for performance isolation while maximizing the overall parallelism. Intuitively, a throughput-oriented workload should have a larger allocation set to meet its demands, but this may harm the performance of other tenants reading from those chips. On the other hand, a QoS-sensitive tenant prefers to have its data isolated from others, but this limits the overall utilization, both in terms of capacity and parallelism. We first explain the utility function that establishes a figure of merit, and then describe the set allocation and data relocation policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Utility Function</head><p>A utility of a tenant is high when its reads experience less traffic. Util(t, S) is the utility of tenant t given the allocation set S, and is defined as follows:</p><formula xml:id="formula_0">Util(t, S) = chips ∑ c N r (t, c) chips ∑ c ( N r (t, c) 1 − Tra f f ic(c, S) )<label>(1)</label></formula><p>Where N r (t, c) is the number of expected reads from tenant t to chip c, and Tra f f ic(c, S) is the expected traffic intensity of chip c given the allocation set S. For N r , we use the number of observed reads in the past to estimate future reads. The denominator of the utility function is a weighted sum of reads, where the weight models the delay in an M/M/1 queue. Thus, the utility of a tenant approaches 1 (maximum) as the traffic from which it reads approaches 0 (idle). Tra f f ic(c, S) ranges from 0 to 1 and indicates the overall busyness of the chip, and is computed as below:</p><formula xml:id="formula_1">Tra f f ic(c, S) = tenants ∑ t (N r (t, c) · τ r + N p (t, c, S) · τ p ) Time window<label>(2)</label></formula><p>Where τ r is the flash memory read latency and τ p is the program latency. Erase latency is ignored as it becomes negligible with a large number of pages per block. While we expect the number of reads in the future N r to be similar to that of the number of observed reads, the number of expected programs N p also depends on the allocation set S and the write amplification factor WAF, and is estimated as follows:</p><formula xml:id="formula_2">N p (t, c, S) =    N w (t) ·WAF(t) |S t | if c ∈ S t 0 otherwise<label>(3)</label></formula><p>Where N w is the number of programs due to tenant's host writes (excluding GC programs). Because GC activities can be sporadic causing wide variations in observation, we compute the number of expected programs based on N w and WAF (approximated by the number of valid page copies during GC). We then normalize N p by the size of the tenant's allocation set |S t | as writes are striped across chips in its set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Set Allocation</head><p>Our objective is to find an allocation set S that minimizes the max-min ratio of utility across all tenants. This problem is difficult as it can be reduced to an NP-complete partition problem (with variables instead of fixed integers). Instead, we approximate the solution by transferring one chip from the allocation set of the tenant with the maximum utility to that of the minimum. The rationale is because increasing the set size improves the utility of the tenant in the following ways. First, writes of the tenant are spread across a larger number of chips, reducing the traffic intensities. Second, reads are no longer affected by writes of other tenants. To prevent thrashing of chips between sets, the transfer is only made when the expected utility of a tenant losing a chip is at least as high as the current utility of the tenant gaining it. Furthermore, the chip with the least number of observed reads is transferred to reduce the inter-tenant I/O interferences.</p><p>Through set allocation, a chip that once belonged to a set of one tenant may belong to that of another. However, this does not mean that all the data must be relocated immediately. In UPI, resources to perform writes and resources to store data are decoupled, and data of one tenant may remain on the chip of another tenant, or relocate to its allocation set over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data Relocation</head><p>SSDs internally handle data relocation to reclaim space (garbage collection), to provide data integrity (read scrubbing), and to prolong storage lifetime (wear leveling). We enhance these existing mechanisms to handle data relocation for performance isolation. This can be achieved by considering the number of reads for a block if the chip belongs to another tenant. By taking into account the number of reads in the victim selection's cost-benefit analysis, frequently read data is relocated back to its set, isolating its data from other I/O activities. Infrequently accessed cold data may remain in another set and need not be actively moved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We implement the proposed UPI scheme on top of the DiskSim environment <ref type="bibr" target="#b0">[1]</ref> by enhancing its SSD extension <ref type="bibr" target="#b5">[4]</ref>. We construct a modest SSD with 3 channels and 4 chips per channel, and a 150GB logical and 192GB physical capacity (28% over-provisioned). <ref type="table" target="#tab_0">Table 1</ref> summarizes the SSD configuration used in our experiments. Host request handling operates in a non-blocking manner to fully utilize the underlying parallelism, and garbage collection (GC) acts as the mechanism for data relocation among sets. Host writes and GC data are written to different blocks as a means to separate hot and cold data. We use a simple priority scheduler at the flash memory subsystem: host requests have precedence over GC requests.</p><p>We use three workloads, DAS-AS, DTRS, and LM-TBE from Microsoft production server traces <ref type="bibr" target="#b15">[14]</ref> to model three tenants with distinct access patterns. DAS-AS has the lowest throughput, but has the highest read-to-write ratio-this is the tenant that wants to be isolated. On the other hand, DTRS is a relatively random workload with bursts of writes and high GC overheads, and LM-TBE sequentially accesses the device with high intensity-these represent tenants that cause I/O interferences. The workload characteristics are summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>All workloads run to completion (about 24 hours), and the address of each request is modified to fit into a 50GB range for each tenant. Prior to replaying the traces, data is randomly written to physical locations in each tenant's allocation set to emulate a pre-conditioned state. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Preliminary Results</head><p>UPI is evaluated against two static allocation schemes:</p><p>Partitioned dedicates separate chips to each tenant and completely isolates performance. Unified shares all resources among tenants to maximize utilization and parallelism. <ref type="figure" target="#fig_6">Figure 3</ref> compares the performance of UPI against the Partitioned and Unified scheme. We first investigate the average performance shown in <ref type="figure" target="#fig_6">Figure 3a</ref>. Compared to the Partitioned scheme, UPI reduces the average read response time for LM-TBE, a throughput-oriented tenant, by 16.1%. The Unified scheme achieves similar an improvement with a 13.8% reduction. The average performance of UPI for DAS-AS is slightly worse than the Partitioned scheme, as the low write intensity of DAS-AS compacts its data to a smaller set. For the 99% QoS performance of reads in <ref type="figure" target="#fig_6">Figure 3b</ref>, UPI improves by 38.5% for DAS-AS, a latency-sensitive tenant, compared to the Unified scheme. For the same workload, we also observe that the Partitioned scheme improves by 47.7% for the 99% QoS <ref type="figure">figure.</ref> Although the Partitioned scheme achieves perfect isolation for DAS-AS, this comes at a cost of performance degradation for LM-TBE in both the average response time and QoS <ref type="figure">figure.</ref> On the other hand, the Unified scheme performs well for LM-TBE, as it dominates the overall traffic across all chips, but this severely degrades the QoS performance for DAS-AS. Our proposed UPI scheme finds a balance between the two extremes: it achieves better average performance for high-throughput tenants compared to the Partitioned scheme, and better QoS performance for latency-critical tenants compared   to the Unified. This property of UPI can also be observed in the response time CDFs in <ref type="figure" target="#fig_7">Figure 4</ref>.</p><p>We microscopically examine the performance of the Partitioned scheme and UPI in <ref type="figure" target="#fig_10">Figure 5</ref>. <ref type="figure" target="#fig_10">Figure 5a</ref> shows the average read response time sampled every 5 seconds during a 600-second window, approximately 17 hours into the workload (when LM-TBE starts to increase its write bandwidth). The performance of the Partitioned scheme in <ref type="figure" target="#fig_10">Figure 5b</ref> is affected by this, resulting in the overall increase in response time. On the other hand, UPI in <ref type="figure" target="#fig_10">Figure 5c</ref> dynamically increases the allocation set for LM-TBE to balance the traffic across chips, thus achieving better performance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussions</head><p>In this work, UPI considers all tenants equally, but the utility function in Equation 1 can be fine-tuned to provide differentiated services among tenants. By taking an exponent to 1 − Tra f f ic(c, S), we can modify the degree in which traffic intensities affect its utility, thereby providing asymmetric performances to tenants. Similarly, UPI can tradeoff between the average and tail latency performance. By applying another exponent value to N r (t, c) of both the numerator and the denominator, the utility function can shift its focus between the common case and the rare.</p><p>By relaxing the mutually exclusive and collectively exhaustive property of allocation sets, UPI can cover other allocation policies. For example, one such policy can exclude a particular chip from any set to only allow reads to be serviced from it.</p><p>Furthermore, the data relocation policy can be adjusted between eager (stronger isolation) and lazy (better efficiency). By scaling down the degree in which the number of reads for a block affects the victim selection, only heavily-read data in another tenant's set will be moved. Scaling it up will cause data to be relocated aggressively, albeit at a cost of higher GC overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented a utilitarian performance isolation (UPI) scheme for multiple tenants sharing a single SSD. UPI considers the utility of each tenant and dynamically allocates resources so that the inherent parallelism of SSDs can be fully exploited all the while mitigating the effects of inter-tenant I/O interferences. Our preliminary results are promising, with UPI reducing the 99% QoS performance by 38.5% for a latency-sensitive workload, and the average performance by 16.1% for a highthroughput workload. Our design can be extended in several directions, such as providing asymmetric services to tenants and exploring the tradeoff between the average and tail latency performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>AS DAS-PS DTRS LM-TBE MSN-CFS RAD-AS RAD-BE Norm. avg. RT 4x4 2x4 1x4 1x2 3.42</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Normalized average read response times of seven workloads with different degrees of parallelism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Underutilized parallelism High GC overhead Lack of read parallelism flash chip 0 flash chip 1 flash chip 2 flash chip 3 flash chip 4 flash chip 5 flash chip 6 flash chip 7 (a) Static partitioning of resources.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Reduced GC overhead Altruistic sharing of parallelism High degree of read parallelism flash chip 0 flash chip 1 flash chip 2 flash chip 3 flash chip 4 flash chip 5 flash chip 6 flash chip 7 (b) Dynamic allocation in UPI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of three tenants (blue , red , and green ) sharing an SSD. Figure 2a illustrates static resource allocation according to capacity. Figure 2b shows how UPI allocates based on workload demand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of the Partitioned, the Unified, and UPI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Response time CDFs for Unified, Partitioned, and UPI under workloads DAS-AS and LM-TBE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Average response time of the Partitioned scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>DAS-AS DTRS LM-TBE (c) Average response time of UPI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Increase in LM-TBE write bandwidth (Figure 5a) causes performance degradation for the Partitioned scheme (Figure 5b). In comparison, UPI (Figure 5c) is less affected by this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 5: Increase in LM-TBE write bandwidth (Figure 5a) causes performance degradation for the Partitioned scheme (Figure 5b). In comparison, UPI (Figure 5c) is less affected by this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : SSD configuration.</head><label>1</label><figDesc></figDesc><table>Parameter 
Value Parameter 
Value 

# of channels 
3 
Read latency 
50µs 
# of chips/channel 4 
Program latency 
500µs 
# of planes/chip 
2 
Erase latency 
5ms 
# of blocks/plane 
1024 
Data transfer rate 400MB/s 
# of pages/block 
512 
Physical capacity 192GB 
Page size 
16KB Logical capacity 
150GB 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Tenant's workload characteristics. DAS-AS DTRS LM-TBE</head><label>2</label><figDesc></figDesc><table>Num. of writes (m) 
0.1 
5.8 
9.2 
Num. of reads (m) 
1.3 
12.0 
34.7 
Avg. wr. size (KB) 
7.2 
31.9 
61.9 
Avg. rd. size (KB) 
31.5 
21.8 
53.2 
Peak write IOPS 
53.2 
408.8 
589.1 
Peak read IOPS 
89.9 
745.3 4173.8 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank the anonymous reviewers for their constructive and insightful comments, and also thank Sang Lyul Min, Jongmoo Choi, and Eunji Lee for reviewing the early stages of this work. This work was supported in part by SK Hynix and the National Research Foundation of Korea under the PF Class Heterogeneous High Performance Computer Development (NRF-2016M3C4A7952587). Institute of Computer Technology at Seoul National University provided the research facilities for this study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The disksim simulation environment version 4.0 reference manual</title>
		<idno>cmu-pdl-08-101</idno>
		<ptr target="http://www.pdl.cmu.edu/PDL-FTP/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Parallel Data Laboratory</title>
		<editor>DriveChar/CMU-PDL-08-101.pdf</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="http://nvmexpress.org/wp-content/uploads/NVM_Express_Revision_1.3.pdf" />
	</analytic>
	<monogr>
		<title level="j">NVM Express revision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2017" />
			<publisher>NVM Express</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Solving latency challenges with NVM express SSDs at scale</title>
		<ptr target="https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2017/20170809_SIT6_" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Flash Memory Summit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petersen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Design tradeoffs for SSD performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrawal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panigrahy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using data clustering to improve cleaning performance for flash memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software-Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="267" to="290" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The tail at scale. Communications of the ACM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barroso</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="74" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pesto: online storage performance management in virtualized datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gulati</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shanmuganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wald-Spurger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uysal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing (SoCC)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Accelerating a burst buffer via user-level I/O isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lockwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Cluster Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient identification of hot data for flash memory storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsieh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Achieving both performance isolation and uniform lifetime for virtualized SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sen-Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qureshi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Flashblox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Workload-aware budget compensation scheduling for NVMe solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Non-Volatile Memory System and Applications Symposium (NVMSA)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The multistreamed solid-state drive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-U</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reinforcement learningassisted garbage collection to mitigate long-tail latency in SSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">134</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Characterization of storage workload traces from production Windows servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavalanekar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharda</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">QoS-aware flash memory controller</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards SLO complying SSDs through OPS isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Autonomous storage management using performance prediction in multi-tenant datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lilja</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Romano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing (SoCC)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Managing flash streams in the file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rho</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-U</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fstream</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance isolation and fairness for multi-tenant cloud storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shue</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaikh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Architecting flash-based solid-state drive for high-performance I/O virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="61" to="64" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Tiny-tail flash: Nearperfect elimination of garbage collection tail latencies in NAND SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunawi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Autotiering: Automatic data placement manager in multi-tier all-flash datacenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hoseinzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Bolt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Bhimani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><surname>Swan-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Performance Computing and Communications Conference (IPCCC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
