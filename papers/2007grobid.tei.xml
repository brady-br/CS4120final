<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Concurrent RDF Queries using RDMA-assisted GPU Graph Exploration Fast and Concurrent RDF Queries using RDMA-assisted GPU Graph Exploration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 11-13. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Parallel and Distributed Systems</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Shanghai Jiao</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Lou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Parallel and Distributed Systems</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Shanghai Jiao</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Parallel and Distributed Systems</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Shanghai Jiao</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
							<email>haibochen@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Parallel and Distributed Systems</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Shanghai Jiao</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><forename type="middle">Jiao</forename><surname>Tong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Parallel and Distributed Systems</orgName>
								<orgName type="institution">University</orgName>
								<address>
									<settlement>Shanghai Jiao</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Lou</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Concurrent RDF Queries using RDMA-assisted GPU Graph Exploration Fast and Concurrent RDF Queries using RDMA-assisted GPU Graph Exploration</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18)</title>
						<meeting>the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18) <address><addrLine>Boston, MA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 11-13. 2018</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 2018 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc18/presentation/wang-siyuan This paper is included in the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>RDF graph has been increasingly used to store and represent information shared over the Web, including social graphs and knowledge bases. With the increasing scale of RDF graphs and the concurrency level of SPARQL queries, current RDF systems are confronted with inefficient concurrent query processing on massive data paral-lelism, which usually leads to suboptimal response time (latency) as well as throughput. In this paper, we present Wukong+G, the first graph-based distributed RDF query processing system that efficiently exploits the hybrid parallelism of CPU and GPU. Wukong+G is made fast and concurrent with three key designs. First, Wukong+G utilizes GPU to tame random memory accesses in graph exploration by efficiently mapping data between CPU and GPU for latency hiding, including a set of techniques like query-aware prefetch-ing, pattern-aware pipelining and fine-grained swapping. Second, Wukong+G scales up by introducing a GPU-friendly RDF store to support RDF graphs exceeding GPU memory size, by using techniques like predicate-based grouping, pairwise caching and look-ahead replacing to narrow the gap between host and device memory scale. Third, Wukong+G scales out through a communication layer that decouples the transferring process for query metadata and intermediate results, and leverages both native and GPUDirect RDMA to enable efficient communication on a CPU/GPU cluster. We have implemented Wukong+G by extending a state-of-the-art distributed RDF store (i.e., Wukong) with distributed GPU support. Evaluation on a 5-node CPU/GPU cluster (10 GPU cards) with RDMA-capable network shows that Wukong+G outperforms Wukong by 2.3X-9.0X in the single heavy query latency and improves latency and throughput by more than one order of magnitude when facing hybrid workloads.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Resource Description Framework (RDF) is a standard data model for the Semantic Web, recommended by W3C <ref type="bibr">[5]</ref>. RDF describes linked data as a set of triples forming a highly connected graph, which powers information retrievable through the query language SPARQL. RDF and SPARQL have been widely used in Google's knowledge graph <ref type="bibr" target="#b18">[22]</ref> and many public knowledge bases, such as DBpedia <ref type="bibr" target="#b0">[1]</ref>, PubChemRDF <ref type="bibr" target="#b34">[38]</ref>, Wikidata <ref type="bibr" target="#b4">[8]</ref>, Probase <ref type="bibr" target="#b55">[59]</ref>, and Bio2RDF <ref type="bibr" target="#b6">[10]</ref>.</p><p>The drastically increasing scale of RDF graphs has posed a grand challenge to fast and concurrent queries over large RDF datasets <ref type="bibr" target="#b13">[17]</ref>. Currently, there have been a number of systems built upon relational databases, including both centralized <ref type="bibr" target="#b36">[40,</ref><ref type="bibr" target="#b8">12,</ref><ref type="bibr" target="#b54">58]</ref> and distributed <ref type="bibr" target="#b44">[48,</ref><ref type="bibr" target="#b40">44,</ref><ref type="bibr" target="#b19">23]</ref> designs. On the other hand, Trinity.RDF <ref type="bibr" target="#b58">[62]</ref> uses graph exploration to reduce the costly join operations in intermediate steps but still requires a final join operation. To further accelerate distributed query processing, Wukong <ref type="bibr" target="#b47">[51]</ref> leverages RDMA-based graph exploration to support massively concurrency queries with low latency requirement and adopts full-history pruning to avoid the final join operation.</p><p>Essentially, many RDF queries have embarrassing parallelism, especially for heavy queries, which usually touch a large portion of the RDF graph on an excessive amount of paths using graph exploration. This poses a significant challenge even for multicore CPUs to handle them efficiently, which usually causes lengthy execution time. For example, the latency differences among seven queries in LUBM <ref type="bibr">[7]</ref> is more than 3,000X (0.13ms and 390ms for Q5 and Q7 accordingly). This may cause one heavy query block all other queries, substantially extending the latency of other queries and dramatically impairing the throughput of processing concurrent queries <ref type="bibr" target="#b47">[51]</ref>. This problem has also gained increased attention <ref type="bibr" target="#b41">[45]</ref>.</p><p>In this paper, we present Wukong+G 1 with a novel design that exploits a distributed heterogeneous CPU/GPU cluster to accelerate heterogeneous RDF queries based on distributed graph exploration. Unlike CPUs pursuing the minimized execution time for single instructions, GPUs are designed to provide high computational throughput for massive simple control-flow operations with little or no control dependency. Such features expose a design space to distribute hybrid workloads by offloading heavy queries to GPUs. Nevertheless, different from many traditional GPU workloads, RDF graph queries are memory-intensive instead of computeintensive: there are limited arithmetic operations and most of the processing time is spent on random memory accesses. This unique feature implies that the key of performance optimizations in Wukong+G is on smart Q L Q H <ref type="figure">Fig. 1</ref>: A sample of RDF data and two SPARQL queries (Q H and Q L ). White circles indicate the normal vertices (subjects and objects); dark circles indicate the (type and predicate) index vertices. Q H is a heavy query, and Q L is a light query. memory usage rather than improving the computation algorithm. Wukong+G is made fast and concurrent with the following key designs:</p><p>GPU-based query execution ( §4.1). To achieve the best performance for massive random accesses demanded by heavy queries, Wukong+G leverages the many-core feature and latency hiding ability of GPUs. Besides making use of hardware advantages, Wukong+G surmounts the limitations of GPU memory size and PCIe (PCI Express) bandwidth by adopting query-aware prefetching to mitigate the constraints on graph size, pattern-aware pipelining to hide data movement cost, and fine-grained swapping to minimize data transfer size.</p><p>GPU-friendly RDF store ( §4.2). To support desired CPU/GPU co-execution pattern while still enjoying the fast graph exploration, Wukong+G follows a distributed in-memory key/value store and proposes a predicatebased grouping to aggregate keys and values with the same predicate individually. Wukong+G further smartly manages GPU memory as a cache of RDF store by supporting pairwise caching and look-ahead replacing.</p><p>Heterogeneous RDMA communication ( §4.3). To preserve better communication efficiency in a heterogeneous environment, Wukong+G decouples the transferring process of query metadata and intermediate results for SPARQL queries. Wukong+G uses native RDMA to send metadata like query plan and current step among CPUs, and uses GPUDirect RDMA to send current intermediate results (history table) directly among GPUs. This preserves the performance boost brought by GPUs from potential expensive CPU/GPU data transfer cost.</p><p>We have implemented Wukong+G by extending Wukong <ref type="bibr" target="#b47">[51]</ref>, a state-of-the-art distributed RDF query system to support heterogeneous CPU/GPU processing. To confirm the performance benefit of Wukong+G, we have conducted a set of evaluations on a 5-node CPU/GPU cluster (10 GPU cards) with RDMA-capable network. The experimental results using the LUBM <ref type="bibr">[7]</ref> benchmark show that Wukong+G outperforms Wukong by 2.3X-9.0X in the single heavy query latency and improves latency and throughput by more than one order of magnitude when facing hybrid workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RDF and SPARQL</head><p>An RDF dataset is composed by triples, in the form of sub ject, predicate, ob ject. To construct a graph (aka RDF graph), each triple can be regarded as a directed edge (predicate) connecting two vertices (from subject to object). In <ref type="figure">Fig. 1</ref>, a simplified sample RDF graph of LUBM dataset <ref type="bibr">[7]</ref> includes two professors (Logan and Erik), three students (Marie, Bobby, and Kurt), and two courses (OS and DS). <ref type="bibr" target="#b1">2</ref> There are also three predicates (teacherOf (to), advisor (ad) and takeCourse (tc)) to link them. Two types of indexes, predicate and type, are added to accelerate query processing on RDF graph <ref type="bibr" target="#b47">[51]</ref>.</p><p>SPARQL, a W3C recommendation, is a standard query language developed for RDF graphs, which defines queries regarding graph patterns (GP). The principal part of SPARQL queries is as follows:</p><formula xml:id="formula_0">Q := SELECT RD WHERE GP</formula><p>where (RD) is the result description and GP consists of triple patterns (TP). The triple pattern looks like a normal triple except that any constant can be replaced by a variable (e.g., ?X) to match a subgraph. The result description RD contains a subset of variables in the triple patterns (TP) to define the query results. For example, the query Q H in <ref type="figure">Fig. 1</ref> asks for professors (?X), courses (?Y) and students (?Z) such that the professor advises (ad) the student who also takes a course (tc) taught by (to) the professor. After exploring all three TPs in Q H on the sample graph in <ref type="figure">Fig. 1</ref>, the exact match of RD (?X, ?Y and ?Z) is only a binding of Logan, OS, and Bobby.</p><p>Query processing on CPU. There are two representative approaches adopted in state-of-the-art RDF systems, (relational) triple join <ref type="bibr" target="#b36">[40,</ref><ref type="bibr" target="#b54">58,</ref><ref type="bibr" target="#b8">12,</ref><ref type="bibr" target="#b19">23]</ref> and graph exploration <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b47">51]</ref>. A recent study <ref type="bibr" target="#b47">[51]</ref> found that graph exploration with full-history pruning can provide low latency and high throughput for concurrent query processing. Therefore, we illustrate this approach to demonstrating the query processing on CPU with the sample RDF graph and SPARQL query (Q H ) in <ref type="figure">Fig. 1</ref>.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, all triple patterns of the query (Q H ) will be iterated in sequence (➊) to generate the results (history table) by exploring the graph, which is stored in an in-memory key/value store. According to the variable (?Y) of the current triple pattern (TP-2), each row of a certain column in the history table (➋) will be combined with the constant (takesCourse) of the triple pattern as the key (➌) to retrieve the value (➍). The value will be appended to a new column (?Z) of the history table (➎).</p><p>Note that an extra triple pattern (TP-0) from an index vertex (teacherOf) will be used to collect all start vertices satisfying a variable (?X) in TP-1.</p><p>Full-history pruning. Since processing RDF query by graph exploration needs to traverse the RDF graph, it is crucial to prune infeasible paths for better performance. There are basically two approaches: partial-history pruning <ref type="bibr" target="#b58">[62]</ref>, by inheriting partial history information (intermediate results) from previous steps of traversing to prune the following traversal paths; and full-history pruning <ref type="bibr" target="#b47">[51]</ref>, by passing the history information of all previous traversal steps for pruning. Wukong has exploited full-history pruning to prune unnecessary intermediate results precisely and make all traversal paths completely independent. Thanks to the fast RDMAcapable network as well as the relative cost-insensitivity of one-sided RDMA operations regarding payload size, full-history pruning is very effective and efficient to handle concurrent queries.</p><p>Workload heterogeneity. Prior work <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b47">51]</ref> has observed that there are two distinct types of SPARQL queries: light and heavy. Light queries (e.g., Q L in <ref type="figure">Fig. 1</ref>) usually start from a (constant) normal vertex and only explore a few paths regardless of the dataset size. In contrast, heavy queries (e.g., Q H in <ref type="figure">Fig. 1</ref>) usually start from an (type or predicate) index vertex and explore massive amounts of paths, which increases along with the growth of dataset size. The top of <ref type="figure" target="#fig_1">Fig. 3</ref> demonstrates the number of paths explored by two typical queries (Q5 and Q7) on LUBM-10240 (10 vs. 16,000,000). The heterogeneity in queries can result in tremendous latency differences on state-of-the-art RDF stores <ref type="bibr" target="#b47">[51]</ref>, even reaching more than 3,000X (0.13ms and 390ms for Q5 and Q7 on LUBM-10240 accordingly). <ref type="bibr" target="#b2">3</ref> Therefore, the multi-threading mechanism is widely used by prior work <ref type="bibr" target="#b19">[23,</ref><ref type="bibr" target="#b58">62,</ref><ref type="bibr" target="#b47">51]</ref> to improve the performance of heavy queries. However, such approach is intrinsically restricted by the limited computation resource of CPU. Currently, the maximum number of cores in a commercial CPU processor is usually less than 16. Moreover, <ref type="bibr" target="#b2">3</ref> Detailed experimental setup and results can be found in §6. the lengthy queries will significantly extend the latency of light queries and impair the throughput of processing concurrent queries. Some CPU systems like Oracle PGX <ref type="bibr" target="#b3">[4]</ref> try to address this issue by adopting priority mechanism. However, with no variation of computing power, the sacrifice of user experience for one type of queries is unavoidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hardware Trends</head><p>Hardware heterogeneity. With the prevalence of computational workloads (e.g., machine learning and data mining applications), it is now not uncommon to see server-class machines equipped with GPUs in the modern datacenter. As a landmark difference compared to CPU, the number of GPU cores (threads) can easily exceed two thousand, which far exceeds existing multicore CPU processors. As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, in a typical heterogeneous (CPU/GPU) machine, CPU and GPU have their private memory (DRAM) connected by PCIe with limited bandwidth (10GB/s). Compared to host memory (CPU DRAM), device memory (GPU DRAM) has much higher bandwidth (288GB/s vs. 68GB/s) but less capacity (12GB vs. 128GB). Generally, GPU is optimized for performing massive, simple and independent operations with intensive accesses on a relatively small memory footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fast communication: GPUDirect with RDMA.</head><p>GPUDirect is a family of technologies that is continuously developed by NVIDIA <ref type="bibr" target="#b2">[3]</ref>. Currently, it can support various efficient communications, including internode, intra-node, and inter-GPU. RDMA (Remote Direct Memory Access) is a networking feature to directly access the memory of a remote machine, which can bypass remote CPU and operating system, and avoid redundant memory copy. Hence, it has unique features like high speed, low latency and low CPU overhead. GPUDirect RDMA has been introduced in NVIDIA Kepler-class GPUs, like Tesla and Quadro series. This technique enables direct data transfer between GPUs by InfiniBand NICs as the name suggests <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Opportunities</head><p>Though prior work (e.g., Wukong <ref type="bibr" target="#b47">[51]</ref>) has successfully demonstrated the low latency and high throughput of running light queries solely by leveraging graph exploration with full-history pruning, it is still incompetent to handle heavy queries efficiently. This leads to suboptimal performance when facing hybrid workloads comprising both light and heavy queries.</p><p>This problem is not due to the design and implementation of existing state-of-the-art systems, which have been heavily optimized by several approaches including multithreading <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b47">51]</ref> and work-stealing scheme <ref type="bibr" target="#b47">[51]</ref>. We attribute the performance issues mainly to the lim- itation of handling hybrid workloads (light and heavy queries) on the homogeneous hardware (CPU), which can provide neither sufficient computation resources (a few cores) nor efficient data accesses (low bandwidth).</p><p>GPU is a good candidate to host heavy queries. First, the graph exploration strategy for query processing heavily relies on traversing massive paths on the graph store, which is a typical memory-intensive workload targeted by GPU's high memory bandwidth. Second, the memory latency hiding capability of GPU is inherently suitable for the random traversal on RDF graph, which is notoriously slow due to poor data locality. Third, every traversal path with the full-history pruning scheme is entirely independent, which can be fully parallelized on thousands of GPU cores.</p><p>In summary, the recent trend of hardware heterogeneity (CPU/GPU) opens an opportunity for running different queries on different hardware; namely, running light queries on CPUs and heavy queries on GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WUKONG+G: AN OVERVIEW</head><p>System architecture. An overview of Wukong+G's architecture is shown in <ref type="figure">Fig. 4</ref>. Wukong+G assumes running on a modern cluster connected with RDMA-capable fast networking, where each machine is equipped with one or more GPU cards. The GPU's device memory is treated as a cache for the large pool of the CPU's host memory. Wukong+G targets various SPARQL queries over a large volume of RDF data; it scales by partitioning the RDF graph into a large number of shards across multiple servers. Wukong+G may duplicate edges to make sure each server contains a self-contained subgraph (e.g., no dangling edges) of the input RDF graph for better locality. Note that there are no replicas of vertices in Wukong+G as no vertex data needs to synchronize. Moreover, Wukong+G also creates index vertices <ref type="bibr" target="#b47">[51]</ref> for types and predicates to assist query processing.</p><p>Similar to prior work <ref type="bibr" target="#b47">[51]</ref>, Wukong+G follows a decentralized, shared-nothing, main-memory model on the server side. Each server consists of two separate layers: query engine and graph store. The query engine layer employs a worker-thread model by running N worker threads atop N CPU cores and dedicates one CPU core to run an agent thread; the agent thread will assist the worker threads on GPU cores to run queries. Each worker/agent thread on CPU has a task queue to continuously handle queries from clients or other servers, one at a time. The graph store layer adopts an RDMA-friendly key/value store over a distributed hash table to support a partitioned global address space. Each server stores a partition of the RDF graph, which is shared by all of worker/agent threads on the same server.</p><p>Wukong+G uses a set of dedicated proxies to run the client-side library and collect queries from massive clients. Each proxy parses queries into a set of stored procedures and generates optimal query plans using a cost-based approach. The proxy will further use the cost to classify a query into one of two types (light or heavy), and deliver it to a worker or agent thread accordingly. 4</p><p>Basic query processing on GPU. In contrast to the query processing on CPU, which has to perform a triple pattern with massive paths in a verbose loop style (see <ref type="figure" target="#fig_0">Fig. 2</ref>), Wukong+G can fully parallelize the graph exploration with thousands of GPU cores. The basic approach is to dedicate one CPU core to perform the control-flow of the query, and use massive GPU cores to parallelize the data-flow of the query. As shown in <ref type="figure" target="#fig_2">Fig. 5</ref>, the agent thread on CPU core will first read the next triple pattern (➊) of the current query and prepare a cache of RDF datasets on GPU memory (➋). After that, the agent thread will leverage all GPU cores to perform the triple pattern in parallel (➌). Each worker thread on GPU core can independently fetch a row in the history table (➍) and combine it with the constant (takesCourse) of the triple pattern (TP-2) as the key (➎). The value retrieved by the key (➏) will be appended to a new column (?Z) of the history table (➐). While the hybrid design seems intuitive, Wukong+G still faces three challenges to run SPARQL queries on GPUs, which will be addressed by the techniques in §4: C1: Small GPU memory. It is well known that GPU can obtain optimal performance only when the device memory (GPU DRAM) gets everything ready. Prior systems <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b47">51]</ref> can store an entire RDF graph in the host memory (CPU DRAM) since it is common that server-class machines equip with several hundred GBs of memory. However, this assumption does not apply to GPU since its current memory size usually stays less than 16GB. We should not allow device memory size to limit the upper bound of the supported working sets.</p><formula xml:id="formula_1">TP-0 TP-1 TP-2 ...</formula><p>C2: Limited PCIe bandwidth. The memory footprint of SPARQL queries may touch arbitrary triples of the RDF graph. Therefore, the data transfer between CPU and GPU memory during query processing is unavoidable, especially for concurrent query processing. However, GPUs are connected to CPUs by PCIe (PCI Express), which has insufficient memory bandwidth (10GB/s). To avoid the bottleneck of data transfer, we should carefully design mechanisms to predict access patterns and minimize the number, volume and frequency of data swapping.</p><p>C3: Cross-GPU communication. With the increasing scale of RDF datasets and the growing number of concurrent queries, it is highly demanding that query processing systems can scale to multiple machines. Prior work <ref type="bibr" target="#b53">[57,</ref><ref type="bibr" target="#b47">51]</ref> has shown the effectiveness and efficiency of the partitioned RDF store and the workerthread model. However, the intra-/inter-node communication between multiple GPUs has a long path: 1) device-to-host via PCIe; 2) host-to-host via networking; 3) host-to-device via PCIe. We should customize the communication flow for various participants to reduce the latency of network traffic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Efficient Query Processing on GPU</head><p>Facing the challenges like small GPU memory and limited PCIe bandwidth, we propose the following three key techniques to overcome them.</p><p>Query-aware prefetching. With the increase of RDF datasets, the limited GPU memory size (less than 16GB) is not enough to host the entire RDF graph. Wukong+G thus treats the GPU memory as a cache of CPU memory, and only ensures the necessary data is retained in GPU memory before running a query. However, it is nontrivial to decide the working set of a query accurately. As shown in the second timeline of <ref type="figure">Fig. 6</ref>, Wukong+G proposes to just prefetch the triples with the predicates involved in a query, which can enormously reduce the memory footprint of a query from the entire RDF graph to the per-query scale. This assumption is based on two observations: 1) each query only touches a part of RDF graph; 2) the predicate of a triple pattern is commonly known (i.e., ?X, predicate, ?Y ). For example, the sample query (Q H ) only requires three predicates (teacherOf, takesCourse, and advisor), occupying about 3.7GB memory (0.3GB, 2.9GB, and 0.5GB respectively) for LUBM-2560.</p><p>Pattern-aware pipelining. For a query with many triple patterns, the total memory footprint of a single query may still exceed the GPU memory size. Fortunately, we further observe that the triple patterns of a query will be executed in sequence. It implies that Wukong+G can further reduce the demand for memory to the per-pattern scale. As shown in the third timeline of <ref type="figure">Fig. 6</ref>, Wukong+G can only prefetch the triples with a certain predicate that is used by the triple pattern will be immediately executed. Thus, for the sample query (Q H ) on LUBM-2560, the demand for GPU memory will further reduce to 2.9GB, the size of the maximum predicate (takesCourse).</p><p>Moreover, since the data prefetching and query processing are split into multiple independent phases, Wukong+G can use a software pipeline to create parallelism between the execution of the current triple pattern and the prefetching of the next predicate, as shown in the fourth timeline of <ref type="figure">Fig. 6</ref>. Note that it will also increase the memory footprint to the maximum size of two successive predicates (takesCourse and advisor).</p><p>Fine-grained swapping. Although the pattern-aware pipelining can overlap the latency of data prefetching and query processing, it is hard to perfectly hide the I/O cost due to limited bandwidth between system and <ref type="table">Table 1</ref>: A summary of optimizations for query processing on GPU.</p><p>"X|Y " indicates XGB memory footprint and Y GB data transfer. ( †) The numbers are evaluated on 6GB GPU memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Granularity</head><p>Main Techniques Q7 (GB) on LUBM-2560 Entire graph</p><p>Basic query processing 16.3 | 16.3 Per-query</p><p>Query-aware prefetching 5.6 | 5.6 Per-pattern Pattern-aware pipelining 2.9 | 5.6 Per-block Fine-grained swapping 2.9 | 0.7 † device memory (e.g., 10GB/s). For example, prefetching 2.9GB triples (takesCourse) requires about 300ms, which is even longer than the whole query latency (100ms). Therefore, Wukong+G adopts a fine-grained swapping scheme to maintain the triples cached in GPU memory. All triples with the same predicate will be further split into multiple fixed-size blocks, and the GPU memory will cache the triples in a best-effort way ( §4.2). Consequently, the demand of memory will be further reduced to the per-block scale. Moreover, the data transferring cost will also become the per-block scale, and all cached data on GPU memory can be reused by multiple triple patterns of the same query or even multiple queries. As shown in the fifth timeline of <ref type="figure">Fig. 6</ref>, when most triples of the required predicates have been retained in GPU memory, the prefetching cost can be perfectly hidden by query processing. Even for the first required predicate, Wukong+G still can hide the cost by overlapping it with the planning time of this query or the processing time of a previous query. <ref type="table">Table 1</ref> summarizes the granularity of data prefetching on GPU memory, and shows the size of memory footprint and data transfer for a real case (Q7 on LUBM-2560). Note that Q7 is similar to Q H but requires five predicates. The memory footprint of Q7 with finegrained swapping is equal to the available GPU memory size (6GB) since Wukong+G only swaps out the triples of predicates on demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GPU-friendly RDF Store</head><p>Prior work <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b47">51,</ref><ref type="bibr" target="#b60">64]</ref> uses a distributed in-memory key/value store to physically store the RDF graph, which is efficient to support random traversals in graphexploration scheme. In contrast to the intuitive design <ref type="bibr" target="#b58">[62]</ref>  This design can prominently reduce the graph traversal cost for both local and remote accesses. However, the triples (both key and value) with the same predicate are still sprinkled all over the store. It implies that the cost of prefetching keys and values for a triple pattern is extremely high or even impossible. Therefore, the key/value store on CPU memory should be carefully re-organized for heterogeneous CPU/GPU processing by aggregating all triples with the same predicate and direction into a segment. Furthermore, the key and value segments should be maintained in a fine-grained way (block) and be cached in pairs. Finally, the mapping between keys and values should be retained in the key/value cache on GPU memory, which uses a separate address space. Wukong+G proposes the following three new techniques to construct a GPU-friendly key/value store, as shown in the right part of <ref type="figure" target="#fig_5">Fig. 7</ref>.</p><p>Predicate-based grouping (CPU memory). Based on the idea of predicate-based decomposition in Wukong, Wukong+G adopts predicate-based grouping to exploit the predicate locality of triples and retains the encoding of keys and values. The basic idea is to partition the key space into multiple segments, which are identified by the combination of predicate and direction (i.e., <ref type="bibr">[pid, d]</ref>). To preserve the support of fast graph exploration, Wukong+G still uses the hash function within the segment but changes the parameter from the entire key (i.e., <ref type="bibr">[vid, pid, d]</ref>) to the vertex ID (vid). The number of keys and values in each segment are collected during loading the RDF graph and aligned to an integral multiple of the granularity of data swapping (block). To ensure that all values belonged to the triples with the same predicate are stored contiguously, Wukong+G groups such triples and inserts them together. Moreover, Wukong+G uses an indirect mapping to link keys and values, where the link is an offset within the value space instead of a direct pointer. As shown in the right part of <ref type="figure" target="#fig_5">Fig. 7</ref>, the triples required by TP-2 (i.e., Kurt,tc, DS and Bobby,tc, OS) are aggregated together in both key and value spaces (the purple boxes).</p><p>Pairwise caching (GPU memory). To support finegrained swapping, Wukong+G further splits each segment into multiple fixed-size blocks and stores them into discontinuous blocks of the cache on GPU memory, like <ref type="bibr">[Logan,to, out]</ref> and <ref type="bibr">[Erik,to, out]</ref>. Note that the block size for keys and values can be different. Wukong+G follows the design on CPU memory to cache key and value blocks into separate regions on GPU memory, namely key cache and value cache. Wukong+G uses a simple table to map key and value blocks, and the link from key to value becomes the offset within the value block. Unlike the usual cache, the linked key and value blocks must be swapped in and out the (GPU) cache in pairs, like <ref type="bibr">[OS, tc, in]</ref> and <ref type="bibr">[Bobby]</ref> (the purple boxes). Thus, Wukong+G maintains a bidirectional mapping between the pair of cached key and value blocks. Moreover, a mapping table of block ID between RDF store (CPU) and cache (GPU) is used to re-translate the link between keys and values, when the pairwise blocks (key and value) are swapped in GPU memory.</p><p>Look-ahead replacement policy. The mapping table of block IDs between RDF store and cache records whether the block has been cached. Before running a triple pattern of the query, all of key and value blocks should be prefetched to the GPU memory. For example, the key [OS,tc, in] and value <ref type="bibr">[Bobby]</ref> should be loaded into the cache before processing TP-2. Wukong+G proposes a look-ahead LRU-based replacement policy to decide where to store prefetched key and value blocks. Specifically, Wukong+G prefers to use free blocks first and then chooses the blocks that will not be used by the following triple patterns of this query (look-ahead), with the highest LRU score. The worst choice is the blocks will be used by the following triple patterns, and then the block of the farthest triple pattern will be replaced. Note that the replacement policies for keys and values are the same and there is at most a pair of key/value blocks will be swapped out due to the pairwise caching scheme. For example, as shown in the right part of <ref type="figure" target="#fig_5">Fig. 7</ref>, before running the triple pattern TP-2, all key/value blocks of the predicate takeCourse (tc) should be swapped in the cache (the purple boxes). The value block with <ref type="bibr">[Bobby]</ref> can be loaded to a free block, while the key block with <ref type="bibr">[OS,tc, in]</ref> will replace the cached block with <ref type="bibr">[Pidx,to, in]</ref>, since it was used by TP-0 with the highest LRU score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distributed Query Processing</head><p>Wukong+G splits the RDF graph into multiple disjoint partitions by a differentiated partitioning algorithm <ref type="bibr" target="#b47">[51,</ref><ref type="bibr" target="#b15">19]</ref>  <ref type="bibr">5</ref> , and each machine hosts an RDF graph partition and launches many worker threads on CPUs and GPUs to handle concurrent light and heavy queries respectively. The CPU worker threads on different machines will only communicate with each other for (light) query processing, and it is the same to GPU worker threads for (heavy) query processing.</p><p>To handle light queries on CPU worker threads, Wukong+G simply follows the procedure (see <ref type="figure" target="#fig_0">Fig. 2</ref>) that has been successfully demonstrated by Wukong <ref type="bibr" target="#b47">[51]</ref>. However, to handle heavy queries on GPU worker threads, the procedure (see <ref type="figure" target="#fig_2">Fig. 5</ref>) becomes complicated due to the assistance of (CPU) agent thread and the maintenance of (GPU) RDF cache.</p><p>Execution mode: fork-join. Prior work <ref type="bibr" target="#b47">[51]</ref> proposes two execution modes, in-place and fork-join, for distributed graph exploration to migrate data and execution respectively. The in-place execution mode synchronously leverages one-sided RDMA READ to directly fetch data from remote machines, while the fork-join mode asynchronously splits the following query computation into multiple sub-queries running on remote machines. Wukong+G follows the design on CPU worker threads but only adopts the fork-join mode for query processing on GPU, because the in-place mode is usually inefficient for heavy queries <ref type="bibr" target="#b47">[51]</ref> and migrating data from remote CPU memory to local GPU memory is still very costly even with RDMA operations.</p><p>In the fork-join mode, the agent thread will split the running query (metadata) with intermediate results <ref type="table">(his- tory table)</ref> into multiple sub-queries for the following query processing, and dispatch them to the task queue of agent threads on remote machines by leveraging onesided RDMA WRITE. Therefore, multiple heavy queries can be executed on multiple GPUs concurrently in a time-sharing way. However, the current history table is located in GPU memory (see <ref type="figure">Fig. 8</ref>), such that it would be inefficient to fetch and split the table by using a single agent thread on CPU (➊ and ➋ in <ref type="figure">Fig. 8(a)</ref>). Therefore, Wukong+G leverages all GPU cores to partition the history table in fully parallel (➊ in <ref type="figure">Fig. 8(b)</ref>) using a dynamic task scheduling mechanism <ref type="bibr" target="#b43">[47,</ref><ref type="bibr" target="#b14">18]</ref>.</p><p>Communication flow. To support fork-join execution, the sub-queries will be sent to target machines with their metadata (e.g., query plan and current step) and history table (intermediate results), and the history table will be sent back with final results at the end. As shown in <ref type="figure">Fig. 8(a)</ref>, the query metadata will be delivered by onesided RDMA operations between the CPU memory of two machines (➌ and ➏). In contrast, the history table has to go through a long path from local GPU memory to the remote GPU memory, and finally goes back to the local CPU memory. A detailed communication flow for history table (see <ref type="figure">Fig. 8(a)</ref>): 1) from local GPU memory to local CPU memory (➊, Device-to-Host); 2) from local CPU memory to remote CPU memory (➌, Host-to-Host); 3) from remote CPU memory to remote GPU memory (➍, Host-to-Device); 4) from remote GPU memory to remote CPU memory (➎, Device-to-Host); 5) from local CPU memory to remote CPU memory (➏, Host-to-Host).</p><p>GPUDirect <ref type="bibr" target="#b2">[3]</ref> opens an opportunity for Wukong+G to directly write history table from local GPU memory to remote GPU and CPU memory. Hence, Wukong+G decouples the transferring process of query metadata and history table (➋ and ➋ in <ref type="figure">Fig. 8(b)</ref>), and further shortens the communication flow for history table by leveraging GPUDirect RDMA. It also avoids the contention on agent thread with the metadata transferring. A detailed communication flow for history table (see <ref type="figure">Fig. 8(b)</ref>): 1) from local GPU memory to remote GPU memory (➋, Device-to-Device); 2) from remote GPU memory to local CPU memory (➌, Device-to-Host).</p><p>Moreover, to mitigate the pressure on GPU memory when handling multiple heavy queries, Wukong+G choose to send the history table of pending queries from local GPU memory to the buffer on remote CPU memory first via GPUDirect RDMA, and delay the prefetching of history table from CPU memory to GPU memory till handling the query on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>Wukong+G prototype is implemented in 4,088 lines of C++/CUDA codes atop of the code base of Wukong. This section describes some implementation details.</p><p>Multi-GPUs support. Currently, it is not uncommon to equip every CPU socket with a separate GPU card for low communication cost and good locality. To support such multi-GPUs on a single machine, Wukong+G runs a separate server for each GPU card and several co-located CPU cores (usually a socket). All servers comply with the same communication mechanism via GPUDirectcapable RDMA operations, regardless of whether two servers share the same physical machine or not.</p><p>Too large intermediate results. In rare cases, the intermediate results may overflow the history buffer on The remaining strips will stay in CPU memory and be swapped in GPU memory one-by-one while processing a single triple pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Hardware configuration. All evaluations are conducted on a rack-scale cluster with 10 servers on 5 machines. We run two servers on a single machine. Each server has one 12-core Intel Xeon E5-2650 v4 CPU with 128GB of DRAM, one NVIDIA Tesla K40m GPU with 12GB of DRAM, and one Mellanox ConnectX-3 56Gbps InfiniBand NIC via PCIe 3.0 x8 connected to a Mellanox IS5025 40Gbps IB Switch. Wukong+G only provides a one-to-one mapping between the work and agent threads on different servers <ref type="bibr" target="#b47">[51]</ref>, which mitigates the scalability issue of RDMA networks with reliable transports <ref type="bibr" target="#b27">[31]</ref> and simplifies the implementation of the task queue. In all experiments, we reserve two cores on each CPU to generate requests for all servers to avoid the impact of networking between clients and servers as done in prior work <ref type="bibr" target="#b50">[54,</ref><ref type="bibr" target="#b52">56,</ref><ref type="bibr" target="#b53">57,</ref><ref type="bibr" target="#b16">20,</ref><ref type="bibr" target="#b47">51]</ref>.</p><p>Benchmarks. Our benchmarks include one synthetic and two real-life datasets, as shown in <ref type="table" target="#tab_0">Table 2</ref>. The synthetic dataset is the Lehigh University Benchmark (LUBM) <ref type="bibr">[7]</ref>. We generate 5 datasets with different sizes (up to LUBM-10240) and use the query set published in Atre et al. <ref type="bibr" target="#b9">[13]</ref>, which are widely used by many distributed RDF systems <ref type="bibr" target="#b32">[36,</ref><ref type="bibr" target="#b58">62,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b47">51]</ref>. The real-life datasets include the DBpedia's SPARQL Benchmark (DBPSB) <ref type="bibr" target="#b0">[1]</ref> and YAGO2 <ref type="bibr" target="#b5">[9,</ref><ref type="bibr" target="#b26">30]</ref>. For DBPSB, we use the query set recommended by its official site. For YAGO2, we collect our query set from both H 2 RDF+ <ref type="bibr" target="#b39">[43]</ref> and RDF-3X <ref type="bibr" target="#b38">[42]</ref> to make sure the test covers both light and heavy queries.</p><p>Comparing targets. We compare our system against two state-of-the-art distributed RDF query systems, TriAD <ref type="bibr" target="#b19">[23]</ref> (RDF relational store) and Wukong <ref type="bibr" target="#b47">[51]</ref> (RDF graph stores). Note that TriAD does not sup- port concurrent query processing, so we only compare to it in the single query performance. As done in prior work <ref type="bibr" target="#b58">[62,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b47">51]</ref>, the string server is enabled for all systems to save memory usage, reduce network bandwidth, and boost string matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Single Query Performance</head><p>We first study the performance of Wukong+G for a single query using the LUBM dataset. <ref type="table" target="#tab_1">Table 3</ref> shows the optimal performance of different systems on a single server with LUBM-2560. For Wukong+G, there is no data swapping during single query experiment since the current memory footprint of all queries on LUBM-2560 (the numbers in brackets) is smaller than the GPU memory (12GB). The query-aware prefetching reduces the memory footprint to the per-query granularity (see <ref type="table">Table 1</ref>). Although Wukong and TriAD have enabled multithreading (10 worker threads), Wukong+G can still significantly outperform such pure CPU systems for heavy queries (Q1-Q3, Q7) by up to 8.3X and 21.9X (from 4.5X and 5.2X) due to wisely leveraging hardware advantages. The improvement of average (geometric mean) latency reaches 5.9X and 8.5X. For the light queries (Q4-Q6), Wukong+G inherits the prominent performance of Wukong by leveraging graph exploration and outperforms TriAD by up to 32.7X.</p><p>We further compare Wukong+G with Wukong and TriAD (multi-threading enabled) on 10 servers using LUBM-10240 in <ref type="table" target="#tab_2">Table 4</ref>. For heavy queries, Wukong+G still outperforms the average (geometric mean) latency of Wukong by 4.6X (ranging from 2.3X to 9.0X), thanks to the heterogeneous RDMA communication for preserving the good performance of GPU at scale. Further, using up all CPU worker threads to accelerate a single query is not practical for concurrent query processing since it will result in throughput collapse. For light queries, Wukong+G incurs about 12% performance overhead (geometric mean) compared to Wukong due to adjusting the layout of key/value store on CPU memory for predicate-based grouping. Wukong+G is still one order of magnitude faster than TriAD due to the in-place execution with one-sided RDMA READ [51].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Factor Analysis of Improvement</head><p>To study the impact of each technique and how they affect the query performance, we iteratively enable each optimization and collect the average latency by repeatedly running the same query on a single server with 3GB GPU memory for LUBM-2560. As shown in Table 5, even using query-aware prefetching (per-query), the memory footprints of query Q1, Q3 and Q7 still exceed available GPU memory (see <ref type="table" target="#tab_1">Table 3</ref>). Hence, they can not run until enabling pattern-aware prefetching (per-pattern). The effectiveness of fine-grained swapping (per-block) varies on different queries. It is quite effective on Q2 and Q3 (8.8X and 5.0X) since all triples required by triple patterns can almost be stored in 3GB GPU memory. Note that Q3 returns an empty history table (intermediate results) half-way and reduces the practical runtime memory footprint to 2.5GB. For Q1 and Q7, although the relative large memory footprint (3.6GB and 5.6GB), incurs massive data swapping (1.5GB by 187 time and 5.1GB by 734 times), the cache sharing with fine-grained mechanism can still notably reduce the query latency by 2.4X and 1.4X. Moreover, pipeline does not work on Q2 and Q3 without data prefetching time.</p><p>The improvement for Q1 and Q7 is still limited since the prefetching and execution time for each triple pattern are quite imbalanced. For example, 88% of blocks are swapped at two triple patterns for Q7.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Scalability</head><p>We evaluate the scalability of Wukong+G with the increase of servers. Since the latency of light queries of Wukong+G mainly inherits from Wukong, We only report the experimental results of heavy queries handled by GPUs. As shown in <ref type="figure" target="#fig_6">Fig. 9</ref>, the speedup of heavy queries ranges from 4.8X to 23.8X. As the number of servers increases from 2 to 10, a good horizontal scalability is shown. After a detailed analysis of the experimental results, we reveal that there are two different factors improving the performance at different stages. In the first stage (from 2 to 4 servers), the increase of total GPU memory provides the main contribution to the performance gains, ranging from 3.2X to 8.5X, by reducing memory swapping cost. In the second stage (from 4 to 10 servers), since Wukong+G stops launching expensive memory swapping operations when enough GPU memory is available, the main performance benefits come from using more GPUs, ranging from 1.5X to 2.8X.</p><p>Discussion. With the further increase of servers, the single query latency may not further decrease due to fewer workload per server and more communication cost. It implies that it is not worth making all resources (GPUs) participate in a single query processing, especially for a large-scale cluster (e.g., 100 servers). Therefore, Wukong+G will limit the participants of a single query and can still scale well in term of throughput by handling more concurrent queries simultaneously on different servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Performance of Hybrid Workloads</head><p>One principal aim of Wukong+G is to handle concurrent hybrid (light and heavy) queries in an efficient and scalable way. Prior work <ref type="bibr" target="#b47">[51]</ref> briefly studied the performance of Wukong with a mixed workload, which consists of 6 classes of light queries (Q4-Q6 and A1-A3 6 ). The light query in each class has a similar behavior except that the start point is randomly selected from the same type of vertices (e.g., Univ0, Univ1, etc.). The distribution of query classes follows the reciprocal of their average latency. Therefore, we first extend original mixed workload by adding 4 classes of heavy queries (Q1-Q3, Q7), and then allow all clients to freely send light and heavy queries 7 according to the distribution of query classes. We compare Wukong+G (WKG) with two different settings of Wukong: Default (WKD) and Isolation (WKI). Wukong/Default (WKD) allows all worker threads to handle hybrid queries, while Wukong/Isolation (WKI) reserves half of the worker threads to handle heavy queries. Each server runs two emulated clients on dedicated cores to send requests. Wukong launches 10 worker threads, while Wukong+G launches 9 worker threads and an agent thread. The multi-threading for heavy queries is configured to 5. We run the hybrid workload over LUBM-10240 on 10 servers for 300 seconds (10s warmup) and report the throughput and median (50 th percentile) latency for light and heavy queries separately over that period in <ref type="figure">Fig. 10</ref>.</p><p>For heavy queries, Wukong+G improves throughput and latency by over one order of magnitude compared to Wukong (WKD and WKI). The throughput of WKD is notably better (about 80%) than that of WKI, since it can use all worker threads to handle heavy queries. For light queries, Wukong+G performs up to 345K queries per second with median latency of 0.6ms by 9 worker threads. The latency can be halved with a small 10% impact in throughput. As expected, WKI can provide about half of the throughput (199K queries/s) with a similar latency since only half of the worker threads (5) are used to handle light queries. However, the throughput and latency of WKD for light queries are thoroughly impacted by the processing of heavy queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">RDF Cache on GPU</head><p>To study the influence of GPU cache for the performance of heavy queries on Wukong+G, we first evaluate the single query latency using LUBM-2560 on a single server with the GPU memory sizes varying from 3GB to 10GB. We repeatedly send one kind of heavy queries until the cache on GPU memory is warmed up, and illustrate the average latency of heavy queries in <ref type="figure">Fig. 11</ref>. Since the memory footprint of Q2 (2.4GB) is always smaller than the GPU memory, the latency is stable, and there is no data swapping. For Q3, although the memory footprint of the query is about 3.6GB, the latency is still stable since the history table becomes empty after the first two triple patterns due to contradictory conditions, where the rest predicate segment (about 1.1GB) will never be loaded. For Q1 and Q7, the latency decreases with the increase of GPU memory due to the decrease of data swapping size. However, the break point of Q7 is later than that of Q1 since it has a relatively larger memory footprint (5.6GB vs. 3.6GB).</p><p>To show the effectiveness of sharing GPU cache by multiple heavy queries, We further evaluate the performance of a mixture of four heavy queries using LUBM-2560 on a single server with 10GB GPU memory. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, the geometric mean of 50 th (median) and 99 th percentile latency is just 84.5 and 93.8 milliseconds respectively, under the peak throughput. Compared to the single query latency (see <ref type="table" target="#tab_1">Table 3</ref>), the performance degradation is just 3% and 14%, thanks to our fine-grained swapping and look-ahead replacing. During the experiment, the number and volume of blocks swapped in per second are about 96 and 750MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Other Workloads</head><p>We further compare the performance of Wukong+G with Wukong on two real-life datasets, DBPSB <ref type="bibr" target="#b0">[1]</ref> and YAGO2 <ref type="bibr" target="#b5">[9]</ref>. As shown in <ref type="table" target="#tab_5">Table 7</ref>, for light queries (D2, D3, Y1, and Y2), Wukong+G can provide a close performance to Wukong due to following the same execution mode and a similar in-memory store. For heavy queries (D1, D4, D5, Y3, and Y4), Wukong+G can notably outperform Wukong by up to 4.3X (from 1.9X).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Wukong+G is inspired by and departs from prior RDF query processing systems <ref type="bibr" target="#b36">[40,</ref><ref type="bibr" target="#b54">58,</ref><ref type="bibr" target="#b37">41,</ref><ref type="bibr" target="#b8">12,</ref><ref type="bibr" target="#b46">50,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b61">65,</ref><ref type="bibr" target="#b56">60,</ref><ref type="bibr" target="#b10">14,</ref><ref type="bibr" target="#b57">61,</ref><ref type="bibr" target="#b58">62,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b47">51,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b60">64]</ref>, but differs from them in exploiting a distributed heterogeneous CPU/GPU cluster to accelerate heterogeneous RDF queries.</p><p>Several prior systems <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b8">12]</ref> have leveraged columnoriented databases <ref type="bibr" target="#b49">[53]</ref> and vertical partitioning for RDF dataset, which group all triples with the same predicate into a single two-column table. The predicate-based grouping in Wukong+G is driven by a similar observation. However, Wukong+G still randomly (hash-based) assign keys within the segment to preserve fast RDMAbased graph exploration, which plays a vital role for running light queries efficiently on CPU.</p><p>Using prefetching and pipelining mechanisms are not new, which have been exploited in many graph-parallel systems <ref type="bibr" target="#b45">[49,</ref><ref type="bibr" target="#b31">35]</ref> and GPU-accelerated systems <ref type="bibr" target="#b33">[37]</ref> to hide the latency of memory accesses. Wukong+G employs a SPARQL-specific prefetching scheme and enables such techniques on multiple concurrent jobs (heavy queries) that share a single cache on the GPU memory. There has been a lot of work <ref type="bibr" target="#b21">[25,</ref><ref type="bibr" target="#b20">24,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr" target="#b22">26,</ref><ref type="bibr" target="#b25">29,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b51">55,</ref><ref type="bibr" target="#b42">46,</ref><ref type="bibr" target="#b24">28]</ref> focusing on exploiting the unique features of GPUs to accelerate database operations. Mega-KV <ref type="bibr" target="#b59">[63]</ref> is an in-memory key/value store that uses GPUs to accelerate index operations by only saving indexes on the GPU memory to ease device memory pressure. CoGaDB <ref type="bibr" target="#b11">[15,</ref><ref type="bibr" target="#b12">16]</ref> uses a column-oriented caching mechanism on GPU memory to accelerate OLAP workload. SABER <ref type="bibr" target="#b30">[34]</ref> is a hybrid high-performance relational stream processing engine for CPUs and GPUs. Wukong+G is inspired by prior work, while the differences in workloads result in different design choices. To our knowledge, none of the above systems exploit distributed heterogeneous (CPU/GPU) environment, let alone using RDMA as well as GPUDirect features.</p><p>To reduce communication overhead between multiple GPUs, NVIDIA continuously puts forward GPUDirect technology <ref type="bibr" target="#b2">[3]</ref>, including GPUDirect RDMA and GPUDirect Async (under development <ref type="bibr">[6]</ref>). They enable direct cross-device data transfer on data plane and control plane, respectively. Researchers have also investigated how to provide network <ref type="bibr" target="#b29">[33,</ref><ref type="bibr" target="#b17">21]</ref> and file system abstractions <ref type="bibr" target="#b48">[52]</ref> based on such hardware features. Our design currently focuses on using GPUs to deal with heavy queries for RDF graphs. The above efforts provide opportunities to build a more flexible and efficient RDF query system through better abstractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>The trend of hardware heterogeneity (CPU/GPU) opens new opportunities to rethink the design of query processing systems facing hybrid workloads. This paper describes Wukong+G, a graph-based distributed RDF query system that supports heterogeneous CPU/GPU processing for hybrid workloads with both light and heavy queries. We have shown that Wukong+G achieves low query latency and high overall throughput in the single query performance and hybrid workloads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The execution flow of query processing on CPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Motivation of Wukong+G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The execution flow of query processing on CPU/GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 6: The timeline of processing sample query (Q H ) on GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>that simply uses vertex ID (vid) as the key, and the in-/out-edge list (each element is a [pid, vid] pair) as the value, Wukong [51] uses a combination of the vertex ID (vid), predicate ID (pid) and in/out direction (d) as the key (in the form of [vid, pid, d]), and the list of neighbor- ing vertex IDs as the value (e.g., [Logan,to, out] −→ [DS] in the left part of Fig. 7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The structure of GPU-friendly RDF store. Fig. 8: Communication flow w/o and w/ GPUDirect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: The latency with the increase of servers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 : A collection of synthetic and real-life datasets. #T, #S, #O and #P mean the number of triples, subjects, objects and predicates respectively. ( †) The size of datasets in raw NT format.</head><label>2</label><figDesc></figDesc><table>Dataset 
#T 
#S 
#O 
#P 
Size † 
LUBM-2560 
352 M 
55 M 
41 M 
17 
58GB 
LUBM-10240 
1,410 M 
222 M 
165 M 
17 
230GB 
DBPSB 
15 M 
0.3 M 
5.2 M 
14,128 
2.8GB 
YAGO2 
190 M 
10.5 M 
54.0 M 
99 
13GB 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 3 : The query performance (msec) on a single server.</head><label>3</label><figDesc></figDesc><table>LUBM-2560 
TriAD 
Wukong 
Wukong+G 

H 

Q1 (3.6GB) 
851 
992 
165 
Q2 (2.4GB) 
211 
138 
31 
Q3 (3.6GB) 
424 
340 
63 
Q7 (5.6GB) 
2,194 
828 
100 
Geo. M 
639 
443 
75 

L 

Q4 
1.45 
0.13 
0.16 
Q5 
1.10 
0.09 
0.11 
Q6 
16.67 
0.49 
0.51 
Geo. M 
2.98 
0.18 
0.21 

GPU memory. For example, we witness this scenario 
in YAGO2 benchmark ( §6.8) that a heavy query keeps 
spanning out without any pruning. Wukong+G can hor-
izontally divide the intermediate results into multiple 
strips by row and only hold a single strip into the his-
tory table on GPU memory. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 : The query performance (msec) on 10 servers.</head><label>4</label><figDesc></figDesc><table>LUBM-10240 
TriAD 
Wukong 
Wukong+G 

H 

Q1 (14.25GB) 
3,400 
480 
211 
Q2 (9.74GB) 
880 
66 
12 
Q3 (14.25GB) 
2,835 
171 
19 
Q7 (22.58GB) 
10,806 
390 
100 
Geo. M 
3,094 
215 
47 

L 

Q4 
3.08 
0.44 
0.46 
Q5 
1.84 
0.13 
0.17 
Q6 
65.20 
0.70 
0.71 
Geo. M 
7.04 
0.34 
0.38 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 5 : The contribution of (cumulative) optimizations to the query latency (msec) evaluated on 3GB GPU memory.</head><label>5</label><figDesc></figDesc><table>LUBM-2560 
Per-query 
Per-parttern 
Per-block 
Pipeline 
Q1 (3.6GB) 
x 
743 
313 
295 
Q2 (2.4GB) 
284 
283 
32 
31 
Q3 (3.6GB) 
x 
309 
62 
63 
Q7 (5.6GB) 
x 
893 
622 
610 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>A comparison of query performance (msec) w/o and w/ 

GPUDirect RDMA (GDR) on 10 servers with LUBM-10240. 

LUBM-10240 
Q1 
Q2 
Q3 
Q7 
Wukong+G w/o GDR 
222 (53.4) 
13 
22 
103 (26.1) 
Wukong+G w/ GDR 
211 (40.1) 
13 
22 
98 (21.3) 

6.4 GPUDirect RDMA 

To shorten communication flow and avoid redundant 
memory copy for history table (intermediate results) 
of queries, Wukong+G leverages GPUDirect RDMA 
(GDR) to write history table directly from local GPU 
memory to remote GPU and CPU memory ( §4.3). To 
study the impact of leveraging GPUDirect RDMA, we 
enforce Wukong+G to purely use native RDMA for both 
query metadata and history table (i.e., Wukong+G w/o 
GDR). As shown in Table 6, the performance of Q2 and 
Q3 is non-sensitive to GPUDirect RDMA because of no 
data transfer among GPUs. For Q1, leveraging GPUDi-
rect RDMA can reduce about 30% communication cost 
(53.4ms vs. 40.1ms), since the query need to send about 
487MB intermediate results by about 990 times RDMA 
operations. For queries with relatively large intermediate 
results or many triple patterns, there are more rooms for 
the overall performance improvement. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 7 : The latency (msec) of queries on DBPSB and YAGO2</head><label>7</label><figDesc></figDesc><table>DBPSB 
D1 
D2 
D3 
D4 
D5 
Geo. M 
Wukong 
1.28 
0.15 
0.25 
4.25 
1.08 
0.74 
Wukong+G 
0.53 
0.16 
0.26 
0.99 
0.52 
0.41 

YAGO2 
Y1 
Y2 
Y3 
Y4 
Geo. M 
Wukong 
0.10 
0.13 
4685 
752 
14.6 
Wukong+G 
0.11 
0.15 
1856 
398 
10.5 

</table></figure>

			<note place="foot" n="1"> The source code and a brief instruction of Wukong+G are available at http://ipads.se.sjtu.edu.cn/projects/wukong.</note>

			<note place="foot" n="2"> Since the special predicate type (ty) is used to group a set of entities, we follow Wukong [51] to treat every type (e.g., professor (P)) as an index, the same as predicates (e.g., advisor (ad)).</note>

			<note place="foot" n="4"> The recent release of Wukong (https://github.com/ SJTU-IPADS/wukong) introduced a new cost-based query planner for graph exploration, where the cost estimation is roughly based on the number of paths may be explored. Wukong+G uses a user-defined threshold for the cost to classify queries.</note>

			<note place="foot" n="5"> The normal vertex (e.g., Logan) will be assigned to only one machine with all of its edges, while the index vertex (e.g., teacherOf) will be split and replicated to multiple machines with edges linked to normal vertices on the same machine.</note>

			<note place="foot" n="6"> Three additional queries (A1, A2, and A3) are from the official LUBM website (#1, #3, and #5).</note>

			<note place="foot" n="7"> In prior experiment [51], only up to one client is used to continually send heavy queries (i.e., Q1).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We sincerely thank our shepherd Howie Huang and the anonymous reviewers for their insightful suggestions. This work is supported in part by the National <ref type="bibr">Key Research &amp; Development Program (No. 2016YFB1000500)</ref>, the National Natural Science Foundation of <ref type="bibr">China (No. 61772335, 61572314, 61525204)</ref>, the National Youth Top-notch Talent Support Program of China, and Singapore NRF (CREATE E2S2).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sparql</forename><surname>Dbpedias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benchmark</surname></persName>
		</author>
		<ptr target="http://aksw.org/Projects/DBPSB" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Developing a Linux Kernel Module using GPUDirect RDMA</title>
		<ptr target="http://docs.nvidia.com/cuda/gpudirect-rdma/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Gpudirect</surname></persName>
		</author>
		<ptr target="https://developer.nvidia.com/gpudirect" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<ptr target="http://www" />
	</analytic>
	<monogr>
		<title level="j">Parallel Graph AnalytiX (PGX</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikidata</surname></persName>
		</author>
		<ptr target="https://www.wikidata.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="http://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago" />
		<title level="m">YAGO: A High-Quality Knowledge Base</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bio2RDF: Linked Data for the Life Science</title>
		<ptr target="http://bio2rdf.org/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Scalable semantic web data management using vertical partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hollenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Very Large Data Bases, VLDB&apos;07</title>
		<meeting>the 33rd International Conference on Very Large Data Bases, VLDB&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sw-store: a vertically partitioned dbms for semantic web data management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hollenbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="385" to="406" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matrix &quot;bit&quot; loaded: A scalable lightweight join query processor for rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chaoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Hendler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web, WWW&apos;10</title>
		<meeting>the 19th International Conference on World Wide Web, WWW&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Building an efficient rdf store over a relational database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bornea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dolby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kementsietsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dantressangle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Udrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;13</title>
		<meeting>the 2013 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The design and implementation of CoGaDB: A column-oriented gpu-accelerated dbms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breß</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DatenbankSpektrum</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="209" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Load-aware inter-coprocessor parallelism in database query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Breß</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Siegmund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bellatreche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Saake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data &amp; Knowledge Engineering</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facebooks distributed data store for the social graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bronson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Amsden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ferris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giardullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 USENIX Annual Technical Conference, USENIX ATC&apos;13</title>
		<meeting>the 2013 USENIX Annual Technical Conference, USENIX ATC&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="49" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tiled-mapreduce: Optimizing resource usages of data-parallel applications on multicore with tiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques, PACT&apos;10</title>
		<meeting>the 19th International Conference on Parallel Architectures and Compilation Techniques, PACT&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="523" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Powerlyra: Differentiated graph computation and partitioning on skewed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth European Conference on Computer Systems, EuroSys&apos;15</title>
		<meeting>the Tenth European Conference on Computer Systems, EuroSys&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast and general distributed transactions using rdma and htm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Conference on Computer Systems, EuroSys&apos;16</title>
		<meeting>the Eleventh European Conference on Computer Systems, EuroSys&apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GPUrdma: Gpuside library for high performance networking from gpu kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Daoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Watad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Workshop on Runtime and Operating Systems for Supercomputers, ROSS&apos;16</title>
		<meeting>the 6th International Workshop on Runtime and Operating Systems for Supercomputers, ROSS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<ptr target="https://googleblog.blogspot.co.uk/2012/05/introducing-knowledge-graph-things-not.html" />
		<title level="m">Google Inc. Introducing the knowledge graph: things, not strings</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Triad: A distributed shared-nothing rdf engine based on asynchronous message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurajada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seufert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Miliaraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Theobald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;14</title>
		<meeting>the 2014 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="289" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relational query coprocessing on graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Sander</surname></persName>
		</author>
		<idno>21:1-21:39</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relational joins on graphics processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="511" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Revisiting co-processing for hash joins on the coupled cpu-gpu architecture. Proceedings of the VLDB Endowment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="889" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">In-cache query co-processing on coupled cpu-gpu architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014-12" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="329" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Self-tuning, gpuaccelerated kernel density models for multidimensional selectivity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;15</title>
		<meeting>the 2015 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1477" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hardware-oblivious parallelism for in-memory column-stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manegold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Markl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="709" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Yago2: Exploring and querying world knowledge in time, space, context, and many languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lewiskelham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference Companion on World Wide Web, WWW&apos;11</title>
		<meeting>the 20th International Conference Companion on World Wide Web, WWW&apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="229" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Design guidelines for high performance rdma systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC&apos;16</title>
		<meeting>the 2016 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zipg: A memory-efficient graph store for interactive queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data, SIGMOD&apos;17</title>
		<meeting>the 2017 ACM International Conference on Management of Data, SIGMOD&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1149" to="1164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GPUnet: Networking abstractions for gpu programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wated</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="6" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Saber: Window-based hybrid stream processing for heterogeneous architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Koliousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Castro Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pietzuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Management of Data, SIGMOD&apos;16</title>
		<meeting>the 2016 International Conference on Management of Data, SIGMOD&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="555" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">G-store: high-performance graph store for trillion-edge processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference forHigh Performance Computing, Networking, Storage and Analysis, SC&apos;16</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="830" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scaling queries over big rdf graphs with semantic hash partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1894" to="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Garaph: efficient gpu-accelerated graph processing on a single machine with balanced replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference, USENIX ATC&apos;17</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title/>
		<ptr target="https://pubchem.ncbi.nlm.nih.gov/rdf/" />
	</analytic>
	<monogr>
		<title level="j">National Center for Biotechnology Information. PubChemRDF</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Monetdb: Two decades of research in column-oriented database architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I F G N</forename><surname>Nes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M S M M</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Engineering</title>
		<imprint>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RDF-3X: A risc-style engine for rdf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="647" to="659" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Scalable join processing on very large rdf graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;09</title>
		<meeting>the 2009 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="627" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The RDF-3X engine for scalable management of rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="113" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">H2rdf+: High-performance distributed joins over large-scale rdf graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papailiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsoumakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koziris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Big Data, IEEE BigData&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="255" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">H2rdf: Adaptive query processing on rdf data in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papailiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsoumakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koziris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web, WWW&apos;12 Companion</title>
		<meeting>the 21st International Conference on World Wide Web, WWW&apos;12 Companion</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="397" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gpu</forename><surname>Blazegraph</surname></persName>
		</author>
		<ptr target="https://www.blazegraph.com/whitepapers/Blazegraph-gpu_InDetail_BloorResearch.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Waste not... efficient co-processing of relational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manegold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE 30th International Conference on Data Engineering, ICDE&apos;14</title>
		<meeting>the 2014 IEEE 30th International Conference on Data Engineering, ICDE&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="508" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluating mapreduce for multicore and multiprocessor systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA&apos;07</title>
		<meeting>the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-performance, massively scalable distributed systems using the mapreduce software framework: The shard triple-store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rohloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Support Innovations for Emerging Distributed Applications, PSI EtA&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">X-stream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP&apos;13</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Al-Naymat</surname></persName>
		</author>
		<title level="m">Relational processing of rdf queries: A survey. SIGMOD Record</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="23" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast and concurrent rdf queries with rdma-based distributed graph exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gpufs: Integrating a file system with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Keidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Witchel</surname></persName>
		</author>
		<idno>1:1-1:31</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">C-store: A column-oriented dbms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Very Large Data Bases, VLDB &apos;05</title>
		<meeting>the 31st International Conference on Very Large Data Bases, VLDB &apos;05</meeting>
		<imprint>
			<publisher>VLDB Endowment</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Speedy transactions in multicore in-memory databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP&apos;13</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles, SOSP&apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Concurrent analytical query processing with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1011" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Using restricted transactional memory to build a scalable in-memory database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems, EuroSys&apos;14</title>
		<meeting>the Ninth European Conference on Computer Systems, EuroSys&apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast inmemory transaction processing using rdma and htm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles, SOSP &apos;15</title>
		<meeting>the 25th Symposium on Operating Systems Principles, SOSP &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="87" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Hexastore: Sextuple indexing for semantic web data management. Proceedings of the VLDB Endowment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1008" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probase: A probabilistic taxonomy for text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;12</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Towards effective partition management for large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;12</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="517" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Triplebit: A fast and compact system for large scale rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="517" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A distributed graph engine for web scale rdf data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th international conference on Very Large Data Bases, PVLDB&apos;13</title>
		<meeting>the 39th international conference on Very Large Data Bases, PVLDB&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="265" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Mega-kv: A case for gpus to maximize the throughput of in-memory key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1226" to="1237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sub-millisecond stateful stream querying over fast-evolving linked data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles, SOSP&apos;17</title>
		<meeting>the 26th Symposium on Operating Systems Principles, SOSP&apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">gStore: Answering sparql queries via subgraph matching. Proceedings of the VLDB Endowment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ozsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-05" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="482" to="493" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
