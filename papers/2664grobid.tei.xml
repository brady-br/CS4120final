<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GreenMap: MapReduce with Ultra High Efficiency Power Delivery</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GreenMap: MapReduce with Ultra High Efficiency Power Delivery</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Energy consumption has become a significant fraction of the total cost of ownership of data centers. While much work has focused on improving power efficiency per unit of computation, little attention has been paid to power delivery, which currently wastes 10-20% of total energy consumption even before any computation takes place. A new power delivery architecture using series-stacked servers has recently been proposed in the power community. However, the reduction in power loss depends on the difference in power consumption of the series-stacked servers: The more balanced the computation loads, the more reduction in power conversion loss. In this preliminary work, we implemented GreenMap, a modified MapReduce framework that assigns tasks in synchronization, and computed the conversion loss based on the measured current profile. At all loads, GreenMap achieves 81x-138x reduction in power conversion loss from the commercial-grade high voltage converter used by data centers, which is equivalent to 15% reduction in total energy consumption. The average response time of GreenMap suffers no degradation when load reaches 0.6 and above, but at loads below 0.6, the response time suffers a 26-42% increase due to task synchronization. For the low-load region, we describe the use of GreenMap with dynamic scaling to achieve a favorable tradeoff between response time and power efficiency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As our reliance on online services continues to grow, so have the sizes of data centers hosting these services. As a result, energy consumption has become a significant fraction of the total cost of ownership (TCO). Electricity bills for large data centers are close to one million dollars per month in 2009 <ref type="bibr" target="#b7">[9]</ref>, and the data center energy usage in 2013 is estimated to be 91 billion kwh <ref type="bibr" target="#b13">[16]</ref>. Consequently, data centers today contribute 2-3% of the global carbon emissions <ref type="bibr">[10]</ref>, and the design of environmentally friendly green data centers is an important societal need <ref type="bibr">[1]</ref>.</p><p>Much work on greening data centers has focused on improving computational power efficiency, where the designers strive to minimize the energy required for each unit of computation <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b8">11]</ref>. For instance, energy consumption is reduced by consolidating demand onto a small number of servers <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b10">13]</ref> via request redirection or virtual machine migration, or by speed gating each server to optimize individual power usage <ref type="bibr" target="#b2">[4]</ref>.</p><p>On the other hand, today's data centers still use a power delivery architecture that is based on the design developed for single server applications. This conventional power delivery technique requires a very large step-down from the grid AC voltage, typically 600 or 480V AC <ref type="bibr" target="#b12">[15]</ref>, to the final CPU load of 12V DC. With today's power delivery architectures, the high voltage conversion efficiency is limited to 80 − 90% <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b12">15]</ref>. That is, 10 − 20% of total energy consumption is wasted before any computation takes place.</p><p>Recently, a new power delivery architecture has been proposed in the power community <ref type="bibr" target="#b3">[5]</ref>. Servers are connected in series to avoid the large step-down from the grid AC voltage, and differential power converters are used to regulate the voltage across each server. However, the differential converters incur a power conversion loss when the computational loads are unbalanced. The amount of loss is proportional to the difference in server computational loads. It was demonstrated in <ref type="bibr" target="#b3">[5]</ref> that with all servers running the Linux "stress" utility, hence an almost perfectly balanced load, 99.89% power efficiency is achieved. No realistic data center traffic has been demonstrated with the new power delivery, and data center traffic is expected to display much more variation than the Linux "stress" utility.</p><p>In this paper, we explore the feasibility of the new power delivery architecture for data centers. We measured the current profile of MapReduce traffic, and observed the tremendous imbalance of computational loads across servers. The imbalance is mainly due to the different levels of resource occupancy, and tasks at different stages consuming different amount of power.</p><p>We implemented GreenMap, a modified MapReduce framework that assigns tasks in synchronization. The preliminary work only includes results on synchronizing map tasks. The conversion losses are computed based on the measured current profile of each server.</p><p>We evaluated GreenMap with the SWIM benchmark <ref type="bibr" target="#b4">[6]</ref>, and found that at all loads, GreenMap achieves 81x-138x reduction in conversion loss from the commercial-grade high voltage converter used by data centers, which is equivalent to 15% reduction in total energy consumption. The amount of reduction from the best available high voltage converter is 27x-46x, but the best available converters are much more costly.</p><p>As GreenMap delays tasks until they can be assigned in synchronization, the average response time below 0.6 load increases by 26 − 42%. However, as load reaches 0.6 and above, no degradation in response time is observed. For the low-load region, we also describe the use of GreenMap together with dynamic scaling of data center clusters so that the load is kept around 0.6. This offers a favorable tradeoff between response time and power efficiency, while at the same time saving the energy consumption and conversion loss of idle servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In data centers, the utility power has to go through several power conversion and storage elements before it reaches the servers. In a conventional power delivery architecture, the grid voltage of 600V or 480V AC is stepped down to 208V or 120V AC for distribution to racks, followed by a further conversion to DC. A DC-DC converter is installed on each server to process the full server power and to convert the high rectified voltage, typically at 208V or 120V, to a lower voltage for servers, typically 12V DC.</p><p>The large voltage step down and the need to process the full server power result in limited system-level efficiency and large converter size. The typical efficiency of a high-voltage converter used in data centers is 80−90%, so the conversion loss will be 10 − 20% of the total energy consumption. The peak efficiency of the best available high-voltage converters is 95%, but they are much more costly and even larger in size <ref type="bibr" target="#b3">[5]</ref>.</p><p>Let L conv denote the conversion loss in conventional converters, P the total power consumption, E the converter efficiency, V the server voltage, I i the current in server i, and n the total number of servers, we have</p><formula xml:id="formula_0">L conv = (1 − E)P = (1 − E)V n ∑ i=1 I i<label>(1)</label></formula><p>where V = 12V, E = { 0.8 − 0.9 for converters in data centers, 0.95 for best available converters.</p><p>Recently, a new power delivery architecture has been proposed in <ref type="bibr" target="#b3">[5]</ref>. Instead of employing a high-voltage step-down for each server, a set of n servers are connected in series to equally share the rectified grid voltage. For a suitable choice of n, the series-stacked architecture provides an inherent step-down, where each server's input voltage is 1/n fraction of the grid voltage.</p><p>However, as the series-connected servers conduct the same current, this leads to a variation in server voltage even if there is only a small mismatch in power consumption between servers. Regulated voltage of all servers in the series stack can be achieved through the use of differential power converters, one for each of the servers. Instead of processing the full server power, the differential power converters only process the difference between the power of each server and the average power in the series stack. As a result, the efficiency of the power converter can be made as high as the best available converter, that is, 95%, at a reasonable cost, and the differential power converters are of a much smaller size.</p><p>Let L diff denote the conversion loss in differential converters. The server voltage V can be considered constant at 12V due to voltage regulation, we have</p><formula xml:id="formula_1">L diff = 1.5(1 − E)V n ∑ i=1 (I i − I avg )<label>(2)</label></formula><p>where V = 12V, E = 0.95,</p><formula xml:id="formula_2">I avg = 1 n n ∑ i=1 I i .</formula><p>The extra factor of 1.5 is due to the specific topology of the server-to-virtual-bus differential power converter <ref type="bibr" target="#b3">[5]</ref>, where the secondary side of the differential converter is connected to a virtual bus. As shown by the term (I i − I avg ), the more balanced the computational loads are (hence the server currents), the smaller the power conversion loss will be.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Load Balancing</head><p>The series stack can be integrated into data center racks as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. As servers are added to data center in racks, a rack can consist of more than one series stacks. This facilitates the installation of a series-connected stack and provides proper ground isolation <ref type="bibr" target="#b3">[5]</ref>. The server hosting the resource manager (RM) is not in a series stack, as its computational load is very different from the other servers.</p><p>The number of servers in a series-stack is upper bounded by the ratio of rectified grid voltage to the server voltage. Apart from the rectified grid voltage of 600V or 480V, we can also use intermediate DC voltages utilized in many data center implementations. For instance, 48V is a standard telecom supply voltage. In this paper, we will compute power conversion loss based on a series stack of 4 servers, as this is the experimental setup in <ref type="bibr" target="#b3">[5]</ref>. This corresponds to a voltage of 48V across the series-stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Current Profiling</head><p>We start by profiling the power consumption of a wordcount job containing one map task and one reduce task on a server with conventional power supply. The server voltage is fixed to 12V. We measure the current consumption of the server using a Yokogawa wt310 digital power meter. <ref type="figure" target="#fig_1">Figure 2</ref> shows the current consumption at different stages of a word-count job. The idle current is around 2.8A. The setup task initializes the job and creates temporary output directories, consuming close-to-peak current at 5.5A for 2.5s. The server goes idle for another 2.5s before launching the map task. The beginning of the map task consumes close-to-peak current as a new thread is initialized and data are read into memory. However, the bulk of the map task experiences an oscillation of current around 4.8A, as it generates &lt;key,value&gt; pairs and outputs them to the intermediate directory. The alternating computationintensive and I/O-intensive operations cause the current to oscillate. The beginning of the reduce task also consumes close-to-peak current as a new thread is initialized, followed by 4 seconds of low current at 2.8A, as &lt;key,value&gt; pairs are copied from intermediate directories on other servers. The later stage of the reduce task is characterized by large oscillations between 2.8A and 5.5A as the high-current computation-intensive operations intersperse among the low-current I/O operations. The cleanup task after the job's completion causes another short period of close-to-peak current consumption.</p><p>In general, a MapReduce job always has a setup task and a cleanup task. It can have multiple map tasks and reduce tasks, whose current consumption can vary depending on user-defined functions, although map tasks (or reduce tasks) of the same job will still have similar current profiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synchronized Task Assignment</head><p>We built GreenMap to balance the computational loads in a series-stack by synchronizing map task assignment. There are three main modifications to the default MapReduce scheduler.</p><p>First, the setup and cleanup tasks are moved to the server where the RM resides. As each setup task (and cleanup task) is executed only once per job, and it consumes close-to-peak power, it is inherently unsuitable for parallelization and balancing across a series-stack of servers. Although we co-locate setup and cleanup tasks with the RM in this experiment, in a more scalable implementation, they can be assigned to any server outside series-stacks. A data center can consist of series-stacks on which parallelized tasks run, and conventional servers for tasks unsuitable for parallelization.</p><p>Second, we minimize load imbalance by assigning the same number of map tasks to each server, and whenever possible, assigning map tasks of the same job in synchronization. This is achieved by delaying task assignment until the number of outstanding tasks is at least that of the servers with idle slots. In particular, when there exist outstanding jobs, a server with an idle slot will be assigned a task immediately, in accordance with the assignment by the default FIFO scheduler based on data locality. When there exist no outstanding jobs, the number of servers with idle slots increases over time. At a new job arrival, if the number of outstanding tasks exceeds that of idle servers, a batch of tasks are assigned in synchronization. If the tasks are insufficient to fill all idle servers, they are delayed till further jobs arrive.</p><p>Third, to prevent the system from delaying tasks for too long, a timer is set to zero whenever tasks are assigned in synchronization or a new job has arrived. In the absence of neither, when the timer reaches a threshold value, all outstanding tasks are assigned. The threshold is set to be the time period during which a new job will arrive with 90% probability at the current load. The exact value of the threshold is not important and has not been optimized for this experiment. A larger value for the threshold will further reduce power conversion loss and increase response time, while a smaller value will increase power loss and reduce response time. To demonstrate the effect of task synchronization, we ran a small trace on four servers with conventional power delivery, and compared the measured current profiles. The trace consists of two jobs of 1 map task, one job of 2 map tasks and one job of 8 map tasks, arriving at random intervals. <ref type="figure">Figure 3(a)</ref> shows the measured current profiles of the four servers respectively, with no task synchronization. Not surprisingly, we observe large difference in currents consumed at each server. <ref type="figure">Figure 3(b)</ref> shows the current profiles of the same four servers with synchronized task assignment. We observe that the map tasks are indeed synchronized, and the difference in currents consumed at different servers becomes small and only occurs at sporadic moments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Our test bed includes five Dell Optiplex SX775 Core 2 Duo workstations. One server hosts the Resource Manager (RM) and is not in a series-stack. The remaining four servers simulate a series-stack of 48V.</p><p>GreenMap is implemented in Hadoop1 for this preliminary work as the centralized design of Hadoop1 scheduler is amenable to task synchronization. Each server has 2 map slots. We do not consider reduce tasks in this experiment.</p><p>We generate traces by selecting jobs from the SWIM benchmark <ref type="bibr" target="#b4">[6]</ref> so that we achieve a good representation of the Pareto job size distribution <ref type="bibr" target="#b0">[2]</ref>, and the length of the trace and the number of files are appropriately scaled for the capacity of one series-stack. Job arrivals are generated as a Poisson process, and each job does not contain any reduce tasks. The data block size is set to 32 MB, and each map task takes an average of 70 seconds. Hence for each load point, the trace takes 1.5 − 6 hours on our cluster. After scaling, our trace contains 447 tasks and 50 jobs.  We connect the 4 servers with a conventional power delivery architecture, and measure the current consumption of each server using a Yokogawa wt310 digital power meter with 10 samples per second per server. We compute the power conversion loss using equations <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_1">(2)</ref>. The advantage of this setup is that it allows us to compare the conventional conversion loss and the differential conversion loss in the exact same setting, with the same run of a trace. Note that the conventional conversion loss depends on the sum of the current, whereas the differential conversion loss depends on the deviation of each current from the average current in the stack.  <ref type="figure" target="#fig_3">Figure 4</ref> shows that at all loads, GreenMap achieves 81x-138x reduction in conversion loss from the conventional power delivery with a commercial-grade high voltage converter of 85% efficiency, which is typical of converters used in data centers today. The power conversion loss is reduced by two orders of magnitude, from an average of 31.4W to 0.3W. This is equivalent to 14.999% reduction in total energy consumption, almost eliminating the 15% conversion loss altogether.   <ref type="figure" target="#fig_4">Figure 5</ref> shows the average job response time of the default Hadoop FIFO scheduler versus that of GreenMap. As GreenMap delays task assignment until tasks can be assigned in synchronization, the average response time below 0.6 load increases by 26 − 42%. However, when the load reaches 0.6 and above, no degradation in response time is observed. This is because there are an abundance of outstanding tasks at high loads, and tasks are seldom delayed, whereas the sparse arrivals of tasks at low loads result in more tasks being delayed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GreenMap with Dynamic Scaling</head><p>The above results show that GreenMap suffers a degradation in response time when the load is below 0.6. In fact, GreenMap delays tasks in order to emulate a higher load, at which there are an abundance of outstanding tasks, hence facilitating assignment in synchronization. We observe that higher loads can be more efficiently achieved by turning off a fraction of stacks in a large cluster with multiple series-stacks.</p><p>For instance, assume that we have 10 series-stacks running at 0.4 load. From <ref type="figure" target="#fig_3">Figure 4</ref>, the total power consumption in each series-stack of 4 servers is 192.2W (= V ∑ 4 i=1 I i ) at 0.4 load, and the conversion loss in each series-stack is 0.29W with GreenMap. With dynamic scaling, we can turn off 3 series-stacks, resulting in 0.57 load for each of the remaining series-stack. The corresponding power consumption in each series-stack is now 215.4W, and the conversion loss is 0.33W with GreenMap. Hence, with GreenMap but not dynamic scaling, total power = (192.2 + 0.29) × 10 = 1924.9W, whereas with GreenMap and dynamic scaling, total power = (215.4 + 0.33) × 6 = 1294.4W, which is a 32.8% reduction. The reduction in total energy consumption is similar as the servers are mostly idle at 0.4 and 0.57 load, and the trace takes a similar amount of time to finish. The average job response time with GreenMap will increase by only 15% as the load increases from 0.4 to 0.57, yielding a favorable tradeoff between power efficiency and response time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>A new DC-DC power delivery architecture was proposed in <ref type="bibr" target="#b12">[15]</ref>. Instead of the conventional step-down of 600V or 480V AC to 208V or 120V AC for distribution to racks, followed by a further conversion to DC for energy storage, the DC-DC architecture uses a single rectification stage. It boosts the efficiency of the best-in-class AC-DC power supply from 90% to 92%. However, the power conversion loss is still directly proportional to the total energy consumption in the entire data center, while GreenMap's conversion loss only depends on the difference in power consumption, which makes it possible to achieve an ultra-high efficiency of 99%.</p><p>Another power delivery architecture also using differential power converters is proposed in <ref type="bibr" target="#b11">[14]</ref>. Instead of connecting each server in a series-stack to a virtual bus, this architecture connects neighboring pairs of servers in the series-stack. While this architecture achieves comparable conversion efficiency as the architecture proposed in <ref type="bibr" target="#b3">[5]</ref>, the conversion loss depends on the difference in power consumption of neighboring servers, instead of the difference between each server and the average. This makes the load balancing problem much more difficult as finding the optimal assignment becomes a combinatorial problem involving the location of a server in the series-stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We explored the feasibility of series-connected stacks in data centers by implementing GreenMap, a modified MapReduce framework that assigns tasks in synchronization. We found that with task synchronization, the conversion loss in data centers can potentially be reduced by two orders of magnitude, which is equivalent to about 15% of total energy consumption. Future work includes implementing GreenMap with multiple series-stacks and heterogeneous jobs, and evaluating the system on actual series-connected stacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Topics</head><p>The most important feedback we seek to receive from this community is the relative priority of energy saving, performance and ease of implementation of future scheduling software.</p><p>For instance, how much performance are we willing to sacrifice for 15% reduction of total energy consumption? The sacrifice is in relative terms rather than in absolute terms: The same logic applies to utilization. While appropriate packing algorithms can increase utilization without degrading performance, beyond a certain threshold, utilization of a cluster is tied with response times due to the stochastic nature of job arrivals. Do we want to run an almost empty cluster at 0.2 load and achieve the best possible response time? Or do we rather run the cluster at 0.7 load with a 10% increase in response time? What if we have a clever scheduling algorithm that will reduce the response time at all loads? With a clever algorithm, the absolute sacrifice might disappear. However, the relative sacrifice always exists because the performance of the clever algorithm will always be better at 0.2 load than at 0.7 load.</p><p>With this in mind, it might be even more important to consider whether the 15% energy saving is worth the additional constraint imposed on the design of scheduling algorithms. GreenMap will require computational loads to be balanced in each series-stack in order to minimize power conversion loss. For instance, it might not be easy to implement the YARN architecture on series-stacks, as map and reduce tasks co-locate with application managers (AM), whose loads might be very different from one another.</p><p>Another related question will be how many servers a series-stack should contain, as it determines the granularity at which batches of tasks are assigned. In an environment with heterogeneous applications, a small granularity will lessen the constraint on scheduling algorithms, while potentially introducing a higher power loss. A larger granularity, on the other hand, will make it more difficult to balance computational loads on all servers in the series-stack.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data center with series-connected stacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Current consumption of a word-count job with one map task and one reduce task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>Figure 3: Current profiles of four servers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: GreenMap reduces power conversion loss from the conventional architecture by two orders of magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: GreenMap achieves comparable response time as Hadoop FIFO scheduler at load 0.6 and above, while increasing response time at lower loads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 also</head><label>4</label><figDesc>Figure 4 also shows the conversion loss of the conventional power delivery with the best available high-voltage converter of 95% efficiency. GreenMap achieves 27x-46x reduction in power conversion loss, from an average of 10.45W to 0.3W. Figure 5 shows the average job response time of the default Hadoop FIFO scheduler versus that of GreenMap. As GreenMap delays task assignment until tasks can be assigned in synchronization, the average response time below 0.6 load increases by 26 − 42%. However, when the load reaches 0.6 and above, no degradation in response time is observed. This is because there are an abundance of outstanding tasks at high loads, and tasks are seldom delayed, whereas the sparse arrivals of tasks at low loads result in more tasks being delayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 shows the job size distribution.</head><label>1</label><figDesc></figDesc><table>Bins 
1 
2 3 4 
5 
6 
Job count 
25 9 6 4 
3 
3 
Map count per job 
1 
2 4 8 16 100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : Job size distribution.</head><label>1</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank Enver Candan for his assistance with setting up the Yokogawa wt310 digital power meter. Yi Lu is supported by NSF grant CNS-1150080.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A storage-centric analysis of mapreduce workloads: File popularity, temporal locality and arrival patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camp-Bell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluating 400v direct-current for data centers -a case study comparing 400 vdc with 480-208 vac power distribution for energy efficiency and other benefits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aldridge</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dupy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint/>
	</monogr>
	<note>Intel Labs</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimality, fairness, and robustness in speed scaling designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wierman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Sigmetrics</title>
		<meeting>of Sigmetrics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A series-stacked power delivery architecture with isolated differential power conversion for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Candan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilawa-Podgurski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Telecommunications Energy Conference (INTELEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The case for evaluating MapReduce performance using workload suites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Griffith</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Symp. Modeling, Analysis &amp; Simulation of Comput. Telecomm. Syst. (MASCOTS)</title>
		<meeting>IEEE Int&apos;l Symp. Modeling, Analysis &amp; Simulation of Comput. Telecomm. Syst. (MASCOTS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Power provisioning for a warehouse-sized computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barroso</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed, robust autoscaling policies for power management in compute intensive server farms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gandhi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kozuch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Open Cirrus Summit</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cost of a cloud: Research problems in data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greenberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigcomm Computer Communication Review</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoelzle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barroso</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan and Claypool Publishers</publisher>
		</imprint>
	</monogr>
	<note>1st ed</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Napsac:design and implementation of a power-proportional web cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krioukov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Keys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Sigcomm workshop on Green Networking</title>
		<meeting>of Sigcomm workshop on Green Networking</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic right-sizing for power-proportional data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thereska</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A series-stacked architecture for high-efficiency data center power delivery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcclurg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pilawa-Podgurski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenoy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Energy Conversion Congress and Exposition (ECCE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="170" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dc power for improved data center efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortenbery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tschudi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Lawrence Berkeley National Laboratory</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data center efficiency assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitney</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delforge</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
<note type="report_type">Tech. rep.</note>
	<note>Natural Resources Defense Council</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
