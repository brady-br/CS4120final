<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lube: Mitigating Bottlenecks in Wide Area Data Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto</orgName>
								<orgName type="institution" key="instit2">University of Toronto</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lube: Mitigating Bottlenecks in Wide Area Data Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Over the past decade, we have witnessed exponential growth in the density (petabyte-level) and breadth (across geo-distributed datacenters) of data distribution. It becomes increasingly challenging but imperative to minimize the response times of data analytic queries over multiple geo-distributed datacenters. However, existing scheduling-based solutions have largely been motivated by pre-established mantras (e.g., bandwidth scarcity). Without data-driven insights into performance bottlenecks at runtime, schedulers might blindly assign tasks to workers that are suffering from unidentified bottlenecks. In this paper, we present Lube, a system framework that minimizes query response times by detecting and mitigating bottlenecks at runtime. Lube monitors geo-distributed data analytic queries in real-time, detects potential bottlenecks, and mitigates them with a bottleneck-aware scheduling policy. Our preliminary experiments on a real-world prototype across Amazon EC2 regions have shown that Lube can detect bottlenecks with over 90% accuracy, and reduce the median query response time by up to 33% compared to Spark&apos;s built-in locality-based scheduler.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With large volumes of data generated and stored at geographically distributed datacenters around the world, it has become increasingly common for large-scale data analytics frameworks, such as Apache Spark <ref type="bibr" target="#b29">[30]</ref> and Hadoop <ref type="bibr" target="#b9">[10]</ref> to span across multiple datacenters. Petabytes of data -including user activities, trending topics, service logs and performance traces -are produced on these geographically distributed datacenters every day, processed by tens of thousands data analytic queries.</p><p>Minimizing response times of geo-distributed data analytic queries is crucial, but far from trivial. Results of these analytics queries are typically used when making real-time decisions and online predictions, all of which depend upon the timeliness of data analytics. However, in contrast to data analytics in a single datacenter, the varying bandwidth on wide-area network (WAN) links and the heterogeneity of the runtime environment across geographically distributed datacenters impose new and unique challenges as query response times are minimized.</p><p>Known as wide-area data analytics in the literature, tasks (or data) are optimally placed across datacenters in order to improve data locality <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. However, all previous works made the simplifying assumption that the runtime environment of wide-area data analytics is temporally stable, and that there are no runtime performance variations in these clusters. Naturally, this may not accurately reflect the reality. In addition, existing works have largely been motivated by a few widely accepted mantras, such as the scarcity of network bandwidth on access links from a datacenter to the Internet. With an extensive measurement study on analytic jobs, Ousterhout et al. <ref type="bibr" target="#b17">[18]</ref> have convincingly pointed out that some of the widely held assumptions in the literature may not be valid in the context of a single cluster.</p><p>Delving into the fluctuating runtime environment of wide-area data analytics, this paper makes a strong case for analyzing and detecting performance bottlenecks in data analytics frameworks at runtime. Shifting gears from a single cluster to the context of wide-area data analytics, we believe that the conclusion from <ref type="bibr" target="#b17">[18]</ref> still holds: it may not always be the same resource -such as bandwidth -that causes runtime performance bottlenecks in wide-area data analytic queries. To generalize a step further, the types of resource that cause performance bottlenecks may even vary over time at runtime, as analytic queries are executed across datacenters. It becomes intuitive that, if we wish to reduce the query response times in wide-area data analytics, these performance bottlenecks need to be detected at runtime, and a new resource scheduling mechanism needs to be designed to mitigate them. Unfortunately, such a high-level intuition has not yet been well explored in the literature and remains a largely uncharted territory.</p><p>In this paper, we propose Lube, a new system that is designed to perform data-driven runtime performance analysis for minimizing query response times. Lube features a closed-loop design: the results of runtime monitoring are used for detecting bottlenecks, and these bottlenecks serve as input to the resource scheduling policy to mitigate them, again at runtime. Our original contributions in this paper are the following:</p><p>First, we propose effective and efficient techniques to detect resource bottlenecks at runtime. We investigate two bottleneck detection techniques, both driven by performance metrics collected in real-time. We start with a simple statistical technique, Autoregressive Integrated Moving Average (ARIMA) <ref type="bibr" target="#b5">[6]</ref>, and then propose machine learning techniques to further explore the implicit correlation between multiple performance metrics. <ref type="bibr" target="#b0">1</ref> As one of the effective algorithms and a case study, we use the Sliding Hidden Markov Model (SlidHMM) <ref type="bibr" target="#b6">[7]</ref>, an unsupervised algorithm that takes time series as input and incrementally updates model parameters for detecting upcoming states.</p><p>Second, we propose a new scheduling policy that, when assigning tasks to worker nodes, mitigates bottlenecks by considering not only data locality (e.g., <ref type="bibr" target="#b25">[26]</ref>), but also the severity of bottlenecks. The upshot of our new scheduling policy is the use of a technique similar to late binding in Sparrow <ref type="bibr" target="#b18">[19]</ref>, that holds a task for a short while before binding it to a worker node. This is designed to avoid the negative implications of false positives when detecting bottlenecks.</p><p>We have implemented a prototype of Lube on a Spark SQL cluster over Amazon EC2 with 37 instances across nine regions. Our experiments of the Big Data Benchmark <ref type="bibr" target="#b22">[23]</ref> with a 1.1 TB dataset show that Lube is able to detect bottlenecks with an accuracy over 90% and reduces the median query response time by as much as 33% (1.5× faster).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lube: a Bird's-Eye View</head><p>Data analytics over geo-distributed datacenters may suffer from a highly volatile runtime environment, due to the lack of load distribution when using resources, or varying bandwidth availability over wide-area network links <ref type="bibr" target="#b0">[1]</ref>. As a result, resource bottlenecks are more likely to occur at runtime, when data analytic queries are executed over the wide area.</p><p>As a motivating example of such runtime bottlenecks, <ref type="figure" target="#fig_0">Figure 1</ref> presents a heat map of real-time memory utilization on the Java Virtual Machine (JVM) heap, cap- tured on a 5-node Spark SQL <ref type="bibr" target="#b2">[3]</ref> cluster running the Big Data Benchmark <ref type="bibr" target="#b22">[23]</ref>. As we can observe, within a specific time window (marked by t current ), memory is heavily utilized on node_1, while other nodes are largely idle on their memory utilization. This implies that memory becomes a bottleneck on node_1, because the Spark SQL scheduler assigned more tasks to this node with no knowledge that its memory may be overloaded at runtime.</p><p>Given the existence of resource bottlenecks, our ultimate objective is to reduce query response times by designing new task scheduling strategies that work around these bottlenecks. To achieve such an objective, we need to monitor performance attributes of data analytic queries at runtime and detect potential bottlenecks with very little overhead. To be more specific, we will need to design and implement the following components:</p><p>Lightweight performance monitors. A collection of performance monitors on each worker node is needed to capture process-level performance metrics in real-time. In Lube, rather than intrusively using code instrumentation, we choose to reuse existing lightweight systemlevel performance monitors on Linux (e.g., jvmtop, iotop, iperf and nethogs).</p><p>Online bottleneck detection. With performance metrics collected in real-time, we will propose algorithms that analyze dependencies between performance metrics and detect potential bottlenecks at runtime.</p><p>Bottleneck-aware scheduling. To react to detected bottlenecks, a bottleneck-aware scheduler will make task assignment decisions by considering both bottleneck sever-  ities and data locality. Besides, the scheduler should be able to tolerate inaccurate detections. <ref type="figure" target="#fig_2">Figure 2</ref> presents the closed-loop design architecture of Lube. On each worker node, a Lube client periodically collects runtime performance metrics, updates the machine learning model and reports detected bottlenecks to the Lube master; on the master node, the task scheduler makes task assignment decisions based on bottleneck intensities at the worker nodes, as well as data locality preferences of tasks. In return, the decisions made by the task scheduler will further influence the performance of data analytic queries at each worker node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Detecting Bottlenecks</head><p>Performance bottlenecks may emerge anytime and anywhere in wide-area data analytics. To mitigate performance bottlenecks in time, we will first need to detect them correctly at runtime. Lube performs online bottleneck detection on performance metrics collected in realtime.</p><p>We investigate two techniques to detect bottlenecks from the time series of performance metrics. One is a simple statistical model -the Autoregressive Integrated Moving Average (ARIMA) algorithm that approximates the future value by a linear function of past values and past errors; the other is an unsupervised machine learning model: the Sliding Hidden Markov Model (SlidHMM) algorithm that can autonomously learn the implicit correlation between multiple performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">ARIMA</head><p>Introduced by <ref type="bibr">Box and Jenkins [6]</ref>, the ARIMA model has been widely applied in time series analysis. As a combination of autoregressive (AR) model and the moving average (MA) model, the ARIMA model is defined by the following equation:</p><formula xml:id="formula_0">y t = θ 0 + φ 1 y t−1 + φ 2 y t−2 + · · · + φ p y t−p +ε t − θ 1 ε t−1 − θ 2 ε t−2 − · · · − θ q ε t−q ,<label>(1)</label></formula><p>where y t and ε t denote the actual value and random error at time t respectively; φ i (i = 1, 2, . . . , p) and θ j ( j = 1, 2, . . . , q) are the coefficients specified by the model. p and q are integers indicating the autoregressive (AR) and moving average (MA) polynomials respectively. A general ARIMA model is represented as ARIMA(p, d, q), in which d is the degree of difference transformation for data stationarity.</p><p>We build an univariate ARIMA model for each performance metric, rather than a vector ARIMA model for all metrics, as it usually becomes "overfitting" due to too many combinations of insignificant parameters <ref type="bibr" target="#b8">[9]</ref>. To support online bottleneck detection, we periodically update the ARIMA model with continuously arriving performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sliding HMM</head><p>The Hidden Markov Model (HMM) <ref type="bibr" target="#b3">[4]</ref> infers a sequence of hidden states that maps to the sequence of observation states. Through feeding a time series of observed performance metrics (O 1 to O d ) to HMM, we can infer the possible performance metrics O k in the future <ref type="figure" target="#fig_3">(Figure 3)</ref>. The HMM is usually defined as a three-tuple: (A, B, π) as the following notations (t is the time stamp): Q = {q 1 , q 2 . . . , q N }, hidden state sequence. HMM learns the hidden states based on an expectation maximization algorithm, the Baum-Welch Algorithm <ref type="bibr" target="#b4">[5]</ref>. This algorithm iteratively searches the model parameters (A, B, π) that maximizes the likelihood of Pr(O|µ) -the best explanation of the observation sequence. Traces of JVM heap utilization in <ref type="figure" target="#fig_4">Figure 4</ref> presents a clear periodical pattern. By learning the hidden states behind this pattern, the HMM infers the future performance metrics for bottleneck-aware scheduling.</p><formula xml:id="formula_1">O = {O 1 , O 2 . . . , O k }, observation state sequence. A = {a i j }, a i j = Pr(q j at t + 1|q i at t), transition matrix. B = {b j (k)}, b j (k) = Pr(O k at t|q j at t), emission matrix. π = {π i }, π i = Pr(q i at t = 1), initial state distribution. past … … future t {time_stamp: mem, net, cpu, disk} A(aij) A(aij) B(bj(k)) B(bj(k)) … Q Q Od Od O2 O2 O1 O1 q 1 q 1 q 2 q 2 q i q i q j q j Ok Ok O O</formula><p>To support bottleneck detection in runtime, HMM must be updated online. However, such online updates incur a heavy cost in both time and space, as the BaumWelch algorithm needs to re-calculate both old and new time series input. Hence, we propose to use the Sliding Hidden Markov Model (SlidHMM) <ref type="bibr" target="#b6">[7]</ref>, which is a sliding version of the classic HMM, and is particularly designed for online characterization of high-density time series. The core of SlidHMM is a sliding window that accepts new observations and evicts outdated ones. A moving average approximation replaces the outdated observations during SlidHMM's training phase. Different from the traditional HMM, SlidHMM updates incrementally with the partial calculation on a fixed window of observations. Thus, it improves the efficiency of bottleneck detection in both time and space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Bottleneck-Aware Scheduling</head><p>To justify the imperatives of bottleneck-aware scheduling, we visualized the performance metrics collected from a Spark SQL cluster running real-world workloads in a geographically distributed fashion on Amazon EC2. <ref type="figure" target="#fig_5">Figure 5</ref> reveals the necessity and feasibility of performing bottleneck-aware scheduling in wide-area data analytics: a single worker node is bottlenecked continuously while all nodes are rarely bottlenecked in chorus. A bottlenecked node slows down the running tasks, and if we keep assigning tasks to the bottlenecked node, performance will be further degraded. Meanwhile, there usually exist available nodes to take over the tasks assigned to bottlenecked nodes.</p><p>Unfortunately, neither existing resource management platforms (e.g., Mesos <ref type="bibr" target="#b12">[13]</ref> and YARN <ref type="bibr" target="#b1">[2]</ref>) nor scheduling solutions (e.g., Iridium <ref type="bibr" target="#b19">[20]</ref> and Sparrow <ref type="bibr" target="#b18">[19]</ref>) support online detection of such performance bottlenecks. The built-in schedulers of Spark and Hadoop make decisions only based on data locality, with the objective of reducing network transmission times <ref type="bibr" target="#b28">[29]</ref>.</p><p>Bottlenecked nodes consume extra time to process tasks. To minimize the response times of data analytic queries, we propose a simple task scheduler to coordinate with our bottleneck detection algorithms and mitigate bottlenecks at runtime. A node will be marked as available if no upcoming bottlenecks have been detected; a task has several levels of locality preferences in descending order. When assigning a task, this scheduler jointly considers data locality and bottleneck severity. Essentially, it searches for an available node that satisfies the highest locality preference level of the task compared to all available nodes. Considering that bottlenecks may not be correctly detected, we introduce a late-binding algorithm to our bottleneck-aware scheduler. Sparrow <ref type="bibr" target="#b18">[19]</ref> applies this algorithm to work around incorrect samplings. The intuition of late-binding is that the worker nodes first verify the correctness of bottleneck detection, and then launch the assigned tasks. If such verification fails, the task will be reassigned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation and Evaluation</head><p>We implement components of Lube in a decoupled fashion. The pluggable performance monitors, the standalone bottleneck detection module and the scheduler exchange messages via in-memory redis <ref type="bibr" target="#b20">[21]</ref> servers. The messages are negligible small key-value pairs for network transmission. We modified the built-in scheduler of Spark to enable bottleneck-aware scheduling.</p><p>We conducted a preliminary evaluation of our design in realistic wide-area settings. The deployment includes 37 EC2 m4.2xlarge instances across 9 regions. Each instance has an 8-core CPU, 32 GB of memory, 1000 Mbps network 2 and a 100 GB SSD disk. All instances run on Ubuntu-14.04 installed with Oracle Java-1.8.0, Spark-1.6.1, HDFS-2.6.4 and Hive-1.2.1. The Big Data Benchmark <ref type="bibr" target="#b22">[23]</ref> includes four queries: Query 1-3 each has three scale levels a, b, c, from small to large; Query 4 is a user-defined-function (UDF) running a python script to count URLs. We run this benchmark with a 1.1 TB dataset.</p><p>The results in <ref type="figure" target="#fig_8">Figure 6</ref> show that with an accuracy of over 90% in bottleneck detection, Lube speeds up median query response times from 26.88% (1.37×) to 33.46% (1.5×). <ref type="figure" target="#fig_8">Figure 6</ref>(a) presents the accuracies of bottleneck detection under different settings. The hit rate is defined as the proportion of detected bottlenecks that are observed by monitors among all detected bottlenecks. We collect the time stamps and the bottleneck sequences during the running of the Big Data Benchmark for 15 times respectively. By comparing the time sequences of detected bottlenecks and observed bottlenecks, we calculate the hit rate offline. In our experiments, the average hit rate of the SlidHMM is 92.1%, while it's 83.57% for ARIMA. The hit rate of ARIMA tends to decrease with the increase of query scale. As a linear combination of autoregression and moving average, ARIMA ignores    nonlinear patterns in the performance metric sequences, which may lower the accuracy of bottleneck detection.</p><p>We show that Lube achieves a faster query response and maintains a low overhead from the task level to query level. At the task level, <ref type="figure" target="#fig_8">Figure 6</ref>(b) plots the task completion times CDF of pure Spark, Lube-ARIMA and LubeSlidHMM. For Query 1, the average (75th percentile) task completion time of pure Spark is 150.928 seconds (246.19 seconds). Lube-ARIMA saves 12.454 seconds (22.075 seconds) for average seconds (75th percentile) tasks compared to pure Spark, while Lube-SlidHMM saves 14.783 seconds (27.469 seconds) for average (75th percentile) tasks. Our bottleneck-aware scheduler brings a substantial improvement to the completion times of long tasks.</p><p>At the query level, we measure query response times under different control groups. Pure Spark is the baseline; Lube-ARIMA and Lube-SlidHMM show the reduction of query response times; and, the Spark default scheduler with Lube-ARIMA (ARIMA + Spark) and Lube-SlidHMM (SlidHMM + Spark) are the control group to evaluate Lube's overhead. <ref type="figure" target="#fig_8">Figure 6</ref>(c) shows that running Lube-ARIMA or Lube-SlidHMM with the Spark default scheduler does not introduce much overhead since the query response times under these three settings are similar. In addition, for median query response times, Lube reduces 26.88% to 33.46% (1.37× to 1.5× faster) of time with the ARIMA algorithm, while reduces 28.41% to 33.18% (1.4× to 1.5× faster) of time with the SlidHMM algorithm. From these results, we can conclude that Lube reduces the overall query response times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>There exists large volumes of existing research on optimizing the performance of wide-area data analytics. Clarinet <ref type="bibr" target="#b24">[25]</ref> pushes wide-area network awareness to the query planner, and selects a query execution plan before the query begins. Graphene <ref type="bibr" target="#b11">[12]</ref> presents a Directed Acyclic Graph (DAG) scheduler with awareness of DAG dependencies and task complexity. Iridium <ref type="bibr" target="#b19">[20]</ref> optimizes data and task placement to reduce query response times and WAN usage. Geode <ref type="bibr" target="#b25">[26]</ref> minimizes WAN usage via data placement and query plan selection. SWAG <ref type="bibr" target="#b13">[14]</ref> adjusts the order of jobs across datacenters to reduce job completion times. These works develop their solutions based on a few widely-accepted mantras, which are shown to be skeptical in a systematic analysis on the performance of data analytics frameworks <ref type="bibr" target="#b17">[18]</ref>. The blocked time analysis proposed in <ref type="bibr" target="#b17">[18]</ref> calls for more attention to temporal performance variations.</p><p>However, there is still very little existing effort on optimizing the performance of data analytics with the awareness of variations in the runtime environment. Hadoop speculative task execution <ref type="bibr" target="#b27">[28]</ref> duplicates tasks that are slow or failed, but not knowing the exact bottlenecks may lead to worse performance. As far as we know, Lube is the first work that leverages machine learning techniques to detect runtime bottlenecks and schedules tasks with awareness of performance bottlenecks.</p><p>Machine learning techniques have been actively applied to predict and classify data analytics workloads. NearestFit <ref type="bibr" target="#b7">[8]</ref> establishes accurate progress predictions of MapReduce jobs by a combination of nearest neighbour regression and statistical curve fitting techniques. Ernest <ref type="bibr" target="#b23">[24]</ref> applies a linear regression model to predict the performance of large-scale analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have presented Lube, a closed-loop framework that mitigates bottlenecks at runtime to improve the performance of wide-area data analytics. Lube monitors runtime query performance, detects bottlenecks online and mitigates them with a bottleneck-aware scheduling policy. Experiments across nine EC2 regions show that Lube achieves over 90% bottleneck detection accuracy and, compared to the default Spark scheduler, reduces the median query response time by up to 33%.</p><p>Preliminary experiments highlight the performance of Lube in reducing query response times achieved through detecting bottlenecks, mitigating bottlenecks at runtime. While this motivates the research on performing datadriven runtime performance analysis to optimize data analytics frameworks, there are a few aspects to discuss.</p><p>Selection of runtime metrics. It is the selected runtime metrics that determine the efficacy of the runtime performance analysis. There are enormous runtime metrics from multiple hierarchies of wide-area data analytics frameworks. To efficiently detect and mitigate bottlenecks in low-level resources (e.g., CPU, memory, disk I/O and network I/O etc.), we have studied several performance monitors and various combinations of performance metrics. However, the space of selecting appropriate metrics has still not been fully explored. We will put more efforts in the assessment of runtime metrics selection.</p><p>Bottleneck detection models. Lube achieves a substantial improvement by applying two simple models, ARIMA and SlidHMM. The emerging data-driven techniques broaden the horizon of data analytics optimization methodologies. We would like to further explore the latest data-driven techniques, such as Generative Adversary Network (GAN) <ref type="bibr" target="#b10">[11]</ref> and Reinforcement Learning <ref type="bibr" target="#b21">[22]</ref>. For example, DeepRM <ref type="bibr" target="#b16">[17]</ref> builds a deep reinforcement learning model for strategies of cluster resource management. However, the surprising accuracy of machine learning models makes us wonder the practical boundary of their effectiveness, which is imperative for robust and reproducible solutions.</p><p>WAN conditions. Most recent work mainly considers the heterogeneity and the variance of wide-area network bandwidths <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>. A few approaches have been applied to measure network conditions in these work. Lube captures the local network throughput by measuring network I/O on each node, which though only reveals a coarse-grained awareness of network; and, measures the pair-wise WAN bandwidths by a cron job running iperf on each node. We plan to exploit the capabilities of Software-Defined Network (SDN) to complement the global wide-area network conditions at runtime.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A potential bottleneck in memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Lube: a closed-loop architecture involving performance monitors, bottleneck detection, and task scheduling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The Hidden Markov Model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The JVM heap utilization traces of a Spark executor process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Heat maps of performance metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evaluation on a 37-node EC2 cluster running the Big Data Benchmark.</figDesc></figure>

			<note place="foot" n="1"> For example, a higher network I/O will lead to higher JVM heap swap frequencies, since network send/receive semantics will trigger memory load/dump operations. 2 EC2 only guarantees intra-region bandwidth. The inter-region traffic runs on public links that are highly fluctuating and intensely competitive.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgement</head><p>This work is supported by a research contract with Huawei Technologies Co. Ltd. and an NSERC Collaborative Research and Development (CRD) grant. We would like to thank the HotCloud anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alipourfard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cherrypick</surname></persName>
		</author>
		<title level="m">Adaptively Unearthing the Best Cloud Configurations for Big Data Analytics. In NSDI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Apache Hadoop Official Website</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/.[Online" />
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Spark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sql</surname></persName>
		</author>
		<ptr target="https://spark.apache.org/sql/.[On-line" />
		<imprint>
			<date type="published" when="2016-07" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Statistical inference for probabilistic functions of finite state Markov chains. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petrie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1966" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiss</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Time series analysis: forecasting and control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ljung</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sliding Hidden Markov Model for Evaluating Discrete Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th Proceedings of Computer Performance Engineering European Workshop</title>
		<imprint>
			<publisher>EPEW</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">8168</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On data skewness, stragglers, and MapReduce progress indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coppa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finocchi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Symposium on Cloud Computing</title>
		<meeting>the Sixth ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">25 years of time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>De Gooijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="443" to="473" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mapreduce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Operating System Design and Implementation (OSDI</title>
		<meeting>USENIX Symposium on Operating System Design and Implementation (OSDI</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goodfellow</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Gio</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphene: Packing and dependency-aware scheduling for data-parallel clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grandl</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kulkarni</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hindman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>USENIX Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<publisher>NSDI</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scheduling Jobs Across Geo-Distributed Datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Golubchik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Network-Aware Scheduling for Data-Parallel Jobs: Plan When You Can</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jalaparti</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makarychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caesar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="page" from="407" to="420" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kloudas</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mamede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Preguiça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And Ro-Drigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pixida</surname></persName>
		</author>
		<title level="m">Optimizing Data Parallel Jobs in WideArea Data Analytics. VLDB Endowment</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Resource Management with Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandula</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM Workshop on Hot Topics in Networks</title>
		<meeting>the 15th ACM Workshop on Hot Topics in Networks</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making sense of performance in data analytics frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousterhout</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sparrow: distributed, low latency scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousterhout</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sto-Ica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low Latency GeoDistributed Data Analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Redis</forename><forename type="middle">Redis</forename><surname>Website</surname></persName>
		</author>
		<ptr target="http://redis.io/" />
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barto</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Big Data Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley Amplab</surname></persName>
		</author>
		<ptr target="https://amplab.cs.berkeley.edu/benchmark/.[Online;accessed1" />
		<imprint>
			<date type="published" when="2016-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ernest: efficient performance prediction for large-scale advanced analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkataraman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clarinet: WAN-aAware optimization for analytics queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viswanathan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akella</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Global Analytics in the Face of Bandwidth and Regulatory Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vulimiri</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jungblut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varghese</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analytics for A GeoDistributed Data-Intensive World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vulimiri</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karanasos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varghese</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wanalytics</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Innovative Data Systems Research (CIDR)</title>
		<meeting>Conference on Innovative Data Systems Research (CIDR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hadoop: The Definitive Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">White</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Delay Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sen Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elmele-Egy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM European Conference on Computer Systems</title>
		<meeting>ACM European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
