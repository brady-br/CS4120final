<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable NUMA-aware Blocking Synchronization Primitives Scalable NUMA-aware Blocking Synchronization Primitives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 12-14, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable NUMA-aware Blocking Synchronization Primitives Scalable NUMA-aware Blocking Synchronization Primitives</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2017 USENIX Annual Technical Conference (USENIX ATC &apos;17)</title>
						<meeting>the 2017 USENIX Annual Technical Conference (USENIX ATC &apos;17) <address><addrLine>Santa Clara, CA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 12-14, 2017</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2017 USENIX Annual Technical Conference is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Application scalability is a critical aspect to efficiently use NUMA machines with many cores. To achieve that, various techniques ranging from task placement to data sharding are used in practice. However, from the perspective of an operating system, these techniques often do not work as expected because various subsystems in the OS interact and share data structures among themselves, resulting in scalability bottlenecks. Although current OSes attempt to tackle this problem by introducing a wide range of synchronization primitives such as spinlock and mu-tex, the widely used synchronization mechanisms are not designed to handle both under-and over-subscribed scenarios in a scalable fashion. In particular, the current blocking synchronization primitives that are designed to address both scenarios are NUMA oblivious, meaning that they suffer from cache-line contention in an under-subscribed situation, and even worse, inherently spur long scheduler intervention, which leads to sub-optimal performance in an over-subscribed situation. In this work, we present several design choices to implement scalable blocking synchronization primitives that can address both under-and over-subscribed scenarios. Such design decisions include memory-efficient NUMA-aware locks (favorable for deployment) and scheduling-aware, scalable parking and wake-up strategies. To validate our design choices, we implement two new blocking synchronization primitives, which are variants of mutex and read-write semaphore in the Linux kernel. Our evaluation shows that these locks can scale real-world applications by 1.2-1.6× and some of the file system operations up to 4.7× in both under-and over-subscribed scenarios. Moreover, they use 1.5-10× less memory than the state-of-the-art NUMA-aware locks on a 120-core machine.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last decade, microprocessor vendors have been pursuing the direction of bigger multi-core and multisocket (NUMA) machines <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref> to provide large chunks of memory, which is accessible by multiple CPUs. Nowadays, these machines are a norm to further scale applications such as large in-memory databases (Microsoft SQL server <ref type="bibr" target="#b25">[26]</ref>) and processing engines <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>. Thus, achieving application scalability is critical for efficiently using these NUMA machines, which today can have up to 4096 hardware threads organized into sockets. To achieve high performance, various applications such as databases <ref type="bibr" target="#b25">[26]</ref>, processing engines <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b40">41]</ref>, and operating systems (OS) often rely on NUMA partitioning to mitigate the cost of remote memory access either by Figure 1: Impact of NUMA-aware locks on a file-system microbenchmark that spawns processes to create new files in a shared directory (MWCM in <ref type="bibr" target="#b26">[27]</ref>). It stresses either the mutex or the writer side of the read-write semaphore and memory allocation. is an in-kernel ported version of NUMA-aware locks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>, and our NUMA-aware lock (CST).</p><p>data or task placement. However, these approaches do not address how to efficiently modify shared data structures such as inodes, dentry cache, or even the structures of the memory allocator that span multiple sockets in a large multi-core machine. As a result, synchronization primitives are inevitably the basic building blocks for such multi-threaded applications and are critical in determining their scalability <ref type="bibr" target="#b1">[2]</ref>. Hence, the state-of-the-art locks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, which are NUMA-aware, are the apt choice to efficiently exploit the NUMA behavior for achieving scalability on these multi-core machines. NUMA-aware locks do improve application scalability, but they are difficult to adopt in practice. They either require application modification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23]</ref> or statically allocate a considerable amount of memory that can bloat shared data structures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11]</ref>, as thousands to millions of lock instances can be instantiated in a large multicore machine. For instance, a similar issue of adopting non-blocking queue-based locks occurred with Linux. Wickizier et al. <ref type="bibr" target="#b1">[2]</ref> showed that a ticket lock suffers from cache-line contention with increasing core count. They replace it with the MCS lock to mitigate such an effect, which improved the system performance. Unfortunately, its adoption faced several challenges due to the change in the structure size and the lock function API <ref type="bibr" target="#b20">[21]</ref>.</p><p>We observe a similar trend in the case of blocking synchronization primitives, which suffer from numerous problems: 1) OS developers rely on TTAS locks or their variant <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref>, as they are simple and cache-line contention is not evident at smaller core count. However, they deter scalability on large multi-core machines <ref type="figure">(Fig- ure 1 (a)</ref>). 2) The proposed blocking synchronization primitives <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref> are NUMA-oblivious and suffer from high memory management cost for every lock acquisition, which impedes scalability. 3) NUMA-aware locks (Cohort locks) suffer from memory bloat as they statically allocate memory for all sockets, which is a serious issue in an OS <ref type="bibr" target="#b2">[3]</ref>  <ref type="figure">(Figure 1 (b)</ref>) and are non-blocking. 4) Finally, current blocking primitives severely suffer from the poor parking strategy because of cache-line contention, use of a global parking list, inefficient scheduling decisions, and inefficient system load estimation.</p><p>In this work, we design and implement two scalable blocking synchronization primitives, namely CST-mutex and CST-rwsem, from an OS perspective. Our primitives are memory-efficient, support blocking synchronization, and are tightly coupled with the scheduler, thereby resulting in better scalability beyond 100 physical cores for both under-and over-subscribed situations (tested up to 5× over-subscription). CST locks support blocking, as they incorporate a timeout capability for waiters, including readers and writers, in which waiters can park and wakeup without hurting the performance of the system. We use four key ideas to implement a scalable blocking synchronization primitive: First, we consciously allocate memory by maintaining a dynamic list of per-socket structures that is a basic building block of NUMA-aware locks. Second, instead of passing the lock to the very next waiter, we pass it to a not-yet-parked (still spinning) waiter, which removes the scheduler intervention while passing the lock to a waiter. Third, we keep track of the parked waiters in a separate, per-socket list without manipulating the actual waiting list maintained by the lock protocol. Lastly, we maintain a per-core scheduling information to efficiently estimate the system load. Thus, our blocking primitives improve the application performance by 1.2-1.6×, and they are 10× faster than existing blocking primitives in over-subscribed scenarios for various micro-benchmarks. Moreover, our approach uses 1.5-10× less memory compared with the state-of-the-art NUMA-aware locks.</p><p>In summary, we make the following contributions:</p><p>• Two blocking synchronization primitives. We design and implement two blocking synchronization primitives (CST-mutex and CST-rwsem) that efficiently scale beyond 100 physical cores.</p><p>• Memory-efficient data structure. We maintain a dynamically allocated list of per-socket structures that address the issue of memory bloat.</p><p>• Scheduling-aware parking/wake-up strategy.</p><p>Our approach mitigates the scheduler interaction by passing the lock to a spinning waiter and batching the wake-up operation.</p><p>• Lightweight schedule information. We extend the scheduler to estimate the system load to efficiently handle both over-and under-subscription cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>We first classify prior research directions into two categories: NUMA-aware locks and runtime contention management. We later give a primer on blocking synchronization primitives used in Linux.</p><p>NUMA-aware locks. NUMA-aware locks address the limitation of NUMA-oblivious locks <ref type="bibr" target="#b24">[25]</ref> by amortizing the cost of accessing the remote memory. Most of the locks are hierarchical in nature such that they maintain multiple levels of lock <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> in the form of a tree. Inspired by prior hierarchical locks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24]</ref>, Cohort locks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref> generalized the design of any two types of locks in a hierarchical fashion for two-level NUMA machines and later extended them for the read-write locks <ref type="bibr" target="#b3">[4]</ref>. However, neither of them addresses the memory utilization issue nor supports blocking synchronization, which leads to sub-optimal performance when multiple instances of locks are used or when the system is overloaded. Besides Cohort locks, another category of locking mechanism is based on combining <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> and the remote core execution approach <ref type="bibr" target="#b22">[23]</ref> in which a thread executes several critical sections without any synchronization. Although it outperforms Cohort locks <ref type="bibr" target="#b22">[23]</ref>, the mechanism requires application modification, which is not practical for applications with a large code base. Our design of NUMA-aware locks is memory conscious, as we defer the allocation of per-socket locks until required, unlike prior ones. In addition, CST locks are blocking, meaning that they support timeout capability while maintaining the locality awareness, unlike the existing NUMA-oblivious locks that allocate memory for each lock acquisition <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Moreover, none of the NUMAaware read-write locks support blocking readers, but the ones that do support <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> are NUMA oblivious and are designed specifically for read-mostly operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contention management.</head><p>The interaction between lock contention and thread scheduling determines application scalability, which is an important criterion to decide whether to spin or park a thread in an under-or over-subscribed scenario. Johnson et al. <ref type="bibr" target="#b16">[17]</ref> addressed this problem by separating contention management and scheduling in the user space. They use admission control to handle the number of spinning threads by running a system-wide daemon that globally measures the load on the system. Similar approaches have been used by runtimes <ref type="bibr" target="#b6">[7]</ref> and task placement strategies inside the kernel without considering the lock subsystem <ref type="bibr" target="#b41">[42]</ref>. Along these lines, the Malthusian lock <ref type="bibr" target="#b8">[9]</ref>, a NUMA-oblivious lock, handles thread over-subscription by randomly moving a waiter from an active list to a passive list (concurrency culling), which is inspired by Johnson et al.</p><p>CST locks handle the over-subscription by maintaining a separate list in which waiters independently add them-selves to a separate list after timing out. Our approach is different from the Malthusian lock and does not lengthen the unlock phase because wake-up and parking strategies are independent. Moreover, CST locks adopt the idea of a separate parking list from existing synchronization primitives <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> or wait queues <ref type="bibr" target="#b37">[38]</ref>, but remove the cache-line bouncing by maintaining a per-socket, separate parking list for both readers and writers.</p><p>Design of Linux's mutex and rwsem. Many OSes, including Linux, do not allow nested critical sections for any blocking locks. The current design of mutex is based on the TTAS lock, which is coupled with a global queuebased instance <ref type="bibr" target="#b21">[22]</ref> and a parking list per-lock instance. The algorithm works by first trying to atomically update the lock variable, called fast path; on failure, the mid-path phase (optimistic spinning) begins in which only a single waiter is queued up if there is no spinning waiter and optimistically spins until its schedule quota expires. If the waiter still does not acquire the lock, it goes to the slow-path phase in which it acquires a lock on the parking list (parking lock), adds itself, and schedules out after releasing the parking lock. During the unlock phase, the lock holder first resets the TTAS variable and wakes up a waiter from the parking list while holding the parking lock. Meanwhile, it is possible that either a new waiter can acquire the lock in the fast path or a spinning waiter in the mid path. Now, once a waiter is scheduled in, it again acquires the parking lock and tries to acquire the TTAS lock. If successful, it removes itself from the parking list and enters the critical section; otherwise, it schedules itself out again and sleeps until a lock holder wakes it up. The current algorithm is unfair because of the TTAS lock; even starves its waiters in the slow-path phase. Moreover, the algorithm also suffers from cache-line contention because of the TTAS lock and waiters maintenance, and even worse is the scheduling overhead in the slow-path phase and the unlock phase for parking and wake up.</p><p>The read-write semaphore is an extension of mutex, with a writer-preferred version. Both the write lock and the reader count are encoded in a word to decide readers, writer, and waiting readers. Moreover, rwsem maintains a single parking list in which both readers and writers are added. Thus, in addition to inheriting the issues of mutex, rwsem also suffers from reader starvation due to the writerpreferred version. Interestingly, developers found that the neutral algorithm suffers from scheduler overhead <ref type="bibr" target="#b19">[20]</ref>, while the writer-preferred version mitigated this overhead and improved the performance by 50% <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Challenges and Approaches</head><p>We present challenges and our approaches in designing practical synchronization primitives that can scale beyond 100 physical cores.</p><p>C1. NUMA awareness. A synchronization primitive should scale under high contention even in NUMA machines. Although locks that are used in practice <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> address cache-line contention by using queue-based locks <ref type="bibr" target="#b21">[22]</ref> for high contention, they do not address the cache-line bouncing (remote socket access) introduced in NUMA machines. The remote access is at least 1.6× slower than the local access within a socket, which is a deterrent to the scalability of an application.</p><p>Approach: To achieve scalability in NUMA machines, hierarchical locks (e.g., Cohort lock) are an apt choice. They mitigate the cache-line bouncing by passing a lock within a socket, which relaxes the strict fairness guarantee of FIFO locks for throughput.</p><p>C2. Memory-efficient data structures. Unfortunately, current hierarchical locks severely bloat the memory due to their large structure size (e.g., a Cohort lock requires 1, 600 bytes in an eight-socket machine 1 ), which they statically allocate for all sockets that may be unused. Memory bloat is a serious concern because it stresses the memory allocator and is alarming for synchronization primitives as they statically allocate the memory. For example, the size of the XFS inode structure increased by 4% after adding 16 bytes to the rwsem structure, which had an impact on the footprint and performance, as there can be millions of inodes cached on a system <ref type="bibr" target="#b7">[8]</ref>. Thus, existing hierarchical locks are difficult to adopt in practice because they statically allocate per-socket structures during initialization.</p><p>Approach: A hierarchical lock should dynamically allocate per-socket structure only when it is being used to avoid the memory bloat problem and reduce the memory pressure on a system.</p><p>C3. Effective contention management for both overand under-subscribed scenarios. Designing synchronization primitives that perform equally well for both over-and under-subscribed situations is challenging. Nonblocking synchronization primitives, such as spinlocks including Cohort locks, work well when a system is underloaded. However, for an over-loaded system, they perform poorly because spinning waiters and a lock holder contend each other, which deters the progress. On the other hand, blocking synchronization primitives such as mutex and rwsem are designed to handle an over-loaded system. Instead of spinning, waiting threads sleep until a lock holder wakes one up upon lock release. However, this procedure imposes the overhead of waking up in every unlock operation, which increases the length of the critical section. Also, frequent sleep and wake-up operations impose additional overhead on the scheduler, which can result in scalability collapse, especially when multiple lock instances are involved. To mitigate this issue, many blocking synchronization primitives <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> employ the spin-then-park strategy: a waiter spins for a while, and then parks itself out. Unfortunately, this approach is agnostic of system-wide contention, which leads to suboptimal performance when multiple locks are contending. Ryan et al. <ref type="bibr" target="#b16">[17]</ref> addressed the problem by designing a system-wide load controller, but its centralized design has memory hot spots for its control variables (e.g., the number of ever-slept threads) to decide whether a thread should sleep or spin.</p><p>Approach: To work equally well in both over-and under-loaded cases, we must address the system-wide load that allows waiters to optimistically spin in underloaded cases and park themselves out in over-loaded cases. In addition, such a decision should be taken in a distributed way to keep the contention management from becoming a scalability bottleneck.</p><p>C4. Scalable parking and wake-up strategy. To implement an efficient blocking synchronization primitive, the most important aspects are how and when to park (schedule out) and wake up waiters with minimal overhead. The current approach <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> maintains a global parking list to keep track of parked waiters and a lock holder wakes one of the parked waiters at the unlock operation. However, this design has several drawbacks:</p><p>The frequent updating of a global parking list becomes a single point of contention in an over-loaded system, which leads to severe performance degradation because a lock holder has to wake up each sleeping waiter during the unlock phase, which adds extra pressure on the scheduler subsystem and lengthens the critical section: the cost of waking up varies from 2,000-8,000 cycles in the kernel-space or from 5,000-50,000 cycles in the userspace (futex() overhead). Thus, according to Amdahl's Law, an increased sequential part can significantly affect the scalability, especially in a large multi-core machine. Approach: Instead of waking up the very next waiter, a lock holder passes the lock to a non-sleeping waiter, if any. Thus, this approach not only avoids waking up other threads under high contention, but also minimizes the access of the parking list and scheduler interactions. Furthermore, we maintain a per-socket parking list to remove costly cache-line bouncing among NUMA domains for accessing the parking list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design Principles</head><p>We present two scalable NUMA-aware blocking synchronization primitives, a mutex (CST-mutex) and a read-write semaphore (CST-rwsem), that can scale beyond 100 physical cores. At a high level, our lock is a two-level NUMAaware lock, where a global lock is an MCS lock <ref type="bibr" target="#b24">[25]</ref> and a per-socket local lock is a K42 lock <ref type="bibr" target="#b14">[15]</ref> (see <ref type="figure" target="#fig_2">Figure 2</ref>). While the first level localizes the cache-line contention within a socket, the second one mitigates the cache-line bouncing among sockets. To enter a critical section, a A CST-mutex is active on sockets: 1, 2, and 3. Currently, socket 1 is being served. T1 now holds the lock (L); T3 is spinning for its turn (UW: unparked waiting); T2, T4, and T5 are sleeping (PW: parked waiting) until a lock holder wakes them up. A lock holder, T1, will pass the lock to T3, which is spinning, skipping the sleeping T2, to minimize the overhead of wake-up.</p><p>thread first acquires the per-socket local lock and then the global lock. During the release phase, it first releases the global lock followed by the local lock. To mitigate memory bloating, we dynamically allocate the per-socket structure (snode) when a thread first tries to acquire the lock on a specific NUMA domain, and maintain it until the life-cycle of the lock. Each snode maintains a perthread qnode in two lists: waiting_list-a K42-style list of waiters and parking_list-a list of parked (or sleeping) waiters. To acquire the lock, a thread first appends its qnode to the waiting_list of the corresponding snode in a UW (unparked waiting or spinning) status (T3 in <ref type="figure">Fig- ure</ref> 2) and spins until its schedule quota is over. On timing out, T3 parks itself by changing its status to PW (parked waiting) and adds itself to the parking_list (T2). A lock holder (T1) that acquires its local and the global lock, passes the lock in the same NUMA domain by traversing the waiting_list during the release phase. It skips the parked waiter (T2) and passes the lock to an active waiter (T3). If there is no active waiter, the lock holder wakes up parked waiters in the same or other NUMA nodes to pass the lock. Our rwsem additionally maintains a separate reader parking list, besides writer parking list, to handle the over-subscription of the readers. We explain our design principles on efficient memory usage (C1 and C2 in §4.1) and parking/wake-up strategy (C3 and C4 in §4.2). We later show how to apply our approaches to design blocking synchronization primitives: CST-mutex ( §5.1) and CST-rwsem ( §5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Memory-efficient NUMA-aware Lock</head><p>Unlike other hierarchical locks that statically allocate per-NUMA structures for all sockets during the initialization, CST defers the snode allocation until the moment it is accessed first. The allocated snodes are active until the lock is destroyed. Our dynamic allocation of snode is especially beneficial in two cases: 1) when the number of objects is unbounded, such as inode and mm_struct in Linux kernel, 2 and 2) when threads are restricted to access  a subset of sockets such as running a multi-core virtual machine on a subset of sockets in a cloud environment.</p><p>For every lock operation, we first check whether a corresponding snode is present, and then get the snode to acquire the local lock. To efficiently determine whether an snode is present, a lock maintains a global bit vector in which each bit denotes the presence of a particular snode. Hence, each thread relies on the bit vector for determining the presence of an snode. We use CAS to atomically update the bit vector, but the number of CAS operations is bounded to the number of sockets in a system during the lifetime of a lock. A lock maintains allocated snodes in snode_list, which is traversed by a thread to find the corresponding snode. We separate the snode into two cache lines, almost-read-only for snode traversal and read-write for the local lock operation, which prevents snode traversal from incurring cache-line bouncing among sockets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scheduling-aware Parking/Wake-up Strategy</head><p>As discussed in the previous section, the most widely used spin-then-park policy fails to address the issue of scalability in NUMA machines. It works by maintaining a single, global parking list to account for the sleeping waiters, and wakes one or some waiters to pass the lock at the time of release. Hence, this approach is not scalable because it incurs contention on the parking list and suffers from scheduler interaction as it passes the lock to a potentially sleeping waiter in an over-subscribed condition.</p><p>To address these issues, the CST lock uses two key ideas: it maintains a per-socket parking_list, which minimizes costly cross-socket cache-line bouncing and passes the lock to a spinning waiter, whose time quota is not over yet, to minimize costly wake-up operations. We wake up a set of skipped sleeping waiters in bulk when there are no active waiters in the serving snode or pass the global lock to the other waiting snode. Thus, by relaxing the strict FIFO guarantee, we mitigate the lock-waiter preemption problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Low-contending List Management</head><p>In a CST lock, each snode maintains the K42-style waiting list that comprises its own tail pointer: qtail. For parked waiters, the snode also maintains a per-socket parking_list to account for the parked waiters, which avoids the costly cache-line bouncing while manipulating the parking_list. For a rwsem, we maintain a separate readers and writers parking_list, which simplifies the list processing in the unlock phase, as the lock holder can pass the lock to all parked readers or to one of the writers. Moreover, this approach enables a distributed parallel waking of readers at a socket level, which can improve the throughput of readers in an over-subscribed scenario (refer §5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Scheduling-aware Parking/wake-up Decision</head><p>For a blocking synchronization primitive, the most important question is how to efficiently pass the lock or wake up a waiter, while maintaining an on-par performance in both the under-and over-subscribed cases. For the scalable parking/wake-up decision, we remove costly scheduler operations (i.e., wake-up) from the common, critical path and employ a distributed parking decision while considering the load on a system. We discuss three key ideas to address the problem of 1) whom to pass the lock to, 2) when to park oneself, and 3) how to take the parking decisions for blocking synchronization primitives. Passing lock to an active spinning waiter. In queuebased locks (e.g., MCS, K42, and CLH), the successor of a lock holder always acquires the lock, which guarantees complete fairness, but, unfortunately, causes severe perfor-mance degradation in an over-subscribed system, as this invariant stresses the scheduler to always issue a call to wake up the parked waiter. To mitigate this issue, we modify the invariant of a succeeding lock holder from the next waiter to a nearest active waiter, which is still spinning for the lock acquisition. Hence, the waiting_list comprises both active and parked waiters in its queue, and the parked waiters are added to a separate list: parking_list. <ref type="figure" target="#fig_3">Figure 3</ref> (a) illustrates this scenario, where T1 passes the lock to T3 instead of T2, since T2 is parked. Later, parked waiters are woken up in batches up to the number of physical cores in a socket once there is no active waiter in the waiting_list. When a parked waiter is woken up, it generally re-queues itself back at the end of the waiting_list, and again actively spins for the lock. This approach is effective because we can avoid scheduler intervention under high contention by passing the lock to an active waiter. In addition, a batched wake-up strategy amortizes the cost of the wake-up phase. Scheduling-aware spinning.</p><p>Current hierarchical locks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref> do not consider the amount of time a waiter should spin before parking itself out. Thus, in an over-loaded system, waiting threads and a lock holder will contend with each other, which deters the system progress. Instead, in CST locks, waiting threads park themselves as soon as their time quota is about to cease. To check the quota, we rely on the scheduler and its APIs for this information. Specifically in the Linux kernel, the scheduler exposes need_resched() to know whether the task should run, and preemption APIs (preempt_disable() / preempt_enable()) to explicitly disable or enable the task preemption. These APIs work with both preemptive and non-preemptive kernels. Limiting the duration of spinning up to the time quota proposed by the scheduler has several advantages: 1) It guarantees the forward progress of the system in an over-loaded system by allowing the current lock holder to do useful work while mitigating its preemption. 2) It allows other tasks to do some useful work rather than wasting the CPU cycles. 3) By only spinning for the specified duration, the primitive respects the fair scheduling decision of the scheduler. Scheduling-aware parking. The current blocking synchronization primitives <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> do not efficiently account for the system load; thus, they naively park waiters even in under-loaded scenarios. Hence, a naive use of the spinthen-park approach results in scheduler intervention, as the waiters park themselves as soon as their time quota ceases, and the lock holder has to do an extra operation of waking them up, which severely degrades the performance of the system in an under-loaded scenario <ref type="bibr" target="#b26">[27]</ref>. Also, previous research <ref type="bibr" target="#b16">[17]</ref> has shown that estimating system load is critical to the spin-then-park approach because it not only removes the scheduler interaction from the parking phase, but also improves the latency of the lock/unlock phase.</p><p>We gauge the system load by peeking at the number of running tasks on a CPU (i.e., the length of scheduling run queue for a CPU). Checking the number of running tasks is almost free because a modern OS kernel, including Linux, has a per-CPU scheduler queue, which already maintains an up-to-date per-CPU active task information. On the other hand, maintaining system-wide, central information, like the approach used by Johnson et al. <ref type="bibr" target="#b16">[17]</ref>, is costly because the cost of collecting the total number of active tasks increases with increasing core count, which may not catch the load imbalance due to the new incoming tasks or the rescheduling of periodic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Scalable Blocking Synchronizations</head><p>We now discuss the design and implementation of the two types of NUMA-aware blocking synchronization primitives (mutex and rwsem) using our design decisions. We first present the design of mutex (CST-mutex) along with the parking strategy and later extend it to rwsem (CST-rwsem). <ref type="figure" target="#fig_4">Figure 4</ref> presents their pseudo-code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Mutex (CST-mutex)</head><p>CST-mutex is a two-level hierarchical lock, which is extended to support blocking behavior by adding several design choices, such as scheduling-awareness, efficient spinning and parking strategy, and passing of the lock to the spinning waiter. The global lock employs an MCS lock, whereas the local lock is a K42 lock <ref type="bibr" target="#b14">[15]</ref>, a variant of the MCS lock. We choose the K42 lock because it does not require an extra argument in the function call as it maintains a qnode structure on the stack, but we can use any queue-based lock for the local lock. The top level lock maintains a dynamically allocated per-socket structure (snode) to keep track of the global lock and local lock information such as its waiting_list and the next waiter (for the K42 lock), and also parking_list information for the parked waiters. The MCS lock protocol has two status values: waiting (lock waiter) and locked state (lock holder). To support the blocking behavior, we keep the locked state (denoted as L) intact and extend the waiting state to the spinning/unparked (UW) and parked (PW) state. We also introduce a special state, called re-queue (R), that notifies the waiter to re-acquire the local lock.</p><p>Extended Cohort lock/unlock protocol.</p><p>A thread starts by trying to acquire a local lock inside a socket. If there are no predecessors during the lock acquisition, it acquires the global lock, thereby becoming the lock holder, and enters the critical section (CS). The other threads that do not acquire the local lock are the local waiters, and the ones waiting for the global lock are the socket leaders. They wait for their respective predecessor to pass the lock. In the release phase, the lock holder locally tries to pass the lock to a successor. Thus, on success, the successor   does not acquire the global lock and immediately enters the critical section. To prevent starvation, a lock holder later passes the global lock to a globally waiting successor (socket leader) after a bounded number of local acquisitions. We now describe the CST-mutex protocol in detail, which is an extension of the aforementioned steps.  <ref type="figure">)</ref>; otherwise, it acquires the lock as the predecessor passed the lock. Note that even after being woken up, T always acquires the global lock without re-queueing itself.</p><p>Release local lock: T gets the current snode (line 42) and tries to locally pass the lock if it is within the batching threshold (line 43). To locally pass the lock, T first tries to CAS the status of its successor from UW to L. On success, the unlock phase is over; otherwise, it traverses the waiting_list to find an actively running waiter (lines 88 -99). <ref type="figure" target="#fig_3">Figure 3</ref> (a) illustrates this scenario, where T1 ends up passing the lock to T3 since T2 has PW state. Note that if all waiters are parked, (line 99), T releases the global lock (line 49) and then the local lock (line 50). T can also initiate both release phases when an snode exceeds the batching threshold. In the local unlock phase, T finds the snode qnext pointer to pass the lock. If qnext is NULL, T updates the qtail of snode with NULL (line 68) and wakes up waiters in the parking list to the R state to re-queue them back to the waiting_list (line 101). <ref type="figure" target="#fig_3">Figure 3 (b)</ref> illustrates the scenario, where T3 is the last one in the waiting_list. In the release phase, after resetting qtail to NULL, T3 wakes up parked T2 after updating its status from PW to UW. If there are waiters (lines 60 -64), then T again tries to pass the lock to a spinning waiter in the waiting_list (line 88). If successful, a waiter acquires the local lock and then goes for the global lock since T has already released that one. If all, including the last waiter, are parked (lines 60-64), T passes the local lock to the last waiter and wakes it up because T cannot reset the qtail pointer, as there maybe some parked waiters; hence, passing the lock to the last waiter is mandatory. <ref type="figure" target="#fig_3">Figure 3</ref> (c) shows this scenario in which T2 is about to release the local lock and finds that T3 is the last one and has PW status. T2 has to wake up T3 with an L state (not R), so that T3 can maintain the K42/MCS lock protocol.</p><p>Release global lock: The protocol differs from the MCS protocol for passing the lock. For an existing snode successor, thread T tries to CASes the status of its succeeding snode from UW to L. If successful, the lock is passed; otherwise, T explicitly updates the status to L and wakes up the succeeding socket leader (lines 72 -74).</p><p>5.2 Read-write Semaphore (CST-rwsem)</p><p>CST-rwsem is a writer-preferred version of the Cohort readwrite lock <ref type="bibr" target="#b3">[4]</ref> (CST-rwsem) with two extensions: 1) application of our parking strategy to the readers and 2) our own version of the mutex algorithm ( §5.1). It relaxes the condition of acquiring the CS by multiple threads in a read mode. Hence, it maintains an active reader count (active_readers) on each snode to localize the contention on each socket at the cost of increasing the latency for the writers. We further extend the snode to support the parking of readers by maintaining a separate parking list for them, which allows readers to separately park themselves without intervening with the writers.</p><p>Write lock: Thread T first acquires the CST-mutex (line 109). Then T traverses all snodes to check whether the value of active_readers is zero (line 111). Due to our writer-preferred algorithm, T blocks new readers from entering the CS because they can only proceed if there is no writer. Once the writer has acquired the mutex lock, it does not park itself, as this is a writer-preferred algorithm and the writer will soon enter the CS (lines 110 -113).</p><p>Read lock: T first finds its snode (line 122) and waits until there are no writers (line 125). On timing out, while waiting, T adds itself to the parking_list and schedules itself out until there are no writers (line 142). The last writer wakes up the first reader in the parking_list, which wakes up remaining sleeping waiters in its own socket. Lines 143 -146 present the waking up of the parked reader and subsequent readers.</p><p>Write unlock: T first releases the writer lock (line 116). If there are no writers (line 117), then T checks for any sleeping waiters across all snodes. If there are any, it wakes up the only very first waiter, which will subsequently wake up remaining waiters to acquire the read lock (line 119). This approach has two advantages: 1) it ensures distributed, parallel wake-up of the readers, and 2) it does not lengthen the writer unlock phase along with the least number of remote memory accesses.</p><p>Read unlock: Thread T searches for its snode from the list of existing sockets and atomically decreases the active_readers count by 1. T does not have to wake up any writer because our approach does not park the writer thread, which is going to be the next lock holder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation</head><p>We implemented CST locks on the Linux kernel v4.6 and v4.7. We also provide a destructor API to reclaim the snode memory while destroying a data structure (e.g., destroy_inode for inode). For our evaluation, we modified the inode structure to use our CST-rwsem in v4.7 and CST-mutex in v4.6 since mutex was replaced with rwsem from v4.7 <ref type="bibr" target="#b39">[40]</ref>. We also modified the virtual memory subsystem (mmap_sem) that manipulates the virtual memory area of a process. We modified 650 and five calls for mmap_sem and inode, respectively. In total, our lock implementation comprises 1,100 lines of code and can substitute most of the lock instances in Linux. 0.5k</p><p>1.0k</p><p>1.5k</p><p>2.0k</p><p>2.5k</p><p>3.0k </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We evaluate the impact of CST locks by answering the following questions:</p><p>• How do locks affect the scalability and memory utilization of real-world applications? ( §7.1) • What is the impact of locks on operations provided by the OS in various scenarios? ( §7.2) • How does each design aspect help improve the performance? ( §7.3, §7.4) Evaluation setup. We evaluate CST locks on three workloads <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref> in an under-subscribed scenario, three micro-benchmarks from FXMARK <ref type="bibr" target="#b26">[27]</ref> that stress various file system components and the kernel memory allocator. Finally, we breakdown the performance implication of each design aspect using a hash table micro-benchmark. We evaluate on an eight-socket, 120-core machine with Intel Xeon E7-8870 v2 processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Application Benchmarks</head><p>We evaluate the scalability of CST-rwsem on three applications, namely <ref type="bibr">Histogram [33]</ref>, Metis <ref type="bibr" target="#b0">[1]</ref>, and Psearchy <ref type="bibr" target="#b0">[1]</ref>, that scale with increasing core count and stress the memory subsystem of the Linux kernel at varying levels. We compare our lock with the Linux's rwsem and an in-kernel port of Cohort locks <ref type="bibr" target="#b13">[14]</ref>. For each benchmark results, we use Vanilla for the native Linux's rwsem, Cohort for the read-write Cohort lock, and CST for the CST-rwsem lock. Histogram is a MapReduce application, which is pagefault intensive. It mmaps an 11 GB file at the beginning and keeps reading this file while each thread performs a simple computation. <ref type="figure" target="#fig_6">Figure 5 (a)</ref> shows that NUMAaware Cohort and CST locks outperform the native implementation after 60 cores. They scale better because both locks localize the number of active readers within a socket, thereby having almost negligible contention across the sockets. Moreover, both locks have 2% idle time because the Cohort lock is non-blocking by design and the CST-rwsem effectively behaves as a non-blocking lock. On the other hand, the vanilla version is idle 10.5% of the time because of its ineffective parking strategy even in the under-subscribed situation. In summary, both locks outperform the native rwsem by 1.2× at 120 cores. Metis is a mix of page-fault and mmap operation workload. It runs a worker thread on each core and mmaps 12 GB of anonymous memory for generating tables for map, reduce, and merge phases. <ref type="figure" target="#fig_6">Figure 5 (b)</ref> shows that both Cohort and CST locks outperform the original version by 1.6× as soon as the frequency of the write operation increases. Since the Cohort lock is non-blocking, it does not sleep, whereas the CST lock efficiently handles the under-subscribed case by not parking the threads, resulting in only 0.5% of idle time. Moreover, both locks batch readers, which improves the throughput of the workload. On the other hand, the original rwsem has 39% of the idle time because of its naive parking strategy and is 1.6× slower than the others at 120 cores. Psearchy is a parallel version of searchy that does text indexing. It is mmap intensive, which stresses the memory subsystem with multiple userspace threads. It does around 96,000 small and large mmap/munmap operations from 96,000 files with multiple threads, which taxes the writer side of the rwsem in the memory subsystem as well as the allocation of the inodes for those files in the virtual file system layer. <ref type="figure" target="#fig_6">Figure 5 (c)</ref> shows that CST-rwsem outperform both the Cohort and native locks by 1.4× at 120 cores. Cohort locks suffer from the static allocation because the kernel has to allocate 96,000 inodes for reading files into a per-core hash table of Psearchy, which not only stresses the memory allocator with large objects, but also suffers from ineffective scheduling because of the involvement of multiple instances of locks. Like prior workloads, the native lock suffers from the scheduler intervention after 45 cores, as it spends up 54.4% being idle, whereas the CST-rwsem is only idle for 11.4% of the time.</p><p>Summary. <ref type="figure" target="#fig_6">Figure 5</ref> shows the impact of scheduler intervention with increasing contention between readers and writers. With our efficient spinning strategy that checks its local load, CST locks have the same benefit as Cohort locks in the case of a highly contended but under-subscribed system. While Cohort locks improve the scalability of applications in highly contended and undersubscribed scenario, they hamper the scalability of applications that allocate multiple instances of locks ( <ref type="figure" target="#fig_6">Figure 5</ref> (c)). Unlike Cohort locks, CST locks consciously allocate memory with increasing socket count, which saves up to 10× of memory for each workload on a single socket, and 1.5 -9.1× at 120 cores. Thus, CST locks show that dynamic allocation is beneficial to real applications, while mitigating the memory bloat issue and maintaining an on-par performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Over-And Under-subscribed Cases</head><p>We compare the performance of CST locks with the kernel and Cohort locks in both an over-and under-subscribed  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance Breakdown</head><p>We evaluate how each component of CST contributes to the overall performance improvement by using an inkernel hash table that is protected by a single lock. To quantify the impacts of NUMA awareness and parking strategy, we keep the read-write ratio at 90/10%. We vary the thread count from 1 to 600 threads on 120 cores to show the effectiveness of our blocking strategy even in the over-subscribed scenario. <ref type="figure" target="#fig_8">Figure 7</ref> (a) shows the throughput of readers with increasing thread count. We evaluate three variants of the reader-side parking strategy: 1) global wake-up of parked readers (CST-Wake) and 2) distributed wake-up (CST-DWake  <ref type="figure" target="#fig_8">Figure 7</ref> (b) presents another micro-benchmark results in which we update a single cache line by multiple threads from 120 to 600. We compare the Linux's mutex with the Cohort lock and two CST locks: 1) CST-WA is the blocking lock that modifies the status invariant and wakes up all parked waiters in a socket, and 2) CST-WS is also blocking but wakes up the selected number of parked waiters in which the number of wake-ups is equal to the number of hardware threads in a socket. At 120 threads, the native mutex suffers from cache-line bouncing and later from contention on its global parking_list while still maintaining a permissible performance beyond 120 threads. On the other hand, all CST variants address the cache-line bouncing issue for 120 threads. However, the Cohort lock suffers from spinning at higher core count since the waiters preempt the lock-holder after 120 threads. CST-WA and CST-WS address the limitation of the Cohort lock and maintain on-par performance even beyond 120 cores. CST-WS further mitigates the lock-holder preemption problem, since it does not wake up all waiters in one shot inside a socket, which has slightly higher throughput than CST-WA. In summary, CST-WS outperforms the Linux version by 1.7× at 6× over-subscription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Critical Section Latency</head><p>We evaluate the lock/unlock pair latency of rwsem to gauge the effectiveness of CST against the Linux version. <ref type="table" target="#tab_7">Table 1</ref> shows that while NUMA-aware lock is a better fit for multiple readers/writers, it suffers in the low contention scenario because of the costly operation of finding the snode for readers and multiple atomic operations to obtain the lock, which we can improve with the hysteresis-based technique <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Limitations</head><p>The current design of CST locks can introduce starvation in two cases: 1) re-queueing of the waiters after they are parked, and 2) the writer-preferred version of the rwsem. Although, in theory, we can devise a non-blocking algorithm that mitigates the overhead of costly scheduler interaction for the first case, we have not come across such an algorithm in practice and the CST lock is a better alternative than the current mutex that also suffers from the same starvation issue. We believe that this can be a plausible future research direction both in the terms of synchronization primitives and lightweight scheduling. For   CST-rwsem, we choose a writer-preferred version because it batches readers, thereby improving the throughput of the application, which is similar to the design ideology of the Linux rwsem <ref type="bibr" target="#b36">[37]</ref>. We can address this limitation by exactly adopting the writer-preferred version of the read-write Cohort lock <ref type="bibr" target="#b3">[4]</ref>. Even though CST locks outperform both Cohort locks and the Linux mutex, we can further scale applications by using combining <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref> or the remote-core locking approach <ref type="bibr" target="#b22">[23]</ref>. However, the only caveat with these approaches is that we need to rewrite some parts of the OS, which is not easy due to the large code base and complicated lock usage. Another area in which we can improve the performance of CST locks is the latency in low contention <ref type="table" target="#tab_7">(Table 1)</ref>. We are investigating the use of hardware transactional memory (TSX) to acquire and release the locks in a transaction as in prior work <ref type="bibr" target="#b4">[5]</ref>. Although CST locks cannot completely replace all of the locks, they are beneficial to a few data structures that are critical and contend as much as inode, mm, dentry, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Synchronization primitives are the basic building blocks of any parallel application, out of which the blocking synchronization primitives are designed to handle both overand under-subscribed scenarios. We find that the existing primitives have sub-optimal performance for machines with large core count. They suffer either from cache-line contention or scheduler intervention in both scenarios, and are oblivious to the existing NUMA machines. In this work, we present scalable NUMA-aware, memoryefficient blocking primitives that exploit the NUMA hardware topology along with scheduling-aware parking and wake-up strategies. We implement CST-mutex and CST-rwsem, which provide the same benefit of existing non-blocking NUMA-aware locks in under-subscribed scenario while maintaining similar peak performance in over-subscribed cases. Our code is available here: https://github.com/sslab-gatech/cst-locks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1: Impact of NUMA-aware locks on a file-system microbenchmark that spawns processes to create new files in a shared directory (MWCM in [27]). It stresses either the mutex or the writer side of the read-write semaphore and memory allocation. Figure (a) presents the results up to 120 threads on a 120-core machine and Figure (b) shows the memory utilized by locks during the experiment. Here, Vanilla is Linux's native version, and Cohort is an in-kernel ported version of NUMA-aware locks [4, 11], and our NUMA-aware lock (CST).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A CST-mutex is active on sockets: 1, 2, and 3. Currently, socket 1 is being served. T1 now holds the lock (L); T3 is spinning for its turn (UW: unparked waiting); T2, T4, and T5 are sleeping (PW: parked waiting) until a lock holder wakes them up. A lock holder, T1, will pass the lock to T3, which is spinning, skipping the sleeping T2, to minimize the overhead of wake-up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Figure (a) shows the passing of a lock to a spinning waiter inside a per-socket structure (snode). (i) T1 is the current lock holder, and T2 and T3 are in the waiting_list, and qtail points to the qnode of T3. (ii) T2 times out, successfully CASes its state from UW to PW, and adds itself to the parking_list. (iii) T1 exits the critical section. It tries to pass the lock to T2, but fails to CAS the state of T2 from UW to L. T1 goes to T3 via next pointer of T2, successfully CASes the state of T3 from UW to L, and leaves the unlock phase. Figure (b) shows the passing of the lock to a parked waiter in the parking_list. (i) T3 (lock holder) is in the unlock phase. It finds that waiting_list is empty as T2 is in the parking_list. T3 successfully CASes qtail to NULL. (ii) Now, T3 checks for parked waiters in parking_list, finds T2, and updates the state of T2 from PW to R. (iii) Since stail is NULL and there are no prior waiters, T2 sets its state to L and acquires the local lock, and later goes to acquire the global lock. Figure (c) illustrates the passing of the lock to a parked waiter at the end of the waiting_list. (i) On exiting the critical section, T2 fails to CAS the state of T3 to L, since it is parked. (ii) T2 then explicitly SWAPs the state of T3 to L and wakes it up. T3 now holds the local lock and goes to acquire the global lock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Pseudo-code of CST-mutex (lines 1 -74), CST-rwsem (lines 108 -148), and their parking/wake up (lines 75 -106). We use three atomic instructions: CAS(addr,new,old) atomically updates the value at addr to new and returns True if the value at addr is old. Otherwise, it returns False without updating addr. SWAP(addr,val) atomically writes val to addr and returns the old value at addr. FAA(addr,val) atomically increases the value at addr by val.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Impact of synchronization primitives on the scalability and memory utilization for three applications: (a) Histogram, (b) Metis, and (c) Psearchy with Linux's native rwsem (Vanilla), Cohort read-write lock, and CST-rwsem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Impact of synchronization primitives on the scalability of micro-benchmarks [27] that affect file system operations such as (a) overwriting a block in a shared file, (b) creating, and (c) enumerating files in a shared directory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Two micro-benchmarks to illustrate the performance impact of various techniques employed by CST-rwsem and CST-mutex. Figure (a) represents the lookup performance of a concurrent hash table for 10% writes, which uses rwsem. Figure (b) shows the time taken to update a single cache line by holding a mutex with increasing thread count.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Latency</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>, or else it goes to acquire it. Acquire global lock: T initializes its snode (line 28) and adds itself to the waiting_list (line 29). It then acquires the global lock if there is no waiter (line 31), or waits until its predecessor snode passes the lock (line 35).</head><label></label><figDesc></figDesc><table>Acquire local lock: A thread T starts by first finding (or 
adding if not present) its snode (line 2). Unlike the Cohort 
lock protocol, T tries to acquire the local lock (line 4) in 
an infinite for loop because it may restart the protocol 
after being parked. In the local lock phase, T initializes 
its qnode (line 10) and then SWAPs the qtail of snode 
with qnode. It then acquires the global lock when no 

waiters are present. Otherwise, T spins on its status, which 
changes to either the L or R state (line 17). While waiting, 
T initiates the parking protocol (lines 75 -86) on timing 
out, where it tries to CAS the status of qnode from UW to 
PW. T returns back on failure; otherwise, it adds itself to 
the parking_list and schedules out. Later, when a lock 
holder wakes it up, it resumes (line 83) and either acquires 
the local lock or restarts the protocol, depending on its 
updated status. If T has L status after being woken up, it 
goes on to acquire the global lock as the previous lock 
holder releases the global lock before waking up sleeping 
waiters. To mitigate cache-line bouncing, T checks for the 
global lock flag (line 5). If not set, T already holds the global lockOn timing 
out, while spinning (line 35), T CASes status of snode from 
UW to PW and schedules out (line 38</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Empty critical section latency for rwsem. 

</table></figure>

			<note place="foot" n="1"> 64-byte cache line size × (3 cache lines for the socket lock × 8 sockets + 1 cache line for the top lock).</note>

			<note place="foot" n="2"> The static allocation of all snodes increases the inode structure size by 3.8× and mm_struct size by 2.6× in an eight-socket machine.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgment</head><p>We thank the anonymous reviewers and our shepherd, Jean-Pierre Lozi, for their helpful feedback. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Analysis of Linux Scalability to Many Cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd-Wickizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pesterev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 9th USENIX Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>OSDI</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Non-scalable locks are dangerous</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd-Wickizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An Overview of Kernel Lock Improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bueso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Norton</surname></persName>
		</author>
		<ptr target="https://events.linuxfoundation.org/sites/events/files/slides/linuxcon-2014-locking-final.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">NUMA-aware Reader-writer Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Luchangco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)</title>
		<meeting>the 18th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)<address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contention-conscious, Locality-preserving Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chabbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)</title>
		<meeting>the 21st ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">High Performance Locks for Multi-level NUMA Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chabbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)</title>
		<meeting>the 20th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">When Less is More (LIMO):Controlled Parallelism For improved Efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahlke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 International Conference on Compilers, Architectures and Synthesis for Embedded Systems, CASES &apos;12</title>
		<meeting>the 2012 International Conference on Compilers, Architectures and Synthesis for Embedded Systems, CASES &apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Re: [regression, 3.16-rc] rwsem: optimistic spinning causing performance degradation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chinner</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2014/7/3/25" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malthusian Locks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1511.06035</idno>
		<ptr target="http://arxiv.org/abs/1511.06035" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flat-combining NUMA Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures, SPAA &apos;11</title>
		<meeting>the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures, SPAA &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lock Cohorting: A General Technique for Designing NUMA Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)</title>
		<meeting>the 17th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A persistent key-value store for fast storage environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
		</author>
		<ptr target="http://rocksdb.org/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting the Combining Synchronization Technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fatourou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Kallimanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)</title>
		<meeting>the 17th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multicore Locks: The Case is Not Closed Yet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guiroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quéma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (ATC)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="649" to="662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibm</forename><surname>Ibm K42</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Group</surname></persName>
		</author>
		<ptr target="http://researcher.watson.ibm.com/researcher/view_group.php?id=2078" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="http://ark.intel.com/products/93790/Intel-Xeon-Processor-E7-8890-v4-60M-Cache-2_20-GHz" />
		<title level="m">Xeon Processor E7-8890 v4 (60M Cache, 2.20 GHz). Intel</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Decoupling Contention Management from Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 15th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The open group base specifications issue 7</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Leroy</surname></persName>
		</author>
		<ptr target="http://pubs.opengroup.org/onlinepubs/9699919799/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable Read-mostly Synchronization Using Passive Reader-writer Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2014 USENIX Annual Technical Conference (ATC)<address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">aim7 performance regression by commit 5a50508 report from LKP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2013/1/29/84" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">qspinlock: Introducing a 4-byte queue spinlock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/582897/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">locking/mutex: Enable optimistic spinning of lock waiter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/696952/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and Portable Locking for Multicore Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Lozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lawall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Muller</surname></persName>
		</author>
		<idno>13:1-13:62</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Hierarchical CLH Queue Lock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Luchangco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Parallel Processing</title>
		<meeting>the 12th International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="801" to="810" />
		</imprint>
	</monogr>
	<note>Euro-Par&apos;06</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithms for Scalable Synchronization on Shared-memory Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Server</surname></persName>
		</author>
		<ptr target="http://www.microsoft.com/en-us/server-cloud/products/sql-server/features.aspx" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding Manycore Scalability of File Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (ATC)<address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Linux rwsem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="http://www.makelinux.net/ldd3/chp-5-sect-3" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generic Mutex Subsystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bueso</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/locking/mutex-design.txt" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Linux percpu-rwsem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nesterov</surname></persName>
		</author>
		<ptr target="http://lxr.free-electrons.com/source/include/linux/percpu-rwsem.h" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<ptr target="http://www.oracle.com/us/products/servers-storage/sparc-m7-16-ds-2687045.pdf" />
		<title level="m">Data Sheet: SPARC M7-16 Server. Oracle</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Executing parallel programs with synchronization bottlenecks efficiently</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yonezawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Workshop on Parallel and Distributed Computing for Symbolic and Irregular Applications (PDSIA)</title>
		<meeting>International Workshop on Parallel and Distributed Computing for Symbolic and Irregular Applications (PDSIA)</meeting>
		<imprint>
			<date type="published" when="1999-07" />
			<biblScope unit="page" from="182" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating MapReduce for Multi-core and Multiprocessor Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ranger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raghuraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Penmetsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA &apos;07</title>
		<meeting>the 2007 IEEE 13th International Symposium on High Performance Computer Architecture, HPCA &apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SAP HANA 2: the transformer</title>
		<ptr target="http://hana.sap.com/abouthana.html" />
	</analytic>
	<monogr>
		<title level="j">SAP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-blocking Timeout in Scalable Queue-based Spin Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<idno>1-58113-485-1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-first Annual Symposium on Principles of Distributed Computing, PODC &apos;02</title>
		<meeting>the Twenty-first Annual Symposium on Principles of Distributed Computing, PODC &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable Queue-based Spin Locks with Timeout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)</title>
		<meeting>the 6th ACM Symposium on Principles and Practice of Parallel Programming (PPOPP)<address><addrLine>Snowbird, Utah</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06" />
			<biblScope unit="page" from="44" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">PATCH] rwsem: steal writing sem for better performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2013/2/5/309" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Linux Wait Queues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torvalds</surname></persName>
		</author>
		<ptr target="http://www.tldp.org/LDP/tlk/kernel/kernel.html#wait-queue-struct" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The Linux Kernel Archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torvalds</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Viro</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/684089/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>parallel lookups</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Spark: Cluster Computing with Working Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud&apos;10</title>
		<meeting>the 2Nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Survey of Scheduling Techniques for Addressing Shared Resources in Multicore Processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuravlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Prieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
