<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Managing Array of SSDs When the Storage Device is No Longer the Performance Bottleneck</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungseok</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNIST (Ulsan National Institute of Science and Technology)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNIST (Ulsan National Institute of Science and Technology)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UNIST (Ulsan National Institute of Science and Technology)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Managing Array of SSDs When the Storage Device is No Longer the Performance Bottleneck</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the advent of high performing NVMe SSDs, the bottleneck of system performance is shifting away from the traditional storage device. In particular, the I/O stack software layers have already been recognized as a heavy burden on the overall I/O. Efforts to alleviate this burden have been considered <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Recently, the spotlight has been on the CPU. With computing capacity as well as the means to get the data to the processor now being limited, recent studies have suggested that processing power be pushed into where the data is residing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8]</ref>. With devices such as 3D XPoint <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b12">11]</ref> in the horizon, this phenomenon is expected to be aggravated.</p><p>In this paper, we focus on another component related to such changes. In particular, it has been observed that the bandwidth of the network that connects clients to storage servers is now being surpassed by storage bandwidth <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b14">13]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the changes that are happening. We observe that the changes in the storage interface is allowing storage bandwidth to surpass that of the network. As shown in <ref type="table" target="#tab_0">Table 1</ref>, recent developments in SSDs have resulted in individual SSDs providing read and write bandwidth in the 5GB/s and 3GB/s range, respectively, which surpasses or is close to that of 10/25/40GbE (Gigabit Ethernet) that comprise the majority of networks being supported today.</p><p>Based on this observation, in this paper, we revisit the organization of disk arrays. Specifically, we target write performance in all-flash arrays, which we interchangeably refer to as SSD arrays, that are emerging as a solution for high-end storage <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. As shown in <ref type="table" target="#tab_1">Table 2</ref>, most major storage vendors carry such a solution and these products employ plenty of SSDs to achieve large capacity and high performance <ref type="bibr">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref>. <ref type="figure">Figure 2</ref> shows how typical all-flash arrays would be connected to the network and the host. Our goal is to provide high, sustained, and consistent write performance in such a storage environment. Even though SSD arrays employ a large number of SSDs, reports have shown that contrary to expectations, they do not provide high, stable performance. In contrast, large latency variations and oscillating bandwidth for I/O requests are commonly reported <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. The major source of such ineffectiveness is the effect of garbage collection (GC) operations within the SSD devices <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref>. <ref type="figure" target="#fig_1">Figure 3</ref> shows our preliminary experimental results where we observe the write bandwidth of a RAID-0 configured SSD array comprising four identical 400GB capacity PCIe SSDs (with specifications of 128KB sequential read and write bandwidth of 2.2GB/s and 900MB/s, respectively) running the synthetic FIO workload <ref type="bibr" target="#b28">[28]</ref>. The measurements are made at the storage host, without going through the network, and the numbers reported are averages of 5 executions of the same workload with  Figure 2: Organization of a typical network connected all-flash array measurements starting with clean empty SSDs (cleaned for every execution). We observe that, at first, the performance is at its peak at roughly 3.5GB/s, which is quite close to the ideal 3.6GB/s write bandwidth. However, after some time, performance drops considerably, dropping even below the line along the x-axis at 1.25GB/s, which is the bandwidth of today's typical 10GbE network. This drop is due to GC. After the performance drop, performance stabilizes at low bandwidth and then increases, but still oscillates considerably. We observe that with just four SSDs the storage system bandwidth can be considerably higher than typical network bandwidth. However, we also observe that due to fluctuations in SSD performance due to GC, performance can drop considerably. In conclusion, we need to find a way to stabilize performance such that services are provided in a steady and expected manner.</p><p>Efforts have been made to alleviate such performance variations at various layers of the system <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>. Despite such effort, performance instability due to GC still remains a problem since GC is a necessary operation for flash memory based storage devices.</p><p>In this paper, we present a storage organization for SSD arrays that totally eliminates garbage collection (GC) within an SSD (that is, internal GC does not occur). This is based on two key, simple observations: 1) host-end storage systems are connected through the network, and 2) the storage device is no longer the bottleneck, but rather, the network bandwidth is. These two observations imply that network bandwidth performance should be the target performance goal for SSD arrays. We show that this can be achieved, not as occasional peak performance, but as sustained, consistent performance as we eliminate all internal GC.</p><p>The remainder of this paper is organized as follows. In Section 2, we discuss the organization that we propose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Serial Management of SSD Arrays</head><p>In the days of HDDs, where disk bandwidth was limited, it was logical to organize disks as an array so that individual disks could be accessed in parallel to increase bandwidth. However, with contemporary SSDs, as individual device bandwidth start to surpass network bandwidth, parallel organization of many SSDs may no longer be advantageous.</p><p>Based on this observation, we propose to organize the array of SSDs in a serial manner. This organization allows only one SSD to service all write requests, though SSDs in the array will take turns as the write servicing SSD. As we will show later, such management allows clients to observe consistent SSD write bandwidth performance. The goal of such an organization is to provide, at all times, 1) consistent unfluctuating SSD write performance, 2) at peak network bandwidth performance. This can be achieved by forbidding GC within SSDs. We now discuss the organization in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design</head><p>In describing our technique, let us for now assume that the ideal SSD write bandwidth as specified on the product is greater than the network bandwidth. This will generally be true with contemporary high-end SSDs and network configurations as we discussed earlier (for example, 3 GB/s for PCIe SSDs shown in <ref type="table" target="#tab_0">Table 1</ref> versus 1.25 GB/s for 10GbE). If this is not the case, we can increase the number of the so-called front-end SSDs, which we describe later, to serve our purpose. For simplicity, but without loss of generality, let us assume that we have one front-end SSD satisfying the above assumption and concentrate on writes. In this case, the network will always be the bottleneck if the SSD can be used at its maximum. This is what we strive for. Let us assume that we have n SSDs, where n is the number of SSDs in an all-flash array, for example, as specified in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Given these assumptions, the array of SSDs in our proposed organization can be viewed as a serial sequence of SSDs, where data is written to only one SSD at a time.</p><p>As the SSD bandwidth is higher than the network bandwidth, there is no loss of performance as one SSD can absorb all write requests. We refer to the SSD serving the write request as the front-end, while the rest of the SSDs are referred to as the back-end as depicted in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. Logically, such an organization can be viewed as a sequential list of blocks as depicted in <ref type="figure" target="#fig_2">Figure 4</ref>(b). However, physically, each SSD is controlled independently, which is different from a typical log-structured layout.</p><p>Let us now see how we can manage the SSDs to achieve consistent, maximal performance. Assume at first that all SSDs are empty. As writes come in, all data are written only in log-structured manner without any GC. Hence, the SSD can be written with maximum performance. However, the front-end SSD will eventually fill up with a mixture of valid and invalid flash pages. Then, the front-end is replaced with a new, clean SSD. (Initially, the selected SSDs will be empty. However, with time, the selected SSD will contain a mix of valid and invalid data.) Evidently, if this keeps going, all the SSDs will fill up. Our past experience from logstructured designs tell us that some form of garbage collection is required.</p><p>Given such a scenario, let us now discuss how our proposed organization is different from the traditional logstructured approach.</p><p>• First, only the front-end SSD is actively performing writes. All back-end SSDs are free to serve read requests if there are any targeted towards these SSDs ( <ref type="figure" target="#fig_3">Figure 5(a)</ref>).</p><p>• Second, no GC occurs within the frond-end SSD so that performance is maximized. • Third, when the front-end SSD fills up, it relieves its role as a front-end SSD to one of the back-end SSDs ( <ref type="figure" target="#fig_3">Figure 5(b)</ref>).</p><p>• Fourth, GC of an SSD occurs not within SSDs, but only among back-end SSDs. Hence, we refer to this GC as external GC (xGC) in contrast to the traditional internal GC that happens within an SSD. . When all valid data is moved, then the old front-end is cleaned by issuing a TRIM command to the entire SSD ( <ref type="figure" target="#fig_3">Figure 5(c)</ref>). This makes GC a deterministic and simple activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Benefits of Serial Organization</head><p>There are two major benefits to our approach. The first is on performance and other is on the efficiency of the SSD. We discuss each of these below. The performance benefits of serial management is as follows, which we quantify in Section 3. Writes: As writes are being done in a log-structured manner, that is, a sequential manner, and no GC occurs performance is optimized. Reads: As writes to the front-end SSD are of recent data, most of the reads targeted towards the frontend SSD will be absorbed by the cache in either the client and/or the host without affecting the front-end SSD. Similarly, reads targeted towards the backend SSDs will not be affected by the front-end SSD write activities. Furthermore, as read latency is greatly affected by GC, and as we rid of internal GC, read performance is stabilized. The second benefit offered by serial management is the simplicity that it brings to the design of the FTL (Flash Translation Layer) within the SSD. Recall that most high-end commodity SSDs employ large amounts of DRAM as buffer space and require substantial spare flash memory to be used as OPS (Over Provisioning Space) <ref type="bibr" target="#b34">[34]</ref>. In addition, for high performance, techniques such as page-level mapping must be employed over simpler, resource thrifty block-level mapping within the FTL. These kinds of restrictions impose a burden on the SSD controller in terms of cost and performance. As there is no need to perform internal GC (though in practice, we believe some form of minimal internal GC may be needed), serial management alleviates these burden on the FTL in the following manner (though these aspects are not quantified in this study):</p><p>1. Less internal resources are required for SSDs used in our organization. This is because block-level mapping, with large block sizes, may be adopted as write requests always arrive in append only manner. Hence, less memory space is required to manage the blocks within the FTL.</p><p>2. SSD capacity increases, in effect, as with no internal (or minimal) GC, no (or less) OPS space is needed to perform GC.</p><p>3. Longer lifetime can be expected for SSDs used in our organization. This is because with internal GC the same valid page may be copied multiple times within an SSD if it, by chance, happens to be in the victim block multiple times. In contrast, with our technique a valid page is written once and never copied within an SSD as there is no internal GC.</p><p>4. Wear-leveling becomes simple and even for all blocks with serially managed SSDs resulting in more predictable SSD lifetime management. This is so as an erasure happens only when a TRIM command is issued to the entire SSD. Hence, all blocks wear out evenly for the entire lifetime of the SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Previous Work on Serial Management</head><p>Most previous studies on an array of disks have attempted to increases access parallelism such as in RAID configurations. In contrast, the Gecko study by Shin et al. takes a similar approach as ours in that they consider serial management of storage devices <ref type="bibr" target="#b35">[35]</ref>. Gecko views the chain of HDDs as a log with new writes being made to the tail of the log. This is, in essence, similar to the traditional log-structured approach, and the general differences between the traditional log-structured approach and ours was described in Section 2.1. Some other specific differences are as follows. Gecko was designed for HDD, while ours is for high-end SSDs that have bandwidth similar or surpassing that of the network. Gecko does not forbid the replacement of HDDs with SSDs, but then, does not consider the peculiarities of SSDs. In particular, our technique completely separates GC writes from first-class writes allowing firstclass writes to make full use of the bandwidth. Gecko, on the other hand, intermixes them, which incurs contention at the disk that holds the tail. Furthermore, GC writes in Gecko are for segment cleaning and is different from SSD internal GC, which is not controlled in Gecko. Hence, the effect of internal GC, which is the key performance distracting factor and which our technique is obviating, still remains with Gecko.</p><p>One similarity between the two is how metadata is managed. Similarly to Gecko, we maintain a logical to physical address map as well as an inverse map in memory. Such mappings are kept for each SSD, and they occupy around 0.2% of the total capacity of the SSDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Environment and Results</head><p>In this section, we discuss the initial implementation of our proposed serial management scheme. We present the experimental setup and the benchmarks used in our evaluations, and then discuss the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head><p>Experimental Platform: For the storage server, we use a Dell R730 equipped with a Xeon E5-2609 CPU, 64GB DRAM, and four Intel 750 400GB NVMe SSDs. Depending on the experiment target, the four SSDs are configured in parallel manner as a volume of RAID-0 or in the serial manner that we propose resulting in 1.6TB storage capacity. The RAID-0 volume with a chunk size of 64KB is created using LVM2 <ref type="bibr" target="#b36">[36]</ref>. For the host system, we use an x86 compatible PC with an i5-6600k CPU, 16GB DRAM, and local storage. The host is directly connected to the storage server via a 10Gb/s Ethernet card. The operating systems for the storage server and the host are Linux kernel-4.4.43 and kernel-4.3.3, respectively, and the Ext4 file system is used.</p><p>Benchmark Parameters: Two types of workloads are used for our experiments. In Section 3.2, we use a simple FTP application to isolate the effect of the network. For this workload, we make use of 10 threads with each thread transmitting a 10 GB file.</p><p>In Section 3.3, where the stability of performance is considered, we use the synthetic FIO benchmark workload. The benchmark is executed with one thread and the thread issues random writes with a queue depth of 32 and to 10 files. Over the entire experiments, we make use of two different FIO workloads. The first is the aging workload that writes for 15 minutes, where the block size is 256KB with a footprint of 1200GB. After aging, we use a second workload where 64KB sized random writes are issued for 30 minutes on a 200GB footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Network as a Bottleneck</head><p>In this section, we observe how the network affects the performance of the parallel configuration, that is, RAID-0 versus the serial configuration that we propose. For both cases, we connect the storage array to the host via 10Gb/s network connection and have 10 threads in the host individually send 10GB files to write to storage via <ref type="figure">Figure 6</ref>: Performance of RAID-0 and serially configured storage connected to the host via 10GbE network the network. We observe the throughput at the storage layer as writing a total of 100GBs saturates the network and storage bandwidth. <ref type="figure">Figure 6</ref> is what we observe at the storage end for RAID-0 and the serial configuration that we propose with the y-axis being the throughput in MB/s. We see that for RAID-0 (the dotted line) the bandwidth fluctuates considerably, periodically reaching the peak and zero values. This phenomenon occurs because as the data arrives through the network, RAID-0 processes the requests in bulks; once that is done, it has to wait for the new data to arrive through the network resulting in zero bandwidth. However, the results reveal that on average, 900MB/s throughput is maintained at the storage layer. This is below the perfect available network bandwidth, but very close to the practical peak bandwidth.</p><p>In contrast, for our serial configuration (red solid line), we also observe fluctuations but at a much smaller scale than the RAID-0 case. We find that the average bandwidth is around 890MB/s for this case, which is slightly below the 900MB/s we saw for RAID-0. However, recall that for RAID-0 all four SSDs are simultaneously being used whereas for our proposed configuration only one is being used. We see that matching the storage bandwidth with the network bandwidth is a logical choice. While this was not possible with past disk technology, with current high-end SSDs, this is easily achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Sustained, Consistent Performance</head><p>We showed in the previous section that the network is the bottleneck with high bandwidth SSDs. In this section, we remove the network aspect, which is the source of performance fluctuation in the previous experiments, and concentrate on storage performance. Recall from <ref type="figure" target="#fig_1">Figure 3</ref> how RAID-0 performance drops after some time due to GC operations. We conduct a similar experiment with our proposed serial configuration on a locally connected storage end with the workload and setting as described in Section 3.1. The average throughput results obtained  <ref type="figure" target="#fig_5">Figure 7</ref>. The line in the middle demarcates the results during and after aging. The demarcating line represents a sudden drop in throughput that is due to a slight pause in generating the workload during the experiments.</p><p>Overall, we see that performance during and after aging is consistent. Some points of interest in <ref type="figure" target="#fig_5">Figure 7</ref> are as follows. First, we see that there are slight differences in the bandwidth observed when front-end and back-end SSDs are swapped as a new front-end is selected. This is because most commercial SSDs do not come with exactly the same performance. We find that performance changes according to the performance characteristics of each SSD selected as the front-end. This, we find, is consistent for each of the SSDs. Second, we see that xGC does not affect performance. Even though GC is happening, this has no bearing on performance. Finally, we attain overall high, sustained performance that we set out to attain even though we do observe occasional small dips in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Based on the observation that storages devices are no longer the performance bottleneck, but that the network is, this paper proposes a serial organization for SSD arrays. Such serial organization allowed us to completely avoid internal garbage collection of SSDs such that high performance of write requests can be sustained. Experimental results showed that this is a promising approach. We are currently looking into specific details concerning the implementation of this approach and other possible design issues that we might have overlooked. To consider the scalability issue, we also plan to implement a prototype all-flash array with many more SSDs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Network and storage bandwidth growth trend</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Observed bandwidth serving the FIO benchmark workload on a RAID-0 configured SSD array of 4 PCIe SSDs Then, in Section 3, we discuss the experimental setup as well as the results of the measurements. Finally, we conclude the paper with Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overall architecture of proposed organization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sequence for handling write requests with xGC</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figures 5 (</head><label>5</label><figDesc>b) and (c) show the sequence in which GC occurs. Once the front-end SSD relieves its role to a back-end SSD, the new front-end absorbs the writes, while the valid data in the old front-end SSD are moved to one of the back-end SSDs as depicted in Figure 5(b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Proposed organization performance results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Comparison of commodity SSDs</head><label>1</label><figDesc></figDesc><table>Product 
SEQ R/W 
RND R/W 
Interface 
(GB/s) 
(IOPS) 

Intel 
P3608 
5/3 
0.8M/0.15M 
PCIe 
P3710 
0.5/0.5 
85K/45K 
SATA 

Samsung 
PM1725a 
6.4/3 
1M/170K 
PCIe 
PM1633a 
1.2/0.9 
190K/30K 
SAS 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : All-flash array products</head><label>2</label><figDesc></figDesc><table>Solid Fire 
EMC 
Pure 
Nimble 
(NetApp) 
Storage 
Storage 
Model 
SF19210 
6X-Brick 
M70 
AF9000 
Capacity 
20TB 
240TB 
136TB 
500TB 
# of SSDs 
10 
150 
68 
96 
IOPS 
100K 
900K 
370K 
350K 
(Random) 
-
@ 8KB 
@ 32KB 
-
Network 
20Gb 
240Gb 
40Gb 
40Gb 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Moneta: A High-Performance Storage Array Architecture for Next-Generation, Nonvolatile Memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arup</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><forename type="middle">I</forename><surname>Mollow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;10</title>
		<meeting>the 43rd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When Poll is Better Than Interrupt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><forename type="middle">B</forename><surname>Minturn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Refactor, Reduce, Recycle: Restructuring the I/O Stack for the Future of Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">and Philippe Bonnet. I/O Speculation for the Microsecond Era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Bjorling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference, ATC&apos;14</title>
		<meeting>the USENIX Annual Technical Conference, ATC&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="475" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunok</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active Disk Meets Flash: A Case for Intelligent SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International ACM Conference on International Conference on Supercomputing, ICS &apos;13</title>
		<meeting>the 27th International ACM Conference on International Conference on Supercomputing, ICS &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Query Processing on Smart SSDs: Opportunities and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyoung</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang-Suk</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jignesh</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;13</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1221" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Biscuit: A Framework for Near-data Processing of Big Data Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boncheol</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duck-Ho</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Insoon</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Uk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moonsang</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duckhyun</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture, ISCA &apos;16</title>
		<meeting>the 43rd International Symposium on Computer Architecture, ISCA &apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="153" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Willow: A Userprogrammable SSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudharsan</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gahagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sundaram</forename><surname>Bhaskaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Bunker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arup</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;14</title>
		<meeting>the 11th USENIX Conference on Operating Systems Design and Implementation, OSDI&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="67" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel Optane Technology</surname></persName>
		</author>
		<ptr target="http://www.intel.com/content/www/us/en/architecture-and-technology/intel-optane-technology.html/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Merritt</surname></persName>
		</author>
		<ptr target="http://www.eetimes.com/document.asp?doc_id=1328682" />
		<title level="m">3D XPoint Steps Into the Light</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Intel&apos;s 140GB Optane 3D XPoint PCIe SSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Cutress</surname></persName>
		</author>
		<ptr target="http://www.anandtech.com/show/10604/intels-140gb-optane-3d-xpoint-pcie-ssd-spotted-at-idf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Davis</surname></persName>
		</author>
		<ptr target="https://www.datanami.com/2016/11/10/network-new-storage-bottleneck/" />
		<title level="m">The Network is the New Storage Bottleneck</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How Networking Affects Flash Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunna</forename><surname>Marripudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Llker Cebeli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Flash Memory Summit</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kenchammana-Hosekote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Conference on File and Storage Technologies, FAST&apos;16</title>
		<meeting>the 14th USENIX Conference on File and Storage Technologies, FAST&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="263" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Purity: Building Fast, Highly-Available Enterprise Flash Storage from Commodity Components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Colgrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cary</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Tamches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;15</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data, SIGMOD&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1683" to="1694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Purestorage All-Flash Array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Purestorage</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="http://info.solidfire.com/rs/solidfire/images/SolidFire_ProductDatasheet.pdf" />
	</analytic>
	<monogr>
		<title level="j">SolidFire. SolidFire All-Flash Array</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Nimble All-Flash Array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nimble</surname></persName>
		</author>
		<ptr target="https://www.nimblestorage.com/technology-products/all-flash-array-specifications/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SDF: Software-defined Flash for Web-scale Internet Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiding</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS&apos;14</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="471" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enabling Cost-Effective Flash based Caching with an Array of Commodity SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongseok</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunjae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choulseung</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Middleware Conference, Middleware&apos;15</title>
		<meeting>the 16th Annual Middleware Conference, Middleware&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flash on Rails: Consistent Flash Performance through Redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Skourtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Maltzahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference, ATC&apos;14</title>
		<meeting>the USENIX Annual Technical Conference, ATC&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="462" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Triple-A: A Non-SSD Based Autonomic All-Flash Array for High Performance Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myoungsoo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonil</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmut Taylan</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS&apos;14</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="441" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">HPDA: A Hybrid Parity-Based Disk Array for Enhanced Performance and Reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Software Orchestrated Flash Array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weafon</forename><surname>Tzi Cker Chiueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Chiang</forename><surname>Tsao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiang-Fang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-Nan</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengding</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Systems and Storage Conference, SYSTOR&apos;14</title>
		<meeting>the 7th ACM International Systems and Storage Conference, SYSTOR&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding Intrinsic Characteristics and System Implications of Flash Memory Based Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIG-METRICS/International Joint Conference on Measurement and Modeling of Computer Systems, SIG-METRICS&apos;09</title>
		<meeting>the 11th ACM SIG-METRICS/International Joint Conference on Measurement and Modeling of Computer Systems, SIG-METRICS&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards SLO Complying SSDs Through OPS Isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on File and Storage Technologies, FAST&apos;15</title>
		<meeting>the 13th USENIX Conference on File and Storage Technologies, FAST&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">FIO: Flexible I/O Tester</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<ptr target="https://github.com/axboe/fio" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sang-Won Lee, and Young Ik Eom. SFS: Random Write Considered Harmful in Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangnyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjin</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="12" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">F2FS: A New File System for Flash Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changman</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on File and Storage Technologies, FAST&apos;15</title>
		<meeting>the 13th USENIX Conference on File and Storage Technologies, FAST&apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="273" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DFTL: a Flash Translation Layer Employing Demand-based Selective Caching of Pagelevel Address Mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvan</forename><surname>Urgaonkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS&apos;09</title>
		<meeting>the 14th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS&apos;09</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reducing SSD Read Latency via NAND Flash Program and Erase Suspension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="10" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Preemptible I/O Scheduling of Garbage Collection for Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><forename type="middle">M</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarp</forename><surname>Oral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongman</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="260" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Understanding SSD Overprovisioning. Flash Memory Summit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gecko: ContentionOblivious Disk Arrays for Cloud Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Yong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Marian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakim</forename><surname>Weatherspoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on File and Storage Technologies, FAST&apos;13</title>
		<meeting>the 11th USENIX Conference on File and Storage Technologies, FAST&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wiki</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Logical_Volume_Manager_" />
		<imprint/>
	</monogr>
	<note>Linux</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
