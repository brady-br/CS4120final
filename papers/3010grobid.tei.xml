<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fair Resource Allocation in Consolidated Flash Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonil</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Myoungsoo Jung KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penn</forename><surname>State</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Myoungsoo Jung KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvan</forename><surname>Urgaonkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Myoungsoo Jung KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penn</forename><surname>State</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Myoungsoo Jung KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Myoungsoo Jung KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penn</forename><surname>State</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Myoungsoo Jung KAIST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fair Resource Allocation in Consolidated Flash Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We argue that, along with bandwidth and capacity, lifetime of flash devices is also a critical resource that needs to be explicitly and carefully managed, especially in emerging consolidated environments. We study the resulting multi-resource allocation problem in a setting where &quot;fairness&quot; across consolidated workloads is desired. Towards this, we propose to adapt the well-known notion of dominant resource fairness (DRF). We empirically show that using DRF with only band-width and capacity (and ignoring lifetime) may result in poor device lifetime. Incorporating lifetime, however, turns out to be non-trivial. We identify key challenges in this adaptation and present simple heuristics. We also discuss possible design choices which will be fully explored in future work.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasing storage capacity and bandwidth of flash devices has enabled scenarios wherein multiple workloads are consolidated within a single flash system. Recent work has studied resource management in this emerging flash system <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17]</ref>. The common design principles identified by this work include (i) the flash system should be workload-aware, (ii) no flash block should be shared among different users, and (iii) resource conflicts among users should be alleviated to provide performance isolation.</p><p>While this trend continues, unfortunately, little attention has been paid to "fair" division of flash system resources among participating users. Notions of fairness are often deemed important in private clouds <ref type="bibr" target="#b4">[5]</ref> or in public clouds with neutralitylike mandates that may emerge in future <ref type="bibr" target="#b7">[8]</ref>. Specifically, we are concerned with the following questions: (i) what are the resources to be divided in consolidated flash systems? and (ii) how one can achieve a fair division of them? For the first question, the resources that come to mind are bandwidth and capacity. However, in addition to these conventional resources, we argue that a flash-specific resource -flash lifetime -should be explicitly and carefully managed. Flash cells wear out as they are subjected to writes (and erases) and eventually become unreliable. Flash lifetime has unique characteristics. First, it is non-renewable (consumable) which sets it apart from bandwidth and capacity because perhaps its treatment should change over time (e.g., different closer to a device's end than when it is newly provisioned). Second, it is "consumed" not just by host-induced (explicit or direct) writes but also by implicit requests; specifically, garbage collection (GC) component of flash systems irregularly and unpredictably contributes writes that contribute to lifetime consumption.</p><p>As this paper's main focus, we seek to answer the second question raised above when considering all three resourcesbandwidth, capacity, and lifetime -together. A recently proposed mechanism for fair division of multiple resources is Dominant Resource Fairness (DRF) <ref type="bibr" target="#b4">[5]</ref>. Motivated by its fairness properties, we propose to employ DRF in the context of flash systems. Especially, we reveal that flash lifetime needs careful management by comparing DRF that considers all three resources vs. that ignores lifetime.</p><p>In managing these three different resources in a coordinated manner, we need to answer the following question: how can one treat lifetime on an equal footing with bandwidth and capacity? Two extreme approaches come to mind. At one extreme, there may be a longer-term goal of ensuring that the device lasts for a certain time (say a year). At another extreme, one may not have any such requirement and be only interested in fairly dividing what lifetime remains at any allocation decision point. For the former, it is natural to spread out available writes across the desired lifetime into allocation "epochs" -periods of relative workload stationarity -where DRF-like allocation is newly performed per epoch with a write "budget" for that epoch. For the latter, since remaining lifetime will keep diminishing with flash usage, a DRF-like allocation will tend to view lifetime as an increasingly bottlenecked resource. Both are plausible scenarios and are partly business decisions, i.e., what kind of behavior do we want -do we want to maintain the illusion of the device acting similarly throughout its life (former) or do we want fewer writes to go to older devices (latter)? More generally, an approach between these two extremes may be appropriate and understanding this is interesting future work. Assuming the former scenario, we make the following contributions:</p><p>• We consider flash lifetime as a first-class resource that needs to be managed together with bandwidth and capacity.</p><p>• We explore the use of DRF as a multi-resource fair allocation mechanism for consolidated flash systems. We empirically confirm that there is a strong need to incorporate lifetime as an explicit resource in the management of these systems.</p><p>• We identify a set of challenges in employing DRF in consolidated flash systems and present preliminary heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries 2.1 Dominant Resource Fairness (DRF)</head><p>In a computer system with multiple resource types, one of the most popular strategies to make a division of the multiple resources is based on Dominant Resource Fairness (DRF) <ref type="bibr" target="#b4">[5]</ref>. Observing that each user can have a different dominant resource, DRF seeks to equalize dominant shares (i.e., the share that the user has been allocated of dominant resource) across all users; and, this allocation is regarded to be fair by satisfying a set of desirable properties <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. If all users have the same dominant resource, the DRF reduces to max-min fairness (MMF) <ref type="bibr" target="#b1">[2]</ref> allocation, which maximizes the minimum allocation received by a user on the dominant resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Our Target System</head><p>Stream-based User: One critical requirement in employing DRF is the standardized resource demand of each user. That is, each user has an executable unit (e.g., VM, task, instance) that needs a certain amount of resources; and, the total amount of resources allocated to a user can increase or decrease by launching more or less executable units. Representative examples include database and server applications where the number of clients varies. We refer to the executable unit as stream. Note that, while the total resource demand of a user increases linearly with the number of launched streams in vanilla DRF, this is not always the case in reality, which is a challenge (i.e., non-linearity between stream count and total demands) when employing DRF in consolidated flash systems. We will discuss this and present our heuristic later ( §5). Flash Cache: As our target system, we consider a consolidated flash "cache" <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> with a backing-store in the next layer, due to two following reasons: (i) allocating limited amount of capacity is a significant task in caching systems (compared to main-storage that should accommodate all of users' data), since the allocated capacity determines hit-ratios and performance of users -the more user capacity a user has, the higher hit-ratio it experiences. (ii) managing (regulating) lifetime is feasible in caching systems; once a user exhausts the allocated amount of writes, its further writes can be prevented by servicing them from the backing-store in the next layer. Note that our proposed resource allocation strategy is agnostic to the exact caching policy; and it is also applicable to main-storage setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Resources Considered</head><p>We have chosen to follow the conventional resource-based notion of fairness. One reason to pursue such an approach is that it may benefit from some properties <ref type="bibr" target="#b4">[5]</ref> that DRF offers. One downside to fairness definitions based on applicationspecific performance metrics such as hit-ratio and latency is that they open up means for workloads to starve others out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conventional Bandwidth and Capacity</head><p>Bandwidth: Across various computing domains (e.g., memory, network), system bandwidth has been one of the representative, prime resource that is divided among multiple users. In particular, (i) how to divide the total bandwidth (e.g., notion of fairness) and (ii) how to guarantee the partitioned bandwidth (e.g., performance isolation) are two major concerns addressed in the literature. For consolidated flash systems, recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16</ref>] demonstrated that provisioning a specified bandwidth for each user in them is feasible. Capacity: In memory and storage systems, device capacity also has been a key resource to be partitioned among users. Particularly, any caching device where all user data cannot be accommodated needs to determine how much capacity is allocated to a user, which directly affects the hit ratio and the performance of the user. Considering various placement of flash devices in the memory-storage hierarchy, various studies have focused on flash capacity partitioning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flash-Specific Device Lifetime</head><p>We pay attention to another significant, flash-specific resource, namely, flash lifetime. As opposed to the other conventional resources, lifetime (or, its proxy, write count) has two unique characteristics. First, it is consumable (non-renewable); in other words, a flash memory can process only a limited number of write operations, beyond which the device becomes unavailable. Since the limited lifetime resource has a direct relationship with the ownership cost (economical condition), it can be considered as the first-class resource to be managed in some cases. Second, the lifetime is affected by other resources. Specifically, the write count is consumed by both explicit write requests (from the host) and implicit write traffic (from the device-internals); and the latter is significantly affected by the storage capacity assigned to it. That is, the number of writes issued by the garbage collection (GC) is determined by the over-provisioned (OP) capacity.</p><p>There is no uniform definition of fairness in device wearout. We are interested in a "multi-resource" fairness definition where lifetime/writes is one resource out of many. If one insists on thinking about wear-out alone, it may help to imagine infinite capacity and bandwidth but finite lifetime. The problem would then boil down to MMF allocation <ref type="bibr" target="#b1">[2]</ref> of lifetime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Coordination of Three Resources</head><p>To treat the three different resource in an orchestrated manner, we define "epoch" (a period of relative workload stationarity) and, in every epoch, our DRF framework newly divides the given total resources to the given competing users. In an epoch, each user and their streams are assumed to be backlogged -each continues to consume a fixed amount of bandwidth while taking a fixed amount of storage capacity. Regarding the lifetime, on the other hand, a total write budget is prescribed to the epoch; and all users and their streams are supposed to share (and divide) the given write budget. In this epoch-based framework, we can manage the three resources on relatively different time scale in a coordinated fashion.  <ref type="figure" target="#fig_0">Figure 1</ref> illustrates our target problem. A flash system provides three types of resources: bandwidth, capacity, and lifetime. In a given epoch, these resources are allocated to multiple users, each of which has its unique resource demand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DRF Allocation Examples</head><p>We assume that each user's bandwidth, capacity, and lifetime demands (resource usages in the worst-case scenario) can be obtained using static workload profiling and analytical models. For example, the total lifetime demand of a user can be calculated by considering (i) # host writes it generates, (ii) its own write amplification (WA) model, and (iii) overprovisioned (OP) capacity allocated to it. We implemented our framework using MATLAB and explored how the resource allocation changes depending on the inputs (device's resources and users' demand).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DRF Considering All Three Resources</head><p>Example: Let us consider an example scenario to see how DRF allocates the three resources among the three participants.  <ref type="figure">Figure 2</ref> show how DRF allocates the three resources among the three users. Specifically, DRF tries to equalize the dominant shares of all users while increasing the number of launched streams. As indicated in <ref type="table" target="#tab_4">Table 2</ref>, the dominant shares of Users A, B, and C are close to each Algorithm 1: DRF allocation of three resources.</p><p>Input: Device's total resources (&lt; B,C,W &gt;, where B, C, and W are bandwidth, capacity, and writes, respectively), the number of users (N), users' capacity demands ( ˆ c). Output: Users' allocated resources (A i =&lt; b i , c i , w i &gt;, where b i , c i , and w i are bandwidth, capacity, and writes allocated to user i), the numbers of launched streams (k i , where the number of streams for user i). &lt; CB,CC,CW &gt;←&lt; 0, 0, 0 &gt; // Device's consumed resources </p><formula xml:id="formula_0">DC i ← ˆ c i (k i + 1) − ˆ c i (k i ) DW i ← ˆ w i (k i + 1) − ˆ w i (k i ) DB i ← ˆ b i (k i + 1) − ˆ b i (k i ) end</formula><p>other, whereas their dominant resources are all different. As a result, Users A, B, and C are allocated the specified amount of resources by launching 2, 4, and 5 streams, respectively. <ref type="figure">Figure 2</ref> also shows that the largest areas of the users across the resources (write allocation of A, capacity allocation of B, and bandwidth allocation of C) are nearly equal. Note that, the bandwidth and write resources are not fully allocated, since the capacity resource is almost exhausted and the remaining capacity is not enough to launch a stream from any user. Other Scenarios: The three users in the above example are representative of various workload types. That is, each of three is bandwidth-dominant, capacity-dominant, or lifetimedominant. We examined many other user (consolidation) scenarios, whose results are left out, due to space limit. One general observation to highlight is that, when consolidated users are similar (i.e., all are lifetime-dominant or bandwidthdominant), the DRF allocation is equal to the MMF <ref type="bibr" target="#b1">[2]</ref> allocation of the common dominant resource.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DRF without Lifetime Management</head><p>2-Resource DRF: Considering non-renewable lifetime as a precious resource that should be carefully and explicitly managed, we included it the DRF allocation above. To see the impact of ignoring lifetime management, we employ a 2-resource DRF where only the two conventional resources (bandwidth and capacity) are considered in the same example <ref type="table" target="#tab_1">(Table 1</ref>). The process of equalizing dominant shares of all users does not change; but, the dominant share (s i ) is calculated by max(b i /B, c i /C) since lifetime is out of the picture. Allocation Result:    from write in 3-resource DRF to capacity. As usual, the DRF tries to equalize the dominant shares while maximizing the number of launched streams. As a result, Users A, B, and C are allocated the specified amount of resources by launching 13, 2, and 4 streams, respectively, which is significantly different from the 3-resource DRF allocation. Since User A's capacity demand is very low and its dominant share also remains low, a lot of streams are launched. <ref type="figure">Figure 3</ref> also shows that the largest areas of the users across the resources (capacity allocation of A, capacity allocation of B, and bandwidth allocation of C) are fairly close to each other. Lifetime Implication: In the case of 2-resource DRF allocation, the expected number of writes to be used by the three users is 41,040,000, which is more than 3× of the write budget of the baseline. Specifically, 95% of the writes are used by User A, since User A is allowed to launch up to 13 streams and; these streams collectively consume a considerable amount of writes. One can conclude from this observation that, if flash lifetime is not carefully managed, (i) while bandwidth and capacity are "fairly" divided to users, their write consumption does not look fair; and, (ii) overall, the end of flash life can be accelerated. Therefore, if one wants his/her flash device to last longer, the DRF-based resource allocation should take the writes (lifetime) into account. Note however that, in this scenario (if device operator does not manage the lifetime), the other resources (bandwidth and capacity) provided by the device can be fairly (or almost fully) allocated to the competing users in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Challenges: User's Demand Estimation</head><p>To adapt the DRF, the following question needs to be addressed: if an additional stream is launched for a user, how much bandwidth, capacity, and write resource does the user ask? That is, one needs to obtain the actual resource demands for each user with a given number of streams. However, this is not a straightforward task, due to the following challenges. Non-Linearity in Demand Increase: If a resource demand increases linearly with streams, estimating the total demand is easy. In fact, this can be done for the bandwidth and write demands of a user, since each stream has its own host I/O and GC traffic. Unfortunately, capacity demand is different, since streams of a user commonly share part of the data. A typical example is a database application where frequently-accessed data are cached and its multiple clients share the data. Let us see how DRF works if users' capacity demands lack the knowledge of shared data and increases linearly with streams. We modify the values of "shared data size" and "perstream data size" of the three users in our baseline <ref type="table" target="#tab_1">Table 1</ref>; shared data sizes are assumed to be zero while per-stream data sizes are a bit adjusted <ref type="table" target="#tab_7">(Table 4)</ref>. In this scenario, User C's dominant resource changes from bandwidth to capacity, as its capacity demand significantly (linearly) increases with streams. As a result, DRF makes an entirely-new allocation against the baseline <ref type="table" target="#tab_4">(Table 2)</ref>, where Users A, B, and C launch 2, 1, and 2 streams, respectively. Actually, this is not a desirable scenario, since resources (bandwidth and write) are significantly under-utilized. Specifically, the total allocated bandwidth and write decrease from 286MB/s and 8,730,000 to 174MB/s and 7,470,000, respectively. This is because the linear increase in capacity demands makes the capacity resource fully allocated with a small number of streams, and as a result, other resources lose opportunity to be further allocated.</p><p>To this end, we propose a simple heuristic to construct the capacity demand by being aware of the shared data among streams. User i's capacity demand with k streams (c i (k)) consists of a fixed capacity for share data (c s i ) and a set of capacities for per-stream data (c</p><formula xml:id="formula_1">p i ). ˆ c i (k) = c s i + c p i × k.</formula><p>(1) Note that the allocation examples so far are based on above heuristic, from which one can conclude that DRF can make a better resource allocation once the capacity demands (nonlinearity with stream) of each user are accurately estimated. Note also that, the scenario of inter-user data sharing (data are shared across users) will be factored in future work. Correlation among Resource Demands: Resource demands of a user are correlated in complex ways; consequently, estimation of total demands of a user should take this correlation into account. For example, let us assume that a user   expects (needs) a high hit ratio from a large capacity; then, its bandwidth demand should correspondingly increase to accommodate the high hit ratio. Among a range of relationships, we focus on an interesting one between capacity demand and write demand. In general, the more (OP) capacity an application has, the less GC overheads (GC writes) it experiences <ref type="bibr" target="#b3">[4]</ref>.</p><p>Here, let us assume that the OP capacity of a user is part of its per-stream capacity ( §6). Accordingly, if a user increases its capacity demand (per-stream data size), its write demand should also decrease correspondingly, since its WA value and GC writes decrease. Keeping this in mind, we propose a simple heuristic to estimate the write demand of a user, given its capacity demand. Specifically, User i's write demand with k streams (w i (k)) can be calculated as follows:</p><formula xml:id="formula_2">ˆ w i (k) = k × w h i (1) × A i (c o i (k)),<label>(2)</label></formula><p>where A i and c o i are the amplification function and the OP capacity of user i. For the amplification function, one can employ an existing analytic model <ref type="bibr" target="#b3">[4]</ref>. Our simple heuristic is based on the assumptions that (i) the number of host writes (w h ) linearly increases with the number of streams (i.e., w h i (k) = k × w h i (1)), and (ii) streams have the same write amplification value since their host write traffic and OP capacity are assumed to be the same.</p><p>We examine another scenario where both capacity and write demands are adjusted. Note that, "per-stream data size" (OP in it) and "amp. value" are adjusted in <ref type="table" target="#tab_8">Table 5</ref>. Under the changed capacity and write demands, the DRF leads to a new allocation against the baseline. Specifically, Users A, B, and C are allowed to launch 3, 5, and 4 streams, respectively. As this example shows, one can construct the most beneficial resource demand vector for a user by exploring the complex correlation among resource demands and adjusting their amounts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DRF Design Parameters</head><p>OP Fraction of Allocated Capacity: One can raise a question: how much OP capacity is given to a user out of the total allocated capacity? We use the following simple heuristic to specify the amount of the OP capacity. The OP capacity of User i with k streams (c o i (k)) can be calculated as follows:</p><formula xml:id="formula_3">c o i (k) = k × c p i × f ,<label>(3)</label></formula><p>where c p is per-stream capacity and f is user-specified fraction of OP capacity to the entire per-stream capacity. It is critical to find the most appropriate f value, since it determines both the capacity and write demands, which can collectively and significantly affect the resulting DRF allocation. Lifetime Management Policy: In our baseline example, the total write resource 11,184,810 <ref type="table" target="#tab_1">(Table 1)</ref> represents the number of 4KB-page writes allowed for a period of 1 hour, assuming that only 1TB write consumption is permitted for a day. However, there can be a range of options for managing a device's lifetime; for example, one might set a tighter budget to last his/her device much longer, while one might not concern the device wear-out and the replacement cost. To see the impact of lifetime management policy, we consider an extreme scenario where the total write resource is set to all the remaining writes to the end of device's life. According to our analysis, when we set the total write resource to a very large number (assuming that our device is young), the DRF results in the same allocation as the 2-resource DRF case where the lifetime resource is not managed <ref type="table" target="#tab_3">(Table 3)</ref>. This is because the large write resource makes User A's dominant resource the capacity (instead of write) and DRF excludes the capacity resource from its interest. However, the situation can change again as the device gets older and its remaining writes becomes small. We will explore the lifetime management choices to best allocate multiple resources in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>DRF is a useful tool to divide multiple resource types across competing users. We propose to employ DRF in consolidated flash systems where bandwidth, capacity, and lifetime resources are correlated in complex ways. We explore challenges in DRF adaptation and design parameters that can affect the resource allocation. In future work, we will study the notion of lifetime fairness in multiple device settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our allocation problem: a flash device's 3 types of resources are to be divided across competing users, each with its unique per-stream resource demand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 : 2 -</head><label>32</label><figDesc>Figure 3: 2-Resource DRF's allocation. While bandwidth and capacity are considered, lifetime is not managed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 lists</head><label>1</label><figDesc></figDesc><table>up the device's total resources and the 
per-stream characteristics (e.g., host I/O, data size, write am-
plification) of the three users. User A mainly needs the write 
resource as its stream generates many host writes and exhibits 
a high write amplification (WA) value. User B dominantly 
needs the capacity resource as its stream loads a large amount 
of data into device. Finally, User C primarily wants the band-
width resource as its stream generates intensive host reads. 
Allocation Process: Let us visit how DRF allocates the three 
resources. Algorithm 1 describes the DRF algorithm. At first, 
(i) it randomly picks a user, launches its stream, and counts 
the allocated resource. Next, (ii) it finds the dominant share 
(s i ) of each user -the dominant share is the maximum frac-
tion of allocated amount to the total amount among the three 
resources. Then, (iii) it finds the user who has the minimum 
dominant share; and, the user acquires more resources by 
launching its stream if there are still available resource to do 
so. Here, (iv) actual resource demands (b i , c i , w i ) in launching 
the additional stream should be estimated from per-stream 
characteristics. We present our simple heuristic in  §5. Finally, 
it repeats (ii)∼(iv) until no more streams can be launched due 
to lack of (remaining) resources. 
Result: Both Table 2 and </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 and</head><label>3</label><figDesc>Figure 3 show how 2-resource DRF allocates the two resources among the three users. One important point is that User A's dominant resource changes</figDesc><table>User 

Dom. Share 
Dom. Share (Alloc./Total) 
# streams 
Alloc. BW 
Alloc. CAP 
Alloc. WR 
A 
Write 
6,000,000/11,184,810 = 0.536 
2 
7 MB/s 
30 GB 
6,000,000 
B 
Capacity 
140GB/256GB = 0.546 
4 
5 MB/s 
140 GB 
480,000 
C 
Bandwidth 
274MBs/512MBs = 0.535 
5 
274 MB/s 
80 GB 
2,250,000 
Alloc. Sum 
286 MB/s 
250 GB 
8,730,000 
Total 
512 MB/s 
256 GB 
11,184,810 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>3-resource DRF: it tries to equalize the dominant shares of all three users. 
Figure 2: 3-resource DRF allocation. 

User 
Dom. Share 
Dom. Share (Alloc./Total) 
# streams 
Alloc. BW 
Alloc. CAP 
Used WR 
A 
Capacity 
85GB/256GB = 0.332 
13 
44 MB/s 
85 GB 
39,000,000 
B 
Capacity 
100GB/256GB = 0.391 
2 
3 MB/s 
100 GB 
240,000 
C 
Bandwidth 
219MBs/512MBs = 0.427 
4 
219 MB/s 
70 GB 
1,800,000 
WR value indicates the sum of used writes. 
Alloc. Sum 
266 MB/s 
255 GB 
41,040,000 
WR value comes from total WR resource of the baseline. 
Total 
512 MB/s 
256 GB 
(11,184,810) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc>2-resource DRF (where lifetime is not considered): it ends up in an entirely new allocation compared to 3-resource DRF, and this causes a significant write consumption.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 : 3-resource DRF which is unaware of non-linearity increase in capacity demands and assumes zero shared data sizes.</head><label>4</label><figDesc></figDesc><table>User 
Per-Stream Data 
Amp. Value 
Dom. Share 
Dom. Share (Alloc./Total) 
# streams 
Alloc. BW 
Alloc. CAP 
Alloc. WR 
A 
10 GB 
1.5 
Write 
4,500,000/11,184,810 = 0.402 
3 
6 MB/s 
50 GB 
4,500,000 
B 
10 GB 
2 
Capacity 
110GB/256GB = 0.429 
5 
7 MB/s 
110 GB 
1,000,000 
C 
15 GB 
1.2 
Bandwidth 
219MBs/512MBs = 0.427 
4 
219 MB/s 
90 GB 
1,440,000 
Note that per-stream data size and amp. value change from the baseline. 
Alloc. Sum 
232 MB/s 
250 GB 
6,940,000 
So, changes in capacity demands lead to changes in write demands. 
Total 
512 MB/s 
256 GB 
11,184,810 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 : 3-resource DRF where the capacity demand (OP size) and the corresponding write (WA value) demand are changed.</head><label>5</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Anirudh Badam, our shepherd, and the anonymous reviewers for their valuable feedback. This research is supported in part by NSF grants <ref type="bibr">1822923,</ref><ref type="bibr">1439021,</ref><ref type="bibr">1629915,</ref><ref type="bibr">1626251,</ref><ref type="bibr">1629129,</ref><ref type="bibr">1763681,</ref><ref type="bibr">1526750,</ref><ref type="bibr">1439057</ref>, and 1717571. Jung's research is in part supported by NRF 2016R1C1B2015312, DOE DEAC02-05CH11231, NRF 2015M3C4A7065645, NRF 2017R1A4A1015498 and Samsung grant (IO181008-05622-01).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">CloudCache: Ondemand Flash Cache Management for Cloud Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dulcardo</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Data Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Bertseka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Gallager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Prentice-Hall Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">HUG: Multi-Resource Fairness for Correlated and Elastic Demands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Sotica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analytic Modeling of SSD Write Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SYSTOR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dominant Resource Fairness: Fair Allocation of Multiple Resource Types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FlashBlox: Achieving Both Performance Isolation and Uniform Lifetime for Virtualized SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Nath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bikash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Multi-streamed Solid-State Drive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Uk</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeeseok</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjoo</forename><surname>Maeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotStorage</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neutrality in Future Public Clouds: Implications and Challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kesidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvan</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neda</forename><surname>Nasiriani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotCloud</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Utilitarian Performance Isolation in Shared SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotStorage</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards SLO Complying SSDs Through OPS Isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centaur: Host-Side SSD Caching for Storage Performance Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">Jose</forename><surname>Mashtizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Rangaswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nitro: A Capacity-Optimized SSD Cache for Primary Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyong</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Smaldone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grant</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">vCacheShare: Automated Server Flash Cache Space Management in a Virtualization Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Uttamchandani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjiang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuangang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S-Rac</surname></persName>
		</author>
		<title level="m">SSD Friendly Caching for Data Center Workloads</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>SYSTOR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FStream: Managing Flash Streams in the File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunhee</forename><surname>Rho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanchan</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seung-Uk</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><forename type="middle">Jagadeesh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jooyoung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeheon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Balancing Fairness and Efficiency in Tiered Storage Systems with BottleneckAware Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AutoStream: Automatic Stream Management for Multi-streamed SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingpei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajinikanth</forename><surname>Pandurangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SYSTOR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
