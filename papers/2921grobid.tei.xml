<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NVMKV: A Scalable and Lightweight Flash Aware Key-Value Store</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Mármol</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Sundararaman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisha</forename><surname>Talagala</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Rangaswami</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushma</forename><surname>Devendrappa</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">work done at FusionIO</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">work done at FusionIO</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Ganesan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">work done at FusionIO</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename><surname>Fusionio</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Florida International University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NVMKV: A Scalable and Lightweight Flash Aware Key-Value Store</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>State-of-the-art flash-optimized KV stores frequently rely upon a log structure and/or compaction-based strategy to optimally organize content on flash. However, these strategies lead to excessive I/O, beyond the write amplification generated within the flash itself, with both the application and the flash device constantly rearranging data. In this paper, we explore the other extreme in the design space: minimal data management at the KV store and heavy reliance on the Flash Translation Layer (FTL) capabilities. NVMKV is a scalable and lightweight KV store that leverages advanced capabilities that are becoming available in modern FTLs. We demonstrate that NVMKV (i) performs KV operations at close to native device access speeds for get operations , (ii) outperforms state of the art KV stores by 50%-300%, (iii) significantly improves performance predictability for the YCSB KV benchmark when compared with the popular LevelDB KV store, and (iv) reduces data written to flash by as much as 1.7X and 29X for sequential and random write workloads relative to Lev-elDB, thereby dramatically increasing device lifetime.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Key-value (KV) stores are ubiquitous, having become the default data management software for many Internet services <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b7">10,</ref><ref type="bibr" target="#b18">21,</ref><ref type="bibr" target="#b19">22,</ref><ref type="bibr" target="#b29">32]</ref>. They serve application needs in a variety of different domains that demand highthroughput and low-latency data access <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>The performance, capacity, and power consumption mix of flash-based storage makes it an attractive medium for KV stores <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b16">19,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b24">27]</ref>. To get the best performance from both HDDs and low-end SSDs, many KV stores use some form of log structured writing to optimize data layout on media. Since log structured updates require eventual compaction or garbage collection, the consequence is auxiliary write amplification, i.e., additional write amplification introduced at the KV store besides the write amplification introduced by the Flash Translation Layer (FTL). For instance, the SILT work introduces an auxiliary write amplification of 5.4 <ref type="bibr" target="#b24">[27]</ref>. The recent LevelDB KV store from Google <ref type="bibr" target="#b19">[22]</ref> also exhibits rather dramatic auxiliary write amplification. in flash demonstrates that the cumulative write amplification is multiplicative, causing even a small user update to result in massive writing to the flash over time <ref type="bibr" target="#b30">[33]</ref>. Three trends force us to rethink KV store design choices. First, NAND flash endurance is getting poorer with every new media generation <ref type="bibr" target="#b22">[25]</ref>. With fewer Program/Erase cycles to begin with, the auxiliary write amplification further reduces the device lifetime and increases the KV store's total cost of ownership. Second, the gap between sequential and random write performance has significantly narrowed in state of the art SSDs today <ref type="bibr" target="#b23">[26]</ref>, calling into question the need for application level log structuring and compaction. Third, modern FTLs are much more powerful than the traditional block devices. New FTL interfaces have recently been developed to provide advanced capabilities to access data to (or from) NAND Flash <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b27">30]</ref>.</p><p>In this paper, we explore a new design for a KV store -one that relies upon cooperative design with an FTL to minimize auxiliary write amplification and maximize application-level performance. The resulting KV store, NVMKV, is lightweight and fully exploits native characteristics of the FTL to achieve get performance equivalent to raw device read speeds and put operations that are significant fractions of raw device write speeds. NVMKV makes several novel contributions. While many flash and disk optimized KV stores exist (see <ref type="figure" target="#fig_1">Figure 2</ref>), NVMKV is the first to leverage native FTL layer primitives such as atomic multi-block write, atomic</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NVMKV</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FTL-Optimized</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flash-Optimized Disk-Optimized</head><p>RocksDB <ref type="bibr" target="#b7">[10]</ref> LevelDB <ref type="bibr" target="#b19">[22]</ref> SILT <ref type="bibr" target="#b24">[27]</ref> FAWN-KV <ref type="bibr" target="#b9">[12]</ref> SkimpyStash <ref type="bibr" target="#b17">[20]</ref> BerkeleyDB <ref type="bibr" target="#b25">[28]</ref> HashCache <ref type="bibr" target="#b12">[15]</ref> MemcacheDB <ref type="bibr">[5]</ref> MongoDB <ref type="bibr" target="#b5">[8]</ref> FlashStore <ref type="bibr" target="#b16">[19]</ref>  multi-block persistent trim, exists, and iterate to implement KV functionality. NVMKV demonstrates how strong consistency and atomic guarantees provided by the underlying FTL can be used to achieve atomicity and isolation, low write amplification, and performance close to that of the raw device. NVMKV is also the first KV store that uses a small (close to zero), constant, amount of in-memory metadata that is independent of both the number of keys stored and the workload intensity.</p><p>Our evaluation of NVMKV reveals the following. NVMKV performs get operations at close to the native device access speeds. Relative to LevelDB, a popular KV store in wide use today, NVMKV reduces auxiliary write amplification by as much as 1.7X and 29X for sequential and random write workloads respectively. For the YCSB KV benchmark, NVMKV outperforms LevelDB by 50%-300% besides significantly reducing the variance of KV operation latencies when compared with LevelDB. Finally, NVMKV provides significantly better performance when using a fourth of the user-level DRAM cache size compared to LevelDB and unlike LevelDB, without using any OS-level DRAM caching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Leveraging Flash Devices</head><p>Modern FTLs are powerful software layers that include functions such as log-structuring, dynamic data remapping, indexing, transactional updates, and thin provisioning <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b26">29]</ref>, which are superficially similar to the functionality being built into many KV stores. For instance, FTLs implement an indirection map to manage the logical to physical block address mapping and write logging to guarantee durability on a medium that implicitly forces out-of-place writes. There is ongoing effort to surface these advanced capabilities through standardized primitives for use by operating systems and user space software <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b6">9]</ref>. <ref type="table">Table 1</ref> lists some of the primitives that can be surfaced by a modern FTL. Recent work has utilized such primitives for implementing efficient file systems <ref type="bibr" target="#b23">[26]</ref>, databases <ref type="bibr" target="#b26">[29]</ref>, and caching <ref type="bibr" target="#b27">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>API Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXISTS</head><p>queries if an address is populated</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ATOMIC-WRITE</head><p>writes an address range as ACID tx.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ATOMIC-TRIM</head><p>deletes an address range as ACID tx.</p><p>ITERATE returns all populated addresses <ref type="table">Table 1</ref>: FTL Primitives. These primitives can be used for either individual or ranges of (both sparse and non-sparse) locations.</p><p>Additionally, batch operations of ATOMIC-WRITE, ATOMIC-TRIM, and combinations are also possible, allowing the write of some locations and the deletion of other locations as a single transaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dynamic Mapping</head><p>FTLs maintain an indirection map translating logical block addresses to physical locations. This mapping is required to organize data for minimal write amplification and best wear leveling. Most KV stores also maintain a mapping engine that converts keys to storage addresses where the values are stored.</p><p>To leverage an FTL based remapping engine for mapping key-value pairs, we extend the FTL indirection map to a sparse map, similar to that used in previous work <ref type="bibr" target="#b23">[26]</ref>. A sparse map provides a few orders of magnitude more addressable logical addresses (LBAs) for the same physical capacity, thinly provisioning physical locations only for LBAs that have been written. Leveraging the underlying sparse addressing, NVMKV replaces the indirection maps found in most KV stores with hashing functions over the sparse address space. Through this approach, put and get operations are simply mapped to write and read operations in the FTL, respectively. A delete of a key removes the KV pair from the storage device using the trim operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Persistence and Transactional Support</head><p>FTLs maintain both data and metadata, and in particular, persistent indirection maps to recover data upon restart. Since FTLs operate as copy-on-write, they can provide high performance transactional write semantics <ref type="bibr" target="#b26">[29]</ref>. KV stores can leverage this capability to ensure that puts of KV pairs are atomic without additional journaling, and providing nearly the same performance as that of conventional writes. Access to the transactional persistence capabilities of the FTL can be provided through two primitives, ATOMIC-WRITE and ATOMIC-TRIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Highly parallel operations</head><p>FTLs support highly parallel read/write operations to match the parallelism available inside the NAND die of most flash devices. By utilizing the atomic operations, we can minimize locking within the KV store and better leverage the inherent parallelism within the flash device.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">NVMKV Design</head><p>NVMKV is designed as a user space library that exports a KV API and leverages the publicly available FTL prim- itives (see <ref type="table">Table 1</ref>) to access flash devices <ref type="bibr" target="#b6">[9]</ref>. These primitives are implemented within Fusion-io's ioMemory FTL <ref type="bibr" target="#b20">[23]</ref> and exported as a set of IOCTLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sparse Addressing</head><p>Leveraging sparse addressing is central to NVMKV since it allows minimizing I/O amplification during both get and put operations. In the absence of collisions, each get or put operation translates into exactly one I/O to flash. The elimination of an indirection map results in fixed (nearly zero) metadata at the KV store.</p><p>To effectively utilize the sparse address space, NVMKV divides the sparse address into two parts: the Key Bit Range (KBR) and the Value Bit Range (VBR). By default, NVMKV uses 36 bits for the KBR and 12 bits for the VBR in a 48 bit address space, with alternatives configurable by the user when the KV store is created. The VBR defines the amount of contiguous address space (i.e., maximum value size) reserved for each KV pair, ensuring that KV pairs mapped into the sparse address space to not overlap each other in logical address space. The KBR determines the maximum number logical hash slots that each KV pair can be placed into. User supplied keys are mapped to LBA addresses through a simple hash model <ref type="figure">(Figure 3</ref>). Keys can be variable length up to the maximum supported key size (2MB for a 12 bit VBR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hashing and Collision Handling</head><p>Since each VBR will contain exactly one KV pair, hash conflicts only occur in the KBR. The design of NVMKV assumes that the KBR is kept sufficiently large, relative to the number of keys that can be stored in a flash device, to reduce the chances of a hash collision. For example, 1TB of flash can contain a maximum of 2 billion 512B KV pairs. With the default KBR of 36 bits which supports 64 billion hash slots, and uniform hashing of KV pairs across KBR space, the chances of a new KV pair inserted into even a full device causing a collision is˜3%is˜ is˜3%.</p><p>Collisions are handled within the library by deterministically computing an alternate hash location (via polynomial probing) within the KBR. Up to eight hash locations are tried before the KV Store refuses to accept a new key. Assuming that the hash function uniformly distributes keys, the probability of a PUT failing in the above example equals the probability of 8 consecutive collisions and is approximately (1/(64 billion / 2 billion) 8 = 1/2 40 is vanishingly small. The sparse address bits can be increased proportionately as device capacities increase to maintain low hash collision probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">KV Storage and Caching</head><p>The minimum unit of storage in NVMKV is a sector where keys and values smaller than 512B will consume a full 512B sector. Each KV pair will also contain some metadata in a header stored on media. NVMKV packs and stores the metadata, the key, and the value in a single sector if the sum of their individual sizes is less than or equal to the sector size. Our instance of NVMKV implements a DRAM cache which can be used to hold KV pairs. Separately, a collision cache holds information about recent hash collisions, reducing the need for additional flash lookups at collision time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluated the performance, overhead, and auxiliary write amplification of NVMKV, comparing it to the raw device and LevelDB 1.14 <ref type="bibr" target="#b19">[22]</ref>. We used the FIO benchmark, the YCSB KV benchmark <ref type="bibr" target="#b14">[17]</ref>, and the LevelDB suite of micro-benchmarks for our workloads. Our experiments were performed on a system with a Quad-Core 2.5 GHz AMD Opteron(tm) Processor, 8GB of DDR2 RAM, and a 825GB Fusion-io ioScale2 drive running Linux Ubuntu 12.04 LTS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Raw Device Performance Comparison</head><p>Our first experiment evaluates the overhead of the NVMKV stack and LevelDB relative to the raw flash device. Get and put used 512B values and key sizes ranging from 1 byte to 128 bytes and for these sizes, NVMKV issues I/O operations of size 1KB. The FIO tool was configured to generate 1KB I/Os to the raw device. <ref type="figure" target="#fig_2">Figure 4</ref> shows that as the thread count increases, the throughput for NVMKV's get operations tracks the FIO benchmark's read rate. For put operations, NVMKV significantly outperforms both the asynchronous and synchronous versions of LevelDB. Additional overheads in NVMKV such as checking for collisions cause performance to be lower than the underlying native device write performance extracted by FIO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LevelDB Comparison using YCSB</head><p>YCSB is a framework for comparing the performance of KV stores and implements six workload personalities A-F <ref type="bibr" target="#b14">[17]</ref>. The YCSB dataset size was 10GB and we evaluated both KV stores with caches of size 256MB (C: 256 MB) and 1GB (C: 1 GB). LevelDB implements write buffering and utilizes the OS page cache (both active during the experiment) while NVMKV does neither. LevelDB was configured to perform asynchronous writes. <ref type="figure" target="#fig_3">Figure 5</ref> demonstrates throughput performance gains of 50%-300% with NVMKV relative to LevelDB when running the YCSB workloads. These performance gains are even more significant when we consider that LevelDB does not provide durability while NVMKV does, and that LevelDB uses both a write buffer and the OS buffer cache for additional DRAM caching/buffering, while NVMKV does neither. We do not report results for YCSB-E because it performs short range scans (short sequential scans at randomly chosen locations), an operation not currently supported by the YCSB Java binding for NVMKV.</p><p>We measured how KV operation latencies varied over time for each of the workloads ( <ref type="figure">Figure 6</ref>). We include data for both async (default) and sync modes for LevelDB writes. There are a few interesting insights here. First, NVMKV, which delivers atomic and durable updates, significantly outperforms even the LevelDB's best performing, but weaker, async mode. Second, we note that the performance variance in LevelDB is greater relative to NVMKV with significant latency spikes. We measured average/maximum KV read latencies of 0.33/1.38 and 0.33/1.39 ms for LevelDB and LevelDB-S respectively, relative to 0.14/0.66 ms for NMVKV. Since the LevelDB latency spikes seem to occur periodically, we believe these are correlated with the internal compaction mechanism. On the other hand, NVMKV offers much more consistent latency performance over time.</p><p>Revisiting <ref type="figure" target="#fig_0">Figure 1</ref>, we see that for the LevelDB suite of microbenchmarks <ref type="bibr" target="#b19">[22]</ref>, NVMKV incurs 1.7X to 29X lower auxiliary write amplification than LevelDB. The performance gains in NVMKV can be attributed to this reduction, as well as eliminating layers of indirection and metadata management overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>Besides NVMKV's performance improvements and endurance gains, atomic durability guarantees for KV operations are an added benefit. The need for such guarantees is a topic of debate within the NoSQL community with KV stores implementing different levels of eventual to strict consistency and varying degrees of durability. In NVMKV, we were able to provide strictly atomic and synchronous durability guarantees by leveraging the underlying FTL capability. Thus, rather than adding complexity and sacrificing performance to achieve strict guarantees (and thereby feeling pressurized to give them up), we found that leveraging their presence in the underlying FTL helped us simplify the NVMKV design without sacrificing performance. We also observed that by extracting more of the native performance of the flash device, we were able to deliver more KV operation performance than LevelDB while consuming less DRAM. We believe that NVMKV represents a sound building block on which scale-out KV stores can be built. This is an area we intend to explore further in the future.</p><p>Due to lack of space, we did not include any descriptions of how the FTL supported the primitives that NVMKV relies upon. However, the implementations of similar primitives have been discussed with descriptions of possible FTL implementation designs <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b27">30]</ref>. In particular, FlashTier discusses sparse maps, showing how an incremental extension to an existing FTL data structure can enable dramatic DRAM reductions in applications while only causing moderate additional DRAM consumption at the FTL.</p><p>A limitation in our current design is the requirement to map individual KV pairs to separate sectors. NVMKV is best utilized for KV pairs which consume over 256 bytes. While many workloads fit this criterion, there are also many that do not. For the second group of workloads, NVMKV will have poor capacity utilization. One way to manage efficient storage of small KV pairs is to follow a multi-level storage mechanism, as provided in SILT <ref type="bibr" target="#b24">[27]</ref>, where small items are initially indexed separately and later compacted into larger units such as sectors. This is also a target area for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We explored a novel concept of a KV store designed cooperatively with an FTL to reduce redundant work across the two layers. The result, NVMKV, is able to extract significant fractions of the raw device performance and outperform a state of the art KV store while minimizing auxiliary write amplification. NVMKV is open source, available at https://github.com/opennvm/nvmkv.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A comparison of write amplification. LevelDB variants are RW: Random asynchronous writes, RW-S: random synchronous writes, SW: sequential asynchronous writes, SW-S: sequential synchronous writes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Categorizing Existing KV stores. This figure shows a broad categorization of existing KV stores based on primarily being hard-disk-or flash-optimized and on leveraging capabilities surfaced by modern FTLs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Microbenchmark comparing LevelDB, NMVKV and Raw Block Device (FIO).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: LevelDB vs. NVMKV for built-in YCSB workloads A-F. Load populates data and precedes all workloads. A is a mix of 50/50 reads and writes. B is 95/5 reads/write mix. C is read only. D is a read latest workload, and F is a read-modify-write workload.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thank the reviewers for their feedback and comments. We also would like to thank the people who helped build NVMKV and the FTL interface primitives.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Aerospike: High performance KV Store use cases</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandra</forename></persName>
		</author>
		<ptr target="http://cassandra.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://www.couchbase.com" />
		<title level="m">Couchbase: NoSQL use cases</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Native Flash Support for Applications</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://www.t10.org/cgi-bin/ac.pl?t=d&amp;f=14-043r2.pdf" />
		<title level="m">SBC-4 SPC-5 Atomic writes and reads</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mongodb</surname></persName>
		</author>
		<ptr target="http://mongodb.org" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvm Primitives Library</surname></persName>
		</author>
		<ptr target="http://opennvm.github.io/nvm-primitives-documents/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocksdb</surname></persName>
		</author>
		<ptr target="http://rocksdb.org" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for SSD Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX ATC</title>
		<meeting>of USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FAWN: A fast array of wimpy nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What consistency does your key-value store actually provide?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tucek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wylie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX HotDep</title>
		<meeting>of USENIX HotDep</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Workload analysis of a large scale key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMETRICS</title>
		<meeting>of ACM SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hashcache: cache storage for the next billion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX NSDI</title>
		<meeting>of USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Many-core key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berezecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGCC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Benchmarking cloud serving systems with ycsb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Chunkstash: Speeding up inline storage deduplication using flash memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX ATC</title>
		<meeting>of USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flashstore: high throughput persistent key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB Endow</title>
		<meeting>of VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skimpystash: Ram space skimpy key-value store on flash-based storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGMOD</title>
		<meeting>of ACM SIGMOD</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGOPS</title>
		<meeting>of ACM SIGOPS</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<ptr target="https://code.google.com/p/leveldb/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fusion-io, Inc. ioMemory Virtual Storage Layer (VSL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Comet: An active distributed key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX OSDI</title>
		<meeting>of USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The bleak future of nand flash memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dfs: A file system for virtualized flash storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bongo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SILT: A memory-efficient, high-performance key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SOSP</title>
		<meeting>of ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bostic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<title level="m">Proc. of USENIX ATC</title>
		<meeting>of USENIX ATC</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
<note type="report_type">Berkeley db</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond block i/o: Rethinking traditional storage primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wipfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE HPCA</title>
		<meeting>of IEEE HPCA</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Flashtier: A lightweight, consistent and durable storage cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM ECCS</title>
		<meeting>of ACM ECCS</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalaris: reliable transactional p2p key/value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schintke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reinefeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGPLAN</title>
		<meeting>of ACM SIGPLAN</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Serving large-scale batch computed data with project voldemort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sumbaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kreps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST</title>
		<meeting>of USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hec: Improving endurance of high performance flashbased cache devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Plasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Talagala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SYSTOR</title>
		<meeting>SYSTOR</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
