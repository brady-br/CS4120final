<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST &apos;16). Open access to the Proceedings of the 14th USENIX Conference on File and Storage Technologies is sponsored by USENIX The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 22-25, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Soundararajan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kenchammana-Hosekote</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Gokul Soundararajan and Deepak Kenchammana-Hosekote, NetApp, Inc.; Andrew A. Chien and Haryadi S. Gunawi</orgName>
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST &apos;16). Open access to the Proceedings of the 14th USENIX Conference on File and Storage Technologies is sponsored by USENIX The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments</title>
					</analytic>
					<monogr>
						<title level="m">USENIX Association 14th USENIX Conference on File and Storage Technologies (FAST &apos;16)</title>
						<imprint>
							<biblScope unit="page">263</biblScope>
							<date type="published">February 22-25, 2016</date>
						</imprint>
					</monogr>
					<note>https://www.usenix.org/conference/fast16/technical-sessions/presentation/hao</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study storage performance in over 450,000 disks and 4,000 SSDs over 87 days for an overall total of 857 million (disk) and 7 million (SSD) drive hours. We find that storage performance instability is not uncommon: 0.2% of the time, a disk is more than 2x slower than its peer drives in the same RAID group (and 0.6% for SSD). As a consequence, disk and SSD-based RAIDs experience at least one slow drive (i.e., storage tail) 1.5% and 2.2% of the time. To understand the root causes, we correlate slowdowns with other metrics (workload I/O rate and size, drive event, age, and model). Overall, we find that the primary cause of slowdowns are the internal characteristics and idiosyncrasies of modern disk and SSD drives. We observe that storage tails can adversely impact RAID performance, motivating the design of tail-tolerant RAID. To the best of our knowledge, this work is the most extensive documentation of storage performance instability in the field.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Storage, the home of Big Data, has grown enormously over the past decade <ref type="bibr" target="#b21">[21]</ref>. This year Seagate projects to ship more than 240 exabytes of disk drives <ref type="bibr" target="#b20">[20]</ref>, SSD market has doubled in recent years <ref type="bibr" target="#b32">[32]</ref>, and data stored in the cloud has also multiplied almost exponentially every year <ref type="bibr" target="#b8">[10]</ref>. In a world of continuous collection and analysis of Big Data, storage performance is critical for many applications. Modern applications particularly demand low and predictable response times, giving rise to stringent performance SLOs such as "99.9% of all requests must be answered within 300ms" <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b48">48]</ref>. Performance instability that produces milliseconds of delay lead to violations of such SLOs, degrading user experience and impacting revenues negatively <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b44">44]</ref>.</p><p>A growing body of literature studies the general problem of performance instability in large-scale systems, specifically calling out the impact of stragglers on tail latencies <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b56">56]</ref>. Stragglers often arise from contention for shared local resources (e.g., CPU, memory) and global resources (e.g., network switches, back-end storage), background daemons, scheduling, power limits and energy management, and many others. These studies are mostly performed at server, network, or remote (cloud) storage levels.</p><p>To date, we find no systematic, large-scale studies of performance instability in storage devices such as disks and SSDs. Yet, mounting anecdotal evidence of disk and SSD performance instability in the field continue to appear in various forums ( §2). Such ad-hoc information is unable to answer quantitatively key questions about drive performance instability, questions such as: How much slowdown do drives exhibit? How often does slowdown occur? How widespread is it? Does slowdown have temporal behavior? How long can slowdown persist? What are the potential root causes? What is the impact of tail latencies from slow drives to the RAID layer? Answers to these questions could inform a wealth of storage systems research and design.</p><p>To answer these questions, we have performed the largest empirical analysis of storage performance instability. Collecting hourly performance logs from customer deployments of 458,482 disks and 4,069 SSDs spanning on average 87 day periods, we have amassed a dataset that covers 857 million hours of disk and 7 million hours of SSD field performance data.</p><p>Uniquely, our data includes drive-RAID relationships, which allows us to compare the performance of each drive <ref type="bibr">(D i</ref> ) to that of peer drives in the same RAID group (i = 1..N ). The RAID and file system architecture in our study ( §3.1) expects that the performance of every drive (specifically, hourly average latency L i ) is similar to peer drives in the same RAID group.</p><p>Our primary metric, drive slowdown ratio (S i ), the fraction of a drive's latency (L i ) over the median latency of the RAID group (median(L 1..N )), captures deviation from the assumption of homogeneous drive performance. Assuming that most workloads are balanced across all the data drives, a normal drive should not be much slower than the other drives. Therefore, we define "slow" (unstable) drive hour when S i ≥ 2 (and "stable" the otherwise). Throughout the paper, we use 2x and occasionally 1.5x slowdown threshold to classify drives as slow.</p><p>(iii) Slowdown temporal behavior and extent ( §4.1.3, §4.1.4): We find that slowdown often persists; 40% and 35% of slow disks and SSDs respectively remain unstable for more than one hour. Slowdown periods exhibit temporal locality; 90% of disk and 85% of SSD slowdowns occur on the same day of the previous occurrence. Finally, slowdown is widespread in the drive population; our study shows 26% of disks and 29% of SSDs have experienced at least one slowdown occurrence.</p><p>(iv) Workload analysis ( §4.2): Drive slowdowns are often blamed on unbalanced workloads (e.g., a drive is busier than others). Our findings refute this, showing that more than 95% of slowdown periods cannot be attributed to I/O size or rate imbalance.</p><p>(v) "The fault is (likely) in our drives": We find that older disks exhibit more slowdowns ( §4.3.2) and MLC flash drives exhibit more slowdowns than SLC drives ( §4.3.3). Overall, evidence suggests that most slowdowns are caused by internal characteristics of modern disk and SSD drives.</p><p>In summary, drive performance instability means the homogeneous performance assumption of traditional RAID is no longer accurate. Drive slowdowns can appear at different times, persist, disappear, and recur again. Their occurrence is "silent"-not accompanied by observable drive events ( §4.3.1). Most importantly, workload imbalance is not a major root cause ( §4.2). Replacing slow drives is not a popular solution ( §4.4.2- §4.4.3), mainly because slowdowns are often transient and drive replacement is expensive in terms of hardware and RAID rebuild costs.</p><p>(vi) The need for tail-tolerant RAID: All of the reasons above point out that file and RAID systems are now faced with more responsibilities. Not only must they handle well-known faults such as latent sector errors and corruptions, now they must mask storage tail latencies as well. Therefore, there is an opportunity to create "tail tolerant" RAID that can mask storage tail latencies online in deployment.</p><p>In the following sections, we present further motivation ( §2), our methodology ( §3), the main results ( §4), an opportunity assessment of tail-tolerant RAID ( §5), discussion ( §6), related work ( §7) and conclusion ( §8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivational Anecdotes</head><p>Our work is highly motivated by the mounting anecdotes of performance instability at the drive level. In the past several years, we have collected facts and anecdotal evidence of storage "limpware" <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b26">26]</ref> from literature, online forums supported by various storage companies, and conversations with large-scale datacenter operators as well as product teams. We found many reports of storage performance problems due to various faults, complexities and idiosyncrasies of modern storage devices, as we briefly summarize below.</p><p>Disk: Magnetic disk drives can experience performance faults from various root causes such as mechanical wearout (e.g., weak head <ref type="bibr">[1]</ref>), sector re-reads due to media failures such as corruptions and sector errors <ref type="bibr" target="#b0">[2]</ref>, overheat from broken cooling fans <ref type="bibr" target="#b1">[3]</ref>, gunk spilling from actuator assembly and accumulating on disk head <ref type="bibr" target="#b2">[4]</ref>, firmware bugs <ref type="bibr" target="#b41">[41]</ref>, RAID controller defects <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b47">47]</ref>, and vibration from bad disk drive packaging, missing screws, earthquakes, and constant "noise" in data centers <ref type="bibr" target="#b17">[17,</ref><ref type="bibr">29]</ref>. All these problems can reduce disk bandwidth by 10-80% and increase latency by seconds. While the problems above can be considered as performance "faults", current generation of disks begin to induce performance instability "by default" (e.g., with adaptive zoning and Shingled-Magnetic Recording technologies <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b33">33]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SSD:</head><p>The pressure to increase flash density translates to more internal SSD complexities that can induce performance instability. For example, SSD garbage collection, a well-known culprit, can increase latency by a factor of 100 <ref type="bibr" target="#b13">[13]</ref>. Programming MLC cells to different states (e.g., 0 vs. 3) may require different numbers of iterations due to different voltage thresholds <ref type="bibr" target="#b51">[51]</ref>. The notion of "fast" and "slow" pages exists within an SSD; programming a slow page can be 5-8x slower compared to</p><formula xml:id="formula_0">L i = S i = T 1</formula><p>Request latency (full stripe): programming fast page <ref type="bibr" target="#b23">[23]</ref>. As device wears out, breakdown of gate oxide will allow charge moves across gate easily, resulting in faster programming (10-50%), but also higher chance of corruption <ref type="bibr" target="#b22">[22]</ref>. ECC correction, read disturb, and read retry are also factors of instability <ref type="bibr" target="#b19">[19]</ref>. Finally, SSD firmware bugs can cause significant performance faults (e.g., 300% bandwidth degradation in a Samsung firmware problem <ref type="bibr" target="#b49">[49]</ref>).</p><p>Although the facts and anecdotes above are crucial, they do not provide empirical evidence that can guide the design of future storage systems. The key questions we raised in the introduction (slowdown magnitude, frequency, scope, temporal behavior, root causes, impacts, etc.) are still left unanswered. For this reason, we initiated this large-scale study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we describe the RAID systems in our study ( §3.1), the dataset ( §3.2), and the metrics we use to investigate performance instability ( §3.3). The overall methodology is illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RAID Architecture</head><p>RAID group: <ref type="figure">Figure 1</ref> provides a simple illustration of a RAID group. We study disk-and SSD-based RAID groups. In each group, disk or SSD devices are directly attached to a proprietary RAID controller. All the disk or SSD devices within a RAID group are homogeneous (same model, size, speed, etc.); deployment age can vary but most of them are the same. RAID and file system design: The RAID layer splits each RAID request to per-drive I/Os. The size of a per-drive I/O (a square block in <ref type="figure">Figure 1</ref>) can vary from 4 to 256 KB; the storage stack breaks large I/Os to smaller I/Os with a maximum size of the processor cache size. Above the RAID layer runs a proprietary file system (not shown) that is highly tuned in a way that makes most of the RAID I/O requests cover the full stripe; most of the time the drives observe balanced workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAID configuration:</head><p>The RAID systems in our study use small chunk sizes (e.g., 4 KB). More than 95% of the RAID groups use a custom version of RAID-6 where the parity blocks are not rotated; the parity blocks live in two separate drives (P and Q drives as shown in <ref type="figure">Figure 1</ref>). The other 4% use RAID-0 and 1% use RAID-4. We only select RAID groups that have at least three data drives <ref type="bibr">(D 1 .</ref>.D N where N ≥ 3 in <ref type="figure">Figure 1</ref>), mainly to allow us measure the relative slowdown compared to the median latency. Our dataset contains RAID groups with 3-26 data drives per group. <ref type="figure" target="#fig_0">Figure 2a</ref> shows the RAID width distribution (only data drives); wide RAID (e.g., more than 8 data drives) is popular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">About the Dataset</head><p>Scale of dataset: A summary of our dataset is shown in <ref type="table">Table 1</ref>. Our dataset contains 38,029 disk and 572 SSD groups within deployment duration of 87 days on average <ref type="figure" target="#fig_0">(Figure 2b</ref>). This gives us 72 and 1 million disk and SSD RAID hours to analyze respectively. When broken down to individual drives, our dataset contains 458,482 disks and 4069 SSDs. In total, we analyze 857 million and 7 million disk and SSD drive hours respectively.</p><p>Data collection: The performance and event logs we analyze come from production systems at customer sites. When the deployed RAID systems "call home", an autosupport system collects hourly performance metrics such as: average I/O latency, average latency per block, and number of I/Os and blocks received every hour. All these metrics are collected at the RAID layer. For each of Latency slowdown of Di compared to the median;</p><formula xml:id="formula_1">Si = Li/L med T k</formula><p>The k-th largest slowdown ("k-th longest tail");   <ref type="figure" target="#fig_0">Figure  2a</ref>. Li, Si and T k are explained in Section 3.3.</p><formula xml:id="formula_2">T 1 = M ax of (S1..N ), T 2 = 2nd M ax of (S1</formula><p>these metrics, the system separates read and write metrics. In addition to performance information, the system also records drive events such as response timeout, drive not spinning, unplug/replug events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Metrics</head><p>Below, we first describe the metrics that are measured by the RAID systems and recorded in the auto-support system. Then, we present the metrics that we derived for measuring tail latencies (slowdowns). Some of the important metrics are summarized in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Measured Metrics</head><p>Data drives (N ): This symbol represents the number of data drives in a RAID group. Our study only includes data drives mainly because read operations only involve data drives in our RAID-6 with non-rotating parity. Parity drives can be studied as well, but we leave that for future work. In terms of write operations, the RAID small-write problem is negligible due to the file system optimizations ( §3.1).</p><p>Per-drive hourly average I/O latency (L i ): Of all the metrics available from the auto-support system, we at the end only use the hourly average I/O latency (L i ) observed by every data drive <ref type="bibr">(D i</ref> ) in every RAID group (i=1..N ), as illustrated in <ref type="figure">Figure 1</ref>. We initially analyzed "throughput" metrics as well, but because the support system does not record per-IO throughput average, we cannot make an accurate throughput analysis based on hourly average I/O sizes and latencies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Derived Metrics</head><p>Slowdown (S i ): To measure tail latencies, RAID is a perfect target because it allows us to measure the relative slowdown of a drive compared to the other drives in the same group. Therefore, as illustrated in <ref type="figure">Figure 1</ref>, for every hour, we first measure the median group latency L med from L 1..N and then measure the hourly slowdown of a drive (S i ) by comparing its latency with the median latency (L i /L med ). The total number of S i is essentially the "#drive hours" in <ref type="table">Table 1</ref>. Our measurement of S i is reasonably accurate because most of the workload is balanced across the data drives and the average latencies (L i ) are based on per-drive I/Os whose size variance is small (see §3.1).</p><p>Stable vs. slow drive hours: Assuming that most workload is balanced across all the data drives, a "stable" drive should not be much slower than other drives. Thus, we use a slowdown threshold of 2x to differentiate slow drive hours (S i ≥ 2) and stable hours (S i &lt; 2). We believe 2x slowdown threshold is tolerant enough, but conversations with several practitioners suggest that a conservative 1.5x threshold will also be interesting. Thus, in some of our findings, we show additional results using 1.5x slowdown threshold.</p><p>Conceptually, drives appear to behave similar to a simple Markov model in <ref type="figure" target="#fig_1">Figure 3</ref>. In a given hour, a drive can be stable or slow. In the next hour, the drive can stay in the same or transition to the other condition.</p><p>Tails (T k ): For every hourly S 1..N , we derive the k-th largest slowdown represented as T k . In this study, we only record the three largest slowdowns (T 1 , T 2 and T 3 ). T 1 represents the "longest tail" in a given RAID hour, as illustrated in <ref type="figure">Figure 1</ref>. The total number of T 1 is the "#RAID hours" in <ref type="table">Table 1</ref>. The differences among T k values will provide hints to the potential benefits of tailtolerant RAID.</p><p>Tail hours: A "tail hour" implies a RAID hour that observes T 1 ≥2 (i.e., the RAID group observes at least one slow drive in that hour). This metric is important for fullstripe balanced workload where the performance will follow the longest tail (i.e., the entire RAID slows down at the rate of T 1 ).  From the above metrics, we can further measure other metrics such as slowdown intervals, extents, and repetitions. Overall, we have performed an in-depth analysis of all the measured and derived metrics. In many cases, due to space constraints, we aggregate some results whenever the sub-analysis does not show different behaviors. For example, we merge read and write slowdowns as I/O slowdown. In some graphs, we break down the slowdowns (e.g., to 2-4x, 4-8x, 8-16x) if their characterizations are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We now present the results of our study in four sets of analysis: slowdown and tail distributions and characteristics ( §4.1), correlations between slowdowns and workload-related metrics ( §4.2) and other available metrics ( §4.3), and post-slowdown analysis ( §4.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Slowdown Distributions and Characteristics</head><p>In this section, we present slowdown and tail distributions and their basic characteristics such as temporal behaviors and the extent of the problem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Slowdown (S i ) Distribution</head><p>We first take all S i values and plot their distribution as shown by the thick (blue) line in <ref type="figure" target="#fig_2">Figure 4</ref> (steeper lines imply more stability). <ref type="table" target="#tab_6">Table 3</ref> details some of the slowdown and percentile intersections. Finding #1: Storage performance instability is not uncommon. <ref type="figure" target="#fig_2">Figure 4</ref> and <ref type="table" target="#tab_6">Table 3b</ref> show that there exists 0.22% and 0.58% of drive hours (99.8 th and 99.4 th percentiles) where some disks and SSDs exhibit at least 2x slowdown (S i ≥ 2). With a more conservative 1.5x slowdown threshold, the percentiles are 99.3 th and 98.7 th for disk and SSD respectively. These observations imply that user demands of stable latencies at 99.9 th percentile <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b54">54]</ref> (or 99 th with 1.5x threshold) are not met by current storage devices.</p><p>Disk and SSD slowdowns can be high in few cases. <ref type="table" target="#tab_6">Table 3a</ref> shows that at four and five nines, slowdowns reach ≥9x and ≥30x respectively. In some of the worst cases, 3-and 4-digit disk slowdowns occurred in 2461 and 124 hours respectively, and 3-digit SSD slowdowns in 10 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Tail (T k ) Distribution</head><p>We next plot the distributions of the three longest tails (T 1−3 ) in <ref type="figure" target="#fig_2">Figure 4</ref>. Storage tails appear at a significant rate. The T 1 line in <ref type="figure" target="#fig_2">Figure 4</ref> shows that there are 1.54% and 2.23% "tail hours" (i.e., RAID hours with at least one slow drive). With a conservative 1.5x threshold, the percentiles are 95.4 th and 95.2 th for disk and SSD respectively. These numbers are alarming for full-stripe workload because the whole RAID will appear to be slow if one drive is slow. For such workload, stable latencies at 99 th percentile (or 96 th with 1.5x threshold) cannot be guaranteed by current RAID deployments.</p><p>The differences between the three longest tails shed light on possible performance improvement from tailtolerant RAID. If we reconstruct the late data from the slowest drive by reading from a parity drive, we can cut the longest tail. This is under an assumption that drive slowdowns are independent and thus reading from the parity drive can be faster. If two parity blocks are available (e.g., in RAID-6), then tail-tolerant RAID can read two parity blocks to cut the last two tails.</p><p>Finding #3: Tail-tolerant RAID has a significant potential to increase performance stability. The T 1 and T 2 values at x=2 in <ref type="figure" target="#fig_2">Figure 4a</ref> suggests the opportunity to reduce disk tail hours from 1.5% to 0.6% if the longest tail can be cut, and furthermore to 0.3% (T 3 ) if the two longest tails can be cut. Similarly, <ref type="figure" target="#fig_2">Figure 4b</ref> shows that SSD tail hours can be reduced from 2.2% to 1.4%, and furthermore to 0.8% with tail-tolerant RAID.</p><p>The T 1 line in <ref type="figure" target="#fig_2">Figure 4b</ref> shows several vertical steps (e.g., about 0.6% of T 1 values are exactly 2.0). To understand this, we analyze S i values that are exactly 1.5x, 2.0x, and 3.0x. We find that they account for 0.4% of the entire SSD hours and their corresponding hourly and median latencies (L i and L med ) are exactly multiples of 250 µs. We are currently investigating this further with the product groups to understand why some of the deployed SSDs behave that way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Temporal Behavior</head><p>To study slowdown temporal behaviors, we first measure the slowdown interval (how many consecutive hours a slowdown persists). <ref type="figure" target="#fig_3">Figure 5a</ref> plots the distribution of slowdown intervals.</p><p>Finding #4: Slowdown can persist over several hours. <ref type="figure" target="#fig_3">Figure 5a</ref> shows that 40% of slow disks do not go back to stable within the next hour (and 35% for SSD). Furthermore, slowdown can also persist for a long time. For example, 13% and 3% of slow disks and SSDs stay slow for 8 hours or more respectively. Next, we measure the inter-arrival period of slowdown occurrences from the perspective of each slow drive. <ref type="figure" target="#fig_3">Figure 5b</ref> shows the fraction of slowdown occurrences that arrive within X hours of the preceding slowdown; the arrival rates are binned by hour.</p><p>Finding #5: Slowdown has a high temporal locality. <ref type="figure" target="#fig_3">Figure 5b</ref> shows that 90% and 85% of disk and SSD slowdown occurrences from the same drive happen within the same day of the previous occurrence respectively. The two findings above suggest that history-based tail mitigation strategies can be a fitting solution; a slowdown occurrence should be leveraged as a good indicator for the possibility of near-future slowdowns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Slowdown Extent</head><p>We now characterize the slowdown extent (i.e., fraction of drives that have experienced slowdowns) in two ways. First, <ref type="figure" target="#fig_4">Figure 6a</ref> plots the fraction of all drives that have exhibited at least one occurrence of at least X-time slowdown ratio as plotted on the x-axis.</p><p>Finding #6: A large extent of drive population has experienced slowdowns at different rates. <ref type="figure" target="#fig_4">Figure 6a</ref> depicts that 26% and 29% of disk and SSD drives have exhibited ≥2x slowdowns at least one time in their lifetimes respectively. The fraction is also relatively significant for large slowdowns. For example, 1.6% and 2.5% of disk and SSD populations have experienced ≥16x slowdowns at least one time.</p><p>Next, we take only the population of slow drives (26% and 29% of the disk and SSD population) and plot the fraction of slow drives that has exhibited at least X slowdown occurrences, as shown in <ref type="figure" target="#fig_4">Figure 6b</ref>.</p><p>Finding #7: Few slow drives experience a large number of slowdown repetitions. <ref type="figure" target="#fig_4">Figure 6b</ref> shows that that around 6% and 5% of slow disks and SSDs exhibit at least 100 slowdown occurrences respectively. The majority of slow drives only incur few slowdown repetitions. For example, 62% and 70% of slow disks and SSDs exhibit only less than 5 slowdown occurrences re- spectively. We emphasize that frequency of slowdown occurrences above are only within the time duration of 87 days on average ( §3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Workload Analysis</head><p>The previous section presents the basic characteristics of drive slowdowns. We now explore the possible root causes, starting with workload analysis. Drive slowdowns are often attributed to unbalanced workload (e.g., a drive is busier than other drives). We had a hypothesis that such is not the case in our study due to the storage stack optimization ( §3.2). To explore our hypothesis, we correlate slowdown with two workload-related metrics: size and rate imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Slowdown vs. Rate Imbalance</head><p>We first measure the hourly I/O count for every drive (R i ), the median (R med ), and the rate imbalance (RI i = R i /R med ); this method is similar to the way we measure S i in <ref type="table" target="#tab_3">Table 2</ref>. If workload is to blame for slowdowns, then we should observe a high correlation between slowdown (S i ) and rate imbalance (R i ). That is, slowdowns happen in conjunction with rate imbalance, for example, S i ≥ 2 happens during R i ≥ 2. <ref type="figure" target="#fig_5">Figure 7a</ref> shows the rate imbalance distribution (RI i ) only within the population of slow drive hours. A rate imbalance of X (on the x-axis) implies that the slow drive serves X times more I/Os. The figure reveals that only 5% of slow drive hours happen when the drive receives 2x more I/Os than the peer drives. 95% of the slowdowns happen in the absence of rate imbalance (the rateimbalance distribution is mostly aligned at x=1).</p><p>To strengthen our conjecture that rate imbalance is not a factor, we perform a reverse analysis. To recap, <ref type="figure" target="#fig_5">Figure  7a</ref> essentially shows how often slowdowns are caused by rate imbalance. We now ask the reverse: how often does rate imbalance cause slowdowns? The answer is shown in <ref type="figure" target="#fig_5">Figure 7b</ref>; it shows the slowdown distribution (S i ) only within the population of "overly" rate-imbalanced drive hours (RI i ≥ 2). Interestingly, rate imbalance has negligible effect on slowdowns; only 1% and 5% of rateimbalanced disk and SSD hours experience slowdowns. From these two analyses, we conclude that rate imbalance is not a major root cause of slowdown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Slowdown vs. Size Imbalance</head><p>Next, we correlate slowdown with size imbalance. Similar to the method above, we measure the hourly average I/O size for every drive (Z i ), the median (Z med ), and the size imbalance (ZI i = Z i /Z med ). <ref type="figure" target="#fig_5">Figure 7c</ref> plots the size imbalance distribution (ZI i ) only within the population of slow drive hours. A size imbalance of X implies that the slow drive serves X times larger I/O size. The size-imbalance distribution is very much aligned at x=1. Only 2.5% and 1.1% of slow disks and SSDs receive 2x larger I/O size than the peer drives in their group. Reversely, <ref type="figure" target="#fig_5">Figure 7d</ref> shows that only 0.1% and 0.2% of size-imbalanced disk and SSD hours experience more than 2x slowdowns.</p><p>Finding #8: Slowdowns are independent of I/O rate and size imbalance. As elaborated above, the large majority of slowdown occurrences (more than 95%) cannot be attributed to workload (I/O size or rate) imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Other Correlations</head><p>As workload imbalance is not a major root cause of slowdowns, we now attempt to find other possible root causes by correlating slowdowns with other metrics such as drive events, age, model and time of day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Drive Event</head><p>Slowdown is often considered as a "silent" fault that needs to be monitored continuously. Thus, we ask: are there any explicit events surfacing near slowdown occurrences? To answer this, we collect drive events from our auto-support system. drive events. However, when specific drive events happen (specifically, "disk is not spinning" and "disk is not responding"), 90% of the cases lead to slowdown occurrences. We rarely see storage timeouts (e.g., SCSI timeout) because timeout values are typically set coarsely (e.g., 60 seconds). Since typical latency ranges from tens of microseconds to few milliseconds, a slowdown must be five orders of magnitude to hit a timeout. Thus, to detect tail latencies, storage performance should be monitored continuously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Drive Age</head><p>Next, we analyze if drive age matters to performance stability. We break the the slowdown distribution (S i ) by different ages (i.e., how long the drive has been deployed) as shown in <ref type="figure" target="#fig_6">Figure 8</ref>.</p><p>For disks, the bold lines in <ref type="figure" target="#fig_6">Figure 8a</ref> clearly show that older disks experience more slowdowns. Interestingly, the population of older disks is small in our dataset and yet we can easily observe slowdown prevalence within this small population (the population of 6-10 year-old disks ranges from 0.02-3% while 1-5 year-old disks ranges from 8-33%). In the worst case, the 8th year, the 95 th percentile already reaches 2.3x slowdown. The 9th year (0.11% of the population) seems to be an outlier. Performance instability from disk aging due to mechanical wear-out is a possibility ( §2).</p><p>For SSD, we do not observe a clear pattern. Although <ref type="figure" target="#fig_6">Figure 8b</ref> seemingly shows that young SSDs experience more slowdowns than older drives, it is hard to make such as a conclusion because of the small old-SSD population (3-4 year-old SSDs only make up 16% of the population while the 1-2 year-old is 83%).</p><p>Finding #10: Older disks tend to exhibit more slowdowns. For SSDs, no high degree of correlation can be made between slowdown and drive age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Drive Model</head><p>We now correlate slowdown with drive model. Not all of our customers upload the model of the drives they use. Only 70% and 86% of customer disks and SSDs have model information. Thus, our analysis in this section is based on partial population.</p><p>We begin by correlating SSD model and slowdown. The SSD literature highlights the pressure to increase density, which leads to internal idiosyncrasies that can induce performance instability. Thus, it is interesting to know the impact of different flash cell levels to SSD performance stability.</p><p>Finding #11: SLC slightly outperforms MLC drives in terms of performance stability. As shown in <ref type="figure" target="#fig_7">Figure 9</ref>, at 1.5x slowdown threshold, MLC drives only reaches 98.2 th percentile while SLC reaches 99.5 th percentile. However, at 2x slowdown threshold, the distribution is only separated by 0.1%. As MLC exhibits less performance stability than SLC, future comparisons with TLC drives will be interesting.</p><p>Our dataset contains about 60:40 ratio of SLC vs. MLC drives. All the SLC drives come from one vendor, but the MLC drives come from two vendors with 90:10 population ratio. This allows us to compare vendors.</p><p>Finding #12: SSD vendors seem to matter. As shown by the two thin lines in <ref type="figure" target="#fig_7">Figure 9</ref>, one of the vendors (the 10% MLC population) has much less stability compared to the other one. This is interesting because the instability is clearly observable even within a small population. At 1.5x threshold, this vendor's MLC drives already reach 94.3 th percentile (out of the scope of <ref type="figure" target="#fig_7">Figure 9</ref>). For disks, we use different model parameters such as storage capacity, RPM, and SAN interfaces (SATA, SAS, or Fibre Channel). However, we do not see any strong correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Time of Day</head><p>We also perform an analysis based on time of day to identify if night-time background jobs such as disk scrubbing cause slowdowns. We find that slowdowns are uniformly distributed throughout the day and night. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Post-Slowdown Analysis</head><p>We now perform a post-mortem analysis: what happens after slowdown occurrences? We analyze this from two angles: RAID performance degradation and unplug/replug events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">RAID Performance Degradation</head><p>A slow drive has the potential to degrade the performance of the entire RAID, especially for full-stripe workload common in the studied RAID systems ( §3.1), it is reasonable to make the following hypothesis: during the hour when a drive slows down, the RAID aggregate throughput will drop as the workload's intensity will be throttled by the slow drive. Currently, we do not have access to throughput metrics at the file system or application levels, and even if we do, connecting metrics from different levels will not be trivial. We leave cross-level analysis as future work, but meanwhile, given this constraint, we perform the following analysis to explore our hypothesis. We derive a new metric, RIO (hourly RAID I/O count), which is the aggregate number of I/Os per hour from all the data drives in every RAID hour. Then, we derive RIO degradation (RAID throughput degradation) as the ratio RIO lastHour to RIO currentHour . If the degradation is larger than one, it means the RAID group serves less I/Os than the previous hour.</p><p>Next, we distinguish stable-to-stable and stable-to-tail transitions. Stable RAID hour means all the drives are stable (Si &lt; 2). Tail RAID hour implies at least one of the drives is slow. In stable-to-stable transitions, RIO degradation can naturally happen as workload "cools down". Thus, we first plot the distribution of stable-tostable RIO degradation, shown by the solid blue line in <ref type="figure" target="#fig_8">Figure 10</ref>. We then select only the stable-to-tail transitions and plot the RIO degradation distribution, shown by the dashed red line in <ref type="figure" target="#fig_8">Figure 10</ref>.</p><p>Finding #13: A slow drive can significantly degrade the performance of the entire RAID. <ref type="figure" target="#fig_8">Figure 10</ref> depicts a big gap of RAID I/O degradation between stable-tostable and stable-to-tail transitions. In SSD-based RAID, the degradation impact is quite severe. <ref type="figure" target="#fig_8">Figure 10b</ref>  example shows that only 12% of stable-to-stable transitions observe ≥2x RIO degradation (likely from workload cooling down). However, in stable-to-slow transitions, there is 23% more chance (the vertical gap at x=2) that RIO degrades by more than 2x. In disk-based RAID, RIO degradation is also felt with 7% more chance. This finding shows the real possibilities of workload throughput being degraded and stable drives being under-utilized during tail hours, which again motivates the need for tailtolerant RAID. We note that RAID degradation is felt more if user requests are casually dependent; RIO degradation only affects I/Os that are waiting for the completion of previous I/Os. Furthermore, since our dataset is based on hourly average latencies, there is no sufficient information that shows every I/O is delayed at the same slowdown rate. We believe these are the reasons why we do not see a complete collapse of RIO degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Unplug Events</head><p>When a drive slows down, the administrator might unplug the drive (e.g., for offline diagnosis) and later replug the drive. Unplug/replug is a manual administrator's process, but such events are logged in our auto-support system. To analyze unplug patterns, we define wait-hour as the number of hours between a slowdown occurrence and a subsequent unplug event; if a slowdown persists in consecutive hours, we only take the first slow hour. <ref type="figure" target="#fig_9">Figures 11a-b</ref> show the wait-hour distribution within the population of slow disks and SSDs respectively.</p><p>Finding #14: Unplug events are common. <ref type="figure" target="#fig_9">Figures  11a-b</ref> show that within a day, around 4% and 8% of slow (≥2x) disks and SSDs are unplugged respectively. For "mild" slowdowns (1.5-2x), the numbers are 3% and 6%. <ref type="figure" target="#fig_9">Figure 11a</ref> also shows a pattern where disks with more severe slowdowns are unplugged at higher rates; this pattern does not show up in SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Replug Events</head><p>We first would like to note that unplug is not the same as drive replacement; a replacement implies an unplug without replug. With this, we raise two questions: What is the replug rate? Do replugged drives exhibit further slowdowns? To analyze the latter, we define recurhour as the number of hours between a replug event and the next slowdown occurrence. <ref type="figure" target="#fig_9">Figures 11c-d</ref> show the recur-hour distribution within the population of slow disks and SSDs respectively.</p><p>Finding #15: Replug rate is high and slowdowns still recur after replug events. In our dataset, customers replug 89% and 100% of disks and SSDs that they unplugged respectively (not shown in figures). <ref type="figure" target="#fig_9">Figures  11c-d</ref> answer the second question, showing that 18% and 35% of replugged disks and SSDs exhibit another slowdown within a day. This finding points out that administrators are reluctant to completely replace slow drives, likely because slowdowns are transient (not all slowdowns appear in consecutive hours) and thus cannot be reproduced in offline diagnosis and furthermore the cost of drive replacement can be unnecessarily expensive. Yet, as slowdown can recur, there is a need for online tail mitigation approaches.</p><p>In terms of unplug-replug duration, 54% of unplugged disks are replugged within 2 hours and 90% within 10 hours. For SSD, 61% are replugged within 2 hours and 97% within 10 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Summary</head><p>It is now evident that storage performance instability at the drive level is not uncommon. One of our major findings is the little correlation between performance instability and workload imbalance. One major analysis challenge is the "silent" nature of slowdowns; they are not accompanied by explicit drive events, and therefore, pinpointing the root cause of each slowdown occurrence is still an open problem. However, in terms of the overall findings, our conversations with product teams and vendors <ref type="bibr" target="#b2">[4]</ref> confirm that many instances of drive performance faults are caused by drive anomalies; there are strong connections between our findings and some of the anecdotal evidence we gathered ( §2). As RAID deployments can suffer from storage tails, we next discuss the concept of tail-tolerant RAID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Tail-Tolerant RAID</head><p>With drive performance instability, RAID performance is in jeopardy. When a request is striped across many drives, the request cannot finish until all the individual I/Os complete ( <ref type="figure">Figure 1)</ref>; the request latency will follow the tail latency. As request throughput degrades, stable drives become under-utilized. Tail-tolerant RAID is one solution to the problem and it brings two advantages.</p><p>First, slow drives are masked. This is a simple goal but crucial for several reasons: stringent SLOs require stability at high percentiles (e.g., 99% or even 99.9% <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52]</ref>); slow drives, if not masked, can create cascades of performance failures to applications <ref type="bibr" target="#b16">[16]</ref>; and drive slowdowns can falsely signal applications to back off, especially in systems that treat slowdowns as hints of overload <ref type="bibr" target="#b24">[24]</ref>.</p><p>Second, tail-tolerant RAID is a cheaper solution than drive replacements, especially in the context of transient slowdowns ( §4.1.3) and high replug rates by administrators ( §4.4.3). Unnecessary replacements might be undesirable due to the hardware cost and the expensive RAID re-building process as as drive capacity increases.</p><p>Given these advantages, we performed an opportunity assessment of tail-tolerant strategies at the RAID level. We emphasize that the main focus of this paper is the large-scale analysis of storage tails; the initial exploration of tail-tolerant RAID in this section is only to assess the benefits of such an approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tail-Tolerant Strategies</head><p>We explore three tail-tolerant strategies: reactive, proactive, and adaptive. They are analogous to popular approaches in parallel distributed computing such as speculative execution <ref type="bibr" target="#b14">[14]</ref> and hedging/cloning <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b13">13]</ref>. To mimic our RAID systems ( §3.2), we currently focus on tail tolerance for RAID-6 with non-rotating parity <ref type="figure">(Fig- ure 1</ref> and §3.1). We name our prototype ToleRAID, for simplicity of reference.</p><p>Currently, we only focus on full-stripe read workload where ToleRAID can cut "read tails" in the following ways. In normal reads, the two parity drives are unused (if no errors), and thus can be leveraged to mask up to two slow data drives. For example, if one data drive is slow, ToleRAID can issue an extra read to one parity drive and rebuild the "late" data.</p><p>Reactive: A simple strategy is reactive. If a drive (or two) has not returned the data for ST x (slowdown threshold) longer than the median latency, reactive will perform an extra read (or two) to the parity drive(s). Reactive strategy should be enabled by default in order to cut extremely long tails. It is also good for mostly stable environment where slowdowns are rare. A small ST will create more extra reads and a large ST will respond late to tails. We set ST = 2 in our evaluation, which means we still need to wait for roughly an additional 1x median latency to complete the I/O (a total slowdown of 3x in our case). While reactive strategies work well in cluster computing (e.g., speculative execution for mediumlong jobs), they can react too late for small I/O latencies (e.g., hundreds of microseconds). Therefore, we explore proactive and adaptive approaches.</p><p>Proactive: This approach performs extra reads to the parity drives concurrently with the original I/Os. The number of extra reads can be one (P drive) or two (P and Q); we name them PROACTIVE 1 and PROACTIVE 2 respectively. Proactive works well to cut short tails (near the slowdown threshold); as discussed above, reactive depends on ST and can be a little bit too late. The downside of proactive strategy is the extra read traffic.</p><p>Adaptive: This approach is a middle point between the two strategies above. Adaptive by default runs the reactive approach. When the reactive policy is triggered repeatedly for SR times (slowdown repeats) on the same drive, then ToleRAID becomes proactive until the slowdown of the offending drive is less than ST . If two drives are persistently slow, then ToleRAID runs PROAC-TIVE 2 . Adaptive is appropriate for instability that comes from persistent and periodic interferences such as background SSD GC, SMR log cleaning, or I/O contention from multi-tenancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>Our user-level ToleRAID prototype stripes each RAID request into 4-KB chunks ( §3.2), merge consecutive perdrive chunks, and submit them as direct I/Os. We insert a delay-injection layer that emulates I/O slowdowns. Our prototype takes two inputs: block-level trace and slowdown distribution. Below, we show ToleRAID results from running a block trace from Hadoop Wordcount benchmark, which contains mostly big reads. We perform the experiments on 8-drive RAID running IBM 500GB SATA-600 7.2K disk drives.</p><p>We use two slowdown distributions: (1) Rare distribution, which is uniformly sampled from our disk dataset <ref type="figure" target="#fig_2">(Figure 4a)</ref>. Here, tails (T 1 ) are rare (1.5%) but long tails exist <ref type="table" target="#tab_6">(Table 3)</ref>. (2) Periodic distribution, based on our study of Amazon EC2 ephemeral SSDs (not shown due to space constraints). In this study, we rent SSD nodes and found a case where one of the locally-attached SSDs periodically exhibited 5x read slowdowns that lasted for 3-6 minutes and repeated every 2-3 hours (2.3% instability period on average). <ref type="figure" target="#fig_0">Figure 12</ref> shows the pros and cons of the four policies using the two different distributions. In all cases, <ref type="bibr">PROAC</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussions</head><p>We hope our work will spur a set of interesting future research directions for the larger storage community to address. We discuss this in the context of performancelog analysis and tail mitigations.</p><p>Enhanced data collection: The first limitation of our dataset is the hourly aggregation, preventing us from performing micro analysis. Monitoring and capturing fine-grained data points will incur high computation and storage overhead. However, during problematic periods, future monitoring systems should capture detailed data. Our ToleRAID evaluation hints that realistic slowdown distributions are a crucial element in benchmarking tail-tolerant policies. More distribution benchmarks are needed to shape the tail-tolerant RAID research area. The second limitation is the absence of other metrics that can be linked to slowdowns (e.g., heat and vibration levels). Similarly, future monitoring systems can include such metrics.</p><p>Our current SSD dataset is two orders of magnitude smaller than the disk dataset. As SSD becomes the front-end storage in datacenters, larger and longer studies of SSD performance instability is needed. Similarly, denser SMR disk drives will replace old generation disks <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b18">18]</ref>. Performance studies of SSD-based and SMRbased RAID will be valuable, especially for understanding the ramifications of internal SSD garbage-collection and SMR cleaning to the overall RAID performance.</p><p>Further analyses: Correlating slowdowns to latent sector errors, corruptions, drive failures (e.g., from SMART logs), and application performance would be interesting future work. One challenge we had was that not all vendors consistently use SMART and report drive errors. In this paper, we use median values to measure tail latencies and slowdowns similar to other work <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b52">52]</ref>. We do so because using median values will not hide the severity of long tails. Using median is exaggerating if (N-1)/2 of the drives have significantly higher latencies than the rest; however, we did not observe such cases. Finally, we mainly use 2x slowdown threshold, and occasionally show results from a more conservative 1.5x threshold. Further analyses based on average latency values and different threshold levels are possible.</p><p>Tail mitigations: We believe the design space of tailtolerant RAID is vast considering different forms of RAID (RAID-5/6/10, etc.), different types of erasure coding <ref type="bibr" target="#b38">[38]</ref>, various slowdown distributions in the field, and diverse user SLA expectations. In our initial assessment, ToleRAID uses a black-box approach, but there are other opportunities to cut tails "at the source" with transparent interactions between devices and the RAID layer. In special cases such as materials trapped between disk head and platter (which will be more prevalent in "slim" drives with low heights), the file system or RAID layer can inject random I/Os to "sweep" the dust off. In summary, each root cause can be mitigated with specific strategies. The process of identifying all possible root causes of performance instability should be continued for future mitigation designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Large-scale storage studies at the same scale as ours were conducted for analysis of whole-disk failures <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b40">40]</ref>, latent sector errors <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b36">36]</ref>, and sector corruptions <ref type="bibr" target="#b7">[9]</ref>. Many of these studies were started based on anecdotal evidence of storage faults. Today, as these studies had provided real empirical evidence, it is a common expectation that storage devices exhibit such faults. Likewise, our study will provide the same significance of contribution, but in the context of performance faults. <ref type="bibr">Krevat et al. [33]</ref> demonstrate that disks are like "snowflakes" (same model can have 5-14% throughput variance); they only analyze throughput metrics on 70 drives with simple microbenchmarks. To the best of our knowledge, our work is the first to conduct a large-scale performance instability analysis at the drive level.</p><p>Storage performance variability is typically addressed in the context of storage QoS (e.g., mClock <ref type="bibr" target="#b25">[25]</ref>, PARDA <ref type="bibr" target="#b24">[24]</ref>, Pisces <ref type="bibr" target="#b42">[42]</ref>) and more recently in cloud storage services (e.g., C3 <ref type="bibr" target="#b45">[45]</ref>, <ref type="bibr">CosTLO [52]</ref>). Other recent work reduces performance variability at the file system (e.g., Chopper <ref type="bibr" target="#b30">[30]</ref>), I/O scheduler (e.g., split-level scheduling <ref type="bibr" target="#b55">[55]</ref>), and SSD layers (e.g., Purity <ref type="bibr" target="#b11">[12]</ref>, Flash on Rails <ref type="bibr" target="#b43">[43]</ref>). Different than ours, these sets of work do not specifically target drive-level tail latencies.</p><p>Finally, as mentioned before, reactive, proactive and adaptive tail-tolerant strategies are lessons learned from the distributed cluster computing (e.g., MapReduce <ref type="bibr" target="#b14">[14]</ref>, dolly <ref type="bibr" target="#b4">[6]</ref>, Mantri <ref type="bibr" target="#b5">[7]</ref>, KMN <ref type="bibr" target="#b50">[50]</ref>) and distributed storage systems (e.g., Windows Azure Storage <ref type="bibr" target="#b31">[31]</ref>, RobuSTore <ref type="bibr" target="#b53">[53]</ref>). The applications of these high-level strategies in the context of RAID will significantly differ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have "transformed" anecdotes of storage performance instability into large-scale empirical evidence. Our analysis so far is solely based on last generation drives (few years in deployment). With trends in disk and SSD technology (e.g., SMR disks, TLC flash devices), the worst might be yet to come; performance instability can be more prevalent in the future, and our findings are perhaps just the beginning. File and RAID systems are now faced with more responsibilities. Not only must they handle known storage faults such as latent sector errors and corruptions <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">28,</ref><ref type="bibr" target="#b39">39]</ref>, but also now they must mask drive tail latencies as well. Lessons can be learned from the distributed computing community where a large body of work has been born since the issue of tail latencies became a spotlight a decade ago <ref type="bibr" target="#b14">[14]</ref>. Similarly, we hope "the tail at store" will spur exciting new research directions within the storage community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RAID width and dataset duration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Other metrics:Figure 3 :</head><label>3</label><figDesc>Figure 3: Conceptual drive slowdown model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Slowdown (S i ) and Tail (T k ) distributions ( §4.1.1- §4.1.2). The figures show distributions of disk (top) and SSD (bottom) hourly slowdowns (Si), including the three longest tails (T 1−3 ) as defined in Table 2. The y-axis range is different in each figure and the x-axis is in log2 scale. We plot two gray vertical lines representing 1.5x and 2x slowdown thresholds. Important slowdown-percentile intersections are listed in Table 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Temporal behavior ( §4.1.3). The figures show (a) the CDF of slowdown intervals (#hours until a slow drive becomes stable) and (b) the CDF of slowdown inter-arrival rates (#hours between two slowdown occurrences).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Slowdown extent ( §4.1.4). Figure (a) shows the fraction of all drives that have experienced at least one occurrence of X-time slowdown ratio as plotted on the x-axis; the y-axis is in log10 scale. Figure (b) shows the fraction of slow drives that has exhibited at least X slowdown occurrences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: CDF of size and rate imbalance ( §4.2). Figure (a) plots the rate imbalance distribution (RIi) within the population of slow drive hours (Si ≥ 2). A rate imbalance of X implies that the slow drive serves X times more I/Os, as plotted in the x-axis. Reversely, Figure (b) plots the slowdown distribution (Si) within the population of rate-imbalanced drive hours (RIi ≥ 2). Figures (c) and (d) correlate slowdown and size imbalance in the same way as Figures (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Finding #9:Figure 8 :</head><label>8</label><figDesc>Figure 8: Drive age ( §4.3.2). The figures plot the slowdown distribution across different (a) disk and (b) SSD ages. Each line represents a specific age by year. Each figure legend is sorted from the left-most to right-most lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: SSD models ( §4.3.3). The figure plots the slowdown distribution across different SSD models and vendors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: RAID I/O degradation ( §4.4.1). The figures contrast the distributions of RIO degradation between stableto-stable and stable-to-tail transitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Unplug/replug events ( §4.4.2- §4.4.3). The figures show the relationships between slowdown occurrences and unplug/replug events. The top and bottom figures show the distribution of "wait-hour" and "recur-hour" respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Primary metrics. The table presents the metrics used in our analysis. The distribution of N is shown in</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>The y-axis range 
is different in each figure and the x-axis is in log2 scale. We 
plot two gray vertical lines representing 1.5x and 2x slowdown 
thresholds. Important slowdown-percentile intersections are 
listed in Table 3. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>1 ) at Y th percentile Disk</head><label></label><figDesc></figDesc><table>(a) 

Y: 
90 th 
95 99 
99.9 
99.99 99.999 
Slowdown (Si) at Y th percentile 
Disk 1.1x 1.2 1.4 
2.7 
9 
30 
SSD 
1.1x 1.2 1.7 
3.1 
10 
39 
Greatest slowdown (T 1.3x 1.5 2.4 
9 
29 
229 
SSD 
1.3x 1.5 2.5 
20 
37 
65 

(b) 

X: 
1.2x 1.5x 
2x 
4x 
Percentile at Si=X 
Disk 97.0 th 
99.3 99.78 99.96 
SSD 95.9 th 
98.7 99.42 99.92 
Percentile at T 1 =X 
Disk 83.3 th 
95.4 98.50 99.72 
SSD 87.0 th 
95.2 97.77 99.67 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Slowdown and percentile intersections. The 

table shows several detailed points in Figure 4. Table (a) de-
tails slowdown values at specific percentiles. Table (b) details 
percentile values at specific slowdown ratios. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 details</head><label>3</label><figDesc></figDesc><table>several T 1 values at 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We thank Bianca Schroeder, our shepherd, and the anonymous reviewers for their tremendous feedback and comments. We also would like to thank Lakshmi N. Bairavasundaram and Shiqin Yan for their initial help and Siva Jayasenan and Art Harkin for their managerial support. This material is based upon work supported by NetApp, Inc. and the NSF (grant Nos. CCF-1336580, CNS-1350499 and CNS-1526304).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Personal Communication from Andrew Baptist of Cleversafe</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Personal Communication from Garth Gibson of CMU</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Personal Communication from NetApp Hardware and Product Teams</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skylight-A Window on Shingled Disk Operation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abutalib</forename><surname>Aghayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Effective Straggler Mitigation: Attack of the Clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 10th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reining in the Outliers in Map-Reduce Clusters using Mantri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Ganesh Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bikas</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 9th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An Analysis of Latent Sector Errors in Disk Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lakshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">R</forename><surname>Bairavasundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM Conference on Measurement and Modeling of Computer Systems (SIGMETRICS)</title>
		<meeting>the 2007 ACM Conference on Measurement and Modeling of Computer Systems (SIGMETRICS)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Analysis of Data Corruption in the Storage Stack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lakshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">R</forename><surname>Bairavasundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 6th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Amazon S3 -905 Billion Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Barr</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">650</biblScope>
			<biblScope unit="page">0</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Requests</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Second</surname></persName>
		</author>
		<ptr target="http://aws.amazon.com/cn/blogs/aws/amazon-s3-905-billion-objects-and-650000-requestssecond" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Speed Matters for Google Web Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Brutlag</surname></persName>
		</author>
		<ptr target="http://services.google.com/fh/files/blogs/google_delayexp.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Purity: Building fast, highly-available enterprise flash storage from commodity components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Colgrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cary</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Tamches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Tail at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz Andr</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 6th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s Highly Available Key-value Store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madan</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunavardhan</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 21st ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Limplock: Understanding the Impact of Limpware on Scale-Out Cloud Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiratat</forename><surname>Patana-Anake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 4th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Startup Takes Aim at Performance-Killing Vibration in Datacenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Feldman</surname></persName>
		</author>
		<ptr target="http://www.hpcwire.com/2010/01/19/startup" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>takes aim at performance-killing vibration in datacenter/</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Shingled Magnetic Recording: Areal Density Increase Requires New Data Management. USENIX ;login: Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data Integrity on 20nm SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Frickey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Flash Memory Summit</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Seagate Q2 solid, 61.3 exabytes shipped</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>Gagliordi</surname></persName>
		</author>
		<ptr target="http://www.zdnet.com/article/seagate-q2-solid-61-3-exabytes-shipped/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The digital universe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Gantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reinsel</surname></persName>
		</author>
		<ptr target="http://www.emc.com/collateral/analyst-reports/idc-the-digital-universe-in-2020.pdf" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Characterizing flash memory: anomalies, observations, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Grupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Yaakobi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">H</forename><surname>Siegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Bleak Future of NAND Flash Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Grupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 10th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">PARDA: Proportional Allocation of Resources for Distributed Storage Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 7th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">mClock: Handling Throughput Variability for Hypervisor IO Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arif</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 9th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What Bugs Live in the Cloud? A Study of 3000+ Issues in Cloud Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiratat</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Patana-Anake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffry</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adityatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kurnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agung</forename><surname>Eliazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Laksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincentius</forename><surname>Lukman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anang</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 5th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swetha</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving File System Reliability with I/O Shepherding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 21st ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">SQCK: A Declarative File System Checker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Rajimwale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 8th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Reducing File System Tail Latencies with Chopper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Erasure Coding in Windows Azure Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huseyin</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Yekhanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2012 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SSD Market On Track For More Than Double Growth This Year</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="http://hothardware.com/news/" />
	</analytic>
	<monogr>
		<title level="m">SSD-Market-On-Track-For-More-Than -Double-Growth-This-Year</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disks Are Like Snowflakes: No Two Are Alike</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elie</forename><surname>Krevat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Tucek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 13th Workshop on Hot Topics in Operating Systems (HotOS XIII)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tales of the Tail: Hardware, OS, and Application-level Sources of Tail Latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">R K</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">D</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gribble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 5th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Amazon found every 100ms of latency cost them 1% in sales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Liddle</surname></persName>
		</author>
		<ptr target="http://blog.gigaspaces.com/amazon-found-every-100ms-of-latency-cost-them-1-in-sales/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">RAIDShield: Characterizing, Monitoring, and Proactively Protecting Against Disk Failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanlin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surendar</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Windsor</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Failure Trends in a Large Disk Drive Population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolf-Dietrich</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz Andre</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 5th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Performance Evaluation and Examination of Open-Source Erasure Coding Libraries for Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">D</forename><surname>Schuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zooko</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-O&amp;apos;</forename><surname>Hearn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 7th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understanding Latent Sector Errors and How to Protect Against Them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Damouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipa</forename><surname>Gill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 8th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 5th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand Lal</forename><surname>Shimpi</surname></persName>
		</author>
		<ptr target="http://www.anandtech.com/show/4142/intel-discovers-bug-in-6series-chipset-begins-recall" />
		<title level="m">Intel Discovers Bug in 6-Series Chipset: Our Analysis</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Performance Isolation and Fairness for Multi-Tenant Cloud Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Shue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Shaikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 10th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Flash on rails: Consistent flash performance through redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Skourtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Maltzahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2014 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<ptr target="http://radar.oreilly.com/2009/07/velocity-making-your-site-fast.html" />
		<title level="m">Steve Souders. Velocity and the Bottom Line</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">C3: Cutting Tail Latency in Cloud Data Stores via Adaptive Replica Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Feldmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 12th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">C3: Cutting Tail Latency in Cloud Data Stores via Adaptive Replica Selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Feldmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 12th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Utah Emulab Testbed. RAID controller timeout problems on boss node</title>
		<ptr target="http://www.emulab.net/news.php3" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The SCADS Director: Scaling a Distributed Storage System Under Stringent Performance Requirements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beth</forename><surname>Trushkowsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armando</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 9th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Samsung Acknowledges the SSD 840 EVO Read Performance Bug -Fix Is on the Way</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Vatto</surname></persName>
		</author>
		<ptr target="http://www.anandtech.com/show/8550/samsung-acknowledges-the-ssd-840-evo-read-performance-bug-fix-is-on-the-way" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The Power of Choice in Data-Aware Cluster Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 11th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Reducing SSD Read Latency via NAND Flash Program and Erase Suspension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 10th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">CosTLO: Cost-Effective Redundancy for Lower Latency Variance on Cloud Storage Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><forename type="middle">V</forename><surname>Madhyastha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 12th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">RobuSTore: Robust Performance for Distributed Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Conference on High Performance Networking and Computing (SC)</title>
		<meeting>the 2007 Conference on High Performance Networking and Computing (SC)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Wrangler: Predictable and Faster Jobs using Fewer Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neeraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 5th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Split-Level I/O Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salini</forename><surname>Selvaraj Kowsalya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Al-Kiswany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rini</forename><forename type="middle">T</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 25th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving MapReduce Performance in Heterogeneous Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 8th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
