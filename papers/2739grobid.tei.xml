<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging the Edge-Cloud Barrier for Real-time Advanced Vision Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiding</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HKUST</orgName>
								<orgName type="institution" key="instit2">HKUST</orgName>
								<orgName type="institution" key="instit3">HKUST</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HKUST</orgName>
								<orgName type="institution" key="instit2">HKUST</orgName>
								<orgName type="institution" key="instit3">HKUST</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HKUST</orgName>
								<orgName type="institution" key="instit2">HKUST</orgName>
								<orgName type="institution" key="instit3">HKUST</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchen</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HKUST</orgName>
								<orgName type="institution" key="instit2">HKUST</orgName>
								<orgName type="institution" key="instit3">HKUST</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">HKUST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">HKUST</orgName>
								<orgName type="institution" key="instit2">HKUST</orgName>
								<orgName type="institution" key="instit3">HKUST</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">HKUST</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging the Edge-Cloud Barrier for Real-time Advanced Vision Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Advanced vision analytics plays a key role in a plethora of real-world applications. Unfortunately, many of these applications fail to leverage the abundant compute resource in cloud services, because they require high computing resources and high-quality video input, but the (wireless) network connections between visual sensors (cameras) and the cloud/edge servers do not always provide sufficient and stable bandwidth to stream high-fidelity video data in real time. This paper presents CloudSeg, an edge-to-cloud framework for advanced vision analytics that co-designs the cloud-side inference with real-time video streaming, to achieve both low latency and high inference accuracy. The core idea is to send the video stream in low resolution, but recover the high-resolution frames from the low-resolution stream via a super-resolution procedure tailored for the actual analytics tasks. In essence, CloudSeg trades additional cloud-side computation (super-resolution) for significantly reduced network bandwidth. Our initial evaluation shows that compared to previous work, CloudSeg can reduce bandwidth consumption by ∼6.8× with negligible drop in accuracy.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have seen an explosive growth of real-world vision-based applications, primarily driven by advances in traditionally challenging vision tasks, e.g. multiple object detection <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29]</ref>, instance segmentation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> and panoptic segmentation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. To obtain adequate inference accuracy, these tasks often require both high computation power and high-resolution images (or video streams). This, however, poses a fundamental challenge to real-time vision-based applications. On the one hand, many video analytics tasks have been optimized for cloud environments (e.g. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref>). This seems to suggest one should send data via the bandwidth-limited connection to the cloud in the hope that the sophisticated cloud-side model can still extract enough information from the limited data. This hope, unfortunately, turns out to be illusory for advanced vision analytics tasks; while reducing video resolution (or frame rate) does save bandwidth, it will nevertheless inflict non-trivial drop in inference accuracy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref>. On the other hand, some realtime advanced vision applications, e.g. autonomous driving, put expensive hardware accelerators <ref type="bibr" target="#b14">[15]</ref> on edge devices to perform local inference. However, this approach does not make much economic sense when future applications require large-scale deployment, e.g. fleets of delivery vehicles <ref type="bibr" target="#b22">[23]</ref>.</p><p>In this paper, we present CloudSeg, an edge-to-cloud video analytics framework that optimizes for both high accuracy and low latency. CloudSeg lowers the quality in which the video is sent to the cloud, but it then runs a super-resolution (SR) procedure at the cloud server to reconstruct high-quality videos before executing the actual video analytics (video segmentation, object detection, etc.). This approach is in the same spirit of prior applications of SR where high-quality images are needed when only low-quality images are available <ref type="bibr" target="#b6">[7]</ref>. What's new is that we found it can potentially strike a desirable balance between accuracy and latency in the edgeto-cloud analytics setting. Essentially, running SR uses much less cloud resource and cause less delay than the actual inference, and it could restore the video quality so that video analytics task could achieve the same accuracy as if the video is streamed in high quality.</p><p>That said, we found that current SR models do not always perform as well as expected. This is because traditional SR models seek to retain pixel-level details (i.e., minimizing visual quality loss), which does not always retain the information needed by vision analytics. A notable example of such mismatch is the recovery of small details such as distant pedestrians. Traditional SR models, trained to uniformly recover all pixels to meet a given target quality, may fail to recover enough details for small object than for big objects, thus making small objects hard to identify or segment. However, these small objects are crucial (just as other large objects) to the accuracy of vision tasks and the practicality of applications e.g. autonomous driving.</p><p>To address the limitations of SR, we train our SR model in such a way that it reduces both quality loss as well as the <ref type="figure">Figure 1</ref>: CloudSeg framework overview accuracy loss of the analytics task. Given an existing SR model, which is essentially a deep neural network (DNN), we use an additional training process to fine-tune the weights of the SR model to minimize the accuracy loss of the superresolved frames on the cloud-side analytics model, as showed in <ref type="figure" target="#fig_0">Figure 2</ref>. To this end, the fine-tuning process uses the difference of inference accuracy between the original frames and the super-resolved frames as the loss function ( §3.1).</p><p>We further integrate CloudSeg with analytics models using the popular pyramid structure <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29]</ref> to reduce unnecessary downsampling overhead by reusing low-resolution data ( §3.2). Besides, we adaptively select useful frames for instance-level tasks with a 2-level frame selector to further reduce overhead while keeping good trackability. Finally, to cope with the bandwidth fluctuation, inspired by prior work <ref type="bibr" target="#b26">[27]</ref>, we adapt the video resolution and frame rate to the available bandwidth ( §3.3). Our preliminary results show that CloudSeg on average can save ∼6.8× bandwidth compared to a recently proposed baseline <ref type="bibr" target="#b26">[27]</ref> while achieving same inference accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Requirements of advanced vision analytics</head><p>This work considers advanced vision analytics tasks that require low latency and high inference accuracy. For example, for autonomous driving and multiple object detection applications, small and distant objects still matter so high-resolution input is necessary; for autonomous driving and robotics applications, high-frame-rate input is essential to ensure trackability because scenes generally change fast and real-time interaction requires low latency.</p><p>To achieve desirable accuracy, these advanced vision analytics needs to run highly complex models, increasingly in the form of deep neural networks (DNNs), with expensive hardware (GPUs) as well as on high-resolution inputs. For example, state-of-the-art real-time object detection model SSD <ref type="bibr" target="#b16">[17]</ref> can run at 300×300 in speed of 59 FPS (frames per second), while real-time accurate semantic segmentation model ICNet <ref type="bibr" target="#b28">[29]</ref> runs at 27 FPS on a 2048×1024 resolution input, both on Nvidia Titan X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video streaming for vision analytics</head><p>In many real-time video analytics applications, it is, however, fundamentally challenging to colocate expensive compute resources with high-fidelity video data considering scalability and cost. With more edge devices deployed in geographically distributed locations, how to collect their video streams to cloud for analytics without using too much bandwidth has attracted much attention.</p><p>The conventional wisdom has been that an edge device should compress its video, via pixel-level (spatial) downsampling and frame-level (temporal) downsampling, and ensure that sufficient information is retained, so that the cloud server can still run the vision analytics model on the downsampled video and produce highly accurate inference as if the video is not compressed. Specifically, AWStream <ref type="bibr" target="#b26">[27]</ref> learns a Paretooptimal policy and adaptively selects a data rate degradation strategy to meet the accuracy and bandwidth trade-off over the wide-area network for video object detection. FilterForward <ref type="bibr" target="#b2">[3]</ref> filters relevant video frames on the edge with small neural networks to save bandwidth and it shares the same spirit of prior filter-based frameworks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>As we will see in §4.2, while this approach <ref type="bibr" target="#b26">[27]</ref> works to some extent, it ultimately imposes a hard trade-off: at some point, when the frame rate needs to be retained high for advanced applications, more aggressive video downsampling always inflict a non-trivial drop in accuracy. As a result, it cannot be directly applied to serve advanced vision analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Super-resolution for vision analytics</head><p>Our solution is based on the recent advance in superresolution (SR) techniques. Ideally, a SR model can reconstruct a high-resolution scene from a low-resolution scene, by inferring details based only on information in the lowresolution input. Recently, DNN-based SR models have significantly improved the performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref>. Prior work has shown that SR is a promising approach to improving video streaming quality <ref type="bibr" target="#b25">[26]</ref> and boosting vision analytics accuracy <ref type="bibr" target="#b6">[7]</ref> when only low-resolution videos are available.</p><p>Our work differs from the prior work in two important aspects. First, we show that by applying SR on the downsampled video, the resulting reconstructed high-resolution video can usually produce almost the same accuracy as if the video was not downsampled. Although such result is not surprising, it suggests that SR could serve as an architectural role of "glue" between the video encoding stack (for saving bandwidth) and the video analytics (for maximizing accuracy). Second, through experiments, we also shed light on the limitations of current SR models, which are tailored to retain visual-based information, rather than maximizing analytics accuracy. Instead, we present a new way of training SR models such that the resulting model maximizes both the post-SR visual quality and the analytics accuracy.</p><p>We present CloudSeg, a new edge-to-cloud framework for real-time advanced video analytics. The workflow of CloudSeg is illustrated in <ref type="figure">Figure 1</ref>. On the edge side, the sensor (camera) adaptively downsamples a high-resolution video, and streams it to the cloud server via network. On the cloud side, the server then processes the video, runs (DNN-based) inference, and finally returns the inference results to the edge device. CloudSeg consists of three main components, which we will explain next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Analytics-aware super-resolution</head><p>To address the challenges of serving advanced vision analytics applications over the cloud as well as the limitations of analytics-agnostic SR discussed in §2, CloudSeg trains the SR model with a novel approach so that the resulting model maximizes both the post-SR visual quality and the analytics accuracy. We first train the SR model offline on the same dataset which was used to train the actual vision model, then fine-tune the SR model with an accuracy-oriented metric to further improve the inference accuracy especially on critical details. The resulting SR model is used to reconstruct highresolution (HR) images from the low-resolution (LR) input images before feeding them to the actual inference model on the cloud server.</p><p>We use a state-of-the-art super-resolution model CARN [2] to illustrate our method <ref type="figure" target="#fig_0">(Figure 2</ref>), which involves two steps:</p><p>• Base SR training: We use a semantic segmentation model ICNet <ref type="bibr" target="#b28">[29]</ref> as the vision-task model. Originally, CARN is trained to minimize the quality loss (structural similarity index, or SSIM) between the original HR frame and the resulting super-resolved (SR) frame.</p><p>• Analytics-aware fine-tuning: Next, we further train the SR model to improve the accuracy of specific vision tasks. We calculate the difference of inference accuracy between running the vision model on the original HR image and running it on the SR image. This difference is then used as the loss function which the new SR model is trained to minimize.</p><p>To better tailor SR model for our purposes, CloudSeg also uses different training parameters make the resulting SR model more amenable to video analytics. CARN adopts a patch-based CNN model, where a patch is any fixed-sized (e.g., 64×64) region which will undergo different forms of random deformations (cropping, flipping, rotation). To make the SR training aware of the analytics task, we use a much more fine-grained path than that used in ICNet (720×720). In our applications, small patches are crucial to identifying and retaining small details, such as distant pedestrians. However, CloudSeg applies the fine-grained patch only when finetuning SR model weights, for practical reasons. First, a small patch is not well-suited for accurate ICNet segmentation inference, so during the SR fine-tuning, we use the same patch size as the original ICNet. Note CloudSeg does not exactly rely on quality recovery since our ultimate goal is accurate vision inference, so a larger patch size is well-suited for the analytics-aware fine tuning. We also found another simple yet effective change that can boost the training of the base SR model. Many machinelearning dataset, including the one we use (Cityscapes <ref type="bibr" target="#b4">[5]</ref>) to train segmentation model, have many labeled images, but contains even more massive unlabeled images, which are collected but not manually labeled. These unlabeled images do not add value to the training of any machine-learning model, but they are as useful as labeled images when training the SR model! Compared to the naive approach where both SR and the vision analytics model are trained on the same labeled dataset (a small subset of the whole image set), incorporating the unlabeled images in the training of the base SR model can significantly boost the effectiveness (quality recovery) of the trained SR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vision analytics pipeline refining</head><p>There are two prevalent machine learning model structures for real-time inference, which are key frame feature propagation on high-frame-rate input for instance-level tasks and pyramid structure (or multi-scale structure) on high-resolution input. Each of them essentially improves the model inference efficiency in temporal (frame rate) or spatial (resolution) aspect. CloudSeg refines the pipeline for models using these two structures, such that (1) the computation overhead of key frame selection can be further saved by integrating it to the edge-side frame filter; or (2) by reusing the lowresolution data, the redundant process of downsampling the super-resolved frames in pyramid structure can be optimized.</p><p>Edge-side 2-level frame selection CloudSeg unifies the frame selection processes required by both the video streaming framework and the vision model. Originally, the video streaming framework skips stale frames to save bandwidth and retain trackability in instance-level tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref>, while in fast-inference vision models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>, key frame feature propagation reduces computation load by only running heavy inference on key frames. CloudSeg conducts a 2-level frame selection only once on the edge side, thus the computation overhead on the server is saved and the frame selection on the edge is more accurate by using the criteria of the vision task.</p><p>We define the frames which are necessary to stream as useful frames, such that key frames can be seen as the most useful frames. Intuitively, when the scene is changing rapidly, useful and key frames are more concentrated than when the scene is stable, so the criteria of frame selection is the pixel deviation of the task outputs (e.g. segmentation maps) of the current frame from that of the previous key frame.</p><p>Previous work <ref type="bibr" target="#b13">[14]</ref> devises a small and fast neural network which takes the differences between the low-level features of the current frame and the previous key frame as input, and predicts the deviation of segmentation maps to select key frames. If the predicted deviation goes beyond a pre-defined threshold, the current frame is set as a key frame, instead of selecting key frames with fixed intervals or simple heuristics. CloudSeg learns CV wisdom. We adapts this filtering method to our 2-level frame selector and deploy it on the edge device. It works in parallel with super-resolution ( §3.1).</p><p>As <ref type="figure" target="#fig_1">Figure 3</ref> shows, two thresholds target different frames: the higher one filters out key frames while the lower one filters out useful frames, and other stale frames will not be streamed to the server. Two thresholds are set by the adaptive controller in §3.3 such that they can be updated according to network conditions and application requirements.</p><p>Useful frames and tagged key frames will be streamed to the server and are compatible with the key frame feature propagation structure. For an instance-level model without key frame scheme, the selector falls back to a single-level useful frame filter to save bandwidth.</p><p>Low-resolution data reusing In parallel with superresolution, if the cloud-side vision model uses pyramid structure, CloudSeg will process received low-resolution data to a set of suitable resolutions and feed them to the model, thus we reduce the overheads of repeated super-resolving and downsampling. The pyramid structure <ref type="bibr" target="#b15">[16]</ref> let the vision model process high-resolution input together with several lower resolutions for fast inference while keeping accuracy <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>. Here we take ICNet <ref type="bibr" target="#b28">[29]</ref> as an example in our refined pipeline.</p><p>ICNet builds an inference path that employs information in the low-resolution frames along with details from the highresolution frames to achieve both low latency and high accuracy. For example, ICNet downsamples the 2048×1024 (HR) input by 2× (MR) and 4× (LR) respectively to feed the pyramid network. We found that for pyramid structure, a naive server-side workflow is to let the SR model upsample the LR input by 4× to HR, then let ICNet downsample HR to MR and LR to run inference with its multiple branches. This naive pipeline introduces repeated computation and the data quality loss.</p><p>CloudSeg refines this pipeline by reusing LR data. CloudSeg can apply the most suitable super-resolution and downsampling policy, then directly feed LR and post-SR frames to ICNet without the unnecessary downsampling process, as illustrated in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive bitrate controlling</head><p>While SR well handles the latency/accuracy trade-off in general (as shown in §4), it may fail in certain extreme cases such as those caused by variance of of scenes, e.g., light and weather changes or glitches (worst cases) of SR. The blue line in <ref type="figure" target="#fig_2">Figure 4</ref> shows the inference accuracy (mIoU) on a 30-second clip (experiment setting in §4). The minimal accuracy (≤ 0.6) is unacceptable for real-world applications, even the average is not that bad. This problem can be addressed by streaming a higher-resolution video to the backend model or even bypassing SR, as the red dashed line shows. To that end, we adopt an adaptive bitrate controller, similar to prior work <ref type="bibr" target="#b26">[27]</ref>, to handle the variance of network conditions, real-world scene changes, or performance drop of SR. Basically, it gathers network information from the transport layer, e.g., bandwidth and network latency, as well as application performance from the application layer, e.g., inference accuracy and computation time. Through offline/online profiling and training, we can learn a model and find a suitable knob policy including downsampling rate, frame rate and frame thresholds with little overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analytics-aware super-resolution</head><p>We compare the similarity criteria (PSNR, SSIM) and the inference accuracy criteria (mIoU) of a semantic segmentation task using the SR model with and without analytics-aware fine-tuning. HR is the 2048×1024 frame. We get the LR frame by resizing HR to 512×256 with bilinear, which is the default resize algorithm of TensorFlow <ref type="bibr" target="#b0">[1]</ref>, and the video size is deducted by 13.3×. Then we upsample LR to the original resolution with three methods: bilinear, content-aware SR and analytics-aware SR (SR-FT). The standard inference model ICNet is trained on the Cityscapes <ref type="bibr" target="#b4">[5]</ref> training set and mIoU is tested on the validation set. The mIoU of HR matches the performance claimed in the ICNet repository <ref type="bibr" target="#b0">1</ref> . PSNR and SSIM are both calculated over the RGB channels, so the exact values are different from the original paper, which are calculated over the luminance channel. Our fine-tuned SR model achieves a better inference accuracy compared with the vanilla SR. It improves the reconstruction of small details e.g. sharper edges of people in the distance which are important for the target advanced vision applications. We further compare the bandwidth consumption of CloudSeg with AWStream. Note that for a pixel-level semantic segmentation task here, we stream all the frames, and frames are only degraded on resolution. To achieve the same accuracy as CloudSeg, AWStream can only downsample the video to 1440×720. It consumes 5.1 Mbps bandwidth which is 6.8× larger than ours, as shown in <ref type="figure" target="#fig_3">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Inference latency</head><p>Besides network latency which is greatly reduced by our SR model, another major latency comes from the SR and vision model inference on the cloud server. We test the average inference time of super-resolving Cityscapes frames from 512×256 to 2048×1024 and semantic segmentation (ICNet) on a single Nvidia V100 GPU. The results are showed in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Can we do better under extremely low bandwidth? Our framework can greatly reduce the bandwidth consumption, but the bandwidth of wireless WAN could be extremely low that even our SR method can not recover sufficient inference accuracy. Again we turn to deep learning. Similar to SR, we consider utilize the computing resource of the cloud server to save bandwidth, with neural frame interpolation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>. We will investigate its impact on the tracking accuracy of instance-level tasks.</p><p>Handling uncertainty when applying ML for system Applying learning-based techniques may increase the uncertainty of the real-world system, especially in applications e.g. autonomous driving <ref type="bibr" target="#b5">[6]</ref>. We propose a method to handle the uncertainty to ensure the performance of the framework, and this is still an important topic for further research.</p><p>Video QoE for vision analytics tasks Traditional QoE of video streaming is designed for user watching experience. From our preliminary results, vision analytics tasks may value different metrics than human audience. With a special QoE for vision tasks, the cloud analytics framework may save more bandwidth and achieve better performance.</p><p>Online improvement of the framework We can improve the performance of DNN-based models online on the server with the raw data as reference. The raw data also serves online profiling. If the client streams raw data, it is collected to train the SR model and the vision analytics model. The logic is decided by the adaptive controller considering the available bandwidth and inference performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Train SR model offline with the new criteria</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Adaptive 2-level frame selection</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Variance of the inference performance with SR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Bandwidth consumption to achieve comparable accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>The pipeline of SR and semantic segmentation works 
at 23.5 FPS. Considering that the framework overhead (e.g. 
image loading, client-side processing) takes a rather small 
fraction, CloudSeg can run in real time. 

Model 
Time (ms) Frame (FPS) 

Super-Resolution 
6.2 
161.3 
Semantic Segmentation 
36.3 
27.5 

Total 
42.5 
23.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Inference time per frame</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="4"> Preliminary results We implement a prototype of CloudSeg and conduct experiments on the Cityscapes [5] dataset. We use semantic segmentation model ICNet [29] as our cloud-side vision model. Preliminary results show that CloudSeg can achieve real-time advanced vision analytics over the cloud with low bandwidth consumption and negligible accuracy loss.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast, accurate, and lightweight super-resolution with cascading residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namhyuk</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungkon</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Ah</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling video analytics on constrained edge nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Canel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramanya</forename><forename type="middle">R</forename><surname>Dulloor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Conference on Systems and Machine Learning (SysML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Glimpse: Continuous, real-time object recognition on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><surname>Yu-Han Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lenin</forename><surname>Ravindranath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramvir</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 13th ACM Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards a framework to manage perceptual uncertainty for safe automated driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Salay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Safety, Reliability, and Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="439" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Is image super-resolution helpful for other vision tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blitznet: A real-time deep network for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2017-International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wideactivated deep residual networks based restoration for bpg-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2621" to="2624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Focus: Querying large video datasets with low latency and low cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramvir</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthai</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Phillip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Noscope: optimizing neural network queries over video at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Abuzaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1586" to="1597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02446</idno>
	</analytic>
	<monogr>
		<title level="m">Panoptic feature pyramid networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00868</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">Panoptic segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Low-latency video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yule</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5997" to="6005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The architectural implications of autonomous driving: Constraints and acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqi</forename><surname>Shih-Chieh Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyThird International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the TwentyThird International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="751" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Phasenet for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelaziz</forename><surname>Djelouah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Schroers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="498" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware synthesis for video frame interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1701" to="1710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinventing video streaming for distributed vision analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisma</forename><surname>Pakha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchen</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th USENIX Workshop on Hot Topics in Cloud Computing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>HotCloud 18</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and accurate object detection in high resolution 4k and 8k video using gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vit</forename><surname>Ruzicka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Franchetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE High Performance extreme Computing Conference (HPEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clockwork convnets for video semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The prime challenges for scout, amazon&apos;s new delivery robot | wired</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arielle</forename><surname>Pardes</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9333" to="9343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multinet: Realtime joint semantic reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Teichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Zoellner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1013" to="1020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural adaptive content-aware internet video delivery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunho</forename><surname>Yeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngmok</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="645" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Awstream: Adaptive wide-area streaming analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the ACM Special Interest Group on Data Communication</title>
		<meeting>the 2018 Conference of the ACM Special Interest Group on Data Communication</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="236" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Matthai Philipose, Paramvir Bahl, and Michael J Freedman. Live video analytics at scale with approximation and delay-tolerance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2349" to="2358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
