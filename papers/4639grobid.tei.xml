<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2016-09">September 2016. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Natural Language and Dialogue Systems Lab</orgName>
								<orgName type="institution">University of California Santa Cruz Santa Cruz</orgName>
								<address>
									<postCode>95064</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Hernandez</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Natural Language and Dialogue Systems Lab</orgName>
								<orgName type="institution">University of California Santa Cruz Santa Cruz</orgName>
								<address>
									<postCode>95064</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
							<email>mawalker@ucsc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Natural Language and Dialogue Systems Lab</orgName>
								<orgName type="institution">University of California Santa Cruz Santa Cruz</orgName>
								<address>
									<postCode>95064</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Fine-Grained Knowledge about Contingent Relations between Everyday Events</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the SIGDIAL 2016 Conference</title>
						<meeting>the SIGDIAL 2016 Conference <address><addrLine>Los Angeles, USA</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="13" to="15"/>
							<date type="published" when="2016-09">September 2016. 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding , information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topic-specific datasets enables learning finer-grained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The original idea behind scripts as introduced by Schank was to capture knowledge about the finegrained events of everyday experience, such as opening a fridge enabling preparing food, or the event of getting out of bed being triggered by an alarm going off ( <ref type="bibr" target="#b25">Schank and Abelson, 1977;</ref><ref type="bibr" target="#b16">Mooney and DeJong, 1985)</ref> This idea has motivated previous work exploring whether commonsense knowledge about events can be learned from text, however, only a few learn from data other than newswire ( <ref type="bibr" target="#b12">Hu et al., 2013;</ref><ref type="bibr" target="#b15">Manshadi et al., 2008;</ref><ref type="bibr" target="#b1">Beamer and Girju, 2009)</ref>. News articles (obviously) cover newsworthy topics such</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Camping Trip</head><p>We packed all our things on the night before Thu (24 Jul) except for frozen food. We brought a lot of things along. We woke up early on Thu and JS started packing the frozen marinatinated food inside the small cooler... In the end, we decided the best place to set up the tent was the squarish ground that's located on the right. Prior to setting up our tent, we placed a tarp on the ground. In this way, the underneaths of the tent would be kept clean. After that, we set the tent up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Storm</head><p>I don't know if I would've been as calm as I was without the radio, as the hurricane made landfall in Galveston at 2:10AM on Saturday. As the wind blew, branches thudded on the roof or trees snapped, it was helpful to pinpoint the place... A tree fell on the garage roof, but it's minor damage compared to what could've happened. We then started cleaning up, despite Sugar Land implementing a curfew until 2pm; I didn't see any policemen enforcing this. Luckily my dad has a gas saw (as opposed to electric), so we helped cut up three of our neighbors' trees. I did a lot of raking, and there's so much debris in the garbage. as bombing, explosions, war and killing so the knowledge learned is limited to those types of events.</p><p>However, much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. These stories are rich with common-sense knowledge. For example, the Camping Trip story in <ref type="figure" target="#fig_0">Fig. 1</ref> contains implicit common-sense knowledge about contingent (causal and conditional) relations between camping-related events, such as setting up a tent and placing a tarp. The Storm story contains implicit knowledge about events such as the hurricane made landfall, the wind blew, a tree fell. Our aim is to learn fine-grained common-sense knowledge about contingent relations between everyday events from such stories. We show that the fine-grained knowledge we learn is simply not found in publicly available narrative and event schema collections <ref type="bibr" target="#b5">(Chambers and Jurafsky, 2009;</ref><ref type="bibr">Bala- subramanian et al., 2013)</ref>.</p><p>Personal stories provide both advantages and disadvantages for learning common-sense knowledge about events. An advantage is that they tend to be told in chronological order , and temporal order between events is a strong cue to contingency <ref type="bibr" target="#b19">(Prasad et al., 2008;</ref><ref type="bibr" target="#b1">Beamer and Girju, 2009</ref>). However, their structure is more similar to oral narrative than to newswire ). Only about a third of the sentences in a personal narrative describe actions, 1 so novel methods are needed to find useful relationships between events.</p><p>Another difference between our work and prior research is that much of the work on narrative schemas, scripts, or event schemas characterize what is learned as "collections of events that tend to co-occur". Thus what is learned is not evaluated for contingency <ref type="bibr" target="#b4">(Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b5">Chambers and Jurafsky, 2009;</ref><ref type="bibr" target="#b15">Manshadi et al., 2008;</ref><ref type="bibr" target="#b17">Nguyen et al., 2015;</ref><ref type="bibr" target="#b0">Balasubramanian et al., 2013;</ref><ref type="bibr" target="#b18">Pichotta and Mooney, 2014</ref>). Historically, work on scripts explicitly modeled causality <ref type="bibr" target="#b13">(Lehnert, 1981;</ref><ref type="bibr" target="#b16">Mooney and DeJong, 1985)</ref> inter alia. Our work is motivated by Penn Discourse Treebank (PDTB) definition of CONTINGENCY that has two types: CAUSE and CONDITION, and is more similar to approaches that learn specific event relations such as contingency or causality ( <ref type="bibr" target="#b12">Hu et al., 2013;</ref><ref type="bibr" target="#b7">Do et al., 2011;</ref><ref type="bibr" target="#b9">Girju, 2003;</ref><ref type="bibr" target="#b21">Riaz and Girju, 2010;</ref><ref type="bibr" target="#b23">Rink et al., 2010;</ref><ref type="bibr" target="#b6">Chklovski and Pantel, 2004</ref>). Our contributions are as follows:</p><p>• We use a corpus of everyday events for learning common-sense knowledge focusing on the contingency relation between events. We first use a subset of the corpus including general-domain stories. Next, we produce a topic-sorted set of stories using a semisupervised bootstrapping method to learn finer-grained knowledge. We use two different datasets to directly compare what is learned from topic-sorted stories as opposed to a general-domain story corpus (Sec. 2);</p><p>• We develop a new method for learning contingency relations between events that is tailored to the "oral narrative" nature of blog stories. We apply Causal Potential <ref type="bibr" target="#b1">(Beamer and Girju, 2009)</ref> to model the contingency relation between two events. We directly compare our method to several other approaches as baselines (Sec. 3). We also identify topicindicative contingent event pairs from our topic-specific corpus that can be used as building blocks for generating coherent event chains and narrative schema for a particular theme (Sec. 4.3);</p><p>• We conduct several experiments to evaluate the quality of the event knowledge learned in our work that indicate our results are contingent and topic-related. We directly compare the common-sense knowledge we learn with the Rel-grams collection and show that what we learn is not found in available corpora (Sec. 4).</p><p>We release our contingent event pair collections for each topic for future use of other research groups 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">A Corpus of Everyday Events</head><p>Our dataset is drawn from the Spinn3r corpus of millions of blog posts ( <ref type="bibr" target="#b3">Burton et al., 2009;</ref><ref type="bibr">Gor- don and Swanson, 2009;</ref><ref type="bibr" target="#b11">Gordon et al., 2012</ref>). We hypothesize that personal stories are a valuable resource to learn common-sense knowledge about relations between everyday events and that finergrained knowledge can be learned from topicsorted stories ( <ref type="bibr" target="#b21">Riaz and Girju, 2010</ref>) that share a particular theme, so we construct two different sets of stories: General-Domain Set. We created a random subset from the Spinn3r corpus from personal blog domains: livejournal.com, wordpress.com, blogspot.com, spaces.live.com, typepad.com, travelpod.com. This set consists of 4,200 stories not selected for any specific topic. Topic-Specific Set. We produced a dataset by filtering the corpus using a bootstrapping method to create topic-specific sets for topics such as going camping, being arrested, going snorkeling or scuba diving, visiting the dentist, witnessing a major storm, and holiday activities associated with Thanksgiving and Christmas (see <ref type="table">Table 1</ref>).</p><p>We apply AutoSlog-TS, a semi-supervised algorithm that learns narrative event-patterns to bootstrap a collection of stories on the same  <ref type="table">Table 1</ref>: Some topics and examples of their indicative events.</p><p>theme <ref type="bibr" target="#b22">(Riloff, 1996)</ref>. These patterns, developed for information extraction, search for the syntactic constituent with the designated word as its head. For example, consider the example in the first row of <ref type="table">Table 2</ref>: NP-Prep-(NP):CAMPING-IN. This pattern looks for a Noun Phrase (NP) followed by a Preposition (Prep) where the head of the NP is CAMPING and the Prep is IN. Our algorithm consists of the following steps for each topic:</p><p>1. Hand-labeling: We manually labeled a small set (∼ 200-300) of stories on the topic. 2. Generating Event-Patterns: Given handlabeled stories on a topic (from Step 1), and a random set of stories that are not relevant to that topic, AutoSlog-TS learns a set of syntactic templates (case frame templates) that distinguish the linguistic patterns characteristic of the topic from the random set. For each pattern it generates frequency and conditional probability which indicate how strongly the pattern is associated with the topic. <ref type="table">Table 2</ref> shows examples of such patterns that we have learned for two different topics. We call them indicative event-patterns for each topic. <ref type="table">Table 1</ref> shows examples of the indicative event-patterns for different topics. They are mapped to our event representation described in Sec 3, e.g., the pattern (subj)-ActVB-Dobj:WENT-CAMPING in <ref type="table">Table 2</ref> is mapped to go(dobj:camp). 3. Parameter Tuning: We use the frequency and probability generated by AutoSlog-TS and apply a threshold for filtering to select a subset of indicative event-patterns strongly associated with the topic. In this step we aim to find optimal val- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Storm</head><p>(subj)-ActVp-Dobj:LOST-POWER (subj)-ActVp:RESTORED (subj)-AuxVp-Dobj:HAVE-DAMAGE (subj)-ActVp:EVACUATED <ref type="table">Table 2</ref>: Examples of narrative event-patterns (case frames) learned from corpus.</p><p>ues for frequency and probability thresholds denoted as f-threshold and p-threshold respectively.</p><p>We divided the hand-labeled data from Step 1 into train and development sets and designed a classifier based on our bootstrapping method: if the number of event-patterns extracted from a post is more than a certain number (n-threshold), it is labeled as positive and otherwise it is labeled as negative meaning that it is not related to the topic. We repeated the classification for several combinations of different values for each of the three parameters and measured the precision, recall and fmeasure. We selected the optimal values for the thresholds that resulted in high precision (above 0.9) and average recall (around 0.4). We compromised on a lower recall to achieve a high precision to establish a highly accurate bootstrapping algorithm. Since bootstrapping is performed on a large set of stories, a low recall stills result in identifying enough stories per topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Bootstrapping:</head><p>We use the patterns learned in previous steps as indicative event-patterns for the topic. The bootstrapping algorithm processes each story, using AutoSlog-TS to extract lexicosyntactic patterns. Then it counts the indicative event-patterns in the extracted patterns, and labels the blog as a positive instance for that topic if the count is above the n-threshold value for that topic.</p><p>The manually labeled dataset includes 361 Storm and 299 Camping Trip stories. After one round of bootstrapping the algorithm identified 971 additional Storm and 870 more Camping Trip stories. The bootstrapping method is not evaluated separately, however, the results in Sec. 4.2 indicate that using the bootstrapped data considerably improves the accuracy of the contingency model and enhances extracting topic-relevant event knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Learning Contingency Relation between Narrative Events</head><p>In this section we describe our representation of events in narratives and our methods for modeling contingency relationship between events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Event Representation</head><p>In previous work different representations have been proposed for the event structure such as single verb and verb with two or more arguments.</p><p>Verbs are used as a central indication of an event in a narrative. However, other entities related to the verb also play a strong role in conveying the meaning of the event. In ( <ref type="bibr" target="#b18">Pichotta and Mooney, 2014</ref>) it is shown that the multi-argument representation is richer than the previous ones and is capable of capturing interactions between multiple events. We use a representation that incorporates the Particle of the verb in the event structure in addition to the Subject and the Direct Object and define an event as a verb with its dependency relations as follows:</p><p>Verb Lemma (subj:Subject Lemma, dobj:Direct Object Lemma, prt:Particle) <ref type="table" target="#tab_1">Table 3</ref> shows example sentences describing an event from the Camping topic along with their event structure. The examples show how including the arguments often change the meaning of an event. In Row 1 the direct object and particle are required to completely understand the event in this sentence. Row 2 shows another example where the verb have cannot implicate what event is happening and the direct object oatmeal is needed to understand what has occurred in the story.</p><p>We parse each sentence and extract every verb lemma with its arguments using Stanford dependencies ( <ref type="bibr" target="#b14">Manning et al., 2014</ref>). For each verb, we extract the nsubj, dobj, and prt dependency relations if they exist, and use their lemma in the event representation. To generalize the event representations, we use the types identified by Stanford's Named Entity Recognizer and map each argument to its named entity type if available, e.g., in Row 3 of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Potential Method</head><p>We define a contingent event pair as a sequence of two events (e 1 , e 2 ) such that e 1 and e 2 are likely to occur together in the given order and e 2 is contingent upon e 1 . We apply an unsupervised distributional measure called Causal Potential to induce the contingency relation between two events. Causal Potential (CP) was introduced by <ref type="bibr" target="#b1">Beamer and Girju (2009)</ref> as a way to measure the tendency of an event pair to encode a causal relation, where event pairs with high CP have a higher probability of occurring in a causal context. We calculate CP for every pair of adjacent events in each topic-specific dataset. We used a 2-skip bigram model which considers two events to be adjacent if the second event occurs within two or less events after the first one.</p><p>We use skip-2 bigram in order to capture the fact that two related events may often be separated by a non-essential event, because of the oralnarrative nature of our data ( ). In contrast to the verbs that describe an event (e.g., hike, climb, evacuate, drive), some verbs describe private states such as as belong, depend, feel, know. We filter out clauses that tend to be associated with private states <ref type="bibr" target="#b28">(Wiebe, 1990)</ref>. A pilot evaluation showed that this improves the results.</p><p>Equation 1 shows the formula for calculating Causal Potential of a pair consisting of two events: (e 1 , e 2 ). Here P denotes probability and P (e 1 → e 2 ) is the probability of e 2 occurring after e 1 in the adjacency window which is equal to 3 due to the skip-2 bigram model. P (e 2 |e 1 ) is the conditional probability of e 2 given that e 1 has been seen in the adjacency window. This is equivalent to the</p><note type="other">353 Event-Bigram model described in Sec. 3.3.</note><p>CP (e 1 , e 2 ) = log P (e 2 |e 1 )</p><formula xml:id="formula_0">P (e 2 ) + log P (e 1 → e 2 ) P (e 2 → e 1 )<label>(1)</label></formula><p>To calculate CP, we need to compute event counts from the corpus and thus we need to define when two events are considered equal. The simplest approach is to define two events to be equal when their verb and arguments exactly match. However, with a close look at the data this approach does not seem adequate. For example, consider the following events: go (subj:PERSON, dobj:camp) go (subj:family, dobj:camp) go (dobj:camp) They encode the same action although their representations do not exactly match and differ in the subject. Our intuition is that when we count the number of events represented as go (subj:PERSON, dobj:camp) we should also include the count of go (dobj:camp). To be able to generalize over the event structure and take into account these nuances, we consider two events to be equal if they have the same verb lemma and share at least one argument other than the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Methods</head><p>Our previous work on modeling contingency relations in film scripts data compared Causal Potential to methods used in previous work: Bigram event models ( <ref type="bibr" target="#b15">Manshadi et al., 2008)</ref> and Pointwise Mutual Information (PMI) <ref type="bibr" target="#b4">(Chambers and Jurafsky, 2008)</ref> and the evaluations showed that CP obtains better results ( <ref type="bibr" target="#b12">Hu et al., 2013)</ref>. In this work, we use CP for inducing contingency relation between events and apply three other models as baselines for comparison: Event-Unigram. This method will produce a distribution of normalized frequencies for events. Event-Bigram. We calculate the bigram probability of every pair of adjacent events using skip-2 bigram model using the Maximum Likelihood Estimation (MLE) from our datasets:</p><formula xml:id="formula_1">P (e 2 |e 1 ) = Count(e 1 , e 2 ) Count(e 1 )<label>(2)</label></formula><p>Event-SCP. We use the Symmetric Conditional Probability between event tuples (Rel-grams) used  </p><formula xml:id="formula_2">SCP (e 1 , e 2 ) = P (e 2 |e 1 ) × P (e 1 |e 2 )<label>(3)</label></formula><p>Like Event-Bigram, we used MLE for estimating Event-SCP from the corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Experiments</head><p>We conducted three sets of experiments to evaluate different aspects of our work. First, we compare the content of our topic-specific event pairs to current state of the art event collections to show that the fine-grained knowledge we learned about everyday events does not exist in previous work focused on the news genre. Second, we run an automatic evaluation test, modeled after the COPA task (Roemmele et al., 2011), on a held-out test set to evaluate the event pair collections that we have extracted from both General-Domain and TopicSpecific datasets, in terms of contingency relations. We hypothesize that the contingent event pairs can be used as basic elements for generating coherent event chains and narrative schema. So, in the third part of the experiments, we extract topicindicative contingent event pairs from our TopicSpecific dataset and run an experiment on Amazon Mechanical Turk (AMT) to evaluate the top N pairs with respect to their contingency relation and topic-relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison to Rel-gram Tuple Collections</head><p>We chose Rel-gram tuples ( <ref type="bibr" target="#b0">Balasubramanian et al., 2013</ref>) for comparison since it is the most relevant previous work to us: they generate pairs of relational tuples of events, called Rel-grams using co-occurrence statistics based on Symmetric Conditional Probability described in Sec 3.3. Additionally, the Rel-grams are publicly available through an online search interface <ref type="bibr">3</ref> and their evaluations show that their method outperforms the previous state of the art on generating narrative event schema. However, their work is focused on news articles and does not consider the causal relation between events for inducing event schema. We compare the content of what we learned from our topicspecific corpus to the Rel-gram tuples to show that the fine-grained type of knowledge that we learn is not found in their events collection. We also applied the co-occurrence statistics that they used on our data as a baseline (Event-SCP) for comparison to our method and present the results in <ref type="bibr">Sec. 4.2.</ref> In this experiment we compare the event pairs extracted from our Camping Trip topic to the Relgram tuples. The Rel-gram tuples are not sorted by topic. To find tuples relevant to Camping Trip, we used our top 10 indicative events and extracted all the Rel-gram tuples that included at least one event corresponding to one of the Camping Trip indicative events. For example, for go(dobj:camp), we pulled out all the tuples that included this event from the Rel-grams collection. The indicative events for each topic were automatically generated during the bootstrapping using AutoSlog-TS (Sec. 2).</p><p>Then we applied the same sorting and filtering methods presented in the Rel-grams work and removed any tuple with frequency less than 25 and sorted the rest by the total symmetrical conditional probability. These numbers are publicly available as a part of the Rel-grams collection. We evaluated the top N = 100 tuples of this list using the Mechanical Turk task described later in Sec. 4.3. The evaluation results presented in <ref type="table" target="#tab_3">Table 4</ref> show that 42% of the Rel-gram pairs were labeled as contingent by the annotators and only 7% were both contingent and topic-relevant. We argue that this is mainly due to the limitations of the newswire data which does not contain the fine-grained everyday events that we have extracted from our corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Automatic Two-Choice Test</head><p>For evaluating our contingent event pair collections we have automatically generated a set of two-choice questions along with the answers, modeled after the COPA task <ref type="bibr" target="#b24">(Roemmele et al., 2011</ref>   Question event: arrange (dobj:outdoor) Choice 1: help (dobj:trip) Choice 2: call (subj:PERSON)</p><p>In this example, arrange (dobj:outdoor) is followed by the event help (dobj:trip) in a document from the test set and call (subj:PERSON) was randomly generated. The model is supposed to predict which of the two choices is more likely to have a contingency relation with the event in the question. We argue that a strong contingency model should be able to choose the correct answer (the one that is adjacent to the question event) and the accuracy achieved on the test questions is an indication of the model's robustness. For the General-Domain dataset, we split the data into train (4,000 stories) and held-out test (200 stories) sets. For each topic-specific set, we divided the hand-labeled data into a train (Train-HL) and held-out test, and created a second train set consisting of Train-HL and the data collected by bootstrapping (Train-HL-BS) as shown in <ref type="table" target="#tab_5">Table 5</ref>. We automatically created a question for every event occurring in the test data which   resulted in 3,123 questions for General-Domain data, 2,058 for the Camping and 2,533 questions for the Storm topic. For each dataset, we applied the baseline methods and Causal Potential model on the train sets to learn contingent event pairs and tested the pair collections on the questions generated from held-out test set. We extracted about 418K contingent event pairs from General-Domain train set, 437K from Storm Train-HL-BS and 630K pairs from Camping Trip Train-HL-BS set using Causal Potential model. We used our automatic test approach to evaluate these event pair collections. The results for General-Domain and Topic-Specific datasets are shown in <ref type="table" target="#tab_6">Table 6</ref> and <ref type="table" target="#tab_8">Table 7</ref> respectively. The Causal Potential model trained on Train-HL-BS dataset achieved accuracy of 0.685 on Camping Trip and 0.887 on Storm topic which is significantly stronger than all the baselines. Our experiments indicate that having more training data collected by bootstrapping improves the accuracy of the model in predicting contingency relation between events. Additionally, the Causal Potential results on Topic-Specific dataset is significantly stronger than General-Domain narratives indicating that using a topic-sorted dataset improves learning causal knowledge about events.  <ref type="table">Table 8</ref>: Results of evaluating indicative contingent event pairs on AMT. <ref type="figure" target="#fig_3">Fig. 2</ref> shows some examples of event pairs with high CP scores extracted from general-Domain set. In the following section we extract topicindicative contingent event pairs and show that Topic-Specific data enables learning of finergrained event knowledge that pertain to a particular theme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Topic-Indicative Contingent Event Pairs</head><p>We identify contingent event pairs that are highly indicative of a particular topic. We hypothesize that these event pairs serve as building blocks of coherent event chains and narrative schema since they encode contingency relation and correspond to a specific theme. We evaluate the pairs on Amazon Mechanical Turk (AMT).</p><p>To identify event sequences that have a strong correlation to a topic (topic-indicative pairs) we applied two filtering methods. First, we selected the frequent pairs for each topic and removed the ones that occur less than 5 times in the corpus. Second, we used the indicative event-patterns for each topic and extracted the pairs that at least included one of these patterns. Indicative event-patterns are automatically generated during the bootstrapping using AutoSlog-TS and mapped to their corresponding event representation as described in Sec. 2. Then we used the Causal Potential scores from our contingency model for ranking the topic-indicative event pairs to identify the highly contingent ones. We sorted the pairs based on the Causal Potential score and evaluated the top N pairs in this list.</p><p>Evaluations and Results. We evaluate the indicative contingent event pairs using human judgment on Amazon Mechanical Turk (AMT). Narrative schema consists of chains of events that are related in a coherent way and correspond to a common theme. Consequently, we evaluate the extracted pairs based on two main criteria:</p><p>• Contingency: Two events in the pair are person -wake up → person -pack up -backpack pack up -tent → check out -video person -head → hike up person -play → person -pick up -sax climb → person -find -rock pack up -material → switch off -projector person -pack up -car → head out person -pick up -photo → person -swim Storm wind -blow -transformer → power -go out restore -community → hurricane -bend tree -fall -eave → crush boil → tree -fall -driveway Ike -blow → knock down -limb clean up -person → people -come out air -push -person → person -fall out blow -sign → person -sit hit -location → evacuate -person person -rock -way → bottle -fall <ref type="table">Table 9</ref>: Examples of event pairs evaluated on AMT.</p><p>likely to occur together in the given order and the second event is contingent upon the first one.</p><p>• Topic Relevance: Both events strongly correspond to the specified topic.</p><p>We have designed one task to assess both criteria since if an event pair is not contingent, it cannot be used in narrative schema for not satisfying the required coherence (even if it is topic-relevant). We asked the AMT annotators to rate each pair on a scale of 0-3 as follows:</p><p>0: The events are not contingent. 1: The events are contingent but not relevant to the specified topic. 2: The events are contingent and somewhat relevant to the specified topic. 3: The events are contingent and strongly relevant to the specified topic.</p><p>To ensure that the Amazon Mechanical Turk annotations are reliable, we designed a Qualification Type which requires the workers to pass a test before they can annotate our pairs. If the workers score 70% or more on the test they will qualify to do the main task. For each topic we created a Qualification test consisting of 10 event pairs from that topic that were annotated by two experts. To make the events more readable for the annotators we used the following representation:</p><p>Subject -Verb Particle -Direct Object For example, hike(subj:person, dobj:trail, prt:up) is mapped to person -hike uptrail. For each topic we evaluated top N = 100 event pairs and assigned 5 workers to rate each one. We generated a gold standard label for each pair by averaging over the scores assigned by the annotators and interpreted the average as follows:</p><formula xml:id="formula_3">Label &gt;2: Contingent &amp; strongly topic-relevant. Label = 2: Contingent &amp; somewhat topic- relevant. 1 ≤ Label &lt; 2: Contingent &amp; not topic-relevant. Label &lt; 1: Not contingent.</formula><p>To assess the inter-annotator reliability we calculated kappa between each worker and the majority of the labels assigned to each pair. The average kappa was 0.73 which indicates substantial agreement. The results in <ref type="table">Table 8</ref> show that 52% of the Camping Trip and 53% of the Storm pairs were labeled as contingent and topic-relevant by the annotators. The results also indicate that our model is capable of identifying event pairs with strong contingency relations: 82% of the Camping Trip pairs and 77% of the Storm pairs were marked as contingent by the workers. Examples of the strongest and weakest pairs evaluated on Mechanical Turk are shown in <ref type="table">Table 9</ref>. By comparison to <ref type="figure" target="#fig_3">Fig. 2</ref>, we can see that we can learn finer-grained type of events knowledge from topic-specific stories as compared to general-domain corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>We learned fine-grained common-sense knowledge about contingent relations between everyday events from personal stories written by ordinary people. We applied a semi-supervised bootstrapping approach using event-patterns to create topic-sorted sets of stories and evaluated our methods on a set of general-domain narratives as well as two topic-specific datasets. We developed a new method for learning contingency relations between events that is tailored to the "oral narrative" nature of the blog stories. Our evaluations indi-cate that a method that works well on the news genre does not generate coherent results on personal stories (comparison of Event-SCP baseline with Causal Potential).</p><p>We modeled the contingency (causal and conditional) relation between the events from each dataset using Causal Potential and evaluated on the questions automatically generated from a heldout test set. The results show significant improvement over the Event-Unigram, Event-Bigram, and Event-SCP (Rel-grams method) baselines on Topic-Specific stories: 25% improvement of accuracy on Camping Trip and 41% on Storm topic compared to Bigram model. In our future work, we plan to explore existing topic-modeling algorithms to create a broader set of topic-sorted corpora for learning contingent event knowledge.</p><p>Our experiments show that most of the finegrained contingency relations we learn from narrative events are not found in existing narrative and event schema collections induced from the newswire datasets (Rel-grams). We also extracted indicative contingent event pairs from each topic and evaluated them on Mechanical Turk. The evaluations show that 82% of the relations between events that we learn from topic-sorted stories are judged as contingent. We publicly release the extracted pairs for each topic. In future work, we plan to use the contingent event pairs as building blocks for generating coherent event chains and narrative schema on several different themes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Excerpts of two stories in the blogs corpus on the topics of Camping Trip and Storm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Topic Event-Pattern (Case Frame) Examples Camping NP-Prep-(NP):CAMPING-IN Trip NP-Prep-(NP):HIKE-TO (subj)-ActVB-Dobj:WENT-CAMPING NP-Prep-(NP):TENT-IN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of event pairs with high CP scores extracted from General-Domain stories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Camping person -pack up → person -go -home person -pick up -cup → person -swim Trip</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 , the Lost Valley River</head><label>3</label><figDesc></figDesc><table>Campground 
is represented by its type LOCATION. We use ab-
stract types for named entities such as PERSON, 
ORGANIZATION, TIME and DATE. We also repre-
sent each pronoun by the abstract type PERSON, 
e.g. Row 5 in Table 3. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Event representation examples from 
Camping Trip topic. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Evaluation of Rel-gram tuples on AMT.</head><label>4</label><figDesc></figDesc><table>in (Balasubramanian et al., 2013) as another base-
line method. The Rel-gram model is the most rele-
vant previous work to our method and outperforms 
the previous state of the art on generating narrative 
event schema. This metric combines bigram prob-
ability considering both directions: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). We produced questions from held-out test sets for each dataset. Each question consists of 3 http://relgrams.cs.washington.edu:10000/relgrams</figDesc><table>Topic 
Dataset 
# Docs 

Camping Hand-labeled held-out test 
107 
Trip 
Hand-labeled train (Train-HL) 
192 
Train-HL + Bootstrap (Train-HL-
BS) 
1,062 

Storm 
Hand-labeled held-out test 
98 
Hand-labeled train (Train-HL) 
263 
Train-HL + Bootstrap (Train-HL-
BS) 
1,234 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Number of stories in the train and test sets 
from topic-specific dataset. 

Model 
Accuracy 

Event-Unigram 
0.478 
Event-Bigram 
0.481 
Event-SCP (Rel-gram) 
0.477 
Causal Potential 
0.510 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Automatic two-choice test results for 
General-Domain dataset. 

one event and two choices. The question event is 
one that occurs in the test data. One of the choices 
is an event adjacent to the question event in the 
document. The other choice is an event randomly 
selected from the list of all events occurring in the 
test set. The following is an example of a question 
from the Camping Trip test set: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Automatic two-choice test results for 
Topic-Specific dataset. </table></figure>

			<note place="foot" n="1"> The other two thirds provide scene descriptions and descriptions of the thoughts or feelings of the narrator.</note>

			<note place="foot" n="2"> https://nlds.soe.ucsc.edu/everyday events</note>

			<note place="foot" n="1"> go (nsubj:PERSON) → go (dobj:trail , prt:down) 2 find (nsubj:PERSON , dobj:fellow) → go (prt:back) 3 see (nsubj:PERSON , dobj:gun) → see (dobj:police) 4 come (nsubj:PERSON) → go (nsubj:PERSON) 5 go (prt:out) → find (nsubj:PERSON , dobj:sconce) 6 go (nsubj:PERSON) → see (dobj:window, prt:out) 7 go (nsubj:PERSON) → walk (dobj:bit , prt:down) 8 go (nsubj:PERSON) → go (nsubj:PERSON , dobj:rafting)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generating coherent event schemas at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1721" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using a bigram event model to predict causal potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="430" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Probabilistic topic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The icwsm 2009 spinn3r dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Soboroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Annual Conference on Weblogs and Social Media (ICWSM)</title>
		<meeting>of the Annual Conference on Weblogs and Social Media (ICWSM)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-08: HLT</title>
		<meeting>of ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative schemas and their participants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 47th Annual Meeting of the ACL</title>
		<meeting>of the 47th Annual Meeting of the ACL</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Verbocean: Mining the web for fine-grained semantic verb relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Chklovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of Conference on Empirical Methods in Natural Language essing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Minimally supervised event causality identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><surname>Quang Xuan Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of the Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic detection of causal relations for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering</title>
		<meeting>the ACL 2003 workshop on Multilingual summarization and question answering</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="76" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying personal stories in millions of weblog entries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Weblogs and Social Media, Data Challenge Workshop</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Different strokes of different folks: Searching for health narratives in weblogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Andrew S Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">Owsley</forename><surname>Wienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on and 2012 International Confernece on Social Computing (SocialCom)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="490" to="495" />
		</imprint>
	</monogr>
	<note>Privacy, Security, Risk and Trust</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised induction of contingent event pairs from film scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larissa</forename><surname>Munishkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Plot units and narrative summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wendy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="331" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a probabilistic model of event sequences from internet weblog stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Manshadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 21st FLAIRS Conference</title>
		<meeting>of the 21st FLAIRS Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning schemata for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Dejong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page">61801</biblScope>
			<pubPlace>Urbana</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative event schema induction with entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiem-Hieu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Besançon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd annual meeting of the Association for Computational Linguistics (ACL-15)</title>
		<meeting>the 53rd annual meeting of the Association for Computational Linguistics (ACL-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Statistical script learning with multi-argument events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">220</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Webber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th International Conference on Language Resources and Evaluation (LREC 2008)</title>
		<meeting>of the 6th International Conference on Language Resources and Evaluation (LREC 2008)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Minimal narrative annotation schemes and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Corcoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Workshop on Intelligent Narrative Technologies</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Another look at causality: Discovering scenario-specific contingency relationships with no supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehwish</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic Computing (ICSC), 2010 IEEE Fourth International Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="361" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatically generating extraction patterns from untagged text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96)</title>
		<meeting>the Thirteenth National Conference on Artificial Intelligence (AAAI-96)</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="1044" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning textual graph patterns to detect causal event relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Rink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanda</forename><forename type="middle">M</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harabagiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew S</forename><surname>Cosmin Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Scripts, plans, goals and understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Abelson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<pubPlace>Lawrence Erlbaum</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A comparison of retrieval models for open domain story generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI 2009 Spring Symposium on Intelligent Narrative Technologies II</title>
		<meeting>of the AAAI 2009 Spring Symposium on Intelligent Narrative Technologies II<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying narrative clause types in personal stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reid</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Corcoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marilyn A</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">171</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Identifying subjective characters in narrative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janyce M Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th Conference on Computational linguistics. V2</title>
		<meeting>of the 13th Conference on Computational linguistics. V2</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="401" to="406" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
