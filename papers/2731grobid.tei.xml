<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Black-Box Approach for Estimating Utilization of Polled IO Network Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshit</forename><surname>Gupta</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † AT&amp;T Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhigyan</forename><surname>Sharma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † AT&amp;T Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Zelezniak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † AT&amp;T Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsung</forename><surname>Jang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † AT&amp;T Labs Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umakishore</forename><surname>Ramachandran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology † AT&amp;T Labs Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Black-Box Approach for Estimating Utilization of Polled IO Network Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cloud management tasks such as performance diagnosis, workload placement, and power management depend critically on estimating the utilization of an application. But, it is challenging to measure actual utilization for polled IO network functions (NFs) without code instrumentation. We ask if CPU events (e.g., data cache misses) measured using hardware performance counters are good at estimating utilization for polled-IO NFs. We find a strong correlation between several CPU events and NF utilization for three representative types of network functions. Inspired by this finding, we explore the possibility of computing a universal estimation function that maps selected CPU events to NF utilization estimates for a wide-range of NFs, traffic profiles and traffic loads. Our NF-specific estimators and universal estimators achieve absolute estimation errors below 6% and 10% respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Network functions (NFs) such as routers, firewalls, and proxies use polled network IO in userspace for high performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref>. Like any application, they require common cloud management tasks such as performance debugging <ref type="bibr" target="#b16">[17]</ref>, workload placement <ref type="bibr" target="#b15">[16]</ref> and power management <ref type="bibr" target="#b19">[20]</ref>. These tasks often depend on NF utilization: the percentage of the peak traffic supported by the NF that the traffic being processed by an NF represents, other factors such as hardware and traffic patterns remaining unchanged. But, due to polling, these NFs always report 100% CPU core utilization, even when they are not processing any packets! There are no serverlevel techniques to estimate the utilization of black-box NFs. The only approach is to instrument NF code to report cycles spent in packet processing and those spent in empty polls <ref type="bibr" target="#b26">[27]</ref>. However, a telco provider such as AT&amp;T deploys dozens of such NFs from vendors across the industry <ref type="bibr" target="#b11">[12]</ref>, which makes it difficult to instrument all NFs to collect a common set of statistics from all of them.</p><p>We propose an approach that requires no modifications to NFs or their network drivers. Our idea is to use CPU events, e.g., branch misses, or L3 cache misses, collected using hardware performance counters <ref type="bibr" target="#b0">[1]</ref>. CPU events have ubiquitous support in server CPUs and software utilities <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28]</ref>. Given a limited number of hardware counters in a CPU, the challenge is to choose which CPU events to monitor and how to map the events' counter values to NF utilization values. At the outset, there are two extreme possibilities for this line of research. It is possible that one finds a single CPU event (out of hundreds of events in a modern CPU <ref type="bibr" target="#b7">[8]</ref>) whose counter values can be mapped to NF utilization for all NFs for all traffic profiles. The other extreme would be if no combinations of CPU events are found to accurately estimate the utilization of any NF for common traffic scenarios. The reality, as we show in this paper, lies somewhere in the middle.</p><p>Our primary contribution is an empirical study of NF utilization estimation using CPU events with three NFs from different layers of the stack: a layer-3 forwarder <ref type="bibr" target="#b10">[11]</ref>, a layer-4 load balancer <ref type="bibr" target="#b23">[24]</ref>, and a layer-7 deep-packet inspection function <ref type="bibr" target="#b3">[4]</ref>. We subject these NFs to a range of workloads with varying packet sizes, connection lengths and traffic volumes for collecting data on counter values for all CPU events. We use the events' data to train a variety of estimation functions or estimators that map counter values for a small number of CPU events to an NF's utilization. Our evaluation of these estimators reveals following insights:</p><p>• Simple local (NF-specific) estimators achieve an absolute error below 6% using just 3 CPU events.</p><p>• A simple universal estimator for all NFs based on 3 events can achieve an absolute error of less than 10%.</p><p>• The accuracy achieved by local estimators on an unseen test profile is close to that on profiles seen previously.</p><p>• An estimator can achieve estimation errors close to the best while measuring hardware counters for 10 ms only.</p><p>We conclude with a discussion on designing improved universal estimators based on hierarchical estimation functions, include additional kinds of NFs virtualized NFs and evaluation of use cases such as workload management and performance debugging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Goal and approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A "top" for polled IO NFs</head><p>We provide background on DPDK -a framework for building polled I/O NFs, and discuss prior work on instrumentationbased techniques to assist management of NFs.</p><p>Anatomy of a DPDK application: DPDK [10] is a toolkit for developing high-performance NFs. DPDK applications use polled IO from userspace, a technique also used by other high-performance NFs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. This approach eliminates the overheads of context switching, packet copying and interrupt handling to maximize performance. But, polling results in 100% CPU use for the cores that the NF is running on.</p><p>Instrumentation-based techniques: Cloud management of NFs (including both polled NFs and others) takes help from instrumentation of NFs, their network interface drivers and cloud infrastructures. <ref type="bibr">Trifonov et al. [27]</ref> measure the number of empty polls to determine utilization and insert artificial delays between consecutive polls to reduce polling frequency. ENVI <ref type="bibr" target="#b4">[5]</ref> uses a combination of NF instrumentation and cloud infrastructure instrumentation to inform elastic scaling decisions for NFs. But, cloud instrumentation needs OS support to measure CPU utilization, which does not exist today for polled IO NFs. <ref type="bibr">Niccolini et al. [20]</ref> use NIC driver instrumentation to estimate queue lengths of NIC packet buffers. Queue lengths are used as a proxy for NF utilization in performing CPU power management. In general, these approaches are difficult to apply to unmodified polled IO NFs.</p><p>Goal: Our position is that getting a good utilization estimate for unmodified NFs can vastly simplify cloud management tasks such as performance diagnosis, workload management and power management for them. Towards this goal, we seek to create a utility like UNIX's top for polled IO NFs. Similar to top, this utility would list the polled IO NFs and their utilization on each of the cores they are running on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NF utilization via CPU events</head><p>CPU events: Our intuition is that a polled IO NF that is processing packets is likely to generate a different type of activity on the CPU, e.g., data cache access due to arriving packets or branch prediction errors in the polling thread due to non-empty polls. A modern CPU defines hundreds of these events (e.g., 714 CPU events on an Intel Xeon E5-2680 v3 in our testbed), which can give a detailed picture of CPU activity. This motivates us to explore if there are some CPU events whose frequency is strongly correlated to NF's utilization.</p><p>Hardware performance counters: A CPU core has a small number of registers called hardware performance counters, each of which can be programmed to count a CPU event as shown in <ref type="figure">Figure 1</ref>. Such counters have long been available in server CPUs such as Intel and AMD <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. Support for using them is available in various utilities such as Linux perf <ref type="bibr" target="#b27">[28]</ref> and Intel EMON <ref type="bibr" target="#b6">[7]</ref>. Hardware counters have been widely used such as for application profiling (e.g., L1/L2/L3 cache misses) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25]</ref> and modeling power use <ref type="bibr" target="#b17">[18]</ref>. msr_fd = open("/dev/cpu/2/msr", O_RDWR | O_SYNC); // descriptor for registers on CPU core 2 c = 0x005300c4; // CPU event for branch hits ret = pwrite(msr_fd, c, sizeof(c), 0x186); // configuration register 0x186 will record branch hits ... ret = pread(msr_fd, &amp;val, sizeof(val), 0xc1); // read counters from counter register 0xc1</p><p>Figure 1: Read/write to performance counter registers.</p><p>Advantages of CPU events: CPU events are deployment friendly for several reasons. They require no modification to NF, network drivers or cloud infrastructures. In comparison, existing instrumentation-based techniques would require considerable effort to instrument run dozens of NFs <ref type="bibr" target="#b11">[12]</ref> and NIC types in a large ISP network. Further, hardware counters can be read within few tens of cycles <ref type="bibr" target="#b18">[19]</ref> and hence can be used by operators on NFs running in production with minimal overhead to NFs.</p><p>Open questions: Are CPU events useful for estimating utilization for polled NFs? Specifically, (1) can we select a small number of CPU events whose values can be accurately mapped to an NF's utilization level? (2) Is the mapping function robust across variations in workload for that NF? <ref type="formula">(3)</ref> Is there a universal estimation function that can map a fixed set of CPU events and their counter values to utilization for all NFs with a low percentage error? (4) How long does it take to estimate an NF's utilization using this approach?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">An empirical study of estimation functions</head><p>An estimation function (or an estimator) on a set of CPU events accepts as input the frequency of occurrences of those events at a CPU core and outputs a percentage value from 0 to 100 that represents the utilization for the NF running on that core. In this section, we define the types of estimation functions we evaluate, describe the experimental setup to collect data for our evaluation and present statistical metrics on the accuracy of our estimation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local and universal estimators</head><p>We evaluate estimation functions under these key assumptions. Since CPU events are dependent on the CPU architecture and its configuration, we assume that the CPU used for evaluation as well as its BIOS settings remain unchanged. In practice, the estimators can be updated upon generational changes in hardware or major configuration changes. An NF can run on one more cores on a physical server. However, each core is assumed to perform identical packet processing so that an estimator for a single core can easily generalize to a multicore NF <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>. Henceforth, we assess the accuracy of single-core NFs. To assess the NF throughput corresponding to a 100% utilization, our evaluation assumes that the CPU is the bottleneck resource for the NF.</p><p>Let N be the set of NFs deployed and C_ALL be the set of CPU events that can be counted.</p><p>Local estimator A local estimator L nC outputs the utilization of NF n ∈ N given the counts of a set of CPU events C ⊂ C_ALL.</p><p>Universal estimator A univeral estimator U C outputs the utilization for any NF given the counts of a set of CPU event C ⊂ C_ALL.</p><p>To train the estimators, we build a set of test profiles T n for each NF n ∈ N. A test profile includes traffic characteristics such as packet sizes and flow lengths as well as an NF's internal configuration such as routing table sizes for a router or the size of memory cache for a proxy. For each traffic profile, the NF is subject to a set of loads L in the range 0-100, where a load of 0 means no traffic and a load of 100 means a traffic volume equal to the maximum throughput of the NF for that test profile. A local estimator L nC is trained based on data from the set of experiments T n × L, and an universal estimator U C is trained based on data from the set of experiments n T n × L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental methodology</head><p>Network functions: We have evaluated three NFs with varying amounts of computation and state.</p><p>A layer 3 forwarder (L3FWD for short) matches the destination IP of an incoming packet to the entries stored in its forwarding table and updates the layer 2 header before sending the packet to the next-hop router. The forwarder is a stateless NF because it does not maintain per-flow state. Our experiments are conducted with the L3FWD application included in the DPDK 18.02 release <ref type="bibr" target="#b10">[11]</ref>.</p><p>A layer 4 load balancer (L4LB for short) multiplexes connections to a pool of servers <ref type="bibr" target="#b23">[24]</ref>. In order to cope with a changing pool of servers, it creates a flow table entry upon the arrival of the first packet of a new connection. The flow table entry stores a connection identifier as the key (e.g., the TCP/IP 5-tuple) and the chosen server as the value. We evaluate a homegrown load balancer implemented as a DPDK application using nearly 3K lines of code <ref type="table">. It performs com- mon flow table management tasks such as flow table resizing  and deletion of flow table entries.</ref> A deep packet inspector (DPI for short) identifies traffic matching a set of pre-configured rules. The rules can specify layer-3 and layer-4 header fields, application-layer rules, e.g., URL patterns, and strings in the packet data. It analyzes data sent over a connection by reconstructing its bytestream from individual packets. A DPI maintains per-flow state and performs non-trivial computation on each flow. We use an opensource DPI library nDPI <ref type="bibr" target="#b3">[4]</ref> and integrated it with a DPDK forwarding application to test its network performance.</p><p>Test profiles: Our test profiles T n for NFs depend on their respective performance metrics.</p><p>L3FWD's performance is typically measured in packets per second <ref type="bibr" target="#b14">[15]</ref>. We stress the computation done by L3FWD by evaluating traffic profiles with minimal-sized packets (64B) as well as larger packet sizes (128B, 256B, 512B). L3FWD is configured to apply a hash-based lookup algorithm on a routing table configured with 10K entries.</p><p>An L4LB should support a high rate of both packets and connections <ref type="bibr" target="#b23">[24]</ref>. Our test profiles vary the number of packets per connection from 10 to 1000. All packets are minimalsized to test the packet throughput. L4LB uses a flow table with 10M entries, which is sufficient to store all flow entries.</p><p>A DPI should support a large number of connections and a high bandwidth (in bits/sec) while examining the packet payloads. Our test profiles include HTTP connections of varying length from 1 KB to 1024 KB. Smaller connections test the connection throughput whereas larger connections test the bandwidth. Our DPI NF is configured to perform transport protocol-based, hostname-based and IP-based matching.</p><p>Estimators: We first generate ground truth values of NF utilization. We determine the peak load for an NF and a test profile using a binary search approach (similar to RFC 2544 <ref type="bibr" target="#b2">[3]</ref>) under the constraint that the drop rate stays below 0.01%. We subject the NF to 10%, 20%, · · · , 100% of the peak load. At each load, we collect all available CPU events using EMON <ref type="bibr" target="#b6">[7]</ref> taking 4 sec measurements for each event.</p><p>We use linear regression to estimate the percentage utilization based on a feature set consisting of event counts of 1, 2 and 3 events. Among |C ALL | c possible estimators with c event counts, we evaluate all estimators for c = 1 and c = 2. For c = 3, we evaluate estimators by uniformly sampling 1% of estimators from a total of 1.4M possible estimators. The aforementioned ground truth utilization data serves as the training dataset for the regression models. We perform 3-fold cross-validation on the training dataset and use the average root mean squared error <ref type="bibr" target="#b28">[29]</ref> as the error metric.</p><p>Testbed: We conduct experiments on two servers, each equipped with 2x10G network interfaces (Intel 82599ES 10 Gbps SFI/SFP+ NIC) and two CPUs (Intel Xeon E5-2680 v3). The two servers running a traffic generator and an NF respectively are connected via a 10G switch. We use a scalable, open-source DPDK traffic generator TRex <ref type="bibr" target="#b25">[26]</ref>. L4LB and DPI are tested using TRex's stateful mode, which can generate millions of new connections per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Estimation errors</head><p>Local estimators: We tabulate the errors for the best estimators in <ref type="table">Table 1</ref>. We find that the best local estimators using 3    CPU events for all NFs achieve an error rate of 0.5%, 5.2% and 6.0%. This finding shows that there exist simple local estimators for NFs that can achieve low error rates despite significant variations in test profiles. Stateful NFs (L4LB and DPI) have higher error rates than the stateless L3FWD, which is likely due to more complex processing and additional state maintained by these NFs. These error rates may be acceptable in many use cases such as power management and workload placement, which can design incorporate a utilization estimation error in making decisions.</p><p>We show the distribution of errors for local estimators of all NFs in <ref type="figure" target="#fig_0">Figure 2</ref>. The median error rates for single-event estimators are 6.5% , 17.8% and 14.9% for the three NFs which are considerably higher than those of the respective best estimator. This validates our data-driven approach to select the estimator functions, since randomly selecting CPU events is likely to yield higher errors. We also note that stateful NFs have higher errors than the stateless NF across the entire distribution, which further shows that stateful NFs are more difficult to model. For all NFs, using two events to estimate the utilization yields lower error rates over a single event estimator across the entire CDF. Median error for estimators using two events is lower than single event estimators by 4.5%, 2.8% and 2.2%. The marginal improvement of adding the 3rd event is smaller in many cases. This trend suggests that using additional events in a linear regression model may yield smaller improvements.</p><p>Universal estimators: We test the accuracy of universal estimators built using data collected across all NFs and their profiles. All data is collected at a fixed CPU frequency (1.2 GHz). We use an equal number of data points for each of the NFs in training and validating the universal estimators.</p><p>The best universal estimator <ref type="table">(Table 1</ref>) using 3 events has an error rate of 9.9%, which is higher than the error of local estimators, especially that of L3FWD. <ref type="figure" target="#fig_2">Figure 3</ref> shows the distribution of errors for universal estimators, where the median error rates are 26.4%, 22.0%, 19.2% for estimators based on 1, 2 and 3 events respectively. Given that there are no techniques to estimate utilization of unmodified polled IO NFs, these estimators represent an advancement of the state of the art. But, due to error rates of 10% or more, we need to be conservative in using results from these universal estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We conduct a preliminary evaluation to test the accuracy of estimators developed in Section 3 on previously unseen profiles. We also measure the impact of the duration of counter measurement on the overall accuracy.</p><p>Experimental setup: We use test profiles with heterogeneous traffic characteristics. L3FWD's profile contains a mix of packet sizes from 64B to 278B. L4LB's profile contains a mix of flows of length 10 to 100 packets. DPI's profile is an HTTP browsing workload included with TRex. For each NF and its test profile, we calculate the peak load (as described in Section 3.3) and test each NF at loads of 10%, 20%, · · · , 100%. For each NF, we selected the local estimator with 2 events which showed the least error <ref type="table">(Table 1)</ref> for evaluation. CPU events of the chosen estimators' are shown in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>We treat the duration of counter measurement as a parameter, which is varied from 10 us to 1 s in this experiment. We measure the counter values 100 times for each value of the   parameter. For each measurement, we calculate the absolute error between the actual utilization and the utilization estimated by the local estimator. <ref type="figure" target="#fig_3">Figure 4</ref> shows the average and the standard deviation of this error across all loads and all measurements of the counter values.</p><p>Results: Estimation errors are the least for L3FWD, followed by L4LB and DPI, which is consistent with the trend observed earlier. The average error of these estimations for a 1 s counter measurement are 0.9%, 1.9% and 4.5% respectively, which are lower relative to the cross-validation errors in Table 1. This result suggests that our estimators can generalize beyond the specific profiles they are trained on. But, we need experiments with real traffic traces to strengthen this claim.</p><p>The interval of counter measurements affects estimation errors. A 10 us measurement interval has an average error of more than 100%. But, increasing the measurement interval beyond 1 ms has smaller improvements for all NFs. What the best value of this interval is may depend on the use case. Workload placement at minutes or hours timescale can use counter measurements over a few seconds for greater accuracy. But, an aggressive CPU power management policy may want an up to date utilization, for which a millisecond measurement may be a better choice despite its higher error rates.</p><p>Our linear regression-based estimator is fast in the common case (median = 0.3 ms). Thus, the estimator is only a small fraction of the overhead provided that the counters are measured for 1 ms or longer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and future work</head><p>Network cloud management and automation require a common interface to report utilization of NFs. To this end, we explore a CPU event-based estimation approach that is applicable to black-box NFs.</p><p>Improved universal estimators: Our experiments show that CPU events are not a "silver bullet" to the complex problem of estimating utilization of polled NFs. Simple universal estimators based on up to three CPU events have an error rate of 10% in the best case. However, simple local estimators do estimate utilization within a percentage point in some cases. We plan to explore universal estimators based on a few tens of CPU events, which can be still be collected in a few milliseconds. Such an estimator could be practical for scenarios that do not need sub-millisecond response time. We can apply non-linear estimation techniques such as those based on neural networks to improve universal estimators.</p><p>Virtualized and/or cross-core processing: We evaluated stateless and stateful NFs and found the estimation errors to be higher for stateful NFs. We plan to explore two more kinds of NFs in the future: virtualized NFs and NFs that perform crosscore processing. Virtualized polled IO NFs using network interfaces that support SR-IOV (a form of a virtual PCI device) can perform close to an NF that has direct physical access to network interfaces <ref type="bibr" target="#b8">[9]</ref>. We are studying whether estimators need to be rebuilt if an NF is run in a VM on an SR-IOV interface. Many multi-core NFs dedicate one or more threads to poll hardware queues for Rx and multiplex them among worker threads via software queues <ref type="bibr" target="#b13">[14]</ref>. Worker threads poll these software queues to read packets. Unlike NFs we have evaluated, these NFs may need different estimator functions for Rx threads and worker threads. Determining whether Rx or worker thread is the bottleneck is a challenge for such NFs.</p><p>Use cases: We see a number of applications of CPU eventbased estimators. A top-like utility for NFs can be made available for system administrators. With community help, the utility can be packaged with a set of estimators for common CPUs, so that the utility can be used out of the box. For cloud orchestration tasks, the utility would implement a northbound API. Cloud orchestration software (e.g., OpenStack <ref type="bibr" target="#b21">[22]</ref>) can query that API to optimize resource allocation (number of cores), workload placement and load distribution. Power management using frequency scaling and/or sleep states seems an urgent need for polled IO NFs, which run constantly at 100% CPU utilization <ref type="bibr" target="#b19">[20]</ref>. A key question for frequency scaling is whether estimators trained at a given CPU frequency work at other frequencies using simple interpolation. Using CPU events to improve the trade-off between power consumption and performance (latency, jitter and packet loss) is an interesting area of exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Workshop discussion</head><p>The central question before the workshop audience is whether our effort to develop a CPU event-based universal estimator worthwhile or is it easier to define a common API and have NF vendors implement and maintain that API for their respective NFs. There appears to be preliminary work in DPDK community towards the second direction. In the long run, other automated techniques are possible such an intelligent compiler to perform automated NF instrumentation, which can simplify the task of implementing a utilization API.</p><p>A limitation of this approach is that the accuracy of utilization estimators cannot be increased arbitrarily by using complex machine-learning models as doing so would affect the end-to-end estimation latency. Such high latency might be unsuitable for critical use-cases like power management. Secondly, the behavior of a universal estimator is dependent on the NFs used to train it. Even in the cross-validation experiments, each estimator was trained on datapoints from all NFs. We haven't tested the accuracy of a universal estimator on an unseen network function. Furthermore, an estimator might have a high error if the traffic profile it is subjected to has significantly different characteristics than the profiles in training data. We rely on the assumption that a network provider like AT&amp;T would have insight about representative profiles for a given NF.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of estimator score (root mean squared error) for tested NFs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Error distribution for universal estimators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Utilization estimation errors on unseen test profiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Events chosen for building evaluated estimators [8]. 

10us 
100us 
1ms 
10ms 
100ms 
1s 
Duration of event measurement 

0.1% 

1% 

10% 

100% 

Error in utilization% 

L3FWD 
L4LB 
nDPI 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exploiting Hardware Performance Counters with Flow and Context Sensitive Profiling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Ammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="1997-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Datacenter RPCs can be General and Fast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">RFC2544-Benchmarking Methodology for Network Interconnect Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Bradner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Mcquaid</surname></persName>
		</author>
		<ptr target="ftp://ftp.rfc-editor.org/in-notes/rfc2544.txt" />
		<imprint>
			<date type="published" when="1999-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://github.com/ntop/nDPI.Accessed" />
		<title level="m">NDPI by NTOP. Open Source Deep Packet Inspection Software Toolkit</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ENVI: Elastic Resource Flexing for Network Function Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Fahmy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX HotCloud</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instruction-Based Sampling: A New Performance Analysis Technique for AMD Family 10h Processors</title>
	</analytic>
	<monogr>
		<title level="m">AMD Corporation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Intel Corporation. EMON: User&apos;s guide</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="https://download.01.org/perfmon/index/.Ac-cessed" />
	</analytic>
	<monogr>
		<title level="j">Intel Corporation. Intel Processor Events Reference</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High Performance Network Virtualization with SR-IOV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaozu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangdeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1471" to="1480" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Data Plane Development Kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dpdk</forename><surname>Dpdk</surname></persName>
		</author>
		<ptr target="https://www.dpdk.org.Accessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Forwarding Sample Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dpdk</forename><surname>Dpdk L3</surname></persName>
		</author>
		<ptr target="https://doc.dpdk.org/guides/sample_app_ug/l3_forward.html.Accessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<ptr target="https://portal.etsi.org/NFV/NFV_White_Paper.pdf.Ac-cessed" />
		<title level="m">ETSI. Network Functions Virtualisation</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Comparison of Memory Mapping Techniques for High-speed Packet Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gallenmüller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Technical University of Munich</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SoftNIC: A Software NIC to Augment Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumik</forename><surname>Palkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<idno>UCB/EECS-2015-155</idno>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Berkeley, Berkeley, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. EECS, Univ. California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dpdk Performance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reports</surname></persName>
		</author>
		<ptr target="https://core.dpdk.org/perf-reports/.Accessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mistral: Dynamically Managing Power, Performance, and Adaptation Cost in Cloud Infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gueyoung</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><forename type="middle">A</forename><surname>Hiltunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaustubh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calton</forename><surname>Schlichting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDCS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">I/O System Performance Debugging using Model-driven Anomaly Characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanpeng</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SoftPower: Fine-grain Power Estimations using Performance Counters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">Yeol</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Porterfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Fowler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM HPDC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optimizing PAPI for Low-Overhead Counter Measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building a Power-proportional Software Router</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Niccolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaideep</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ntop</forename><surname>Pf_Ring</surname></persName>
		</author>
		<ptr target="https://github.com/ntop/PF_RING.Accessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openstack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openstack</surname></persName>
		</author>
		<ptr target="https://www.openstack.org.Accessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">NetBricks: Taking the V out of NFV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurojit</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangjin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keon</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parveen</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemant</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<title level="m">ACM SIG-COMM</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Cloud Scale Load Balancing</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Intel Performance Counter Monitor: A better way to measure CPU Utilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick Fay Thomas</forename><surname>Willhalm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Dementiev</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/intel-performance-counter-monitor.Accessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">TRex: Cisco&apos;s Realistic Traffic Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Cisco</forename><surname>Trex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Systems</surname></persName>
		</author>
		<ptr target="https://trex-tgn.cisco.com.Ac-cessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Traffic-aware Adaptive Polling Mechanism for High Performance Packet Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trifonov</forename><surname>Hristo Georgiev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Limerick Institutional Repository</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiki</forename><surname>Linux Perf</surname></persName>
		</author>
		<ptr target="https://perf.wiki.kernel.org/index.php/Main_Page.Accessed" />
		<title level="m">Linux Profiling with Performance Counters</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Root-mean-square deviation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Root-mean-square_deviation" />
		<imprint>
			<biblScope unit="page" from="2019" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hardware-assisted Isolation in a Multitenant Function-based Dataplane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhigyan</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustubh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
