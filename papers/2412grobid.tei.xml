<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST &apos;17). Chronix: Long Term Storage and Retrieval Technology for Anomaly Detection in Operational Data Chronix: Long Term Storage and Retrieval Technology for Anomaly Detection in Operational Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 27-March 2, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Lautenschlager</surname></persName>
							<email>florian.lautenschlager@qaware.dejosef.adersberger@qaware.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qaware</forename><surname>Gmbh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Philippsen</surname></persName>
							<email>michael.philippsen@fau.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kumlehn</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Lautenschlager</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">QAware GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Philippsen</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">University Erlangen-Nürnberg (FAU) Programming Systems Group</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kumlehn</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">University Erlangen-Nürnberg (FAU) Programming Systems Group</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Adersberger</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">QAware GmbH</orgName>
								<address>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Friedrich-Alexander-Universität Erlangen-Nürnberg; Josef Adersberger</orgName>
								<orgName type="department" key="dep2">QAware GmbH</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST &apos;17). Chronix: Long Term Storage and Retrieval Technology for Anomaly Detection in Operational Data Chronix: Long Term Storage and Retrieval Technology for Anomaly Detection in Operational Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February 27-March 2, 2017</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 15th USENIX Conference on File and Storage Technologies is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Anomalies in the runtime behavior of software systems, especially in distributed systems, are inevitable, expensive , and hard to locate. To detect and correct such anomalies (like instability due to a growing memory consumption, failure due to load spikes, etc.) one has to automatically collect, store, and analyze the operational data of the runtime behavior, often represented as time series. There are efficient means both to collect and analyze the runtime behavior. But traditional time series databases do not yet focus on the specific needs of anomaly detection (generic data model, specific built-in functions, storage efficiency, and fast query execution). The paper presents Chronix, a domain specific time series database targeted at anomaly detection in operational data. Chronix uses an ideal compression and chunking of the time series data, a methodology for commissioning Chronix&apos; parameters to a sweet spot, a way of enhancing the data with attributes, an expandable set of analysis functions, and other techniques to achieve both faster query times and a significantly smaller memory footprint. On benchmarks Chronix saves 20%-68% of the space that other time series databases need to store the data and saves 80%-92% of the data retrieval time and 73%-97% of the runtime of analyzing functions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Runtime anomalies are hard to locate and their occurrence is inevitable, especially in distributed software systems due to their multiple components, different technologies, various transport protocols, etc. These anomalies influence a system's behavior in a bad way. Examples are an anomalous resource consumption (e.g., high memory consumption, growing numbers of open files, low CPU usage), sporadic failures due to synchronization problems (e.g., deadlock), or security issues (e.g., port scanning activity). Whatever their root causes are, the resulting behavior is critical and may in general lead to economic or reputation loss (e.g., loss of sales, productivity, or data). Almost every software system has hidden anomalies that occur sooner or later. Hence one needs to detect them in an automated manner soon after their occurrence in order to initiate measures.</p><p>There are efficient means both to collect and analyze the runtime behavior. Tools <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">44]</ref> can collect all kinds of operational data like metrics (e.g., CPU usage), traces (e.g., method calls), and logs. They represent such operational data as time series. Analysis tools and research papers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47]</ref> focus on the detection of anomalies in that data. But there is a gap between collection and analysis of operational time series, because typical time series databases are general-purpose and not optimized for the domain of this paper. They typically have a data model that focuses on series of primitive type values, e.g., numbers, booleans, etc. Their built-in aggregations only support the analysis of these types. Furthermore, they do not support an explorative and correlating analysis of all the raw operational data in spontaneous and unanticipated ways. With a domain specific data model and with domain specific built-in analysis functions we achieve better query analysis times. Generalpurpose time series databases already have a good storage efficiency. But we show that by exploiting domain specific characteristics there is room for improvement.</p><p>Chronix, a novel domain specific time series database, addresses the collection and analysis needs of anomaly detection in operational data. Its contributions are a multi-dimensional generic time series data model, builtin domain specific high-level functions, and a reduced storage demand with better query times. Section 2 covers the requirements of such a time series database. Section 3 presents Chronix. Section 4 discusses the commissioning of Chronix and describes a methodology that finds a sweet performance spot. The quantitative evaluation in Section 5 demonstrates how much better Chronix works than general-purpose time series databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Requirements</head><p>Three main requirements span the design space of a time series database that better suits the needs of an anomaly detection in operational data: a generic data model for an explorative analysis of all types of operational data, analysis support for detecting runtime anomalies, and a time-and space-efficient lossless storage.</p><p>As shown in <ref type="table" target="#tab_0">Table 1</ref>, the established general-purpose time series databases Graphite <ref type="bibr" target="#b11">[12]</ref>, InfluxDB <ref type="bibr" target="#b23">[24]</ref>, KairosDB <ref type="bibr" target="#b24">[25]</ref>, OpenTSDB <ref type="bibr" target="#b32">[32]</ref>, and Prometheus <ref type="bibr" target="#b36">[36]</ref> do not or only partially fulfill these requirements.</p><p>Generic data model. Software systems are typically distributed and a multi-dimensional data model allows to link operational data to its origin (host, process, or sensor). We argue that a generic multi-dimensional data model is necessary for an explorative and correlating analysis to target non-trivial anomalies. Explorative means that a user can query and analyze the data without any restrictions (as by a set of pre-defined queries) to verify hypotheses. Correlating means that the user can combine queries on all different types of operational data (e.g., traces, metrics, etc.) without any restrictions (as by pre-defined joins and indexes). Imagine an unanticipated need to correlate the CPU usage in a distributed system with the executed methods. Such an analysis first queries the CPU usage on the hosts (metrics) for a time range. Second, it queries which methods were executed (traces). Finally, it correlates the results in a histogram.</p><p>The traditional time series databases have a specific multi-dimensional data model for storing mainly scalar/numeric values. But in the operational data of a software system there are also traces, logs, etc. that often come in (structured) strings. As explicit string encodings require implementation work, often only such raw data is encoded and collected that appears useful at the time of collection. Other raw data is lost, even though an explorative analyses may later need it. Moreover, any string encoding loses the semantics that come with the data type. Therefore, while the traditional time series databases support explorative and correlating anal- yses on scalar values, they often fail to do so efficiently for generic time series data (e.g., logs, traces, etc.). InfluxDB also supports strings and booleans. KairosDB is extensible with custom types. Both lack operators and functions and hence only partly fulfill the requirements. Analysis support. <ref type="table" target="#tab_1">Table 2</ref> is an incomplete list of basic and high-level analysis functions that a storage and retrieval technology for anomaly detection in operational data must support in its plain query language. Graphite, InfluxDB, KairosDB and Prometheus only have a rich set of basic functions for transforming and aggregating operational data with scalar values. OpenTSDB even supports only a few of them. But domain specific high-level functions that other authors <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b47">47]</ref> successfully use for anomaly detection in operational data (lower part of <ref type="table" target="#tab_1">Table 2</ref>) should be built in natively to execute them as fast as possible, without network transfer costs, etc. The evaluation in Section 5 shows the runtime benefits of having them built-in instead of emulating them.</p><p>As such a set of functions can never be complete its extensibility is also a requirement.</p><p>Efficient lossless long term storage. Complex analy- </p><formula xml:id="formula_0">distinct × × × × integral × × × × min/max/sum count × avg/median bottom/top × × × first × × last × × percentile (p) stddev derivative × nnderivative × × × diff × × movavg × × × divide/scale × High-level sax [33] × × × × × fastdtw [38] × × × × × outlier × × × × × trend × × × × × frequency × × × × × grpsize × × × × × split × × × × ×</formula><p>ses to identify and understand runtime anomalies, trends, and patterns use machine learning, data mining, etc. They need quick access to the full set of all the raw operational data, including its history. This allows interactive response times that lead to qualitatively better explorative analysis results <ref type="bibr" target="#b30">[30]</ref>. Thus a lossless long term storage is required that achieves both a small storage demand and good access times. A domain specific storage should also allow to store additional pre-aggregated or in other ways pre-processed versions of the data that enhance domain specific queries. The operational characteristics of anomaly detection tasks are also specific as there are comparatively few batch writes and frequent reads/analyses, hence the storage should optimized for this.</p><p>As <ref type="table" target="#tab_0">Table 1</ref> shows, the traditional time series databases (except for Graphite due to its round-robin storage, see Section 6) can be used as long term storage for raw data. Their efficiency in terms of domain specific storage demands and query runtimes is insufficient as the evaluation in Section 5 will point out. Prometheus is designed as short-term storage with a default retention of two weeks. Furthermore it does not scale, there is no API for storing data, and it uses hashes for time series identification (the resulting collisions may lead to a seldom loss of a time series value). It is therefore not included in the quantitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design and Implementation</head><p>Generic data model. Chronix uses a generic data model that can store all kinds of operational data. The key element of the data model is called a record. It stores an ordered chunk of time series data of an arbitrary type (n pairs of timestamp and value) in a binary large object. There is also type information (e.g., metric, trace, or log) that defines the available type-specific functions and the ways to store or access the data (e.g., serialization, compression, etc.). A record stores technical fields (version and id of the record, as needed by the underlying Apache Solr <ref type="bibr" target="#b4">[5]</ref>), two timestamps for start and end of the time range in the chunk, and a set of user-defined attributes, for example to describe the origin of the operational data, e.g., host and process. Thus the data model is multi-dimensional. It is explorative and correlating as queries can use any combination of the attributes stored in a chunk as well as the available fields.</p><p>Listing 1 shows a record for a metric (type) time series, with dimensions for host name, process, group (a logical group of metrics), and metric. Description and version are optional attributes. The underlying Solr finds the data according to q. Then we apply the optional Chronix functions cf before we ship the result back to the client. Cf is a ;-separated list of filters, each of the form</p><formula xml:id="formula_1">&lt;type&gt;'{'&lt;functions&gt;'}'</formula><p>The ;-separated list of &lt;type&gt;-specific &lt;functions&gt; is processed as a pipeline. A &lt;function&gt; has the form &lt;analysis-name&gt; <ref type="bibr">[:&lt;params&gt;]</ref> for analysis names from First, q takes all time series from all hosts whose name starts with prod and that also hold data of the UNIX commands lsof or strace. Then cf applies two functions. To the lsof data cf applies grpsize to select the group named name and counts the occurrences of the value/word pipe in that group. On the strace data cf performs a split-operation on the command-column. For each of the arguments (here just for the file handle ID 2030) it produces a split of the data that contains "2030".</p><p>Analysis support. Chronix offers all the basic and high-level functions listed in <ref type="table" target="#tab_1">Table 2</ref>, e.g., there are detectors for outliers and functions that check for trends, similarities (fastdtw), and patterns (sax). The plug-in mechanism of Chronix allows to add functions that run server-side. They are fast as they operate close to the data without shipping (e.g., through HTTP). For security reasons only an administrator is capable (and thus responsible) for installing plug-ins while regular users can only use them afterwards. The plug-in developer has to implement an interface called ChronixAnalysis. It defines three methods: execute holds the code that analyzes a time series, getQueryName is the function's name in Chronix' Query Language, and getTimeSeriesType binds the function to a time series type. An additional Google Guice <ref type="bibr" target="#b18">[19]</ref> module is needed before (after a reboot of Chronix with the operator code in the classpath) the newly added function is available. Chronix parses a given query and delegates the execution to the pluggedin function. Listing 2 illustrates how to add the grpsize analysis for the time series type lsof. The function runs on the server, groups the time series data with respect to a field (given as its first argument), and returns the sizes of the groups. The raw time series data is never shipped to the client. With this function added, regular users just need to call one function instead of several queries to emulate the semantics themselves.</p><p>Functionally-lossless long term storage. Chronix optimizes storage demands and query times and is designed for few batch writes and frequent reads. The storage is functionally-lossless and preserves all the data that can potentially be useful for anomaly detection. We clarify the two situations below when functionally-lossless is different from lossless.</p><p>Chronix' pipeline architecture has four building blocks that run multi-threaded: Optional Transformation, Attributes and Chunks, Compression, and MultiDimensional Storage, see <ref type="figure" target="#fig_1">Fig. 1</ref>. Not shown is the input buffering (called Chronix Ingester in the distribution) that batches incoming raw data points.</p><p>The Optional Transformation can enhance or extend what is actually stored in addition to the raw data, or instead of it. The goal of this optional phase is an opti- mized format that better supports use-case specific analyses. For example, it is often easier to identify patterns in time series when a symbolic representation is used to store the numerical values of a time series <ref type="bibr" target="#b33">[33]</ref>. Other examples are averaging, a Fourier transformation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr">Wavelets [35]</ref>, or a Piecewise Aggregate Approximation <ref type="bibr" target="#b25">[26]</ref>. The more is known about future queries and about the shape or structure of a time series or its pattern of redundancy, the more can a transformation speed up analyses. In general, this phase can add an additional representation of the raw data to the record. Storing the raw data can even be omitted if it can be reconstructed. In rare situations, an expert user knows for sure that certain data will never be of interest for any of her/his anomaly detection tasks. Think of a regular log file that in addition to the application's log messages also has entries that come from the framework hosting the application. The latter often do not hold anomaly-related information. It is such data that the expert user can decide to drop, keeping everything else and leaving the Chronix store functionally-lossless. The decision is irreversiblebut this is the same with the other time series databases.</p><p>Attributes and Chunks breaks the raw time series (or the result of the optional transformation) into chunks of n data points that are serialized into c bytes. Instead of storing single points, chunking speeds up the access times. It is known to be faster to read one record that contains n points instead of reading n single points. A small c leads to many small records with potentially redundant user-defined attributes between records. The value of c is therefore a configuration parameter of the architecture.</p><p>This stage of the pipeline also calculates both the required fields and the user-defined attributes of the records. The required fields (of the data model) are the binary data field that holds a chunk of the time series, and the fields start and end with timestamps of the first and the last point of the chunk. In addition, a record can have user-defined attributes to store domain specific information more efficiently. For instance, information that is known to be repetitive for each point, e.g., the host name, can be stored in an attribute of the record instead of encoding it into the chunk of data multiple times.</p><p>As it is specific for a time series type which fields are repetitive Chronix leaves it to an expert user to design the records and the fields. Redundancy-free data chunks are a domain specific optimization that general-purpose time series databases do not offer. To design the records and fields the expert has two tasks: (1) define the conversion of a time series chunk to a record (see the example RecordConverter interface in Listing 3) and <ref type="formula">(2)</ref> define the static schema in Apache Solr that lists the field names, the types of the values, which fields are indexed, etc. Chronix stores every record that matches the schema.</p><p>Compression processes the chunked data. Chronix exploits domain specific characteristics in three ways.</p><p>First, Chronix compresses the operational data significantly as there are only small changes between subsequent data points. The cost of compression is acceptable as there are only few batch writes. When querying the data, compression even improves query times as compressed data is transferred to the analysis client faster due to the reduced amount.</p><p>Second, time series for operational data often have periodic time intervals, as measurements are taken on a regular basis. The problem in practice is that there is often a jitter in the timestamps, i.e., the time series only have almost-periodic time intervals because of network latency, I/O latency, etc. Traditional time series databases use optimized ways to store periodic time series (run-length encoding, delta-encoding, etc.). But in case of jitter they fall back to storing the full timestamps/deltas. In contrast, Chronix' Date-Delta-Compaction (DDC) exploits the fact that in its domain the exact timestamps do not matter that much, at least if the difference between the expected timestamp and the actual timestamp is not too large. Here Chronix' storage is functionally-lossless because by default it drops timestamps if it can almost exactly reproduce them. If in certain situations an expert user knows that the exact timestamps do matter, they can be kept. In contrast to timestamp jitter, Chronix never drops the exact values of the data points. They always matter in the domain of anomaly detection. Nevertheless,  <ref type="figure">Figure 2</ref>: DDC calculates timestamp deltas (0, 10000, 10002, 10004) and compares them (0, 10000, 2, 4). It removes deltas below a threshold of 4 ( , 10000, , ). Without additional deltas, the reconstruction would yield timestamps with increments of 10000. Since the fourth of them would be too far off from t4 (drift of 6) DDC stores the correcting delta 10006. With the resulting two stored deltas ( , 10000, , 10006) the reconstructed timestamps are error-free, even for r4.</p><p>some parts of the related work are lossy, see <ref type="table" target="#tab_0">Table 1</ref>.</p><p>The central idea of the DDC is: When storing an almost-periodic time series, the DDC keeps track of the expected next timestamp and the actual timestamp. If the difference is below a threshold, the actual timestamp is dropped as its reconstructed value is close enough. The DDC also keeps track of the accumulated drift as the difference between the expected timestamps and actual timestamps adds up with the number of data points stored. As soon as the drift is above the threshold, DDC stores a correcting delta that brings the reconstruction back to the actual timestamp. DDC is an important domain specific optimization. See Section 4 for the quantitative effects. The DDC threshold is another commissioning parameter. <ref type="figure">Fig. 2</ref> holds an example.</p><p>There are related approaches <ref type="bibr" target="#b34">[34]</ref> that apply a similar idea to both the numeric values and the timestamps. Chronix' DDC avoids the lossy compression of values as the details matter for anomaly detection. Chronix also exploits the fact that deltas are much smaller than full timestamps and that they can be stored in fewer bits. Chronix' serialization uses Protocol Buffers <ref type="bibr" target="#b19">[20]</ref>.</p><p>Third, to further lower the storage demand, Chronix compresses the records' binary data fields. The attributes remain uncompressed for faster access. Chronix uses one of the established lossless compression techniques t = bzip2 <ref type="bibr" target="#b39">[39]</ref>, gzip <ref type="bibr" target="#b13">[14]</ref>, LZ4 <ref type="bibr" target="#b10">[11]</ref>, Snappy <ref type="bibr" target="#b20">[21]</ref>, and XZ <ref type="bibr" target="#b42">[42]</ref> (others can be plugged in). Since they have a varying effectiveness depending on the size of the data blocks that are to be compressed, the best choice t is another commissioning parameter.</p><p>The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Commissioning</head><p>Many of the available general-purpose time series databases come with a set of parameters and default values. It is often unclear how to adjust these values to tune the database so that it performs well on the use-case at hand. We now discuss the commissioning that selects values for Chronix' three adjustable parameters: d (DDC threshold), c (chunk size), and t (compression technique). Commissioning has two purposes: (1) to find default parameters for the given domain (and to show the effects of an unfortunate choice) and (2) to describe a tailoring of Chronix for use-case specific characteristics.</p><p>Let us start with a typical domain specific dataset and query mix (composed from real-world projects). We then sketch the measurement infrastructure that the evaluation in Section 5 also uses and discuss the commissioning.</p><p>Commissioning Data = Dataset plus Query Mix. To determine its default parameter configuration, Chronix relies on three real-world projects that we consider typical for the domain of anomaly detection in operational data. From these projects, Chronix uses the time series data and the queries that analyze the data.</p><p>Project 1 is a web application for searching car maintenance and repair instructions. In production, 8 servers run the web application to visualize the results and 20 servers perform the users' searches. The operational time series data is analyzed to understand the resource consumption for growing numbers of users and new functions, i.e., to answer questions like: 'How do multiple users and new functions affect the CPU load, memory consumption, or method runtimes?', or 'Is the time needed for a user search still within predefined limits?' Project 2 is a retail application for orders, billing, and customer relations. The production system has a central database, plus two servers. From their local machines, users run the application on the servers via a remote desktop service. The analysis goals are to investigate the impact of a new JavaFX-based UI Framework that replaces a former Swing-based version, and to locate the causes of various reported problems, e.g., memory leaks, high CPU load, and long runtimes of use-cases.</p><p>Project 3 is a sales application of a car manufacturer. There are two application servers and a central database server in the production environment. The analysis goals are to understand and optimize the batch-import, to identify the causes of long-running use-cases reported by users, to improve the database layer, and understand the impact that several code changes have.</p><p>In total, the projects' operational data have about 500 million pairs of timestamp and (scalar) value in 14,185 time series of diverse time ranges and metrics, see Table 3(a). Two projects have an almost-periodic time interval of 30 or 60 seconds. All projects also have eventdriven data, e.g., the duration of method calls. There are recurring patterns (e.g., heap usage), sequences of constant values (e.g., size of a connection pool), or even erratic values (e.g., CPU load).</p><p>All time series of the three projects have the same userdefined attributes (host, process, group, and metric) that takes 46 bytes on average, see <ref type="table" target="#tab_6">Table 3</ref>(b). The required fields of the records take 40 bytes, leading to a total of m = 40 + 46 = 86 uncompressed bytes per record.</p><p>The three projects have 96 queries in total.  Commissioning of the DDC threshold. The higher the DDC threshold is, the higher is the deviation between the actual and the reconstructed timestamp. On the other hand, higher DDC thresholds result in fewer deltas that hence need less storage. Since for anomaly detection, accuracy is more important than storage consumption, and since the acceptable degree of inaccuracy is use-case specific, the commissioning of the DDC threshold focuses on accuracy and only takes the threshold's impact on the storage efficiency into account if there is a choice.</p><p>The commissioning works as follows: For a broad range of potential thresholds, apply the DDC to the time series in the dataset. For each timestamp, record whether the reconstructed value is different from the actual timestamp, and if so, how far the reconstructed value is off. The fraction of the number of inaccurately reconstructed timestamps to error-free ones is the inaccuracy rate. For all inaccurately reconstructed timestamps compute their average deviation. With those two values plotted, the commissioner selects a threshold that yields the desired level of accuracy.</p><p>Default Values. The commissioning of the DDC threshold can be done for individual time series or for all time series in the dataset. For the default value we use all time series that are not event-driven. For thresholds from 0 milliseconds up 1 second. <ref type="figure" target="#fig_3">Fig. 3</ref>. shows that the inaccuracy rate grows up to 67%; the average deviation of the reconstructed timestamps grows up to 90 ms.</p><p>From the experience gained from anomaly detection projects (the three above projects are among them) an average deviation of 30 ms seems to be reasonably small -recall that the almost-periodic time series in the dataset have intervals of 30 or 60 seconds. The acceptable jitter is thus below 0.1% or 0.05%, resp. Therefore we choose a default DDC threshold of d = 200 ms which implies an inaccuracy rate of about 50% that we deem acceptable because of the absolute size of the deviation.</p><p>Note that the DDC is effective as the resulting data only take 27% to 19% of the original space. But for the dataset the curve of the space reduction is too flat to affect the selection of the DDC threshold.  Commissioning of the compression parameters. Many of the standard compression techniques achieve better compression rates on larger buffers of bytes <ref type="bibr" target="#b6">[7]</ref>. On the other hand, it takes longer to decompress larger buffers to get to the individual data point. Because of the conflicting goals of storage efficiency and query runtimes, Chronix uses a domain specific way to find a sweet spot. There are three steps.</p><p>1. Minimal Chunk Size. First, Chronix finds a minimal chunk size for its records. Chronix is not interested in the compression rate in a memory buffer. For anomaly detection, what matters instead is the total storage demand, including the size of the index files (smaller for larger and fewer records). On the other hand, smaller chunks have more redundancy in the records' (potentially) repetitive uncompressed attributes which makes the compression techniques work better. We call the quotient of this total storage demand and the size of the raw time series data the materialized compression rate.</p><p>Finding a minimal chunks size works as follows: For a range of potential chunk sizes, construct the records (with their chunks and attributes) from the DDCtransformed time series data and compress them with the standard compression techniques. With the materialized compression rate plotted, the commissioner finds the minimal chunk size where saturation sets in.</p><p>Default Values. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the materialized compression rates that t= bzip2, gzip, LZ4, Snappy, and XZ achieve on records with various chunk sizes that are constructed from the (DDC-transformed) dataset. Saturation sets in around a minimal chunk size of c min =32 KB. Larger chunks do not improve the compression rate significantly, regardless of the compression technique.</p><p>2. Candidate Compression Techniques. Then Chronix drops some of the standard compression techniques from the set of candidates. Papers on compression techniques usually include benchmarks on decompression times. But for the domain of anomaly detection, the decompression of a whole time series that is in a memory buffer is irrelevant. What matters instead is the total time needed to find and access an individual record, to then ship the compressed record to the client, and to decompress it there. Note that the per-byte cost of shipping goes down with growing chunks due to latency effects.</p><p>Finding the set of candidate compression techniques works as follows: For potential chunk sizes above c min , find a record, ship it, and reconstruct the raw data. For meaningful results, process all records, and compute the average runtime. The commissioner drops those compression techniques that have a slow average access to single records. The reason is that in anomaly detection data is read much more often than written.</p><p>Default Values. On the compressed dataset, all of the standard compression techniques take longer to find, access, and decompress larger chunks, see <ref type="figure" target="#fig_5">Fig. 5</ref>. It is obvious that bzip2 and XZ can be dropped from the set of compression techniques because the remaining ones clearly outperform them.</p><p>3. Best Combination. Now that the range of potential values for c is reduced and the set of candidates for the compression technique t is limited, the commissioning considers all combinations. Since query performance is more important than storage efficiency for the domain of anomaly detection, commissioning works with a typical (or a use-case specific) query mix. The access time to a single record as considered above can only be an indicator, because real queries request time ranges that are either part of a single record (a waste of time in shipping and decompression if the record is large) or that span multiple records (a waste of time if records are small).</p><p>This commissioning step works as follows: Randomly retrieve q time ranges of size r from the data. The values of r and q reflect the characteristics of the query mix, see <ref type="table" target="#tab_6">Table 3</ref>(c). Repeat this 20 times to stabilize the results. For a query of size r the commissioning does not pick a time series that is shorter than r. From the resulting plot, the commissioner then picks the combination of chunk size c and compression technique t that achieves the shortest total runtime for the query mix.</p><p>Default Values. For chunk sizes c from 32 to 1024 KB and for the remaining three compression techniques t, <ref type="figure" target="#fig_6">Fig. 6</ref> shows the total access time of all the 20 · 96 = 1920 randomly selected data retrievals that represent the query mix. There is a bath tub curve for each of the three compression techniques t, i.e., there is a chunk size c that results in a minimal total access time. As t=gzip achieves the absolute minimum with a chunk size of c=128 KB, Chronix selects this sweet spot, especially as the other options do not show better materialized compression rates for that chunk size (see <ref type="figure" target="#fig_4">Fig. 4</ref>). The default parameters are good-natured, i.e., small variations do not affect the results much. <ref type="figure" target="#fig_6">Fig. 6</ref> also shows the effect of suboptimal choices for c and t.</p><p>Commissioning and re-commissioning for a use-case specific set of parameters are possible but costly as the latter affects the whole data. At the end of the next section, we discuss the effect of a use-case specific commissioning compared to the default parameter set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We quantitatively compare the memory footprint, the storage efficiency, and the query performance of Chronix (without any optional transformation and without any pre-computed analysis results) to InfluxDB, OpenTSDB, and KairosDB. After a sketch of the setup, we describe two case-studies whose operational data serve as benchmark. Then we discuss the results and demonstrate that Chronix' default parameters are sound.</p><p>Setup. For the minimal realistic installation all databases run on one computer and the Java process issuing the queries via HTTP runs on a second computer (for hardware details see Section 4). InfluxDB, OpenTSDB, and KairosDB store time series with different underlying storage technologies: InfluxDB (v.1.0.0) uses a custom storage called time structured merge tree, OpenTSDB (v.2.2.0) uses the wide-column store HBase <ref type="bibr" target="#b22">[23]</ref> that stores data in the Hadoop Distributed File System (HDFS) <ref type="bibr" target="#b41">[41]</ref>, and KairosDB (v.1.1.2) stores the data in the wide-column store Cassandra <ref type="bibr" target="#b2">[3]</ref>. They also have different strategies for storing the attributes: InfluxDB stores them once with references to the data points they belong to. OpenTSDB and KairosDB store them in key-value pairs that are part of the row-key. We use default configurations for each database. To measure the query runtimes and the resource consumptions each database is called separately and runs it in its own Docker container <ref type="bibr" target="#b15">[16]</ref> to ensure non-interference. We batch-import the data once and run the queries afterwards. We do not discuss the import times as they are less important for the analysis.</p><p>Case-studies/benchmark. We collected 108 GByte of operational data from two real-world applications. In contrast to the dataset used in the commissioning there are also data of lsof and strace, see <ref type="table" target="#tab_9">Table 4</ref>(a).</p><p>Project 4 detects anomalies in a service application for modern cars (such as music streaming). The goal is to locate the root cause of a file handle leak that forces the application to nightly rebootings of its two application servers. The collected operational data covers 3 hours with an almost-periodic interval of 1 second.</p><p>Since we have mentioned an example query of this project in Section 3 and since we have shown how the grpsize function (Listing 2) can be plugged-in, let us give some more details on this project.</p><p>The initial situation was that the application kept opening new file handles without closing old ones. After reproducing the anomaly in a test environment, for the explorative analysis we employed lsof to show the open files, and stored this operational data in Chronix. The results of queries like q=type:lsof &amp; cf=lsof{grpsize:name,*} were the key to explain the rise in the number of open file handles as about 2,000 new file handles were pipes or anon inodes that are part of the java.nio package. Hence it was necessary to dig deeper and to link file handles to system calls. To do so, we used strace and also stored the data in Chronix. By narrowing down the time series data to individual file handle IDs, with correlating queries like the one shown in Section 3 we found  that epoll ctl was often the last function call before the anomaly (see Listing 4). By then analyzing which third party libraries the application uses and by gathering information on epoll ctl we deduced that the application used an old version of Grizzly that leaks selectors <ref type="bibr">[1]</ref> when it tries to write to an already closed channel. The solution was to upgrade the affected library. Project 5 detects anomalies in an application that manages the compatibility of software components in a vehicle. The production system has a central database and six application servers. The operational data is analyzed to find and understand production anomalies, such as long running method calls, positive trends of resource consumption, etc. The dataset has an almost-periodic interval of 1 min. and holds seven months of operational data. <ref type="table" target="#tab_9">Table 4</ref>(b) shows the mix of the 133 queries that the projects needed to achieve their analysis goals. There are different ranges (r) for the 58 raw data retrievals (q) that do not have a cf-part and also for the 32 queries that use basic analysis functions (b-queries) and for the 43 h-queries that use high-level analyses. <ref type="table" target="#tab_13">Table 8</ref> lists which of the built-in analysis functions from <ref type="table" target="#tab_1">Table 2</ref> the projects actually use (and how often).</p><p>Memory Footprint. <ref type="table" target="#tab_10">Table 5</ref> shows the memory footprint of all database processes at different times. This is relevant as analyses on large amounts of time series data are often memory intensive. The first line shows the memory consumption just after a container's start, when all components of the time series database are up and running. The next two lines show the maximal memory footprints that we encountered while the data of the two benchmark projects was imported and while the query mix was executed. All databases stay below the maximal available memory of 31.42 GB. The import (buffering, index construction, etc.) needs more memory than the query mix (reading, decompression, serialization, ship- ping, etc.). OpenTSDB and KairosDB clearly take the most memory due to their various underlying components. InfluxDB is better but still takes 1.5 times more memory than Chronix. The reasons for Chronix' lower memory usage are: (a) it does not hold large amounts of data in memory at once, (b) it runs as a single process with lightweight thread parallelism, and (c) its implementation avoids expensive object allocations.</p><p>Storage Efficiency. <ref type="table" target="#tab_11">Table 6</ref> shows the storage demands of the data, including the index files, commit logs, etc. There are three aspects to note. First, out-ofthe-box none of the general-purpose databases can handle the lsof and strace data. Extra programming was needed to make these databases utilizable for the casestudies. (For both OpenTSDB and InfluxDB we had to encode the non-numeric data in tags or in timestamps plus strings, including some escape mechanisms for special characters. For KairosDB we had to explicitly implement and add custom types.) Second, both OpenTSDB and KairosDB cannot handle the nanosecond precision of the strace data. We chose to let them store imprecise data instead, because (explicitly) converted timestamps would have taken even more space. Third, all measurements are done after the optimizations and compactions. During the import the databases temporarily take more disk space (e.g., for commit logs etc.).</p><p>Chronix only needs 8.7 GBytes to store the 108.2 GBytes of raw time series data. Compared to the generalpurpose time series databases Chronix saves 20%-68% of the storage demand. This is caused by Chronix' domain specific optimizations and by differences in the underlying storage technologies. By default, OpenTSDB does not compress data, but for a fair comparison we used it with gzip. InfluxDB stores rows of single data points using various serialization techniques, such as Pelkonen et al. <ref type="bibr" target="#b34">[34]</ref> for numeric data. KairosDB uses LZ4 that has a lower compression rate.</p><p>Data Retrieval Performance. The case-studies have 58 raw data queries (q) in their mix, with various time ranges (r). <ref type="table" target="#tab_12">Table 7</ref> gives the retrieval times. They include the time to find, load, and ship the data and the time to deserialize it on the side of client. For the measurements, the data retrieval mix is again repeated 20 times to stabilize the results, with q randomly picked time ranges r.  InfluxDB is the slowest, followed by KairosDB and OpenTSDB. Chronix is the fastest and saves 80%-92% of the time needed for the raw data retrieval. For all databases the retrieval times grow with larger ranges. But for Chronix, they grow more slowly. There are several reasons for this: (a) Chronix uses an index to access the chunks and hence avoids full scans, (b) its pipeline architecture ships the raw chunks to the client that can process (decompress, deserialize) them in parallel, and (c) Chronix selects its chunk size and compression to suit these queries.</p><p>Built-in Function Advantages. In addition to raw data retrieval, anomaly detection in operational data also needs analyzing functions, several of which the generalpurpose time series databases do not natively support (see <ref type="table" target="#tab_1">Table 2</ref>) and whose functionality has to be implemented by hand and typically with more than one query. <ref type="table" target="#tab_13">Table 8</ref> shows the runtimes (20 repetitions for stabilization) that use basic (b) and high-level (h) functions and how often the projects use them (first column). In total, Chronix saves 73%-97% of the time needed by the general-purpose time series databases. We discuss the results for queries with basic functions (b-queries) and with high-level functions (h-queries) in turn.</p><p>In total, the 32 b-queries that other time series databases also natively support account for not more than about 17% of the total runtime. Thus, speed variations for b-queries do not matter that much for anomaly detection tasks. Nevertheless, let us delve into the upper part of <ref type="table" target="#tab_13">Table 8</ref>. OpenTSDB and KairosDB are often slower than InfluxDB or Chronix. Whenever InfluxDB can use its pre-computed values (for instance for average, maximum, etc.) it outperforms Chronix. When on-thefly computations are needed (deviation and percentile), Chronix is faster.</p><p>The lower part of <ref type="table" target="#tab_13">Table 8</ref> illustrates the runtimes of the 43 h-queries. They are important for the anomaly detection projects as they are used much more often than the other functions. Here Chronix has a much more pronounced lead over the general-purpose databases. The reason is that Chronix offers built-in means to evaluate these functions server-side, whereas they have to be manually implemented on the side of the client in the other systems, with additional raw data retrievals.</p><p>Let us look closer at the penalties for the lack of such built-in functions. To implement an outlier detector in the other systems, one has to calculate the threshold value as (Q3 -Q1) · 1.5 + Q3 where Q1 is the first and Q3 is the third quartile of the time series values. With InfluxDB this needs one extra query. KairosDB needs two extra queries, one for getting Q1 and one for Q3, plus a clientside filter. OpenTSDB does neither provide a function for getting Q1 nor for filtering values. In the other systems a trend detector (that checks if the slope of a linear regression through the data is above or below a threshold) has to be built completely on the side of the client. A frequency detector (that splits a time series into windows, counts the data points, and checks if the delta between two windows is above a predefined threshold) is more costly to express and to run in the other systems as well. InfluxDB needs one extra query and a client-side validation. OpenTSDB and KairosDB need a query plus code for an extra function on the side of the client. The grpsize and the split functions that run through this paper are crucial for project 4 both have to be implemented on the side of the client with an extra query for raw values.</p><p>Although it was possible to emulate the high-level functions, we ran into problems that are either caused by the missing support of nanosecond timestamps (KairosDB and OpenTSDB) or the string encoding (lsof/strace) in tags (OpenTSDB). Missing precision causes the split function to construct wrong results -we ignored this and measured the times nevertheless. String decoding and serialization simply took too long, so we measured the time of the raw data retrieval only.</p><p>The online distribution of Chronix holds the code and also the re-implementation of the queries with other time series databases.</p><p>Extra queries and client-side evaluations cause a significant slowdown. This can be seen in the lower part of <ref type="table" target="#tab_13">Table 8</ref>  For the default DDC threshold of d=200 ms we see an inaccuracy rate of 20% for both projects. The average deviations are around 42 ms and 80 ms, resp. From our experience, this is acceptable. With the DDC threshold set to the period of the almost-periodic time series, inaccuracy reaches the worst case as only the first timestamp is stored. But even then the resulting materialized compression rate would only be 1.1% lower but for the costs of a high inaccuracy rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>We discuss related work along the main requirements of Sec. 2 and the domain specific design decisions of Sec. 3.</p><p>Generic data model. We are not aware of any time series database that has such a generic data model as Chronix. Often only scalar values are supported <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>. InfluxDB <ref type="bibr" target="#b23">[24]</ref> has also strings and boolean types. KairosDB <ref type="bibr" target="#b24">[25]</ref> is extensible but the types lack support for custom operators and functions. As discussed in Section 2, this is too restrictive for the operational data of software systems.</p><p>Analysis support. There are indexing approaches for an efficient analysis and retrieval of time series data, e.g., approximation techniques and tree-structured indexes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref>. They optimize an approximate representation of the time series with pointers to the files that contain the raw data for example for a similarity search. In contrast, Chronix is not tailored to a specific analysis but it is optimized for explorative and correlating analyses of operational time series data. Note that the Optional</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association 15th USENIX Conference on File and Storage Technologies 239</head><p>Transformation stage can add indexing values. Several researchers presented methods that detect anomalies in time series <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b46">46]</ref>. Chronix implements in its API most of them and also offers plug-ins.</p><p>Efficient long term storage. While many time series databases focus on cloud computing and PBytes of general time series data, Chronix is domain specific for the anomaly detection in operational data. Chronix builds upon and extends the architectural principles proposed by Shafer et al. <ref type="bibr" target="#b40">[40]</ref> and Dunning et al. <ref type="bibr" target="#b16">[17]</ref>. Its strengths and the reasons for the better performance are its pipeline architecture with domain specific optimizations and the commissioning methodology.</p><p>Many time series databases are distributed systems that run in separate processes <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref> on multiple nodes. Some are affected by synchronization costs and inter-process communication even when configured to run on a single node <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref>. In contrast, on a single node Chronix uses lightweight thread parallelism within a process. Moreover, since OpenTSDB and KairosDB use an external storage technology (HBase <ref type="bibr" target="#b22">[23]</ref> and Cassandra <ref type="bibr" target="#b2">[3]</ref>) with a built-in compression, they cannot save time by shipping compressed data to/from the analysis client <ref type="bibr" target="#b21">[22]</ref>, whereas Chronix uses the chunk size and the compression technique that not only achieve the best results on the data but also cut down on query latency. OpenTSDB and KairosDB have a large memory footprint due to their building blocks.</p><p>In-memory databases or databases that keep large parts of the data in memory, like Gorilla <ref type="bibr" target="#b34">[34]</ref> or BTrDB <ref type="bibr" target="#b1">[2]</ref>, can quickly answer queries on recent data, but they need a long term storage like Chronix for the older data that some anomaly detectors need.</p><p>Lossless storage. A few of the related time series databases are not lossless. RRDtool <ref type="bibr" target="#b31">[31]</ref>, Ganglia <ref type="bibr" target="#b29">[29]</ref> (that uses RRDtool), and Graphite <ref type="bibr" target="#b11">[12]</ref> store data points in a fixed-size cyclic buffer, called Round Robin Archive (RRA). If there is more data than fits into the buffer, they purge old data. This may cause wrong analysis results. For identification purposes Prometheus <ref type="bibr" target="#b36">[36]</ref> uses a 64-bit fingerprint of the attribute values to find data. Potential hash collisions of fingerprints may cause missed data.</p><p>Focus on Queries. While most of the databases are optimized for write throughput <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref> and suffer from scan and filter operations when data is requested, Chronix optimizes the query performance (mainly by means of a reverse index). This is an advantage for the needs of anomaly detection. Chronix delays costly index reconstructions when a batch import of many small chunks of data is needed.</p><p>Chronix processes raw data for aggregations. To optimize such aggregates Chronix can be enriched with techniques of related time series databases to store preaggregated values <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Domain specific compression. Most time series databases use some form of compression. There are lossy approaches that do not fit the requirements of anomaly detection in operational data. (For instance, one idea is to down-sample and override old data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">31]</ref>.) Many time series databases <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref> use a lossless compression. (Tsdb <ref type="bibr" target="#b12">[13]</ref> applies QuickLZ <ref type="bibr" target="#b37">[37]</ref> and only achieves a mediocre rate of about 20% <ref type="bibr" target="#b12">[13]</ref>.) Most of them also use a value encoding that is similar in spirit to the DDC. The difference is that Chronix only removes jitter from the timestamps in almost-periodic intervals as exact values matter for anomaly detection.</p><p>Commissioning. For none of the other time series databases there is a commissioning methodology to tune it to the domain specific or even use-case specific needs of anomaly detection in operational data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The paper illustrates that general-purpose time series databases impede anomaly detection projects that analyze operational data. Chronix is a domain specific alternative that exploits the specific characteristics of the domain in many ways, e.g., with a generic data model, extensible data types and operators, built-in high-level functions, a novel encoding of almost-periodic time series, records with attributes and binary-encoded chunks of data, domain specific chunk sizes, etc. Since the configuration parameters need to be chosen carefully to achieve an ideal performance, there is also a commissioning methodology to find values for them.</p><p>On real-world operational data from industry and on queries that analyze these data, Chronix outperforms general-purpose time series databases by far. With a smaller memory footprint, it saves 20%-68% of the storage space, and it saves 80%-92% on data retrieval time and 73%-97% of the runtime of analyzing functions.</p><p>Chronix is open source, see www.chronix.io.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>class GroupSize implements ChronixAnalysis&lt;LsofTS&gt;{ 2 public GroupSize(String[] args) { 3 field = args[0];filters = args[1]; 4 } 5 public void execute(LsofTS ts, Result result) { 6 result.put(this,new Map()) 7 for (Group group : ts.groupBy(field)) { 8 if (filters.contains(group.key())) 9 result.get(this).put(group.key(), group.value().size()); 10 } 11 } 12 public String getQueryName() {return "grpsize";} 13 public String getTimeSeriesType() {return "lsof";} 14 } Listing 2: Plug-in for a function GroupSize.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Building blocks of Chronix: (1) optional transformation, (2) grouping of time series data into chunks plus attributes, (3) compression of chunks, (4) storage of chunks with their attributes. The data flows from left to right for imports. For queries it is the other way round.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label></label><figDesc>public interface RecordConverter&lt;T&gt; { 2 //Convert a record to a specific type. 3 //Use queryStart and queryEnd to filter the record. 4 T from (Record r, long queryStart, long queryEnd); 5 //Convert a specific type to a record. 6 Record to (T tsChunk); 7 } Listing 3: The time series converter interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impact of different DDC thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Materialized compression rate in the data store.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Access time for a single chunk (in ms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Total access times for 20 · 96 queries (in sec.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1</head><label></label><figDesc>//End of strace for file handle ID = 2030 2 epoll ctl(2129, EPOLL CTL ADD, 2030, 3 {EPOLLIN, {u32=2030, u64=2030}}) = 0 4 //End of strace for file handle ID = 2032 5 epoll ctl(2032, EPOLL CTL ADD, 1889, 6 {EPOLLIN, {u32=1889, u64=1889}}) = 0 Listing 4: Last strace calls for two file handle IDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>where Chronix is faster. But this effect is also visible in the b-queries. For instance, InfluxDB needs 343.8 s / (58 · 20) = 0.3 s on average for a raw data q- query without evaluating any function at all. Its average for the 43 · 20 b-queries instead is only 0.03 s because the function is evaluated server-side and only the result is shipped. This is similar for the other databases. Built- in functions are therefore a clear advantage. Default values of Chronix. All the results show that even with its default parameters Chronix outperforms the general-purpose time series databases on anomaly detec- tion projects. A use-case specific commissioning with both projects' datasets as input did not change the values for c and t and did not buy any extra performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Design space requirements.</head><label>1</label><figDesc></figDesc><table>Time Series 
Database 

Generic 
data model 

Analysis 
support 

Lossless 
long term 
storage 
Graphite 



InfluxDB 



OpenTSDB 



KairosDB 



Prometheus 



Chronix 



= No, = Partly, = Yes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Common query functions.</head><label>2</label><figDesc></figDesc><table>Basic 
G ra p h it e 
In fl u x D B 
O p e n T S D B K a ir o sD B 
P ro m e th e u s C h ro n ix 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Here is an example 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Raw Time Series</head><label></label><figDesc></figDesc><table>10000 
_ 
10006 

_ 
10000 
_ 
10006 

_ 
d2-d1 = 10000 

r1: 
1473060010326 

r2: 
1473060020326 

r3: 
1473060030326 

r4: 
1473060040332 

_ 
10000 
_ 
_ 

_ 

d1: 
start-t1 = 0 

d2: 
t2-t1 = 10000 

d3: 
t3-t2 = 10002 

d4: 
t4-t3 = 10004 

Calculate Deltas 

Compare Deltas 

d3-d2 = 2 
d4-d3 = 4 

Check Drift 

Reconstructed 

Start:1473060010326, DDC Threshold: 4 

Remove Deltas 

t1: 
1473060010326 

t2: 
1473060020326 

t3: 
1473060030328 

t4: 
1473060040332 

Stored 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Multi-Dimensional Storage handles large records of pure data in a compressed binary format. Only the log- ical fields, attributes, and dimensions that are necessary to locate the records are explicitly visible to the data stor- age which uses a configurable set of them for construct- ing indexes. (Dimensions can later be changed to</head><label></label><figDesc></figDesc><table>match 
future analysis needs.) Queries can use any combination 
of the dimensions to efficiently locate records, i.e., to find 
a matching (compressed) chunk of time series data. In-
formation about the data itself is not visible to the storage 
and hence it is open to future changes as needed for ex-
plorative and correlating analyses. Chronix is based on 
Apache Solr as both the document-oriented storage for-
mat of the underlying reverse-index Apache Lucene [4] 
and the query opportunities match Chronix' require-
ments. Furthermore Lucene applies a lossless compres-
sion (LZ4) to all stored fields in order to reduce the index 
size and Chronix inherits the scalability of Solr as it runs 
in a distributed mode called SolrCloud that implements 
load balancing, data distribution, fault tolerance, etc. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Project Statistics. 
Project 
1 
2 
3 
total 
pairs (mio) 
2.4 331.4 162.6 
496.4 
time series 1,080 8,567 4,538 
14,185 

(a) Pairs and time series per project. 

Project 
1 
2 
3 average 
Attributes (bytes) 43 47 48 
46 

(b) Average size of attributes per record. 

r 0.5 
1 
7 14 21 28 56 91 
q 15 30 30 10 
5 
3 
1 
2 

(c) Time ranges (in days) and their occurrence. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 (</head><label>3</label><figDesc></figDesc><table>c) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Project Statistics. 
Project 
4 
5 
total 
time series 
500 
24,055 
24,555 

pairs 
(mio) 
metric 
3.9 
3,762.3 
3,766.2 
lsof 
0.4 
0.0 
0.4 
strace 
12.1 
0.0 
12.1 

(a) Pairs and time series per project. 

r 0.5 1 7 14 21 28 56 91 180 
q 
2 11 15 8 12 5 1 2 
2 58 
b 
1 6 5 7 2 4 4 1 
2 32 
h 
2 6 10 8 6 6 3 2 
0 43 

(b) Time ranges r (days); # of raw data queries (q), of 
queries with basic (b) and high-level (h) functions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Memory footprint in MBytes. 

I n fl u x D B 
O p e n T S D B 
K a i r o s D B 
C h r o n i x 
Initially 
33 
2,726 
8,763 
446 
Import (max) 10,336 10,111 18,905 7,002 
Query (max) 
8,269 
9,712 11,230 4,792 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="true"><head>Table 6 : Storage demands of the data in GBytes.</head><label>6</label><figDesc></figDesc><table>Project 
R a w 

d a t a 
I n fl u x D B 
O p e n T S D B 
K a i r o s D B 
C h r o n i x 
4 
1.2 
0.2 
0.2 
0.3 0.1 
5 107.0 10.7 16.9 26.5 8.6 
total 108.2 10.9 17.1 26.8 8.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><head>Table 7 : Data retrieval times for 20 · 58 queries (in s).</head><label>7</label><figDesc></figDesc><table>r 
q 
I n fl u x D B 
O p e n T S D B 
K a i r o s D B 
C h r o n i x 
0.5 
2 
4.3 
2.8 
4.4 
0.9 
1 11 
5.2 
5.6 
6.6 
5.3 
7 15 
34.1 
17.4 
26.8 
7.0 
14 
8 
36.2 
14.2 
25.5 
4.0 
21 12 
76.5 
29.8 
55.0 
6.0 
28 
5 
7.9 
3.9 
5.6 
0.5 
56 
1 
35.4 
12.4 
24.1 
1.2 
91 
2 
47.5 
15.5 
33.8 
1.1 
180 
2 
96.7 
36.7 
66.6 
1.1 
total 
343.8 138.3 248.4 27.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Times for 20 · 75 b-and h-queries (in s). 

Basic (b) 
I n fl u x D B 
O p e n T S D B 
K a i r o s D B 
C h r o n i x 
4 
avg 
0.9 
6.1 
9.8 
4.4 
5 
max 
1.3 
8.4 
9.1 
6.0 
3 
min 
0.7 
2.7 
5.3 
2.8 
3 
stddev. 
6.7 
16.7 
21.1 
2.3 
5 
sum 
0.7 
6.0 
12.0 
2.0 
4 
count 
0.8 
5.5 
10.5 
1.0 
8 
perc. 
10.2 
25.8 
34.5 
8.6 
High-level (h) 
12 
outlier 
30.7 
29,1 117.6 
18.9 
14 
trend 162.7 
50.4 100.6 
30.2 
11 frequency 
47.3 
23.9 
45.7 
16.3 
3 
grpsize 218.9 2927.8 206.3 
29.6 
3 
split 123.1 2893.9 
47.9 
37.2 
75 
total 
604.0 5996.3 620.4 159.3 

</table></figure>

			<note place="foot" n="242"> 15th USENIX Conference on File and Storage Technologies USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was in part supported by the Bavarian Ministry of Economic Affairs and Media, Energy and Technology as an IuK-grant for the project DfD -Design for Diagnosability.</p><p>We are grateful to the reviewers for their time, effort, and constructive input.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">All online resources accessed on</title>
		<imprint>
			<date type="published" when="2016-09-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing Storage System Design for Timeseries Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andersen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Culler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Btrdb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conf. File and Storage Techn. (FAST)</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Manage massive amounts of data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="http://cassandra.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A full-featured text search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Lucence</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Open source enterprise search platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Solr</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/solr" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The TS-tree: Efficient Time Series Search and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assent</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Afschari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seidl</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Extending database technology: Advances in database technology</title>
		<meeting><address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A Blocksorting Lossless Data Compression Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wheeler</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Systems Research Center</publisher>
			<pubPlace>Palo Alto, CA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep. 124</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">iSAX 2.0: Indexing and Mining One Billion Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camerra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keogh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Data Mining (ICDM)</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<title level="m">Efficient Time Series Matching by Wavelets. In Intl. Conf. Data Eng</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="126" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The system statistics collection daemon</title>
		<ptr target="https://collectd.org" />
		<imprint/>
		<respStmt>
			<orgName>COLLECTD</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">LZ4 is lossless compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collet</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<ptr target="http://www.lz4.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Graphite project -a highly scalable realtime graphing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<ptr target="https://github.com/graphite-project" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">tsdb: A Compressed Database for Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deri</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mainardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fusco</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Traffic Monitoring and Analysis</title>
		<meeting><address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="143" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">GZIP file format specification version 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deutsch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<ptr target="https://www.ietf.org/rfc/rfc1952.txt" />
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Querying and Mining of Time Series Data: Experimental Comparison of Representations and Distance Measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Scheuermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keogh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Very Large Databases Endowment</title>
		<meeting><address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1542" to="1552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="https://www.docker.com" />
		<title level="m">DOCKER. Docker is the leading software containerization platform</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Time Series Databases: New Ways to Store and Access Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dunning</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elastic</forename><surname>Logstash</surname></persName>
		</author>
		<ptr target="https://www.elastic.co/products/logstash" />
		<title level="m">Collect, Parse, Transform Logs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Guice</surname></persName>
		</author>
		<ptr target="https://github.com/google/guice" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Protocol buffers -google&apos;s data interchange format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://github.com/google/protobuf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Snappy</surname></persName>
		</author>
		<ptr target="http://google.github.io/snappy" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data Compression and Database Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graefe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. Applied Computing</title>
		<meeting><address><addrLine>Kansas City, MO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="22" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The hadoop database, a distributed, scalable, big data store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hbase</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<ptr target="http://hbase.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Influxdata</forename><surname>Influxdb</surname></persName>
		</author>
		<ptr target="https://influxdata.com/time-series-platform/influxdb" />
	</analytic>
	<monogr>
		<title level="j">Time-Series Storage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<ptr target="https://kairosdb.github.io/" />
	</analytic>
	<monogr>
		<title level="j">KAIROSDB. Fast Time Series Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keogh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrotra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information Systems</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generic and scalable framework for automated time-series anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laptev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Amizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flint</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<editor>Intl. Conf</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Knowledge Discovery and Data Mining (KDD</title>
		<imprint>
			<biblScope unit="page" from="1939" to="1947" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DataGarage: Warehousing Massive Amounts of Performance Data on Commodity Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loboz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Smyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nath</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Very Large Databases Endowment</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1447" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The ganglia distributed monitoring system: design, implementation, and experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Culler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="817" to="840" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dremel: Interactive Analysis of Web-Scale Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melnik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gubarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Romer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Very Large Databases Endowment</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">RRDtool: Data logging and graphing system for time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oetiker</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<ptr target="http://oss.oetiker.ch/rrdtool" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<ptr target="http://opentsdb.net" />
		<title level="m">OPENTSDB. The Scalable Time Series Database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mining Motifs in Massive Time Series Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lonardi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Data Mining</title>
		<meeting><address><addrLine>Maebashi City, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="370" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pelkonen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cav-Allaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And Veer-Araghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gorilla</surname></persName>
		</author>
		<title level="m">A Fast, Scalable, InMemory Time Series Database. In Conf. Very Large Databases</title>
		<meeting><address><addrLine>Kohala Coast, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1816" to="1827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Similarity Search Over Time-Series Data Using Wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Popivanov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Data Eng</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<ptr target="http://prometheus.io" />
		<title level="m">PROMETHEUS. Monitoring system and time series database</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">QUICKLZ. Fast compression library</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Toward Accurate Dynamic Time Warping in Linear Time and Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fastdtw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="561" to="580" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seward</surname></persName>
		</author>
		<ptr target="http://www.bzip.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Specialized Storage for Big Numeric Time Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sambasivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conf. Hot Topics Storage and File Systems</title>
		<meeting><address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Hadoop Distributed File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shvachko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chansler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symp. Mass Storage Systems and Technologies</title>
		<meeting><address><addrLine>Lake Tahoe, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tukaani</forename><surname>Xz</surname></persName>
		</author>
		<ptr target="http://tukaani.org/xz" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A Novel Technique for Long-Term Anomaly Detection in the Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vallis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hochenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejari-Wal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conf. Hot Topics Cloud Computing</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Kieker: A framework for application performance monitoring and dynamic software analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Hoorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Waller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassel-Bring</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Perf. Eng. (ICPE)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="247" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Statistical Techniques for Online Anomaly Detection in Data Centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Satterfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schwan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. Integrated Network Management (IM)</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="385" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Supporting Swift Reaction: Automatically Uncovering Performance Problems by Systematic Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Happe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Happe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. Soft. Eng. (ICSE)</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="552" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Mining console logs for large-scale system problem detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Tackling Computer Systems Problems with Machine Learning Techniques</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
