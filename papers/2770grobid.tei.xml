<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Erasure Code with Shingled Local Parity Groups for Efficient Recovery from Multiple Disk Failures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Miyamae</surname></persName>
							<email>miyamae.takeshi@jp.fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Fujitsu Laboratories Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Nakao</surname></persName>
							<email>nakao.takanori@jp.fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Fujitsu Laboratories Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensuke</forename><surname>Shiozawa</surname></persName>
							<email>shiozawa.kennsu@jp.fujitsu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Fujitsu Laboratories Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Erasure Code with Shingled Local Parity Groups for Efficient Recovery from Multiple Disk Failures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The ever-growing importance and volume of digital content generated by ICT services has led to the demand for highly durable and space-efficient content storage technology. Erasure code can be an effective solution to such requirements, but the current research outcomes do not efficiently handle simultaneous multiple disk failures. We propose Shingled Erasure Code (SHEC), an erasure code with local parity groups shingled with each other, to provide efficient recovery for multiple disk failures while ensuring that the conflicting properties of space efficiency and durability are adjustable according to user requirements. We have confirmed that SHEC meets the design goals using the result of a numerical study on the relationships among the conflicting properties, and a performance evaluation of an actual SHEC implementation on Ceph, a type of open source scalable object storage software.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The ever-growing importance of digital content generated by ICT service vendors has led to an increasing demand for highly durable content storage technology. For ICT services, triple replication technique has met this demand for years, but its low space efficiency (three times the storage capacity of user data) has made this solution less attractive for vendors. Erasure codes are a durable data storage technique with less redundant information (parities), and are rapidly gaining popularity.</p><p>The space efficiency and durability of erasure codes come with increasing computational overhead. In particular, the recovery overhead associated with disk failures severely affects the availability and performance of ICT services unless the performance interference of the recovery operation is controlled properly. Throttling the CPU and I/O bandwidth of the recovery operation to minimize interference is one of the common practices <ref type="bibr" target="#b17">[18]</ref>, but controlled interference entails a longer recovery operation, which, in turn, makes the storage less durable. Hence recovery performance enhancement itself has been a focus of research in erasure code field lately <ref type="bibr" target="#b9">[10]</ref>.</p><p>Recent, remarkable achievements in recovery performance enhancement are Microsoft LRC (MS-LRC) <ref type="bibr" target="#b8">[9]</ref> and Facebook Xorbas <ref type="bibr" target="#b9">[10]</ref>. They use the concept of local parity, where a few instances of parity are calculated based only on subsets of an entire dataset. Given that the amount of data read and transferred is reduced only in those subsets of a dataset, the recovery consumes a smaller amount of each ICT resource and becomes faster.</p><p>However, in the event of multiple disk failures, MS-LRC and Xorbas use global parities, which include parity information calculated over an entire dataset, and a large amount of data is transferred in the recovery operation. Because each of the disk failures is often corelated in the real world <ref type="bibr" target="#b15">[16]</ref>, we are motivated to develop a new erasure code that is robust against multiple disk failures.</p><p>In this paper, we propose Shingled Erasure Code (SHEC), whose local parity groups overlap each other. The code is designed to recover efficiently from multiple disk failures, and space efficiency and durability are user-adjustable. To confirm the design goals of SHEC, we first study in section 2 the relationships among the three key properties of erasure codes (space efficiency, durability and recovery efficiency) with our various local parity group layouts. Then, we demonstrate that the layout is adjustable to achieve the optimal combination of the intended properties to users. In section 3, we show the aforementioned recovery efficiency of SHEC through evaluation with the SHEC implementation in Ceph <ref type="bibr" target="#b13">[14]</ref>, a type of open source scalable object storage software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ANALYSES OF LOCAL PARITY GROUPS AND A NEW CODE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Properties of Local Parity Groups</head><p>In this paper, we call each split data element in an erasure code stripe a 'data chunk' and each of the local parity groups a 'parity chunk.' We define the local parity's locality as the number of data chunks to calculate each parity chunk.</p><p>We pick three main properties in the erasure code with local parity groups.</p><formula xml:id="formula_0"> Space efficiency  Durability  Recovery efficiency</formula><p>The first property, space efficiency, is defined as the ratio of data chunks and calculated as k/(k+m) (k: the number of data chunks, m: the number of parity chunks), indicating cost efficiency.</p><p>The second property, durability, is defined as the probability of data loss, as shown in equation <ref type="formula">(1)</ref>  <ref type="bibr" target="#b16">[17]</ref>.</p><formula xml:id="formula_1">1 ) / ( ) / 1 (     f f n MTBF MTTR MTBF P PDL (1)</formula><p>PDL: Probability of data loss MTTR: Mean time to recovery MTBF: Mean time between failures n: Total number of disks f: Number of concurrent disk failures n P f : n!/(n-f)! The last property, recovery efficiency, is defined as the inverse number of recovery overhead (recovery overhead means the ratio of read chunks to all data chunks during the recovery). Generally, the smaller the locality is, the higher the recovery efficiency is. The recovery efficiency raises the availability or performance of ICT services.</p><p>Next, we now explain the three-way trade-off relationship among the abovementioned erasure code properties, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. First, we mention the relationship between space efficiency and durability. This trade-off is obvious because if we add parities, durability increases whereas space efficiency decreases (tradeoff #1). The second relationship is between durability and recovery efficiency. If we reduce the locality to increase recovery efficiency, the number of parity chunks covering each data chunk decreases, indicating a decrease in durability (trade-off #2). The last relationship is between space efficiency and recovery efficiency. To reduce the locality with equal durability, we must add more local parities to keep the number of parity chunks covering each data chunk, indicating a decrease in space efficiency (trade-off #3).</p><p>Customers often opine that durability should not be sacrificed to increase recovery efficiency. In such a case, we usually suggest sacrificing space efficiency instead of durability because space efficiency and recovery efficiency also share a trade-off relationship (#3).</p><p>In the remainder of this paper, we will propose and evaluate the new erasure code with local parity groups, based on the above analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Shingled Erasure Code</head><p>We propose a new erasure code, Shingled Erasure Code (SHEC), which is designed for efficient recovery in the event of multiple disk failures, with space efficiency and durability adjustable according to user requirements. SHEC is an erasure code with local parity groups, and the calculation ranges of local parities are shifted and partly overlap with each other, similar to arranging shingles on the roof of a house. All local parity groups have the same locality and are shifted at almost regular intervals. SHEC(k,m,l) represents a layout with k data chunks, m parity chunks and locality l.</p><p>The average number of parity chunks that have relation to each data chunk is ml/k. Because the failure of ml/k+1 data or parity chunks can cause data loss, we use ml/k as an estimator of SHEC's durability. For example, in the case of SHEC(10,6,5), the estimator is 3 (= ml/k = 6*5/10) and the four failures of D1/P1/P5/P6 cause data loss because D1 cannot be recovered from the remaining chunks <ref type="figure">(Figure 2</ref>).</p><p>In the case of multiple data chunk failures, SHEC recovers data from multiple parity chunks. Usually, there are multiple combinations of parity chunks that recover the failed chunks. SHEC selects the one which requires the lowest number of disks read. For example, when D6/D9 fail in <ref type="figure">Figure 2</ref>, SHEC selects P3/P4 because the union of P3/P4's calculation ranges results in six contiguous data chunks, and the size of the union (that indi- To show SHEC's improvement factor in durability, we compare SHEC(6,4,3) with an instance of simple local parity groups (SLPG) <ref type="figure">(Figure 3</ref>). If D1/D2/D3 fail simultaneously, the SLPG cannot be recovered because only P1/P3 have relation to D1/D2/D3. In contrast, SHEC(6,4,3) can be recovered from P1/P2/P4. Among all failure patterns, SHEC(6,4,3)'s data loss cases number half those of the SLPG. That means that SHEC(6,4,3)'s durability is twice as high as SLPG with the same space efficiency and recovery efficiency. Moreover, the higher the durability estimator is, the more the improvement factor of durability is <ref type="figure" target="#fig_1">(Figure 4)</ref>. The improvement is ascribed to the shifting of the calculation range for each local parity.</p><p>Formulating the SHEC generator matrix is quite simple. First, we create a generator matrix of Reed Solomon systematic code (abbreviated as RS(k,m) in this paper). Next, each matrix element whose corresponding data chunk is not used for calculating the corresponding parity chunk is set to zero ( <ref type="figure">Figure 5</ref>). CPU utilization is directly proportional to the number of non-zero matrix elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Comparison among SHEC Parameter Sets</head><p>In this section, we show that SHEC provides layouts with less recovery overhead, ensuring that conflicting properties of space efficiency and durability are adjustable according to user requirements.</p><p>Restricted to the meaningful ones, SHEC can generate over 100 different parameter sets in the threedimensional property space. We pick some sets from them and compare their erasure-code properties. Let us start with SHEC(4,2,4), RAID6's equivalent SHEC parameter set, and search for alternative candidates that are more recovery-efficient with almost equal durability <ref type="table">(Table 1)</ref>. In this case, we sacrifice the space efficiency, and we can get the candidates SHEC(4,3,3) and SHEC <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b2">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Comparison with Other Erasure Codes</head><p>We compare the theoretical recovery overhead between the Reed Solomon code, MS-LRC, Xorbas and SHEC, under the condition of almost equal durability ( <ref type="figure">Figure 6</ref>). SHEC's recovery overhead is less than the others in cases of double or more disk failures. The others are worse because they must use global parities in those cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IMPLEMENTATION AND EVALUA-TION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Ceph Architecture</head><p>We evaluated SHEC on Ceph. A Ceph cluster includes a large number of object storage daemons (OSDs). Each OSD corresponds to a storage device. In this paper, an OSD indicates a whole disk device. A placement group (PG) is a set of OSDs over which the data and parity chunks are distributed randomly.</p><p>When a data is written on a Ceph cluster, (a) the data is divided into 4MB Ceph objects. Next, (b) the PG is determined by a hash value for the name of the object. Finally, (c) each item of data or parity chunk is stored in one of the OSDs assigned to the PG. When (d) an OSD fails, (e) another OSD is newly assigned to the degraded PG, and the lost chunk is recovered to the OSD from the remaining chunks in the acting-set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SHEC Implementation</head><p>The Ceph object storage has an interface plugin for erasure code from v0.80.1 (Firefly) release. It supported the Reed Solomon code as a default plugin, and we implemented SHEC as an alternative. The interface is sufficient to implement SHEC because it includes a useful function minimum_to_decode(), which yields the set of chunk numbers required to decode the lost chunks. SHEC yields the subset of the chunk numbers that the Reed Solomon code yields.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Test Conditions</head><p>We evaluated the SHEC(6,4,3)'s recovery performance and selected RS(6,4) as a reference <ref type="figure">(Figure 8</ref>). The comparison with Xorbas or MS-LRC is beyond the scope of this paper.</p><p>Hardware setup and software version for testing is described in <ref type="figure">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">SHEC Recovery Performance</head><p>We prepared 100GB (25,000 * 4MB objects) of data and measured the recovery time from double OSD failures in each of the cases of RS(6,4) and SHEC <ref type="figure" target="#fig_1">(6,4,3)</ref>. We set the number of Ceph recovery threads as five. At first, we found that the SHEC(6,4,3)'s total CPU overhead was about 20% less than that of RS(6,4) as shown in <ref type="figure" target="#fig_0">Figure 10</ref>. We consider that this was mainly due to the effect of our simplified generator matrix ( <ref type="figure">Figure 5</ref>) because CPU utilization is directly proportional to the number of non-zero matrix elements.</p><p>Next, RS(6,4)'s recovery overhead is 1.00 (the number of read chunks is equal to k), whereas that of SHEC(6,4,3) is 0.74 <ref type="table">(Table 1)</ref>. Therefore, we estimated SHEC(6,4,3)'s recovery time at 74% of RS(6,4) in the beginning. However, this experiment showed that, though the amount of data read from the disks was 74%, the actual recovery time was 81.4% of RS(6,4) ( <ref type="figure" target="#fig_0">Figure  10</ref>).</p><p>The reason was a partial bottleneck. We assumed that one of the system resources must be bottlenecked, and in fact, the disk seemed bottlenecked <ref type="figure" target="#fig_0">(Figure 11</ref>), while the CPU and network <ref type="figure" target="#fig_0">(Figure 10, Figure 12</ref>) did not at all. However, the disk bottleneck did not continue constantly. Seeing <ref type="figure" target="#fig_0">Figure 11</ref>, the disk bandwidth was not fully utilized during 35% of entire recovery time (in the rectangle), and we could re-estimate SHEC(6,4,3)'s recovery time as follows. The result was 83% of RS(6,4), almost the same as the actual ratio, 81.4%.</p><p>Finally, we concluded that SHEC's recovery overhead was decreased in comparison with the Reed Solomon code. However, in our test conditions, 70% of SHEC's recovery efficiency emerges as decreasing latency of recovery completion, and 30% emerges as decreasing disk bandwidth used for recovery processing. Moreover, we obtained a similar result when we tried the comparison between RS(5,3) and SHEC <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b2">3</ref>) in the event of single OSD failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>Various storage systems have used erasure codes <ref type="bibr" target="#b0">[1]</ref> to realize a higher durability of storage data for a long time. Especially, the well-known RAID <ref type="bibr" target="#b0">[1]</ref>, based on the Reed Solomon code, was deployed in almost all of the highly reliable storage systems.</p><p>After Google disclosed Google File System <ref type="bibr" target="#b2">[3]</ref>, which used triple replication with the diffusion of commodity disks, other major cloud storages such as Apache HDFS <ref type="bibr" target="#b3">[4]</ref> and Microsoft Azure storage <ref type="bibr" target="#b4">[5]</ref> followed the trend. However, because the volume of data generated by ICT services started to grow explosively, the erasure code's space efficiency was revalued again <ref type="bibr" target="#b5">[6]</ref>.</p><p>In recent years, recovery overhead has become regarded as a serious problem of erasure code, especially in distributed or scalable storages <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Many researchers have proposed methods including local parity techniques to decrease the recovery overhead. WEAV-ER Codes <ref type="bibr" target="#b6">[7]</ref> suggested a generic method which includes most of the possible local parity layouts. Microsoft Pyramid Codes <ref type="bibr" target="#b7">[8]</ref>, followed by Azure's Local Reconstruction Codes (MS-LRC) <ref type="bibr" target="#b8">[9]</ref> and Facebook Xorbas <ref type="bibr" target="#b9">[10]</ref>, discussed the durability of local parities. MS-LRC insisted that the probability of data loss (information-theoretically non-decodable case) is limited to the trivial level. On the other hand, Xorbas discussed the relationship between locality and code distance highly theoretically. Regenerating Codes <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> suggested an interesting approach which discussed the properties of the optimal trade-off between space efficiency and recovery bandwidth via 'cut-based' analysis. Rotated Reed-Solomon Codes <ref type="bibr" target="#b12">[13]</ref> suggested local parities and has similar layouts to SHEC. However, it is characterized by the number of data disks being limited to the product of a pair of integers. Fountain code <ref type="bibr" target="#b18">[19]</ref> also seems to use a sliding window, but takes a highly probabilistic approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>First, we proposed SHEC, a new erasure code designed for high recovery efficiency, especially from multiple disk failures. Second, we showed that SHEC provides the layouts with more recovery efficiency than the Reed Solomon codes, ensuring that the conflicting properties of space efficiency and durability are adjustable according to user requirements. Finally, we showed through experiments that the SHEC's recovery is actually faster than the Reed Solomon codes.</p><p>However, it may not be easy to show the normal SHEC's superiority to state-of-the-art codes (Xorbas and MS-LRC) without any sacrifices of space efficiency or durability. Therefore, we will expand the normal SHEC concept into an asymmetric one or one bundled with global parities in the future. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three-Way Trade-Off</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SHEC's Improvement Factor of Durability</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ceph Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Recovery Progress and CPU Utilization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Candidates with Equal Space Efficiency</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Effective erasure codes for reliable computer communication protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Communication Review</title>
		<imprint>
			<date type="published" when="1997-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A case for redundant arrays of inexpensive disks (RAID)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 1988 ACM SIGMOD International Conference on Management of Data<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-09" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Google File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th Symposium on Operating Systems Principles. Lake</title>
		<meeting><address><addrLine>George, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-12" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The hadoop distributed file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shvachko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chansler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)</title>
		<meeting>the 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Windows Azure storage: A highly available cloud storage service with strong consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nilakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Skjolsvold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mckelvie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Haridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Uddaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Khatri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bedekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mainali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fahim Ul Haq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ikram Ul Haq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dayanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adusumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manivannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Principles</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Erasure coding vs. replication: A quantitiative comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Weatherspoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Procs. of IPTPS</title>
		<meeting>s. of IPTPS</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">WEAVER Codes: Highly fault tolerant erasure codes for storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hafner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-12" />
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pyramid codes: Flexible schemes to trade space for access efficiency in reliable data storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented at the IEEE Int. Symp. Network Computing and Applications</title>
		<imprint>
			<date type="published" when="2007-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Erasure coding in Windows Azure Storage. Presented at the USENIX Annu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yekhanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Conf</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">XORing elephants: novel erasure codes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sathiamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asteris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vadali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Very Large Data Bases conference (VLDB)</title>
		<meeting>of the Very Large Data Bases conference (VLDB)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deterministic regenerating codes for distributed storage. Presented at the Allerton Con. Control, Computing, and Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-09" />
			<pubPlace>Urbana-Champaign, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explicit and optimal exact-regenerating codes for the minimum-bandwidth point in distributed storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Rashmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Information Theory (ISIT)</title>
		<meeting>IEEE Int. Symp. Information Theory (ISIT)<address><addrLine>Austin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="1938" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rethinking erasure codes for cloud file systems: Minimizing I/O for recovery and degraded reads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ceph: A scalable, high-performance distributed file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maltzahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 7th Symposium on Operating Systems Design and Implementation</title>
		<meeting>of the 7th Symposium on Operating Systems Design and Implementation<address><addrLine>Seattle, WA, November</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CRUSH: Controlled, scalable, decentralized placement of replicated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maltzahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Supercomputing (SC)</title>
		<meeting>Supercomputing (SC)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Availability in globally distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-A</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A story of two MTTDL models. Ramblings from Richard&apos;s Ranch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Elling</surname></persName>
		</author>
		<ptr target="https://blogs.oracle.com/relling/entry/a_story_of_two_mttdl" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lazy means smart: Reducing repair bandwidth costs in erasure-coded distributed storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alvisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SYSTOR</title>
		<meeting>SYSTOR</meeting>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Repairable fountain codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asteris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Inform. Theory</title>
		<meeting>Int. Symp. Inform. Theory<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
			<biblScope unit="page" from="1752" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
