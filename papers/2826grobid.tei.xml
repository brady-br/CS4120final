<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SaFace: Towards Scenario-aware Face Recognition via Edge Computing System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Advanced Institute of Information Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Advanced Institute of Information Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenren</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojie</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Advanced Institute of Information Technology</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SaFace: Towards Scenario-aware Face Recognition via Edge Computing System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Deep Convolutional Neural Networks (CNNs) have achieved remarkable progress in the field of face recognition (FR). However, developing a robust FR system in the real-world is still challenging due to vast variance of illumination, visual quality, and camera angles in different scenarios. These factors may result in significant accuracy drop, if the pre-trained model doesn&apos;t have perfect generalization ability. To mitigate this issue, we present a solution named SAFACE, which helps to improve FR accuracy through unsupervised online-learning in an edge computing system. Specifically, we propose a novel scenario-aware FR flow, then decouple the flow into different phases and map each of them to different levels of a three-layer edge computing system. For evaluation, we implement a prototype and demonstrate its advantages in both improving recognition accuracy and reducing processing latency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, deep learning algorithms have revolutionized the field of face recognition (FR), attributing to the emergence of powerful deep CNN architectures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref> delicate loss functions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>, and large scale (public) face datasets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21]</ref>. However, deploying these FR models in the real-world is still challenging. Recent studies have shown that those FR algorithms trained with static datasets will suffer significant performance degradation when applied to real-world scenarios <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>. This is caused by vast variance of face poses, illumination and visual quality of the captured pictures in different scenarios, as well as the limited generalization ability of FR models trained with static datasets <ref type="bibr" target="#b9">[10]</ref>. A straightforward solution is to collect more training data from the target scenario and then fine-tune the FR models. Unfortunately, this method will introduce several issues: First, the procedure of collecting and labeling training data is both technically challenging and labor-intensive. Second, as data *Equal contribution †Corresponding author volume increases infinitely, it seems to be impossible to manage the data and build/train a model atop to best fit every scenario to deliver seamless service. These issues altogether suggest that such a solution cannot scale in practice.</p><p>To tackle these challenges, we present SAFACE, a practical face recognition system. It employs the idea of unsupervised online learning, which can gradually fine-tune the FR model and improve its accuracy in the targeted scenarios. It is realized by a novel computing flow to unify the inference (face detection and recognition) and online model training. SAFACE performs face detection, verification, and tracking simultaneously. By leveraging implicit information from continuous images in time series, SAFACE learns the discriminative features to retrain the FR model in an unsupervised manner with such online data.</p><p>From the deployment perspective, SAFACE borrows edge computing paradigm <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref> as the substrate to natively solve the scalability issue. We decouple the scenario-aware FR flow into different phases and map each of them to different levels of a three-layer edge computing system. We further introduce the following optimization techniques to improve efficiency. First, we use a strategy only to fine-tune a portion of the FR model. This strategy can improve training efficiency, without sacrificing the accuracy and latency. Second, we propose a context-aware scheduling strategy based on the flow density of persons (faces). This strategy is capable of coordinating the FR inference and online training tasks so that both high inference throughput and high training efficiency are promised. For evaluation,we implement a prototype and demonstrate its advantages in both improving recognition accuracy and reducing processing latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Basic FR Flow</head><p>We start with the typical open-set FR problem. In a standard surveillance process, we first employ a face detection model FD to detect the probes (the faces that need to be recognized) in a frame. Then, these probes are aligned based on the facial landmarks obtained from a face aligning model FA. We 񮽙 face detection and alignment, step 3 񮽙 feeding probes into FR model, step 4 񮽙 extracting face representations, and step 5 񮽙 comparing and determining the identity. denote p as the aligned version of a probe captured from the original frame. Formally, the aligned probes in I t can be obtained by {p i } = FA • FD(I t ). After that, we can feed these aligned face images into the FR model to calculate their face representations, denoted as {x i }. At last, we compare the representations with those pre-registered in the gallery and determine their identities. The pipeline for open-set face recognition is also illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>The overall structure of the SAFACE is depicted in <ref type="figure" target="#fig_1">Figure 2</ref>. On the right side, the flow of SAFACE consists of three parts. The first one (top) is initializing and training a model using a public dataset. This is a one-time step, which is performed on a cloud server. Then, this pre-trained model is sent to edge servers at the starting point for FR inference. As addressed before, the performance may be degraded in different scenarios. The second part (middle) of SAFACE is responsible for FR inferences and online learning tasks, which are deployed on edge servers. The middle piece in this part is a context-aware scheduler. It is responsible for run-time adjusting computation resource utilization of on-line learning tasks. The goal is to avoid interfering the FR inference tasks. The input data for both FR inference and online learning tasks come from the last part (bottom), which is deployed on each smart camera. This part is responsible for face detection, alignment, and tracking tasks. It generates probes and extracts related timing information from live video streams and send them to edge servers.</p><p>The partition and mapping of SAFACE flow highly depends on the computation and communication characteristics of each sub-tasks. Face detection <ref type="bibr" target="#b23">[24]</ref>, alignment <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>, and tracking are considered as light-weight tasks, which can be processed directly on smart cameras. It is also called scenarioshared stage of the flow. On the contrary, the middle part running on edge servers are called scenario-aware stage. The original FR model is adjusted into different versions by online learning, according to the specific environment of each camera. The reasons why we choose edge servers to deploy scenario-aware stage, rather than cloud servers are that, online learning algorithm involves significant extra data transmission between cameras and servers. This is not scalable as the number of surveillance cameras increases with a speed of 50 million per year <ref type="bibr" target="#b17">[18]</ref>. In addition, the communication latency between surveillance cameras and the remote cloud server is relative high (typically range from 50 to 200 ms). It is not acceptable for some real-time FR scenarios. Using edge servers can efficiently solve these problems. Finally, since the pre-traning of FR model demands tremendous computing resources <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and only needs to be done once, we can naturally deploy it in cloud servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FR Flow of SaFace</head><p>In this section, we depict our scenario-aware FR flow. It can dynamically adjust a pre-trained FR model through unsupervised face representation learning. In order to achieve this, we divide the traditional flow into two stages, namely, scenarioshared stage and scenario-aware stage. Illustration of this scenario-aware FR flow is shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which is introduced in detail in the rest of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Scenario-shared Stage</head><p>The scenario-shared stage contains all essential steps in a traditional one. Besides, a face tracking step is introduced to leverage implicit information contained in live video streams. In fact, a probe with the same identity may appear in several consecutive frames of a stream. Thus, we use a face tracking algorithm to obtain the track associated with one specific identity. In this paper, we propose to track the faces by detection, as introduced in the prior study <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>. All steps of this stage are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Given a probe p (associated with a face box) in the t-th frame I t ,we firstly extract a set of face boxes in the next frame I t+1 . The overlap between these boxes and p should be larger than a given threshold. Then, the face box with the maximum overlap is selected into the track. The tracking is terminated when there is no matched face in the next frame. Specifically, to reduce the noises in the face tracking caused by false alarm, we set a minimum length (10 in our design) for each track. We ignore the tracks that are shorter than this threshold. These tracks are used for online training in next stage. Note that in some very crowded scenes, the risk of tracking wrong faces may increase. Adopting more advanced tracking methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref> will mitigate this risk. In our test cases, we find that simply using tracking-by-detection is good enough.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scenario-aware Stage</head><p>In scenario-aware stage, we use the probe tracks from the last stage to incrementally improve the FR model with online learning. Motivated by prior works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>, we adopt triplet loss to fine-tune the FR model. To introduce the way to leverage triplet loss for online learning, we first describe the triplets generation procedure with the face tracks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Triplet Generation</head><p>A triplet consists of three probes, namely, an anchor probe, a negative probe, and a positive probe. For each track, we first select a probe as the anchor probe. Then, we go through all other probes in the same frame containing the anchor probe. Obviously, the identities of these probes are different from that of the anchor probe. Thus, they are named as negative samples in this work. The feature distance between the anchor probe and negative samples is calculated. The negative probe is the negative sample that has the minimum distance from the anchor probe. Similarly, we go through all other probes in the same track, which are supposed to be positive samples. The positive sample having the maximum distance with the Face triplets P = {} 5 for t ← 0 to N v − 1 do 6 for p a in T t do</p><formula xml:id="formula_0">7 p p = arg max p k ({dist(FR(p a ), FR(p k )) : p k ∈ T (p a ) ∧ p k 񮽙 = p a }) 8 p n = arg min p k ({dist(FR(p a ), FR(p k )) : p k ∈ I t ∧ p k 񮽙 = p n }) 9 P.push( 񮽙 p a , p p , p n 񮽙 ) 10 end 11 end 12 return P</formula><p>anchor probe is selected as the positive probe in the triplet. An example is illustrated in <ref type="figure">Figure 4</ref>. Note that, if there is only one probe (i.e., anchor probe) in a frame, we cannot find the corresponding negative probe for this anchor probe. Thus, we skip this anchor probe. For each track, we need to extract all triplets. The corresponding process is listed in Algorithm 1. Each triplet is represented as &lt; p a , p p , p n &gt;. These generated triplets are used for providing the supervised signal for the online learning, which is introduced as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Online Training</head><p>We adopt the triplet-loss function <ref type="bibr" target="#b7">[8]</ref> to learn from the generated triplets. In the context of training deep CNN model, the common practice is to train on all the weight parameters, but in our case, it suffices to adjust a part of the parameters of the FR model. To be specific, we divide the FR CNN model into two separated parts: scenario-shared extractor and scenarioaware learner, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. During the online learning phase, the parameters in the scenario-shared extractor are fixed once the initialized FR model is deployed. All scenarios share the parameters in this part. On the other hand, to learn dedicated face representations from a specific scenario, the SGD is performed on the parameters in the scenario-aware learner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Context-aware Scheduling</head><p>When deploying SAFACE on an edge computing system, both FR model inferences and online training tasks are assigned to edge servers. Each edge server is responsible for multiple cameras, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Obviously, the original FR tasks should be given a higher priority to guarantee real-time responses. The online learning tasks, on the contrary, are performed incrementally without any real-time requirement. Thus, the online learning tasks should not interfere with the FR tasks and only leverage the idle resources on edge servers. Thus, we propose a context-aware scheduling method for online training tasks.</p><p>In practice, the number of video frames are sampled in a fixed rate (e.g. 25fps) at each surveillance camera. This rate is denoted as R C . Assume that the maximum number of cameras that can be connected to an edge server is denoted as N C . We assume that the maximum number of probes contained in a frame is limited as N P max . Thus, the total number of probes that can be processed on an edge server in a time interval ∆t = 1/R C is defined as its computation capability, denoted as N E , which should satisfy the following requirement,</p><formula xml:id="formula_1">N E ≥ N C × N P max<label>(1)</label></formula><p>Since the incoming rates of probes vary a lot in a real-work application, the pivot is to select a proper number (batch size) of triplets for training in a time interval ∆t = 1/R C . It should adjust according to the run-time computation utilization occupied by FR. An example is illustrated in <ref type="figure" target="#fig_4">Figure 5</ref>. In the daytime, the incoming rate is high so that a small batch size is used for online training. During the night, a larger batch size is used.</p><p>The maximum batch size that can be processed in a time interval can be obtained in advance. This is denoted as B max . Then, the run-time batch size B t can be calculated with the following equation,</p><formula xml:id="formula_2">B t = max(0, B max × (1 − α ∑ N C i=1 N P i N E ))<label>(2)</label></formula><p>The term Parameter α is a pre-defined coefficient to adjust so-called effective computation utilization. It accounts for the overhead caused by task switching, data movement, etc. This is a scenario dependent parameter, which should be adjusted in deployment.</p><formula xml:id="formula_3">∑ N C i=1 N P i N E</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Prototype</head><p>For evaluation, we implement a prototype. We adopt several models of InsightFace <ref type="bibr" target="#b2">[3]</ref> to serve as the baseline FR models. To simulate the edge computing system, We adopt Hisilicon Hi3516CV500 IP Camera to serve as the camera node, whose computation power is sufficient to conduct realtime face detection and tracking. The edge node is emulated with a desktop PC equipped with an Intel i7-6700k CPU and Nvidia GTX1080 GPU. For communication between the edge server and camera nodes, we use a TP-Link WDR5620 router through a 100Mbps Ethernet cable. The round trip time between camera nodes and the edge server is less than 20ms. The cloud side is a powerful GPU server with 4×GTX 1080TI GPU.</p><p>We build the training and testing datasets based on a private dataset used in <ref type="bibr" target="#b15">[16]</ref>, which is obtained by the real surveillance cameras located in two different scenarios. As shown in <ref type="figure">Fig- ure</ref> 6, Scene0 and Scene1 have different camera angles, illumination and face resolution. We split each video sequence into two time-independent parts, one for training and another for test. The ratio of training and test frames is 5 to 1. Note that the test data is manually labeled, while the training data has no labels, and our system learns with it through unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation Results</head><p>In this section we present some preliminary results that demonstrate the efficiency of our designed FR system, including the accuracy improvement, speedup of partial fine-tuning and the throughput improvement of context-aware scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Accuracy Improvement</head><p>As introduced before, for each scenario there is a training dataset and a testing dataset based on the experiment settings. For unsupervised online learning, we fine-tune the initialized model via the proposed flow using video streams from the training dataset of the specific scenario. The verification accuracy on the testing dataset is used to measure the performance of the FR model quantitatively. We perform experiments on three different FR models with different CNN models. The overall results are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>As the results show, there is a considerable performance gap between the research benchmark and the benchmark built on real surveillance data. For example, ResNet50 achieves a superior accuracy of 99.80% on LFW <ref type="bibr" target="#b2">[3]</ref>. There is an obvious drop when applying this model to our surveillance images. Specifically, ResNet50 achieves an accuracy of 96.74% on Scenario1 and 95.62% on Scenario2, respectively. With the help of the proposed flow, ResNet50 fine-tuned using online learning (denoted as After) has obviously improved the accuracy. It indicates that the fine-tuned model has a better generalization in the real scenario than the original FR model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Partial Fine-tuning</head><p>Motivated by previous practice in transfer learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, we introduce a strategy in Section 4, which is to fine-tune a part of parameters while freezing others in the phase of online learning. Here, we quantitatively show how the strategy affects the performance of the online learning. To this end, we conduct experiments to fine-tune different parts of the Sphere-20 network and show how the performance changes. <ref type="figure">Figure 7</ref> and <ref type="figure" target="#fig_6">Figure 8</ref> show the speed-accuracy trade-off in the two scenarios. We can easily find that in Sphere-20 model, even if we freeze he first 12 layers in the CNN model and only fine-tune the last 8 layers, the FR accuracy drops a little compared to fine-tuning the whole model. However it achieves about 1.8x speed up. If we further freeze more layers, say 16 layers, the accuracy drops a lot in both two scenarios.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Context-aware Scheduling</head><p>As mentioned in Section 4, we introduce a context-aware scheduling strategy (denoted as dynamic) to coordinate the online learning and inference tasks. In this part, we validate the effectiveness of this strategy on both benchmarks compared with a fixed strategy, which uses a fixed training batch size (denoted as fixed). For the fixed strategy, we test different batch sizes. Note that using different batch size leads to different training speed of the fixed strategy. Therefore, for fair comparison, we adjust al pha in Equation 2 to ensure dynamic strategy has a similar training speed to fixed strategy. The results are shown in <ref type="figure" target="#fig_7">Figure 9</ref>. As shown in results, our dynamic strategy achieves consistent improvements (measured as throughput) on different batch sizes, also in different scenarios. The throughput in our context is defined as the number of probes that the system can process in one second. For example, given a batch size of 40 for fixed strategy in scenario1, dynamic achieves a throughput of 51.03 probes per second while fixed achieves the throughput of 38.52 probes per second. Our strategy shows a 32.4% relative improvements. All these results indicate that our dynamic strategy can achieve better inference throughput while at a high training efficiency compared with the fixed strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper we introduce SAFACE. It exploits implicit timing series information from live video to perform un-supervised online training. It can improve performance of the FR model according to the real environment of a scenario. In addition, SAFACE deploys the whole algorithm flow on an edge computing system rather than a centric cloud system. With further assistance of a run-time task scheduler, efficient execution of both FR inference and online training tasks can be promised.</p><p>• Generality of SAFACE. In this work, we focus on face recognition problems. However, we believe that SAFACE can be generalized to support many other tasks such as person reidentification tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. Since they share similar dataflow with CNN based face recognition. More experiments need to be done to confirm this conjecture.</p><p>• Better Offloading Strategy. SaFace currently adopts an offloading strategy which allocates the face detection/alignment to the IoT device and offloads other tasks into edge nodes. This method reduces the average face detection and recognition latency from about 120ms to less than 60ms in our prototype. Theoretically, we can build better analytic model to achieve optimal latency based on the current states of the edge and its managing IoT node. We mark this as future work.</p><p>• Different Training Modes. For online-learning, there are two different modes. 1) Always-on mode: SAFACE collects triplets all the time and performs fine-tuning immediately after a batch of training data is prepared. 2) Periodical training mode: SAFACE periodically fine-tune the FR model. It collects triplets, stores it, and then fine-tunes the model every one or two hours. The differences of the two modes with respect to performance/accuracy remain to be discussed.</p><p>• Evaluate in More Realistic Scenarios. For fast evaluation, we use two video sequences to evaluate SAFACE. It is better to deploy SAFACE in a more realistic scenarios to evaluate its effectiveness, stability and scalability, etc. More works remain to be done to make SAFACE a more practical system for actual use.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of a basic FR flow: step 1 񮽙 FR model training, step 2 񮽙 face detection and alignment, step 3 񮽙 feeding probes into FR model, step 4 񮽙 extracting face representations, and step 5 񮽙 comparing and determining the identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SAFACE overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Detailed flow of SAFACE: (A) model pre-training, (B) face detection&amp; tracking, (C) FR inference, (D) triplet generation, and (E) online learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Face triplets generation 1 Input: 2 Face tracks T 3 Frame number = N v 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Context-aware Scheduling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Dataset visualization. (The faces are intentionally covered in consideration of privacy)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 7: Speed-accuracy trade-off (#Scenario1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The comparison of context-aware scheduling (denoted as dynamic) and the strategy that uses fixed batch size (denoted as fixed). The x-axis is the fixed batch size, while the y-axis represents the throughput (triplets/min)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Face verification accuracy (%).</head><label>1</label><figDesc></figDesc><table>Model 
Scenario1 
Scenario2 
Before 
After 
Before 
After 
MobileNet 
95.70 
96.12 
92.69 
93.51 
Sphere20 
96.22 
97.13 
94.71 
96.20 
ResNet50 
96.74 
97.33 
95.62 
96.43 

0 

20 

40 

60 

80 

24 
32 
40 
48 

fixed dynamic 

(a) Scenario1 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised learning of face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="142" />
		</imprint>
	</monogr>
	<note>13th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07698</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Trunk-branch ensemble convolutional neural networks for video-based face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taigman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Face recognition in real-world surveillance videos with deep learning method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abdelrahman Eldesokey, et al. The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Luka Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukezic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1949" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning iot in edge: deep learning for the internet of things with edge computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaoru</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mianxiong</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Network</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="101" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-domain adversarial feature learning for sketch re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 FPS via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The worldwide installed surveillance cameras</title>
		<ptr target="https://www.sdmmag.com/articles/92407-rise-of-surveillance-camera-installed-base-slows" />
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Edge computing: Vision and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on the edge computing for the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Grant</forename><surname>Hatcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6900" to="6919" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tracking persons-of-interest via unsupervised representation adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwoo</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02139</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
