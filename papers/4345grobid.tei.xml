<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX. Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><forename type="middle">Alfred</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Morley</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">Qi Alfred Chen</orgName>
								<orgName type="institution" key="instit3">UC Irvine; Z. Morley Mao</orgName>
								<orgName type="institution" key="instit4">University of Michigan</orgName>
								<orgName type="institution" key="instit5">University of Michigan</orgName>
								<orgName type="institution" key="instit6">University of Michigan</orgName>
								<address>
									<settlement>Irvine</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX. Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures Towards Robust LiDAR-based Perception in Autonomous Driving: General Black-box Adversarial Sensor Attack and Countermeasures</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-17-5</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Perception plays a pivotal role in autonomous driving systems, which utilizes onboard sensors like cameras and LiDARs (Light Detection and Ranging) to assess surroundings. Recent studies have demonstrated that LiDAR-based perception is vulnerable to spoofing attacks, in which adversaries spoof a fake vehicle in front of a victim self-driving car by strategically transmitting laser signals to the victim&apos;s LiDAR sensor. However, existing attacks suffer from effectiveness and generality limitations. In this work, we perform the first study to explore the general vulnerability of current LiDAR-based perception architectures and discover that the ignored occlusion patterns in LiDAR point clouds make self-driving cars vulnerable to spoofing attacks. We construct the first black-box spoofing attack based on our identified vulnerability, which universally achieves around 80% mean success rates on all target models. We perform the first defense study, proposing CARLO to mitigate LiDAR spoofing attacks. CARLO detects spoofed data by treating ignored occlusion patterns as invariant physical features, which reduces the mean attack success rate to 5.5%. Meanwhile, we take the first step towards exploring a general architecture for robust LiDAR-based perception, and propose SVF that embeds the neglected physical features into end-to-end learning. SVF further reduces the mean attack success rate to around 2.3%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, self-driving cars, or autonomous vehicles (AV), are undergoing rapid development, and some are already operating on public roads, e.g., self-driving taxis from Google's Waymo One <ref type="bibr" target="#b2">[3]</ref> and Baidu's Apollo Go <ref type="bibr" target="#b0">[1]</ref>, and self-driving trucks from TuSimple <ref type="bibr" target="#b1">[2]</ref> used by UPS. To enable self-driving, AVs rely on autonomous driving (AD) software, in which perception is a fundamental pillar that detects surrounding obstacles using sensors like cameras and LiDARs (Light Detection and Ranging). Since perception directly impacts safety-critical driving decisions such as collision avoidance, it is imperative to ensure its security under potential attacks.</p><p>In AD perception, 3D object detection is indispensable for ensuring safe and correct autonomous driving. To achieve this, almost all AV makers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref> adopt LiDAR sensors, since they capture high-resolution 360 • 3D information called point clouds and are more reliable in challenging weather and lighting conditions than other sensors such as cameras. Due to such heavy reliance on LiDAR, a few prior studies have explored the security of LiDAR and its usage in AD systems <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b53">55]</ref>. Among them, <ref type="bibr">Cao et al.</ref> are the first to discover that the deep learning model for LiDAR-based perception used in a real-world AD system can be fooled to detect a fake vehicle by strategically injecting a small number of spoofed LiDAR points <ref type="bibr" target="#b15">[17]</ref>. Such LiDAR spoofing attacks could lead to severe safety consequences (e.g., emergency brake operations that may injure passengers). However, the attack proposed was evaluated on only one specific model (i.e., Baidu Apollo 2.5), assuming white-box access, which may be unrealistic. Moreover, it is unclear 1) whether the attack generalizes to other machine learning models, and 2) how to mitigate such spoofing attacks.</p><p>In this work, we perform the first study to systematically explore, discover, and defend against a general vulnerability existing among three state-of-the-art LiDAR-based 3D object detection model designs: bird's-eye view-based, voxel-based, and point-wise (introduced in §2). More specifically, we first demonstrate that existing LiDAR spoofing attacks <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b53">55]</ref> cannot directly generalize to all three model designs ( §4). Meanwhile, we find that in these prior works the required sensor attack capabilities to succeed in fooling AD perception are quite intriguing: Cao et al. <ref type="bibr" target="#b15">[17]</ref> found that an attack trace with merely 60 points is sufficient to spoof a front-near vehicle in Apollo 2.5, while a valid one should have ∼2000 points <ref type="bibr" target="#b29">[31]</ref>, which is almost two magnitudes more. Thus, there must exist certain LiDAR-related physical invariants that are not correctly learned in the model, which could also be generalizable to other state-of-the-art 3D object detection model designs.</p><p>To explore the cause, we perform experiments based on hypotheses formed by empirical observations of deep learning models and unique physical features of LiDAR, and discover that all the three state-of-the-art 3D object detection model designs above generally ignore the occlusion patterns in LiDAR point clouds, a set of physical invariants for LiDAR ( §5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association 29th USENIX Security Symposium 877</head><p>For example, when a vehicle is behind another vehicle, its point cloud can legitimately have much fewer points due to the front vehicle's occlusion of the LiDAR beams. However, such point cloud with much fewer points should not be detected as a vehicle at front-near locations with no occlusions, due to the physical law. Unfortunately, all three model designs today fail to differentiate these two cases. This allows an adversary to spoof almost two magnitudes fewer points into the victim's LiDAR but can still fool the perception model into detecting a fake front-near vehicle ( §5.3). Based on this general vulnerability, we design the first black-box (i.e., without any knowledge about the models) adversarial sensor attack on LiDAR-based perception models to spoof a front-near vehicle to a victim AV that can alter its driving decisions ( §6). To realize this, we enumerate different occlusion patterns of a 3D vehicle mesh (e.g., different occluded postures) to fit the sensor attack capability, and leverage ray-casting techniques <ref type="bibr" target="#b16">[18]</ref> to render the attack traces. We perform large-scale experiments on the three target model designs with around 15,000 point cloud samples from the KITTI <ref type="bibr" target="#b29">[31]</ref> dataset. Evaluations show that with the same sensor attack capability as prior works <ref type="bibr" target="#b15">[17]</ref> (i.e., 60 spoofed points), adversaries can generally achieve over 80% success rates on all three model designs.</p><p>Since these spoofed point clouds directly violate the physical laws of the LiDAR occlusion patterns mentioned above, we then leverage them as physical invariants to defend against this class of LiDAR spoofing attacks. First, we design a model-agnostic defense solution, CARLO: oCclusionAware hieRarchy anomaLy detectiOn, which can be applied to LiDAR-based perception immediately without changing the existing models. CARLO exploits two occlusion-related characteristics: 1) the free space inside a detected bounding box, and 2) the locations of points inside the frustum corresponding to a detected bounding box. Large-scale evaluations of CARLO show that it can efficiently and effectively defend both white-and black-box LiDAR spoofing attacks <ref type="bibr" target="#b15">[17]</ref>. CARLO is also found to have high resilience to adaptive attacks since it exploits physical invariants that are highly difficult, if not impossible, for attackers to break.</p><p>While the model-agnostic defense is already useful, it is also beneficial if we can improve the robustness of the model designs themselves. Thus, we further design a general architecture for robust LiDAR-based perception in AVs. We observe that LiDAR measures range data by nature; hence the front view (FV) of the LiDAR sensor preserves the physical features as well as the occlusion information <ref type="bibr" target="#b36">[38]</ref>. Recent studies present view fusion-based models that combines the FV and 3D representations <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b70">72]</ref>. However, our experiment results show that current designs are still vulnerable to LiDAR spoofing attacks since features from the 3D representation dominate the fusion process. To address such limitations, we propose sequential view fusion (SVF), a novel view fusion-based model design that sequentially fuses the FV and 3D representations to ensure that the end-to-end learning makes sufficient use of the features from FV ( §8.2). Evaluations show that SVF can effectively reduce the attack success rate to 2.3% without sacrificing the original performance, which is a 2.2× improvement compared to CARLO. We find that SVF is also resilient to white-box attacks and adaptive attacks.</p><p>Overall, our key contributions are summarized as follows:</p><p>• We perform the first study to explore the general vulnerability of current LiDAR-based perception architectures. We discover that current LiDAR-based perception models do not learn occlusion information in the LiDAR point clouds, which enables a class of spoofing attacks. We construct the first black-box spoofing attack based on this vulnerability. Large-scale evaluations show that attackers can achieve around 80% mean success rates on all target models.</p><p>• To defend against LiDAR spoofing attacks, we design a model-agnostic defense CARLO that leverages the ignored occlusion patterns as invariant physical features to detect spoofed fake vehicles. We also perform large-scale evaluations on CARLO, and demonstrate that CARLO can effectively reduce the mean attack success rate to 5.5% on all target models without sacrificing the original performance.</p><p>• We design a general architecture for robust LiDARbased perception in AVs by embedding the front view (FV) representation of LiDAR point clouds. We find that existing view fusion-based models are dominated by features from the 3D representation, meaning they are still vulnerable to LiDAR spoofing attacks. To address such limitations, we propose sequential view fusion (SVF). SVF leverages a semantic segmentation module to better utilize FV features. Evaluations show that SVF can further reduce the mean attack success rate to 2.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LiDAR-based Perception in AVs</head><p>LiDAR-based perception leverages 3D object detection models to understand driving environments, in which the models output 3D bounding boxes for detected objects. Deep learning has achieved great success in computer vision tasks for 2D images. However, standard convolutional pipelines cannot digest point clouds due to their sparsity and irregularity. To this end, significant research efforts have been made for 3D object detection recently <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b71">73]</ref>, among which the state-of-the-art models can be grouped into three classes:</p><p>1. Bird's-eye view (BEV)-based 3D object detection. Due to the remarkable progress made in 2D image recognition tasks, a large number of existing works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b66">68]</ref> attempt to transform LiDAR point clouds into the 2D structure for 3D object detection in AD systems. Most state-of-the-art methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b66">68]</ref> conduct the transformation by projecting point clouds into the top-down view, also known as the BEV, and utilize convolutional neural networks (CNNs) to perform the final detection. <ref type="figure" target="#fig_0">Figure 1 (a)</ref> shows the architecture of Apollo 5.0 1 , an industry-level BEV-based model, that has six hard-coded feature maps in the BEV and follows a UNetlike <ref type="bibr" target="#b50">[52]</ref> pipeline to output the grid-level confidence score. The final stage heuristically clusters the grids that belong to the same object.</p><p>2. Voxel-based 3D object detection. VoxelNet <ref type="bibr" target="#b71">[73]</ref> is the first model that slices the point clouds into voxels and extracts learnable features by applying a PointNet <ref type="bibr" target="#b47">[49]</ref> to each voxel, after which a 2D convolutional detection layer is applied in the final stage. Many recent works <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b65">67]</ref> adopt this voxel-based architecture and achieve state-of-the-art performance <ref type="bibr" target="#b8">[9]</ref>. <ref type="figure" target="#fig_0">Figure 1 (b)</ref> shows the architecture of PointPillars that creatively voxelizes the point cloud into pillars (a representation of point clouds organized in vertical columns) to enhance the efficiency and follows the general design of voxel-based detection architectures. Notably, PointPillars is adopted by Autoware <ref type="bibr" target="#b5">[6]</ref>, an industry-level AV platform.</p><p>3. Point-wise 3D object detection. Instead of transforming point clouds to regular 2D structures or voxels for feature extraction, recent studies propose to directly operate on point clouds for 3D object detection <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b51">53,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b69">71]</ref> and achieve the state-of-the-art performance. Most existing works in this category use a classic two-stage architecture similar to Faster RCNN <ref type="bibr" target="#b49">[51]</ref> in 2D object detection. The first stage is responsible for generating high-quality region proposals in the 3D space. Based on these proposals, the second stage regresses the bounding box parameters and classifies the detected objects. As shown in <ref type="figure" target="#fig_0">Figure 1 (c)</ref>, PointRCNN adopts a bottomup method that generates point-wise region proposals in the first stage and regresses these proposals in the later stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">KITTI Benchmark</head><p>KITTI <ref type="bibr" target="#b29">[31]</ref> is a popular dataset for benchmarking AD research, of which the point cloud data are by design divided into a trainval set containing 7481 samples and a test set containing 7518 samples. We follow the methodology by Chen et al. to split the trainval set to a training set (3712 samples) and a validation set (3769 samples) for better experimental studies <ref type="bibr" target="#b21">[23]</ref>. KITTI evaluates 3D object detection performance by average precision (AP) using the PASCAL <ref type="bibr" target="#b25">[27]</ref> criteria and requires a 3D bounding box overlap (IoU) over 70% for car detection. KITTI also defines objects into three difficulty classes: Easy, Moderate, and Hard <ref type="bibr" target="#b8">[9]</ref>. The difficulties correspond to different occlusion and truncation levels. We train PointPillars and PointRCNN on the training set, and <ref type="table">Table 1</ref> shows their APs evaluated on the validation set. We utilize the publicly released Apollo 5.0 model since it has its own labeling, which is incompatible with KITTI. In this work, we target car detection on the KITTI benchmark as the APs of pedestrian and cyclist detection are not yet satisfactory. However, our methodology can be generalized to other categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">LiDAR Sensor and Spoofing Attacks</head><p>LiDAR sensor. A LiDAR instrument measures the distance to surroundings by firing rapid laser pulses and obtaining the reflected light with a sensor. Since the speed of light is constant, the accurate distance measurements can be derived from the time difference between laser fires and returns. By firing laser pulses at many predetermined vertical and horizontal angles, a LiDAR generates a point cloud that can be used to make digital 3D representations of surroundings. Each point in a point cloud contains its xyz-i information, corresponding to its location and the intensity of the captured laser return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Sensor-level LiDAR Spoofing Attack</head><p>In the context of sensors, a spoofing attack is the injection of a deceiving physical signal into a victim sensor <ref type="bibr" target="#b44">[46]</ref>. Since they share the same physical channels, the victim sensor accepts the malicious signal, trusting it as legitimate. Prior works <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b53">55]</ref> have shown that LiDAR is vulnerable to laser spoofing attacks. Specifically, Petit et al. showed the feasibility to relay LiDAR laser pulses from other locations to inject fake points into the point cloud <ref type="bibr" target="#b46">[48]</ref>. Shin et al. further improved the attack to control fake points at different locations in the point cloud, even very close to the victim vehicles <ref type="bibr" target="#b53">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Adv-LiDAR: Model-level LiDAR Spoofing Attack</head><p>Besides directly spoofing fake points into LiDAR point clouds, a recent study proposes Adv-LiDAR that uses adversarial machine learning to not only spoof a set of fake points into the point cloud but also manage to deceive the LiDAR-based perception model <ref type="bibr" target="#b15">[17]</ref>. The authors formulate the attack on Apollo 2.5 2 as an optimization problem:</p><formula xml:id="formula_0">min L(x ⊕ t ; M ) s.t. t ∈ {Φ(T ) | T ∈ A} &amp; x = Φ(X) (1)</formula><p>where X is the pristine point cloud and x represents the hard-coded feature maps in Apollo ( §2.1). Φ(·) is the preprocessing function for crafting the feature maps. T and t are the spoofed point cloud and its corresponding feature maps, respectively. A stands for the sensor attack capability, and ⊕ merges the pristine and adversarial feature maps.</p><p>The attack goal is to spoof a fake vehicle right in front of the victim AV that leads to safety issues, and the success condition is that the confidence score of the optimized spoofed points (T ) exceeds the default threshold so that Apollo 2.5 (M ) will detect T as a valid vehicle. The authors formulate the sensor attack capability (A) for general LiDAR spoofing attacks and design a specific loss function (L) and a merging function (⊕) for Apollo 2.5 (M ). By strategically controlling the spoofed points, Adv-LiDAR achieves around 75% attack success rate towards Apollo 2.5 and is considered as the state-of-the-art LiDAR spoofing attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Threat Model</head><p>Sensor attack capability. We perform the sensor-level spoofing attack experiments towards a Velodyne VLP-16 PUCK LiDAR <ref type="bibr" target="#b30">[32]</ref>. The attack setup is the same as Cao et al. <ref type="bibr" target="#b15">[17]</ref>, and the utilized devices are detailed in Appendix A.</p><p>We adopt the formulation in Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref> to describe the sensor attack capability (A):</p><p>• Number of spoofed points. Compared to Adv-LiDAR, we fine-tune the comparator circuit that bridges the photodiode and delay components to calculate the time delay more accurately. Moreover, we also use a better COTS lens put in front of the attack laser to refract the laser beams to a slightly wider azimuth range. Based on these improvements, we can stably spoof at most 200 points. Thus, we assume that attackers can spoof at most 200 points in the pristine point cloud. Such a capability is constrained by the attack hardware devices.</p><p>• Location of spoofed points. Similar to Adv-LiDAR, we assume that attackers are able to modify the distance, altitude, and azimuth of a spoofed point to the victim LiDAR by changing the delay intervals of the attack devices. Especially, the azimuth of a spoofed point can be modified within a horizontal viewing angle of 10 • . Black-box model-level spoofing attack. We consider Li-DAR spoofing attacks as our threat model, which has been shown as a practical attack vector for LiDAR sensors <ref type="bibr" target="#b46">[48,</ref><ref type="bibr" target="#b53">55]</ref>. We adopt the attack goal of Adv-LiDAR: to spoof a frontnear vehicle located 5-8 meters in front of the victim AV. To perform the attack, adversaries can place an attack device at roadsides to shoot malicious laser pulses to AVs passing by, or launch attacks in another vehicle in front of the victim car (e.g., on the adjacent lane) <ref type="bibr" target="#b15">[17]</ref>. LiDAR spoofing attack has been demonstrated to cause severe safety consequences in Sim-control, an AV simulator provided by Baidu Apollo <ref type="bibr" target="#b6">[7]</ref>. For example, spoofing a front-near vehicle to a high-speed AV will make it trigger a hard brake, which may injure the passengers. Adversaries can also launch a spoofing attack on an AV waiting for the traffic lights to freeze the local transportation system <ref type="bibr" target="#b15">[17]</ref>. We assume that attackers can control the spoofed points within the observed sensor attack capability (A). Note that attackers do not have access to the machine learning model nor the perception system. We deem such an attack model realistic since we adopt the demonstrated sensor attack settings by <ref type="bibr">Shin et al. [55]</ref> and relax the white-box assumptions in Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref>.</p><p>Defense against general spoofing attacks. We also consider defending such LiDAR spoofing attacks. We assume a stronger attack model that adversaries have white-box access to the machine learning model and the perception systems. We also assume that defenders can only strengthen the software-level design, but cannot modify the AV hardware (e.g., sensors) due to cost concerns. We deem it a realistic setting since we propose to defend state-of-the-art spoofing attacks, and software-level countermeasures can be easily adopted in current AD systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Limitations of Existing Attacks</head><p>In this section, we first study whether existing LiDAR spoofing attacks can realize the attack goal on three target models, and further discuss their limitations accordingly.</p><p>Limitations of sensor-level LiDAR spoofing attacks:</p><p>1. Blind attack limitation. The sensor-level spoofing attack suffers from the effectiveness issue due to no control strategies for the spoofed points. We apply the reproduced sensor attack traces to three target models and further explore whether they will be detected as vehicles at target locations. The results (detailed in §6.1.1) show that blindly spoofing cannot effectively achieve the attack goal other than Apollo 5.0, which also confirms the findings by <ref type="bibr">Cao et al. [17]</ref>.</p><p>Limitations of Adv-LiDAR:</p><p>1. White-box attack limitation. Adv-LiDAR, the state-ofthe-art spoofing attack by Cao et al., demonstrates the feasibility of leveraging adversarial machine learning techniques to enhance its effectiveness <ref type="bibr" target="#b15">[17]</ref>. However, it suffers from the white-box limitation. Adv-LiDAR assumes that attackers have access to the deep learning model parameters and its pre-and post-processing modules. However, very few AV companies publicly release their perception systems, making Adv-LiDAR challenging to launch in the real world.</p><p>2. Attack generality limitation. Adv-LiDAR cannot be easily generalized. First, as introduced in §2.2.2, Adv-LiDAR only targets Apollo 2.5 and utilizes a specific pre-processing function (Φ(·)) and merging function (⊕) which are not applicable to other models. Constructing such functions is nontrivial since they need to be differentiable so that the optimization problem can be solved by gradient descent-based methods <ref type="bibr" target="#b17">[19]</ref>. For example, the Φ(·) and ⊕ correspond to the voxelization and stacking processes, respectively, in PointPillars. It is still unknown whether such processes can be properly approximated differentiablely. Second, adversarial examples generated by Adv-LiDAR cannot transfer between models. We construct 20 optimized attack traces using AdvLiDAR that successfully fool Apollo 5.0, and apply them to the other two models. However, none can achieve the attack goal in either PointPillars or PointRCNN. Third, the attack trace T is optimized with one specific point cloud at a time (Equation 1), which indicates that T may not succeed in attacking other point cloud samples. The robustness analysis by Cao et al. also validates that the attack success rate consistently drops with the change of the pristine point cloud <ref type="bibr" target="#b15">[17]</ref>.</p><p>Overall, existing spoofing attacks cannot easily achieve the attack goal on all target models. Though Adv-LiDAR shows the feasibility to attack Apollo 2.5, more work is needed to understand the potential reasons that lead to its success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">A General Design-level Vulnerability</head><p>Motivated by the limitations of existing attacks, in this section, we leverage an in-depth understanding of the intrinsic physical nature of LiDAR to identify a general design-level vulnerability for LiDAR-based perception in AD systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Behind the Scenes of Adv-LiDAR</head><p>Despite a lack of generality, Adv-LiDAR was able to spoof a fake front-near vehicle by injecting much fewer points than required for a valid vehicle representation. For example, Cao et al. have demonstrated that an attack trace with merely 60 points and 8 • of horizontal angles is sufficient to deceive Apollo 2.5 <ref type="bibr" target="#b15">[17]</ref>. However, a valid front-near vehicle ( §3) contains around 2000 points and occupies about 15 • of horizontal angles in KITTI point clouds <ref type="bibr" target="#b29">[31]</ref>. It remains unclear why such spoofing attacks can succeed despite a massive gap in the number of points between that of a fake and a valid vehicle. To answer this question and comprehend the general vulnerability exposed by Adv-LiDAR, it is necessary to consider the distinct physical features of LiDAR. In particular, we identify two situations where a valid vehicle contains a small number of points in LiDAR point clouds: 1) an occluded vehicle and 2) a distant vehicle, each corresponding to a unique characteristic (C) of LiDAR.</p><p>C1: Occlusions between objects will make occluded objects partially visible in the LiDAR point cloud. As introduced in §2.2, a LiDAR sensor functions by firing laser pulses and capturing their returns. As a result, each point in a point cloud represents the distance to the nearest solid object along the laser ray. Similar to human eyes, a LiDAR sensor can only perceive parts of an object (e.g., a vehicle) if other obstacles, that obstruct the laser beams, are standing between the LiDAR and the object. Consequently, an occluded vehicle contains significantly fewer points than a fully exposed one since only a portion of it is visible.</p><p>In this paper, we name such occluded objects as occludees and the obstacles that occlude others as occluders. Particularly, as shown in <ref type="figure">Figure 2</ref>, we use O(v) to represent the point set that occludes a vehicle v, and V to denote the point set that belongs to the vehicle v in a point cloud F.</p><p>C2: The density of data decreases with increasing distance from the LiDAR sensor, due to the working principles of LiDAR sensors ( §2.2). Since the generated point clouds are collected uniformly in vertical and horizontal angles, the density of point clouds varies in the 3D space. Similar to human eyes in which a far object occupies much fewer pixels than a near one with identical size, a distant vehicle contains significantly fewer points since its point set is much sparser than that of a front-near vehicle in LiDAR point clouds ( <ref type="figure">Figure 3</ref>).</p><p>Based upon these observations, we propose two hypotheses of potential false positive (FP) conditions for current LiDARbased perception models, which could contribute to the success of Adv-LiDAR:</p><p>FP1: If an occluded vehicle can be detected in the pristine point cloud by the model, its point set will still be detected as a vehicle when directly moved to a front-near location.</p><p>FP2: If a distant vehicle can be detected in the pristine point cloud by the model, its point set will still be detected as a vehicle when directly moved to a front-near location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Validation</head><p>We design experiments (E) to test the existence of such potential erroneous predictions (i.e., FP) on three target models using the KITTI validation set.</p><p>E1: To validate FP1, we first randomly pick 100 point</p><formula xml:id="formula_1">cloud samples F = {F i } 100 i=1</formula><p>that contain 100 target occluded vehicles {v i } 100 i=1 with their point sets V i ⊆ F i . We then feed F into three target models and record the confidence scores (i.e., outputs of models to represent the confidence of detection) of the occluded vehicles as s i for each v i .</p><p>Second, we leverage a global translation matrix H(θ, τ) (Equation 2) to move every V i to a front-near location (i.e., 5-8 meters in front of the victim AV) in the point cloud F i as V i , where θ and τ correspond to the azimuth and distance of the translation, respectively: . <ref type="figure" target="#fig_2">Figure 4</ref> shows the CDF of e for three target models. As shown, 99.5% of the picked occluded vehicles only have below 10% fluctuations of their confidence scores, which successfully validate FP1.</p><formula xml:id="formula_2">V i w i = V iw i     V i w x V i w y V i w z 1     =     cos θ − sin θ 0 τ cos (θ + α) sin θ cos θ 0 τ sin (θ + α) 0 0 1 0 0 0 0 1     ·     V iw x V iw y V iw z 1     (2) (V iw x ,V iw y ,V iw z ,V iw i ) denotes</formula><p>The success of E1 comes from the fact that LiDAR-based 3D object detection models perform amodal perception, where given only the visible portions of a vehicle v, the model attempts to reason about occlusions and predict the bounding box for the complete vehicle <ref type="figure">(Figure 2</ref>). However, convolutional operations exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. Such architecture thus ensures to produce the strongest response to a spatially local input pattern. Since the occludee's and occluder's point sets V and O(v) stand apart from each other in the 3D space, deep learning models may fail to identify the causality between V and O(v) and thus learns to regress the bounding box for v by V only.</p><p>E2: To validate FP2, similarly, we first randomly pick 100 point cloud samples that contain 100 target distant vehicles {v i } 100 i=1 that locate farther than 30 meters away from the AV, and follow the same procedure with E1 to record the confidence score changes. Experimental results show that all of the translated points V i are detected by three target models, and we calculate the relative errors e = |s i −s i | s i</p><p>. <ref type="figure" target="#fig_2">Figure 4</ref> shows the CDF of e for three target models. As shown, 99.5% of the picked distant vehicles only have below 7.5% fluctuations of their confidence scores, which successfully validate FP2.</p><p>The success of E2 comes from that 3D object detection models are designed to be non-sensitive to the locations of objects. For example, Apollo 5.0 does not incorporate location information in its hard-coded feature maps, and PointRCNN regards the centers of each bounding box as the origins of their coordinates. Hence the global locations of objects are not valued by the 3D object detection models in AD systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Vulnerability Identification</head><p>As mentioned earlier, the sensor attack capability A is far from spoofing a fully exposed front-near vehicle's point set. However, E1 and E2 provide two strategies for adversaries to launch spoofing attacks with fewer points and horizontal angles. As a result, attackers can directly spoof a vehicle imitating various occlusion and sparsity patterns that satisfy the sensor attack capability A to fool the state-of-the-art models.</p><p>For example, the V (red points) in <ref type="figure">Figure 2</ref> only contains 38 points and occupies 4.92 • horizontally when translated to 6 meters in front of the AV. We confirm that it can deceive all three target models successfully, as visualized in Appendix E.</p><p>The vulnerability comes from the observation that the stateof-the-art 3D object detection architectures ignore the distinct physical features of LiDAR. Therefore, they leave a gap, as well as an attack surface, between the model capacity and Li-DAR point clouds. We further abstract the neglected physical features as two occlusion patterns inside the LiDAR point clouds, described below.</p><p>Inter-occlusion. We abstract the typical occlusion introduced in §5.1 as inter-occlusion. As its name indicates, interocclusion describes a causal relationship between occludee and the corresponding occluders (i.e., the occluders cause the occludee partially visible). FP1 violates the physical law of inter-occlusion since a translated "occluded" vehicle's point set V no longer has its valid occluder O(v). However, E1 demonstrates that state-of-the-art LiDAR-based perception models overlook such inter-occlusions in the point clouds.</p><p>Intra-occlusion. We abstract the other occlusion pattern hidden inside an object as intra-occlusion. The facing surface of a solid object (e.g., a vehicle) occludes itself in the point cloud, which indicates that the LiDAR cannot perceive the interior of the object <ref type="figure" target="#fig_7">(Figure 9</ref>). FP2 violates the physical law of intra-occlusion since the abnormal sparseness of a translated "distant" vehicle's point set V can no longer fully  occlude a valid vehicle since other laser pulses could penetrate its "surface". However, E2 demonstrates that state-of-the-art LiDAR-based perception models are unable to differentiate reflected points of real solid objects from sparse injected points of the same overall shape so that they also overlook the intra-occlusions in the LiDAR point clouds.</p><p>To demonstrate the potential real-world impacts of this identified vulnerability, we construct the first black-box spoofing attack on state-of-the-art LiDAR-based perception models in §6. We find the violations of the physical law of occlusion generally enable LiDAR spoofing attacks. Therefore, we perform the first defense study, exploiting the occlusion patterns as physical invariants to detect spoofing attacks in §7. Lastly, in §8, we present a general architecture for robust LiDARbased perception that embeds occlusion patterns as robust features into end-to-end learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Black-box Spoofing Attack</head><p>Constructing black-box attacks on deep learning models is non-trivial. Prior works have studied black-box attacks on image classification <ref type="bibr" target="#b43">[45]</ref> and speech recognition models <ref type="bibr" target="#b7">[8]</ref>. However, none explored LiDAR-based perception models, and their approaches usually suffer from efficiency limitations (e.g., building a local substitute model). In this section, we present the first black-box LiDAR spoofing attack based on our identified vulnerability ( §5.3) that achieves both high efficiency and success rates.</p><p>1. Constructing original attack traces. As demonstrated in §5.3, occluded or distant vehicles' point sets that meet the sensor attack capability can be utilized to spoof frontnear vehicles. Therefore, our methodology attempts to closely represent realistic physical attacks using traces from realworld datasets (e.g., KITTI). In order to test different sensor attack capability, we extract occluded vehicles' point sets with varying numbers of points (5-200 points) from the KITTI validation set. Furthermore, we take 10 points as interval, and divide the extracted point sets into 20 groups per their number of points (The first group contains traces with the number of points from 0 to 10, and the second group contains traces with the number of points from 10 to 20, etc.). We then randomly pick five traces in each group forming a small dataset K containing 100 point sets.</p><p>Besides collecting existing real-world traces, the identified vulnerability also supports adversaries in generating customized attack traces, which are more efficient for pipelining the attack process. We leverage ray-casting techniques to generate customized attack traces. More specifically, we utilize a 3D car mesh and implement a renderer <ref type="bibr" target="#b16">[18]</ref> simulating the function of a LiDAR sensor that probes the car mesh by casting lasers. By doing so, we can render the car mesh's point cloud. We further simulate different occlusion and sparsity patterns on the car mesh to fit the sensor attack capability, as shown in <ref type="figure" target="#fig_4">Figure 6</ref>. Similar to K , we collect rendered point clouds with different numbers of points by using different postures and occlusion patterns. We also follow the same procedure to build a small dataset R containing 100 rendered point sets. More figures of R are shown in Appendix E.</p><p>2. Spoofing original attack traces at target locations. To trigger severe security and safety consequences, adversaries need to inject the constructed attack traces at target locations in the point cloud. We consider spoofing K and R in both digital and physical environments. For digital spoofing, we make sure the injection of attack traces meets the sensor attack capability A and real-world requirements. We follow the high-level formulation in Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref> utilizing a global transformation matrix H(θ, τ) (Equation 2) to translate the attack traces (i.e.,</p><formula xml:id="formula_3">V T = H(θ, τ) ·V T , where V ∈ K ∪ R ).</formula><p>Here the translation interprets the attack capability (A) in terms of modifying the azimuth and distance of attack traces. We further calibrate each point in the translated attack trace to its nearest laser ray's direction and prune the translated attack trace to fit the attack capability (i.e., V ∈ A). Finally, we merge the attack trace with the pristine point cloud according to the physics of LiDAR. We feed the modified point cloud samples containing the attack traces into three target models.</p><p>For physical spoofing, we program attack traces from R as input to the function generator so that we can control the spoofed points and launch the spoofing attack <ref type="bibr" target="#b53">[55]</ref> in our lab. We further collect the physical attack traces and feed them into target models. Due to the limitation of our attack devices, we only conduct preliminary physical spoofing experiments. More details of physical spoofing can be found in §9.1.2. It is worth noting that such limitations do not hurt the validity of USENIX Association 29th USENIX Security Symposium 883 our attack model ( §3) since the attack capability A is adopted from Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref>, in which has been demonstrated in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Attack Evaluation and Analysis</head><p>We perform large-scale evaluations on our proposed blackbox attack in terms of effectiveness and robustness. Experimental setup. The evaluations are performed on the KITTI trainval and test sets (introduced in §2.1.1), which are collected in the physical world. As mentioned before, limited by our attack devices, we leverage K , R to launch digital spoofing attacks. We also utilize attack traces (S ) generated by the sensor-level spoofing attack ( §4) as a baseline. S is collected from blindly physical spoofing attacks on a real Velodyne VLP-16 PUCK LiDAR <ref type="bibr" target="#b30">[32]</ref>. We further inject all the attack traces from above three constructed datasets into the KITTI point clouds at front-near locations (i.e., 5-8 meters in front of the victim AV) to test their effectiveness.</p><p>Evaluation metrics. Object detection models often have default thresholds for confidence scores to filter out detected objects with low confidence (potential false positives). We leverage the default thresholds used by three target models to measure the attack success rate (ASR). We label an attack successful as long as the model detects a vehicle at the target location whose confidence score exceeds the default threshold:</p><formula xml:id="formula_4">ASR = # of successful attacks # of total point cloud samples<label>(3)</label></formula><p>Besides the default threshold, we also define a new metric that leverages multiple thresholds to evaluate LiDAR spoofing attacks. The corresponding definitions and evaluations are described in Appendix B, which provide insights that point-wise features appear to be more robust than voxel-based features. <ref type="figure" target="#fig_3">Figure 5</ref> shows the ASR of the digital spoofing attack with different attack capabilities (i.e., number of points). As expected, the ASR increases with more spoofed points. The ASRs are able to universally achieve higher than 80% in all target models with more than 60 points spoofed, and it also stabilizes to around 85% with more than 80 points spoofed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Attack Effectiveness</head><p>Notably, the attack traces from R can achieve comparable ASR with K on all target models, which demonstrates that adversaries can efficiently leverage a customized renderer to generate attack traces ( <ref type="figure" target="#fig_4">Figure 6</ref>). Such rendered traces can be directly programmed into hardware for physical spoofing attacks (Appendix A). Interestingly, S achieves much higher ASR on Apollo 5.0, indicating that BEV-based features are less robust to spoofing attacks than the other two categories, which could be attributed to the information loss of feature encoding from BEV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Robustness Analysis</head><p>We analyze the robustness of the proposed attack to variations of attack traces V and the average precision (AP) of target  models M . We also evaluate the attack robustness against state-of-the-art defense strategies <ref type="bibr" target="#b64">[66,</ref><ref type="bibr" target="#b68">70]</ref> designed for imagebased adversarial attacks. We find that spoofed traces with around 60 points to trigger major changes in ASR. Note that Cao et al. also utilized spoofed traces with 60 points for analysis <ref type="bibr" target="#b15">[17]</ref>. Therefore, we use attack traces with <ref type="bibr" target="#b58">(60,</ref><ref type="bibr" target="#b68">70]</ref> points from R for the robustness analysis.</p><p>Robustness to variations in attack traces. First, we apply a scaling matrix S to the attack traces V with different-level randomness to simulate the inaccuracy of sensor attack:</p><formula xml:id="formula_5">V w i = V w i   V w x V w y V w z   =   s 0 0 0 s 0 0 0 s   ·   V w x V w y V w z  <label>(4)</label></formula><p>where s subjects to a uniform distribution U(1 − ε, 1 + ε). We use the mean l-2 norm to measure the distance between V and V . <ref type="figure" target="#fig_5">Figure 7</ref> shows the ASR drops with larger l-2 distances which is expected. However, as shown, the ASR still reaches around 70% while the distance is around 10 cm. We also observe that the ASR for PointRCNN drops faster than for PointPillars and Apollo 5.0, which also validates that pointwise features are arguably more robust than voxel-based and BEV-based features (detailed in Appendix B).</p><p>Robustness to variations in model performance. To understand the relationship between ASR and the original performance of models (i.e., AP), we first extract the intermediate models when we trained PointPillars and PointRCNN. We then try to launch attacks on these models. Surprisingly, we find that the ASR increases with higher AP 3 <ref type="figure" target="#fig_6">(Figure 8</ref>), which implies that a model with better performance could be more vulnerable to such attacks. Our results indirectly demonstrate that the identified vulnerability could be attributed to an ignored dimension (i.e., occlusion patterns) by current models. Since the models do not notice such a hidden dimension, they will be overfitted to be more vulnerable during training.</p><p>Robustness to adversarial training. Adversarial training is not rigorously applicable because it targets classification models, and requires norm-bounded perturbations to make the optimization problem tractable <ref type="bibr" target="#b40">[42]</ref>. In contrast, our study targets 3D object detection models, and the proposed attack is constrained by the sensor attack capability (A), which does not fit any existing norm-bounded formulations. Thus, we perform this robust analysis in an empirical setting. Specifically, we generate another 100 attack traces with 60 points using the customized renderer and randomly inject two of them into each point cloud sample in the KITTI training set at areas without occlusions. We further train PointPillars and PointRCNN on this modified dataset and evaluate our proposed attack using the same 60-point attack traces with §6.1.1 on them. We observe that the ASRs drop from 83.6% to 70.1% and 88.3% to 79.7% on PointPillars and PointR-CNN, respectively, on the KITTI validation set. However, the "Hard" category's original detection performance has significant degradation of over 10% on both models. Our results empirically show that current LiDAR-based perception model designs cannot learn the occlusion information correctly. The slight drop of the ASRs comes from the under-fitting effect of existing occluded vehicles (i.e., significant AP degradation), which is not acceptable in real AD systems.</p><p>Robustness to randomization-based defenses. We leverage state-of-the-art image-based defenses: feature squeezing <ref type="bibr" target="#b64">[66]</ref> and ME-Net <ref type="bibr" target="#b68">[70]</ref> to test the attack robustness on Apollo 5.0 since it has similar pipelines with image-based models. We demonstrate that none of them can defend the black-box spoofing attack without hurting the original AP. More details can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Physics-Informed Anomaly Detection</head><p>Our results show that a lack of awareness for occlusion patterns enables the proposed black-box attack in §6. Since adversaries exploit an ignored hidden dimension, such attacks can succeed universally in target models and appear to be robust to existing defenses ( §6.1). Since anomaly detection methods have been widely adopted in different areas <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b37">39]</ref>, one intuitive and immediate mitigation is to detect such violations of physics. We find that no existing open-source AV platforms enable such physical checking <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. In this section, we present CARLO: oCclusion-Aware hieRarchy anomaLy detectiOn, that harnesses occlusion patterns as invariant physical features to accurately detect such spoofed fake vehicles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">CARLO Design</head><p>CARLO consists of two building blocks: free space detection and laser penetration detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Free Space Detection</head><p>Free space detection (FSD) integrates both inter-and intraocclusions ( §5.3) to detect spoofed fake vehicles. As introduced in §2.2, each laser in a LiDAR sensor is responsible for perceiving a direction in the spherical coordinates. Due to resolution limits, such a laser direction actually corresponds to a thin frustum in the 3D space. As shown in <ref type="figure" target="#fig_7">Figure 9</ref>, the ) is considered as free space (drivable space occupied by air only). Therefore, combined with all laser beams of the LiDAR, the entire 3D space is divided into free space (FS) and occluded space (OS) (i.e., space behind the hit point from the LiDAR sensor's perspective). FS information is embedded at the point level. Occlusions, on the other hand, exist at the object level. FS, thus, is more fine-grained and incorporates occlusion information since the OS of an object directly reflects its occlusion status <ref type="figure" target="#fig_7">(Figure 9</ref>). Due to inter-occlusion and intra-occlusion, we observe that the ratio f of the volume of FS over the volume of a detected bounding box should be subject to some distribution and upper-bounded by ∃ b ∈ (0, 1), implying f ∈ (0, b] ( <ref type="figure" target="#fig_7">Figure  9</ref>). Nevertheless, since the fake vehicles do not obey the two occlusion patterns, their ratio f should be large enough and lower-bounded by ∃ a ∈ (0, 1) such that f ∈ [a, 1). Clearly, as long as a &gt; b, we have opportunities to distinguish valid vehicles with the spoofed fake vehicles statistically. To estimate the ratio f , we grid the 3D space into cells and calculate:</p><formula xml:id="formula_6">f B = ∑ c∈B 1 · FS(c) |B|<label>(5)</label></formula><p>where FS(c) indicates whether the cell c is free or not, and |B| denotes the total number of cells in the bounding box B. The algorithm to derive FS(c) can be found in Appendix C. We then estimate the distributions of valid and fake vehicles.We empirically set the cell size to 0.25 3 m 3 , and utilize the KITTI training set and 600 new attack traces generated by the implemented renderer ( §6) for estimation. <ref type="figure" target="#fig_0">Figure 10</ref> shows that the CDF of f and f clearly separate from each other. We further take the models' error into considerations (0.7 IoU), and estimate the distributions again. The two distributions still do not overlap with each other, as shown in <ref type="figure" target="#fig_0">Figure 10</ref>, which demonstrate the feasibility to leverage the ratio f as an invariant indicator for detecting anomalies.</p><p>However, though FSD provides a statistically significant method to detect adversarial examples, it is too timeconsuming to perform ray-casting to all the detected bounding  boxes in real-time. The mean processing time of one vehicle is around 100 ms in our implementation using C++ on a commodity Intel i7-6700K CPU @ 4.00GHz, which is already comparable to the inference time of deep learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Laser Penetration Detection</head><p>Laser penetration detection (LPD) is a variant of FSD that aims to provide better efficiency for CARLO. As introduced in §7.1, each point in the point cloud represents one laser ray and the boundary between free space and occluded space. Given a vehicle's point set, its bounding box B also divides the corresponding frustum into three spaces which are: 1) the space between the LiDAR sensor and the bounding box B ↑, 2) the space inside the bounding box B, and 3) the space behind the bounding box B ↓. Intuitively, only a small number of laser rays can penetrate the bounding box <ref type="figure" target="#fig_7">(Figure 9)</ref>. As a result, from the perspective of the LiDAR sensor, the ratio g of the number of points located in the space behind the bounding box B ↓ over the total number of points in the whole frustum should be upper bounded by ∃ b ∈ (0, 1). For the same reason in §7.1, the ratio g of the spoofed vehicles is supposed to be large enough and lower bounded by ∃ a ∈ (0, 1).</p><p>Therefore, the ratio g is derived from:</p><formula xml:id="formula_7">g B = ∑ p∈B↓ 1( p) ∑ p∈B∪B↓∪B↑ 1( p)<label>(6)</label></formula><p>Since LPD leverages information directly from the output of models, it is a good fit for parallel acceleration. The mean processing time of LPD is around 5 ms for each bounding box using Python on a commodity GeForce RTX 2080 GPU.</p><p>Similarly, <ref type="figure" target="#fig_0">Figure 11</ref> shows the CDF of g and g for valid vehicles from the KITTI training set and the 600 generated attack traces, respectively. As shown, though the distributions of ground-truth are separate, the error-considered distributions overlap with each other (i.e., b &gt; a ). We verify that the overlap comes from the noise introduced by points of the ground plane. As a result, LPD will cause erroneous detection of potential anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Hierarchy Design</head><p>To achieve both robustness and efficiency, CARLO hierarchically integrates FSD and LPD. In the first stage, CARLO  accepts the detected bounding boxes and leverages LPD to filter the unquestionably fake and valid vehicles by two thresholds ( §7.1.2). The rest bounding boxes are uncertain and will be further fed into FSD for final checking. CARLO achieves around 8.5 ms mean processing time for each vehicle. The entire algorithm of CARLO is detailed in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">CARLO Evaluation</head><p>Experimental setup. We evaluate the defense performance of CARLO on the KITTI trainval and test sets. We apply all the attack traces from K , R to all point cloud samples at target locations (5-8 meters in front of the victim), and feed them into three CARLO-guarded models CARLO(M (·)). We also evaluate CARLO against Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref> on Apollo 5.0. The defense goal is to successfully detect the spoofed fake vehicles from the output bounding boxes without hurting the original performance (i.e., AP) of the target models. Evaluation metrics. We evaluate the performance of CARLO in two aspects, which are the ASR of the CARLOguarded models, and the precision and recall of CARLO itself. ASR directly relates to the defense performance, while the precision and recall of CARLO reflect whether it will harm the original AP of target models. We test the ASR on the validation set and test set since the distributions are estimated from the training set. We only evaluate the precision and recall of CARLO on the validation set as we do not have the ground-truth for the test set. <ref type="figure" target="#fig_0">Figure 12</ref> (a-c) shows the ASR of three CARLO-guarded models. As shown, CARLO reduces the ASR from more than 95% to below 9.5% with the maximum attack capability, and reduce the mean ASR to around 5.5%. We observe that the remaining 5.5% comes from the detection errors (i.e., the detected bounding box of the fake vehicle cannot match well with the ground-truth) that shift the f and g to the distribution of valid vehicles. The errors occur randomly in the point clouds so that it is hard for adversaries to utilize. The recall in <ref type="figure" target="#fig_0">Figure 12 (d)</ref> reaches around 95% in all targets <ref type="table">Table 1</ref>: PointPillars' and PointRCNN's APs (%) of 3D car detection on the KITTI validation set. "Mod." refers to the Moderate category introduced in §2.1.1; "Original" refers to the original performance of two models; "Attack" refers to the performance after spoofing attacks; "CARLO" refers to the performance after CARLO applied. models which also validate the results in <ref type="figure" target="#fig_0">Figure 12 (a-c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Besides delivering satisfactory defense performance, CARLO barely introduces misdetections (i.e., false negatives) to the models. <ref type="figure" target="#fig_0">Figure 12 (d)</ref> shows that the precision of anomaly detection reaches at least 99.5% for all target models. We manually verify the 0.5% misdetections, and find they are all vehicles at least 40 meters away from the AV, which will not affect its immediate driving behavior. Table 1 also shows that the AP will slightly increase after CARLO being applied to the original model because the original model has internal false positives such as detecting a flower bed as a vehicle. CARLO detects some of those false positives generated by the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Defense against White-box and Adaptive Attacks</head><p>To further evaluate CARLO against white-box attacks, we first leverage Adv-LiDAR to generate adversarial examples that fool Apollo 5.0 and test whether they can succeed in attacking CARLO-guarded Apollo 5.0. <ref type="figure" target="#fig_0">Figure 13</ref> demonstrates that CARLO can effectively defend Adv-LiDAR, where the ASR drops from more than 95% to below 5% consistently. We observe that the defense effects are better than the results shown in <ref type="figure" target="#fig_0">Figure 12 (a)</ref>. We find out that Adv-LiDAR tends to translate the attack traces to a slightly higher place along z axis. Such translations will isolate the adversarial examples in the point cloud, making themselves easier to be detected by CARLO.</p><p>We also try our best efforts to evaluate CARLO on the adaptive attacks. We assume attackers are aware of the CARLO pipelines and utilize Adv-LiDAR to break CARLO's defense. Due to the sensor attack capability, adversaries have limited ability to modify the absolute free space (∑ c∈B 1 · FS(c)) in Equation 5. However, attackers can try to shrink the volume of the bounding box (|B|) to shift the distribution of f , since it is controlled purely by models. Therefore, the attack goal is to spoof a vehicle at target locations at the same time, minimize the size of the bounding box. We formulate the loss function and follow Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref> to utilize a global transformation matrix H(θ, τ) (Equation 2) for solving the optimization problem:</p><formula xml:id="formula_8">min θ,τ L(x ⊕V · H(θ, τ) T ) + λ · V B (V · H(θ, τ) T ) (7)</formula><p>where L(·) is the loss function defined in Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref>, V B (·) is the volume of the target bounding box B, and λ  is a hyper-parameter. <ref type="figure" target="#fig_0">Figure 14</ref> shows that such adaptive attacks cannot break CARLO, either. We attribute the reason to H(θ, τ) that holistically modifies the spoofed points so that it can barely change the size of the bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Physics-Embedded Perception Architecture</head><p>In this section, we take a step further to explore the feasibility of embedding physical features into end-to-end learning that provides better robustness for AD systems. We find that, despite BEV or 3D representations, which are used by most models, the front view (FV) is a better representation for learning occlusion features by nature. However, prior works adopting FV are still vulnerable to the proposed attacks due to their model architecture designs' fundamental limitations. To improve the design and further enforce the learning of occlusion features, we propose sequential view fusion (SVF), a general architecture for robust LiDAR-based perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Why should FV Representations help?</head><p>We observe that LiDAR natively measures range data ( §2.2). Thus, projecting the LiDAR point cloud into the perspective of the LiDAR sensor will naturally preserve the physical features of LiDAR. Such projecting is also known as the FV of LiDAR point clouds <ref type="bibr" target="#b36">[38]</ref>. Given a 3D point p = (x, y, z), we can compute its coordinates in FV p FV = (r, c) by:</p><formula xml:id="formula_9">c = arctan(y, x)/∆θ r = arctan(z, x 2 + y 2 )/∆φ<label>(8)</label></formula><p>where ∆θ and ∆φ are the horizontal and vertical fire angle intervals ( §2.2). As shown in <ref type="figure">Figure 2</ref>, since the occluder O(v) and occludee V neighbor with each other in the FV, deep learning models have opportunities to identify the interocclusion. The abnormal sparseness of a fake vehicle will also be exposed, as valid vehicles' points are clustered, while the spoofed points scatter in the FV <ref type="figure" target="#fig_3">( §5.3)</ref>. Therefore, the FV representation of point clouds embeds both ignored occlusion patterns.</p><p>Although prior works have utilized FV for object detection, little is known about its robustness to LiDAR spoofing attacks. LaserNet <ref type="bibr" target="#b41">[43]</ref> is the latest model that only takes the FV representation of point clouds as input for 3D object detection. However, LaserNet cannot achieve state-of-the-art performance compared to models in the three classes introduced in §2. Other studies <ref type="bibr" target="#b36">[38]</ref>  leveraging the FV representation, models cannot provide satisfactory detection results. The failure of FV-based models comes from the scale variation of objects as well as occlusions between objects in a cluttered scene <ref type="bibr" target="#b70">[72]</ref>.</p><p>Besides the models that only take FV as input, several studies <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b70">72]</ref> present fusion-based architectures for LiDAR-based perception that utilize the combinations of data from different sensors and views as input. MV3D <ref type="bibr" target="#b22">[24]</ref> is a classic fusion-based design that takes both LiDAR point clouds and RGB images as input and predicts 3D bounding boxes, where the point cloud is projected into multi-views (i.e., FV and BEV) for feature encoding. Zhou et al. recently proposed multi-view fusion (MVF), which combines FV with 3D representations <ref type="bibr" target="#b70">[72]</ref>. MVF builds on top of PointPillars. Instead of only voxelizing points in 3D, MVF also voxelizes the point cloud into FV frustums and integrates the two voxels' features based on coordination mapping in the 3D space.</p><p>To better understand the robustness of fusion-based architectures, we reproduce MV3D and MVF based on PointPillars. For MV3D, we ignore the RGB images, and take the FV and BEV as the model input since we focus on LiDAR-based perception. We use a VGG-16 <ref type="bibr" target="#b54">[56]</ref> for FV feature learning in MV3D. <ref type="figure" target="#fig_0">Figure 15</ref> shows the architectures we adopt and reproduce. We train the two reproduced models on the KITTI training set and evaluate them on the KITTI validation set. As <ref type="table" target="#tab_2">Table 2</ref> shows, the FV-augmented models can achieve comparable performance than the original PointPillars. The reproduced results also align well with the evaluations in <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b70">72]</ref>. We then evaluate their robustness against our proposed black-box attack. The experimental setups are identical to the settings in §6.1. <ref type="figure" target="#fig_0">Figure 16</ref> shows that the ASR of reproduced models are as high as the original PointPillars. It indicates that existing view fusion-based architectures both cannot help with defending against LiDAR spoofing attacks, although they provide marginally gain on the AP. We further perform abla-  tion studies and find that the BEV (or 3D) features dominate the model decisions (elaborated in Appendix D). Since the identified vulnerability exists in 3D space, the models are still vulnerable to LiDAR spoofing attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Sequential View Fusion</head><p>The insights drawn from existing view fusion schemes show that existing fusion designs cannot provide better robustness compared to the original models. The 3D (or BEV) representation dominates the model leaving the FV representation not critical in the end-to-end architectures. Based on the above understandings, we propose a new view fusion schema called sequential view fusion (SVF). SVF comprises of three modules <ref type="figure" target="#fig_0">(Figure 17</ref>), which are: 1) semantic segmentation: a semantic segmentation network that utilizes the FV representation to computes the point-wise confidence scores (i.e., the probability that one point belongs to a vehicle). 2) view fusion: the 3D representation is augmented with semantic segmentation scores. 3) 3D object detection: a LiDAR-based object detection network that takes the augmented point clouds to predict bounding boxes. Instead of leaving the models to learn the importance of different representations by themselves, we attach a semantic segmentation network to the raw FV data. By doing so, we enforce the end-to-end learning to appreciate the FV features, so that the trained model will be resilient to LiDAR spoofing attacks.</p><p>Semantic segmentation. The semantic segmentation networks accept the FV represented point clouds and associate each point in FV with a probability score that it belongs to a vehicle. These scores provide aggregated information on the FV representation. Semantic segmentation over FV has several strengths. First, as mentioned before, the FV representation is noisy because of the nature of LiDAR. Compared to 3D object detection or instance segmentation, which is intractable over FV, semantic segmentation is an easier task as it does not need to estimate object-level output. Second, there are extensive studies on semantic segmentation over FV represented point clouds <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b61">63]</ref>, and the segmentation networks achieve much more satisfactory results than the 3D object detection task over FV.</p><p>In our implementation, we adopt the high-level design in LU-Net <ref type="bibr" target="#b13">[15]</ref>. It is worth noting that the end-to-end SVF architecture is agnostic to the semantic segmentation module. View fusion. The fusion module re-architects existing symmetric designs which integrate the 3D representation with the confidence scores generated by the semantic segmentation module. Specifically, we use Equation 8 for mapping between p = (x, y, z) and p FV (r, c), and augment each p with the pointwise confidence score from its corresponding p FV .</p><p>3D object detection. SVF is also agnostic to the 3D object detection module. In this paper, we utilize PointPillars and PointRCNN in our implementation. Most of the models introduced in §2 can fit into the end-to-end SVF architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">SVF Evaluation</head><p>Experimental setup. We train SVF-PointPillars and SVFPointRCNN on the KITTI training set. The setup of robustness analysis against LiDAR spoofing attacks is identical to the settings in §6.1. We also try to evaluate SVF against Adv-LiDAR <ref type="bibr" target="#b15">[17]</ref> on Apollo 5.0 and the adaptive attacks.</p><p>Evaluation metrics. We evaluate the AP of SVFPointPillars and SVF-PointRCNN on the KITTI validation set, and leverage ASR to test their robustness against LiDAR spoofing attacks.</p><p>As shown in <ref type="table" target="#tab_3">Table 3</ref>, both SVF models achieve comparable AP compared to the original models. The marginal degradation comes from two-state training. More specifically, the distributions of the semantic segmentation outputs in the training and validation sets do not align well with each other. We find that the drop of AP will indeed cause a tiny amount of false negatives but will not influence the driving behaviors. Moreover, such degradation could be compensated with better training strategies (e.g., finer-tuning of the parameters) since the capacity of SVF is larger than the original models. We then perform the robustness evaluation of SVF models. <ref type="figure" target="#fig_0">Figure 18</ref> shows the ASR of our proposed spoofing attacks. As shown, the attacks are no longer effective in SVF models. The ASR reduces from more than 95% (original models) to less than 4.5% on both models with the maximum attack capability, which is also an around 2.2× improvement compared to CARLO-guarded models. The mean ASR also drops from 80% to around 2.3%. We also perform ablation study on SVF,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Defense against White-box and Adaptive Attacks</head><p>Since SVF requires re-training for the model, we cannot directly evaluate Adv-LiDAR on SVF-Apollo ( §2.1.1). As a result, we decouple the problem to whether Adv-LiDAR can fool both the semantic segmentation and 3D object detection modules. We first directly apply the attack traces that successfully fool Apollo 5.0 to the segmentation network and record the mean confidence score of all the points belonging to the attack trace. <ref type="figure" target="#fig_0">Figure 19</ref> shows that the mean confidence scores are consistently below 0.08, which is too low to be classified as a valid vehicle with mean confidence scores of around 0.73 in our trained model. Model-level defenses are usually vulnerable to simple adaptive attacks <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b17">19]</ref>. To demonstrate the effectiveness of SVF against adaptive attack, we assume that the adversaries are aware of the SVF architecture. The attack goal is to both fool the semantic segmentation and 3D object detection modules. We also leverage the formulation in <ref type="bibr" target="#b15">[17]</ref> to utilize the global transformation matrix H(θ, τ) to control the spoofed points.</p><formula xml:id="formula_10">min θ,τ −L seg (x V · H(θ, τ) T ) (9)</formula><p>where represents the point cloud merge in the front view and L seg (·) defines the average confidence score of the attack trace (i.e., V · H(θ, τ) T ). <ref type="figure" target="#fig_21">Figure 20</ref> shows that none of the attack traces' average confidence score reaches 0.2 in the segmentation module, which is still far from the mean average confidence score of valid vehicle 0.73. Therefore, the adaptive attacks also cannot break the robustness of SVF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion and Future Work</head><p>In this section, we discuss the distinct features of our proposed black-box attack along with its practicality and completeness. We further discuss the comparisons between the presented defense strategies and their limitations, accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Attack Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.1">Comparison with Physical Adversarial Attacks</head><p>First, we distinguish the spoofing attacks on LiDAR with extensive prior work on physical-world adversarial machine learning attacks in mainly three aspects:</p><p>USENIX Association 29th USENIX Security Symposium 889  1. Different perturbation methods. Images and point clouds have different data structures, which further lead to different perturbation methods applied. Images have compact and ordered structures. In contrast, point clouds are irregular, represented as N × C, where N is the number of points, and C contains the location and intensity information (i.e., xyz-i) <ref type="bibr" target="#b8">[9]</ref>. Attackers are able to generate adversarial examples by modifying the RGB values in images. For attacks on LiDAR, however, attackers can directly shift the point in the 3D Euclidean space as long as it obeys the physics of LiDAR.</p><p>2. Different perturbation capabilities. Prior attacks on 2D images treat the whole target area as the attack surface since the threat model assumes that attackers have full controls over the target object (e.g., attackers can potentially modify any area of a stop sign in <ref type="bibr" target="#b26">[28]</ref>). However, due to the characteristics of LiDAR spoofing attacks, the attack surface is limited by the sensor attack capability (A) in §3. Such a small attack surface also introduces difficulties in launching the attack.</p><p>3. Different perturbation constraints. Prior attacks on 2D images leverage L p norms as the main constraints for the formulated optimization problem <ref type="bibr" target="#b26">[28]</ref> whose goal is to minimize the perturbation to be stealthy. Such constraints do not apply to attack LiDAR because point clouds are not perceived by humans. Thus, stealthiness is not a focus in such attacks. Instead, the optimized attack traces must not exceed the sensor attack capability (A) boundary, in which case A naturally becomes the primary constraint for attacking LiDAR.</p><p>Second, the high-level methodology of our proposed attack is similar to replay attacks, in which adversaries playback the intercepted data to deceive target systems <ref type="bibr">[11]</ref>. However, different from existing replay attacks <ref type="bibr" target="#b42">[44]</ref> that retransmit logically correct data to launch attacks, the limited sensor attack capability (A) cannot support the injection of a physically valid vehicle's trace into the LiDAR point cloud <ref type="bibr" target="#b15">[17]</ref>. Thus, the success of our black-box attack indeed relies on the identified vulnerability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.2">Attack Practicality and Completeness</head><p>One major limitation of our proposed attack is that the presented results cannot directly demonstrate attack practicality in the physical world. First, due to the limitations of our delay component (i.e., the function generator in our implementation), we can only control spoofed points at 10cm-level precision. Therefore, we only construct two fine-controlled physical attack traces for a proof-of-concept demonstration. The two attack traces contain 140 and 47 points. We evaluated them on the KITTI trainval set, and they achieve 87.68%, 98.12%, and 74.91% ASRs on Apollo 5.0, PointPillars, and PointRCNN, respectively. Second, launching our black-box attack on a real AV requires accurate aiming of attack lasers at a target LiDAR, which is challenging to perform without realworld road tests and precision instruments <ref type="bibr" target="#b15">[17]</ref>. Since this paper aims to explore and expose the underlying vulnerability, we leave real-world testing as future work.</p><p>Although we demonstrate that our proposed black-box attack achieves high attack success rates, the identified vulnerability does not provide completeness. This means that there may exist other potential vulnerabilities hidden in the AD systems to be discovered and exploited. Future research may include verification of the AD models and comprehensive empirical studies to explore the underlying vulnerabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Defense Discussion</head><p>CARLO vs. SVF. Both CARLO and SVF achieve satisfactory defense performance while maintaining comparable AP with the original model. In addition, both of them are modelagnostic so that they can be incorporated into most existing LiDAR-based perception systems. CARLO is a practical postdetection module. It does not require re-training the model, which can be quite labor-intensive. CARLO is also realistic because it does not assume that users have white-box access to the model. SVF, on the other hand, is a general architecture for ensuring robust LiDAR-based perception. SVF embeds physical information into model learning, which requires retraining. Compared to CARLO, SVF achieves better defense performance but suffers from a slight drop in AP, indicating that it may require more training efforts.</p><p>Limitations. The main limitation of our mitigation strategies is the lack of guarantees. First, although both defenses can effectively defend against LiDAR spoofing attacks under the current sensor attack capability, our countermeasures may not work at some point with the increasing capability of sensor attacks. We argue that if attackers can spoof a set of points located in the distribution of physical invariants for valid vehicles (e.g., injecting around 1500 points into the point cloud), there is arguably no way to distinguish them at the model level and it is safer for AVs to engage emergency brakes in that situation. Second, both defenses have a small portion of false alarms (i.e., the 0.5% false negatives in CARLO and the slight AP drop of SVF). However, we manually verify that they are not front-near vehicles; hence they would not impact the AV's driving behaviors, as mentioned before. Third, since the adaptive attacks are formulated with our efforts, future research may present more powerful attacks or advanced perturbation methods to break our defenses. In the future, we plan to improve SVF to provide guaranteed robustness by combining multiple sensors' inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Related Work</head><p>Vehicular system security. Extensive prior works explore security problems in vehicular systems and have identified vulnerabilities in in-vehicle networks of modern automobiles <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b32">34]</ref>, in-vehicle cache side channels <ref type="bibr" target="#b10">[12]</ref>, and Connected Vehicle (CV)-based systems <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b59">61]</ref>. Other studies try to provide robustness vehicular systems, such as secured in-vehicle communications <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b60">62]</ref>, secured invehicle payment transactions <ref type="bibr" target="#b28">[30]</ref>, and secured CV communications <ref type="bibr" target="#b48">[50]</ref>. In comparison, our work focuses on the emerging autonomous vehicle systems and specifically targets the robustness of LiDAR-based perception in AVs, which are under-explored in previous studies.</p><p>3D adversarial machine learning. Adversarial attacks and defenses towards 3D deep learning have been increasingly explored recently. Point cloud classification models have been demonstrated vulnerable to adversarial perturbations <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b62">64]</ref>. Xiao et al. generate adversarial examples for 3D mesh classification <ref type="bibr" target="#b63">[65]</ref>. <ref type="bibr">Liu et al. and Yang et al.,</ref> on the other hand, leverage heuristics to detect the adversarial examples for point cloud classification <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b67">69]</ref>. In comparison, our work targets LiDAR-based 3D object detection in AVs.</p><p>As introduced in §2.2, LiDAR point clouds only have measurements of the object's facing surface, which are different from full 3D point cloud data or meshes. Our attack method is motivated to generate adversarial examples in a black-box manner based on the discovered vulnerability. The mitigation strategies are designed to defend against current sensor attack capability, thus provide better robustness against both whiteand black-box LiDAR spoofing attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>In this paper, we perform the first study to explore the general vulnerability of LiDAR-based perception architectures. We construct the first black-box spoofing attack based on the identified vulnerability, which universally achieves an 80% mean success rate on target models. We further perform the first defense study, proposing CARLO to accurately detect spoofing attacks which reduce their success rate to 5.5%. Lastly, we present SVF, the first general architecture for robust LiDAR-based perception that reduces the mean spoofing attack success rate to 2.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Spoofing Attack Details</head><p>The attack consists of three modules: a photodiode, a delay component, and an infrared laser <ref type="bibr" target="#b53">[55]</ref>. The photodiode functions as a synchronizer that triggers the delay component whenever it captures laser signals from the victim LiDAR sensor. The delay component triggers the laser module after a configurable time delay to attack the following firing cycles of the victim LiDAR sensor. The attack can be programmatically controlled so that an adversary can target different locations and angles in the point cloud. Specifically, we use an OS-RAM SFH 213 FA as the photodiode, a Tektronix AFG3251 function generator as the delay component, and a PCO-7114 laser driver that drives the attack laser OSRAM SPL PL90 in our setups. <ref type="figure" target="#fig_0">Figure 21</ref> shows the physical spoofing attack conducted in a controlled environment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Supplementary Attack Evaluation</head><p>We define a new metric for general evaluations on the object detection-based attacks called average attack success rate (A 2 SR). As mentioned before, the default thresholds are empirically set. Thus, evaluations of ASR provide limited insights. Similar to AP defined in PASCAL <ref type="bibr" target="#b25">[27]</ref> criteria, we average the ASR in 11 recall intervals to better understand the impact of the proposed attacks and the characteristics of different architectures:</p><formula xml:id="formula_11">A 2 SR = 1 11 ∑ r∈{0.0,0.1,...,1.0} ASR t r<label>(10)</label></formula><p>where t r represents the threshold that makes the recall of the target model at r. The evaluation of recall follows the description of the Moderate category in §2.1.1. In this paper, we test A 2 SR on PointPillars and PointRCNN since Apollo model is not designed to be evaluated on KITTI ( §2.1.1). <ref type="figure" target="#fig_23">Figure 22</ref> shows that the A 2 SR of PointPillars is generally higher than PointRCNN, which means the spoofed points can achieve higher confidences in PointPillars. Such results are expected since point-wise features contain more detailed information than voxel-based features; hence point-wise features could be more resilient to spoofing attacks.  We also leverage feature squeezing <ref type="bibr" target="#b64">[66]</ref> and ME-Net [70] to evaluate our proposed attack on Apollo 5.0. We utilize median smoothing as the method for feature squeezing, and follow general settings in <ref type="bibr" target="#b68">[70]</ref> for matrix estimation. We perform evaluations on 100 samples from the KITTI validation set. Results show that ME-Net can eliminate the fake vehicle but introduce new false negatives, which will lead to more USENIX Association 29th USENIX Security Symposium 893  severe safety issues. In contrast, feature squeezing cannot effectively eliminate fake vehicles, as shown in <ref type="figure" target="#fig_1">Figure 23</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C CARLO Algorithm Details</head><p>Algorithm 1 shows the detailed CARLO algorithm combined with its two building blocks: FSD and LPD. Especially, to estimate the free space inside a bounding box B, we first extract all the laser fires that have chances to hit B, which form a frustum in the 3D space. We then grid the 3D Euclidean space of the frustum into small 3D cells and initialize all the cells as occluded cells in the beginning. For each laser, we use 3D Bresenham's line algorithm <ref type="bibr" target="#b14">[16]</ref> to compute the cells it traversed from the origin of the laser beam (i.e., the LiDAR sensor) to the end (i.e., the hit point). If a cell is traversed by a laser beam, we label it as a free cell because it does not belong to a solid object. Finally, we union the free cells for all the possible laser rays to get the total free cells in the frustum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Ablation Study of View Fusion Models</head><p>We perform ablation studies to explore the reasons behind the results shown in <ref type="figure" target="#fig_0">Figure 16</ref>. In particular, we find most of the existing fusion-based models utilize a symmetric design where the FV and 3D (or BEV) features are fed into similar modules for learning, and the learned features are simply stacked or averaged for later stages ( <ref type="figure" target="#fig_0">Figure 15</ref>). We design experiments to study the effectiveness of such a design empirically. Explicitly, we zero out the features from FV to measure how much the FV representation contributes to the final detection. As shown in <ref type="figure" target="#fig_2">Figure 24</ref>, the APs only have relatively small degradation, which implies that BEV (or 3D) features dominate the model decisions. Kim et al. also empirically demonstrates that current sensor fusion-based models are vulnerable to single-source perturbations <ref type="bibr" target="#b31">[33]</ref>. Similarly, we showcase that current view fusion-based models are vulnerable to the perturbation represented in the dominated view. To better understand why SVF models can provide better robustness, we analyze how the FV representation helps in SVF models. Similarly, we zero out the augmented confidence score features and evaluate the AP. <ref type="figure" target="#fig_3">Figure 25</ref> shows the weakened models' performance, which empirically demonstrates that the features from FV account more in SVF models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Supplementary Figures</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: State-of-the-art LiDAR-based perception models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Illustration of an occluded vehicle (C1) in LiDAR point clouds. The yellow points from another vehicle occlude the vehicle v from the perspective of the LiDAR sensor. The blue 3D cubes are the bounding boxes of detected vehicles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: CDF of the relative errors e in E1. Right: CDF of the relative errors e in E2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Attack success rates (ASRs) of proposed black-box spoofing attacks on target state-of-theart models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The process of generating attack traces for R from the implemented renderer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Attack robustness to variations in generated attack traces from R .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Attack robustness to variations in target models' performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Illustration of free space (FS) and occluded space (OS) in a frustum corresponding to a detected bounding box. frustum (as well as the straight-line path p − o) from the Li-DAR sensor ( o = (0, 0, 0)) and any point in the point cloud (∀ p = (x, y, z)) is considered as free space (drivable space occupied by air only). Therefore, combined with all laser beams of the LiDAR, the entire 3D space is divided into free space (FS) and occluded space (OS) (i.e., space behind the hit point from the LiDAR sensor's perspective). FS information is embedded at the point level. Occlusions, on the other hand, exist at the object level. FS, thus, is more fine-grained and incorporates occlusion information since the OS of an object directly reflects its occlusion status (Figure 9). Due to inter-occlusion and intra-occlusion, we observe that the ratio f of the volume of FS over the volume of a detected bounding box should be subject to some distribution and upper-bounded by ∃ b ∈ (0, 1), implying f ∈ (0, b] (Figure 9). Nevertheless, since the fake vehicles do not obey the two occlusion patterns, their ratio f should be large enough and lower-bounded by ∃ a ∈ (0, 1) such that f ∈ [a, 1). Clearly, as long as a &gt; b, we have opportunities to distinguish valid vehicles with the spoofed fake vehicles statistically. To estimate the ratio f , we grid the 3D space into cells and calculate:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: CDF of f and f , and the two distributions are clearly separate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: CDF of g and g , but the two distributions overlap with each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(b) CARLO-guarded PointPillars. (c) CARLO-guarded PointRCNN. (d) Precision and recall of CARLO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Attack success rates (ASRs) of proposed black-box spoofing attacks on three CARLO-guarded models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Attack success rates (ASRs) of Adv-LiDAR on Apollo 5.0 and CARLO-guarded model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Attack success rate (ASR) of the adaptive attack on CARLO-guarded Apollo 5.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Existing view fusion-based architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>(</head><label></label><figDesc>a) ASR of the reproduced MV3D. (b) ASR of the reproduced MVF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Attack success rates (ASRs) of proposed black-box spoofing attacks on MV3D and MVF models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Sequential view fusion (SVF) architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>ASR of SVF-PointRCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Attack success rates (ASRs) of proposed black-box spoofing attack on SVF models. and demonstrate that the FV features are more important in SVF models (detailed in Appendix D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Average confidence score of Adv-LiDAR on the segmentation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Average confidence score of the adaptive attack on the segmentation network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Physical spoofing in in-lab environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Average attack success rates (A 2 SRs) of proposed black-box attack on PointPillars and PointRCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>valid vehicle attack trace Both can be detected. Both cannot be detected, due to reconstruction noise. valid vehicle attack trace Both can still be detected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Illustrative example: the left figure is the original feature map of a point cloud sample from Apollo 5.0; the middle one is the feature map after ME reconstruction; and the right one is the feature map after squeezing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Figure 24 :</head><label>24</label><figDesc>Figure 24: APs of weakened view fusion-based models (w-: weakened models).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 25 :</head><label>25</label><figDesc>Figure 25: APs of weakened SVF models (PP: PointPillars; PR: PointRCNN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Figure 26</head><label>26</label><figDesc>Figure 26 shows an illustrative example that translated points from Figure 2 can be detected as a valid vehicle in PointR-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>Figure 27 :FF</head><label>27</label><figDesc>Figure 27: More examples of our rendered traces with occlusions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : MV3D-PointPillars' and MVF-PointPillars' APs (%) of 3D car detection on the KITTI validation set.</head><label>2</label><figDesc></figDesc><table>Model 
Car Detection 
Easy 
Moderate 
Hard 

MV3D-PointPillars 
85.67 
77.12 
71.65 
MVF-PointPillars 
86.77 
79.15 
75.72 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : SVF-PointPillars' and SVF-PointRCNN's APs (%) of 3D car detection on the KITTI validation set.</head><label>3</label><figDesc></figDesc><table>Model 
Car Detection 
Easy 
Moderate 
Hard 

SVF-PointPillars 
85.93 
74.12 
70.19 
SVF-PointRCNN 
88.12 
76.56 
74.81 

</table></figure>

			<note place="foot" n="1"> In this paper, we use &quot;Apollo 5.0&quot; to denote the Baidu Apollo 5.0 model.</note>

			<note place="foot" n="2"> Apollo 2.5 was the latest version when Adv-LiDAR [17] was published. In this work, we target Apollo 5.0, the latest version at the time of writing.</note>

			<note place="foot" n="3"> We evaluate the ASRs until the training procedures (i.e., APs) converge on both models.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Acknowledgements</head><p>We appreciate our shepherds, Xiaoyu Ji and Wenyuan Xu, and the anonymous reviewers for their insightful comments. We thank Xiao Zhu, Shengtuo Hu, Jiwon Joung, and Xumiao Zhang for proofreading our paper. This project is partially supported by Mcity and NSF under the grants <ref type="bibr">CNS-1930041, CNS-1850533, CNS-1929771, and CNS-1932464.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Baidu debuts robotaxi ride hailing service in China, using self-driving electric taxis</title>
		<ptr target="https://www.marketwatch.com/story/baidu-debuts-robotaxi-ride-hailing-service-in-china-using-self-driving-electric-taxis-2019-09-26" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">UPS joins race for future of delivery services by investing in self-driving trucks</title>
		<ptr target="https://abcnews.go.com/Business/ups-joins-race-future-delivery-services-investing-driving/story?id=65014414" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Waymo has launched its commercial self-driving service in Phoenixand it&apos;s called &apos;Waymo One</title>
		<ptr target="https://www.businessinsider.com/waymo-one-driverless-car-service-launches-in-phoenix-arizona-2018-12" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://media.gm.com/media/us/en/gm/news.detail.html/content/Pages/news/us/en/2017/oct/1009-lidar1.html" />
		<title level="m">GM Advances Self-Driving Vehicle Deployment With Acquisition of LIDAR Developer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Introducing Laser Bear Honeycomb by Waymo</title>
		<ptr target="https://waymo.com/lidar/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ai</surname></persName>
		</author>
		<ptr target="https://www.autoware.ai/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apollo</forename><surname>Baidu</surname></persName>
		</author>
		<ptr target="http://apollo.auto" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Devil&apos;s whisper: A general approach for physical adversarial attacks against commercial black-box speech recognition devices</title>
	</analytic>
	<monogr>
		<title level="m">29th USENIX Security Symposium (USENIX Security 20)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d" />
		<title level="m">KITTI Vision Benchmark: 3D Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comprehensive vulnerability analysis of obd-ii dongles as a new over-the-air attack surface in automotive iot</title>
	</analytic>
	<monogr>
		<title level="m">29th USENIX Security Symposium (USENIX Security 20)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stealthy tracking of autonomous vehicles with cache side channels</title>
	</analytic>
	<monogr>
		<title level="m">29th USENIX Security Symposium (USENIX Security 20)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00420</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An intelligent secured framework for cyberattack detection in electric vehicles&apos; can bus using machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Avatefipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Al-Sumaiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>El-Sherbeeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Awwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Elmeligy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="127580" to="127592" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lu-net: An efficient network for 3d lidar point cloud semantic segmentation based on end-to-end-learned 3d features and u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Biasutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brédif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bugeau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A linear algorithm for incremental digital display of circular arcs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bresenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="100" to="106" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial sensor attack on lidar-based perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rampazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2267" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial objects against lidar-based autonomous driving systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05418</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ghostbuster: Detecting the presence of hidden eavesdroppers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassanieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Roy</forename><surname>Choudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Conference on Mobile Computing and Networking</title>
		<meeting>the 24th Annual International Conference on Mobile Computing and Networking</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="337" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Comprehensive Experimental Analyses of Automotive Attack Surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Checkoway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Czeskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th USENIX Conference on Security, SEC&apos;11</title>
		<meeting>the 20th USENIX Conference on Security, SEC&apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exposing Congestion Attack on Emerging Connected Vehicle based Traffic Signal Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Annual Network and Distributed System Security Symposium, NDSS &apos;18</title>
		<meeting>the 25th Annual Network and Distributed System Security Symposium, NDSS &apos;18</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Error handling of in-vehicle networks makes them vulnerable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS&apos;16</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zisserman. The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust physical-world attacks on deep learning visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1625" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vulnerability of Traffic Control System Under Cyber-Attacks Using Falsified Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation Research Board 2018 Annual Meeting (TRB)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mechanism for secure in-vehicle payment transaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aissi</surname></persName>
		</author>
		<idno>App. 14/466</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">405</biblScope>
		</imprint>
	</monogr>
<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">VLP-16 User Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">L</forename><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">On single source robustness in deep fusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04691</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Experimental Security Analysis of a Modern Automobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Czeskis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Checkoway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kantor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE Symposium on Security and Privacy, SP&apos;10</title>
		<meeting>the 2010 IEEE Symposium on Security and Privacy, SP&apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Patch refinement-localized 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitterecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmarcher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04093</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07916</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting anomaly in large-scale network using mobile crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2019-IEEE Conference on Computer Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2179" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep continuous fusion for multi-sensor 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="641" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extending adversarial attacks and defenses to deep 3d point cloud classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2279" to="2283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Lasernet: An efficient probabilistic 3d object detector for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vallespi-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Wellington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12677" to="12686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Phantom of the adas: Phantom attacks on driver-assistance systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ben-Netanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Elovici</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia conference on computer and communications security</title>
		<meeting>the 2017 ACM on Asia conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">This ain&apos;t your dose: Sensor spoofing attack on medical infusion pump</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th USENIX Workshop on Offensive Technologies (WOOT 16)</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards a distributed secure in-vehicle communication architecture for modern vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Patsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dellios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bouroche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; security</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Remote attacks on automated vehicles sensors: Experiments on camera and lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stottelaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kargl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">11</biblScope>
			<pubPlace>Black Hat Europe</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Secure v2v communication with certificate revocations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sangwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kherani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bellur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shorey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 Mobile Networking for Vehicular Environments</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="127" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Part-aˆ2aˆ2 net: 3d part-aware and aggregation neural network for object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Illusion and dazzle: Adversarial optical channel exploits against lidars for automotive applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cryptographic Hardware and Embedded Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="445" to="467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Robust adversarial objects against deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Voxel-fpn: multi-scale voxel feature aggregation in 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05286</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pointseg: Real-time semantic segmentation based on 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06288</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11171</idno>
		<title level="m">Geometry-aware generation of adversarial and cooperative point clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Trajectory-Based Hierarchical Defense Model to Detect Cyber-Attacks on Transportation Infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation Research Board 2019 Annual Meeting (TRB)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A practical security architecture for in-vehicle can-fd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2248" to="2261" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generating 3d adversarial point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Meshadv: Adversarial meshes for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<title level="m">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10899</idno>
		<title level="m">Adversarial attack and defense on point sets</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">ME-Net: Towards effective adversarial robustness with matrix estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">End-to-end multi-view fusion for 3d object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06528</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
