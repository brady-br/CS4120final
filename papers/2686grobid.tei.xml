<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Experiment on Bare-Metal BigData Provisioning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ata</forename><surname>Turk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><forename type="middle">S</forename><surname>Gudimetla</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emine</forename><forename type="middle">Ugur</forename><surname>Kaynar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hennessey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahil</forename><surname>Tikale</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orran</forename><surname>Krieger</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Boston University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<orgName type="institution" key="instit3">Boston University</orgName>
								<orgName type="institution" key="instit4">Boston University</orgName>
								<orgName type="institution" key="instit5">Boston University</orgName>
								<orgName type="institution" key="instit6">Northeastern University</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Experiment on Bare-Metal BigData Provisioning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many BigData customers use on-demand platforms in the cloud, where they can get a dedicated virtual cluster in a couple of minutes and pay only for the time they use. Increasingly, there is a demand for bare-metal big-data solutions for applications that cannot tolerate the unpredictability and performance degradation of virtu-alized systems. Existing bare-metal solutions can introduce delays of 10s of minutes to provision a cluster by installing operating systems and applications on the local disks of servers. This has motivated recent research developing sophisticated mechanisms to optimize this installation. These approaches assume that using network mounted boot disks incur unacceptable run-time overhead. Our analysis suggest that while this assumption is true for application data, it is incorrect for operating systems and applications, and network mounting the boot disk and applications result in negligible run-time impact while leading to faster provisioning time.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, virtualized IaaS based BigData analytics solutions such as those provided by Amazon EMR <ref type="bibr" target="#b0">[1]</ref> and IBM BigInsights <ref type="bibr" target="#b1">[2]</ref> are boasting significant shares of the BigData analytics market <ref type="bibr" target="#b2">[3]</ref>. Virtualization, at least in the way it is enabled in today's clouds, can introduce significant overhead, unpredictability, and security concerns, which is not tolarable for certain applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. To address the needs of applications that are sensitive to these overheads, cloud vendors like IBM <ref type="bibr" target="#b6">[7]</ref>, Rackspace <ref type="bibr" target="#b7">[8]</ref>, and Internap <ref type="bibr" target="#b8">[9]</ref> have started to serve bare-metal IaaS cloud solutions, with much of the focus being on supporting on-demand bare-metal BigData platforms.</p><p>All these Bare-metal cloud solutions install the tenant's operating system and application into the server's local disks, incurring long delays for the user of the platform. Projects such as Ironic <ref type="bibr" target="#b9">[10]</ref>, MaaS <ref type="bibr" target="#b10">[11]</ref>, Emulab <ref type="bibr" target="#b11">[12]</ref> have developed sophisticated mechanisms to make this process efficient. A recent ASPLOS paper by Omote et al. <ref type="bibr" target="#b12">[13]</ref> goes a step further to reduce these delays, lazily copying the image to local disk while running the application virtualized, and then de-virtualizing when copying is complete.</p><p>Is all this effort really necessary? In fact Omote et al <ref type="bibr" target="#b12">[13]</ref> observe that network booting was actually faster than their approach, but asserted it would incur a "continual overhead", directing every disk I/O over the network. However, it is not clear if they considered the natural approach of having a network-mounted boot drive (with OS files and applications) and using the local drive for just application data.</p><p>To evaluate this option we create a simple prototype where client machines (24-core 10GbE servers, RHEL 7.1) access their kernel and init ramdisk via standard network boot mechanisms (PXE), mount their root file system with pre-installed applications (Hadoop benchmarks) from an iSCSI volume located on a remote server, and use local disk for ephemeral storage (i.e. /swap, /tmp, and Hadoop data). With this approach, which involved a few lines of config file changes, we found that the run time overhead of having a network mounted boot drive is in fact negligible. After a short startup phase there are very few subsequent reads from the boot disk (around 3KB/s over 10 hours) suggesting that file caching is very effective for the boot drive. Boot disk writes, mostly to application log files, average 14KB/s.</p><p>These results strongly suggest that the enormous effort by on-demand bare-metal platforms to reduce the delay and overhead of installing tenants operating systems on local disks may be misguided. The much simpler approach, of separating boot and data disks and handling them differently, appears to offer improved provisioning time with little or no runtime degradation. Moreover, a system based on this approach, can allow the boot drives to be stored in a centralized repository, bringing to bare metal environments many of the same capabilities available on virtualized platforms today. We are starting to develop a new Bare Metal Imaging (BMI) service based on this approach.</p><p>In the remainder of the paper, Section 2 describes the prototype we built to evaluating our approach to baremetal BigData cluster provisioning. Section 3 presents the evaluation results we obtained. Related works are discussed in Section 4, and in Section 5 we conclude with a discussion of our findings. <ref type="figure" target="#fig_0">Figure 1</ref> shows the simple prototype we developed to evaluate our approach. HaaS <ref type="bibr" target="#b13">[14]</ref> is a service we previously developed to allow users to allocate and provision physical nodes out of a shared pool. A single VM was used in the prototype for PXE services (DHCP, TFTP) and as an iSCSI server, with images (exposed to nodes as iSCSI targets) stored in a shared Ceph file system. Ceph provides us with a distributed storage system that supports efficient cloning of files. Most of the functionality in the prototype was implemented as bash scripts that interact with Ceph (to clone images), the provisioning VM, and the HaaS service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Prototype</head><p>We chose iSCSI, rather than NFS, as the protocol for mounting the drives because of the simplicity of installation-rather than crafting a shareable file system, we were able to connect a server to a blank iSCSI volume, perform a standard operating system installation, and then copy the resulting image file. Moreover, with the right hardware support, it should be possible to boot an iSCSI mounted drive with no changes to the operating system being booted. In addition, iSCSI does not incur the overhead of NFS to validate that potentially shared files have not been modified.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, provisioning a cluster has four main steps:</p><p>1. Node Reservation: Provisioning scripts interact with HaaS to allocate physical servers. 3. Per-node Configuration: Each image is modified (using loopback mount) to perform per-node configuration such as SSH keys, cluster IP addresses (/etc/hosts), and specifying applicationspecific functionality. Each image is then exposed as an iSCSI volume by the iSCSI server.</p><p>4. PXE Boot: On boot the node requests configuration information via DHCP, downloading its kernel, initial ramdisk, and a configuration file giving the iSCSI address for that node's remote boot disk.</p><p>This prototype is designed to provide us with basic performance information, and has obvious limitations from both a functionality and performance perspective. A real implementation would have all the functionality implemented as bash scripts provided by an APIaccessible service. The single provisioning VM will obviously be a performance bottleneck in the long term, as e.g. multiple iSCSI servers will be needed to scale to large numbers of nodes. Despite these issues, the current implementation provides a proof of concept, as a more carefully-constructed system would provide even better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>We tested the prototype on a HaaS-managed 48-node cluster; each server was equipped with two Intel Xeon E5-2630L CPUs, 128 GB memory, 300 GB 10K SAS HDDs (two nodes had 1 TB 7.2K SATA HDDs), and two Intel 82599ES 10 Gbit NICs. Storage was provided by a four-node Fujitsu CD10000 Ceph storage appliance, with 4 10 Gbit external NICs and internal 40 Gbit InfiniBand interconnect. <ref type="figure" target="#fig_1">Figure 2</ref> iSCSI bar shows the time taken to start up from scratch a bare-metal Hadoop image using our prototype. As we can see from the figure almost half the time is spent in firmware initialization, and the overall boot time is very rapid (260 seconds) and comparable to network boot results presented in prior work <ref type="bibr" target="#b12">[13]</ref>. As a comparison point, the Local Disk bar shows the time for a full install of a Hadoop environment using standard tools (RedHat Foreman for OS installation, Apache BigTop for Hadoop installation, . . . ), as is typically done in managed system environments. The remainder of this section compares the runtime overheads of these two installation mechanisms.</p><p>We have made no effort to make the prototype scalable. The provisioning scripts are sequential, cloning each image in turn, and then starting the nodes booting. Moreover, there is only a single provisioning VM in the prototype. <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates that even this very simple design is sufficient to provision a modest number of nodes in parallel with relatively modest degradation as we increase the number of concurrently provisioned nodes from two to eight.</p><p>The main goal of the prototype was to understand what is the run time impact of a network mounted boot drive for a Big Data platform. <ref type="figure" target="#fig_3">Figures 4 and 5</ref> show the per node cumulative read and write iSCSI traffic during initial provisioning and then over five consecutive runs of random data generation followed by sorting, using the Hadoop Sort example, covering a duration of 7 to 17 hours for 128 GB and 256 GB of data respectively. These experiments were performed on two nodes allocated out of the HaaS cluster with local data stored in the one terabyte drive.</p><p>While we do not have a comparison to provisioning systems that copy (rather than install) an image to the local disk, one interesting data point is how much data would be transferred in the two cases. For the iSCSI case, <ref type="figure">Figures 4</ref>   to transfer 2.9GB over the network to each node. Worse yet, it would then need to write this data to the local disk, at typical speeds of 100 MB/s or less for single-disk systems, or 1 10 of network speed. In <ref type="figure">Figure 4</ref>, both curves flatten after repeated runs, demonstrating that (even with the 256GB case where total data handled-at minimum five runs times 128GB per machine, times a replication factor of two-is substantially larger than system memory) that the file cache is effective at caching the boot drive. After initial boot and application startup, the sustained read bandwidth incurred is around 3KBytes per second; effectively negligible. <ref type="figure" target="#fig_3">Figure 5</ref> shows the writes to the network mounted storage; in contrast to the read case, log writes continue throughout the experiment, at an average rate of approximately 14 KB/s. On further examination these writes target paths such as /var/log, /hadoop/log, and /var/run. 1 Most of these writes are log file updates made by Hadoop; although they could be directed to local storage, we did not do so due to their utility for debugging and negligible rate.</p><p>The above figures examined the read/write overhead for relatively large data sets for just two nodes, and took more than 17 hours to run. To examine the performance difference between the two configurations, we also timed a series of experiments on 8 node clusters as we varied the data set from 8GB to 128GB. In <ref type="figure" target="#fig_4">Figure 6</ref> we compare the runtime of standard Hadoop benchmarks (Sort, Grep, WordCount) running on local disk-installed and network mounted clusters. Reported numbers are average of five runs; we observed that deviations among runs on the same configuration are negligible. As seen from the figure, the difference in runtime performances are negligible, with the exception of Sort experiments for 32GB data and 128GB data; we hypothesize that this may be caused by the non-deterministic behavior of random sorting benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Network booting of computers came into widespread use almost 30 years ago <ref type="bibr" target="#b14">[15]</ref>, with remote access to both initial boot files (e.g. kernel) and file system enabling the creation of clusters of diskless workstations. More recently, remote storage has become popular in the highperformance computing field <ref type="bibr" target="#b15">[16]</ref>. Some of the largest installations (e.g. those from Cray <ref type="bibr" target="#b16">[17]</ref>) use this technique to allow smaller and more reliable diskless compute nodes, while others (Gordon <ref type="bibr" target="#b17">[18]</ref>) add high-speed local storage (e.g. SSD) for ephemeral data. In other fields, initial network booting (i.e. PXE) is widely used to initiate OS installation to local disk, but network boot with remote storage access is rarely used. Instead, a rich set of open source and commercial products have been developed for automated provisioning of bare-metal systems. Chandrasekar and Gibson <ref type="bibr" target="#b18">[19]</ref> provide a comparative analysis of commonly used provisioning systems, namely Emulab <ref type="bibr" target="#b11">[12]</ref>, OpenStack Ironic <ref type="bibr" target="#b9">[10]</ref>, Crowbar <ref type="bibr" target="#b19">[20]</ref>, Razor <ref type="bibr" target="#b20">[21]</ref>, and Cobbler <ref type="bibr" target="#b21">[22]</ref> and evaluate in detail Emulab and OpenStack Ironic. All of these bare-metal provisioning frameworks copy a disk image to the nodes and use additional configuration management systems to set up the desired applications on provisioned systems. Canonical's Metal-as-aService (MAAS) <ref type="bibr" target="#b10">[11]</ref> provides a similar solution to host a cloud on the hardware owned by a customer.</p><p>Although there are many commercial offerings for BigData as a Service, such as Amazon Elastic MapReduce <ref type="bibr" target="#b0">[1]</ref>, Google Cloud DataProc <ref type="bibr" target="#b22">[23]</ref>, and others, most of these are based on virtual machine deployment. Internap <ref type="bibr" target="#b8">[9]</ref> and Rackspace <ref type="bibr" target="#b7">[8]</ref> offer Hadoop on baremetal solutions using the OpenStack Ironic <ref type="bibr" target="#b9">[10]</ref> provisioning solution, typically coupled with BigData application platforms such as Hortonworks Data Platform <ref type="bibr" target="#b23">[24]</ref>, Cloudera Enterprise <ref type="bibr" target="#b24">[25]</ref>, or MapR Converged Data Platform <ref type="bibr" target="#b25">[26]</ref>.</p><p>In addition to open source products and commercial solutions, there exists a flurry of academic studies that investigate problems related to the focus of this study. Ekanayake and Fox. <ref type="bibr" target="#b26">[27]</ref> study the overhead incurred by Hadoop based applications run on bare metal vs virtual nodes in a Cloud infrastructure. Their studies corroborate the performance gains achieved by bare-metal deployment of BigData solutions. Omote et al. <ref type="bibr" target="#b12">[13]</ref> investigate mechanisms for fast provisioning of operating systems and reducing boot time on bare-metal systems in clouds. They argue that long startup times of bare metal servers act as a significant inhibitor in providing agility and elasticity in bare-metal clouds, and propose BMCast, an OS deployment system with a special purpose devirtualizable Virtual Machine Manager that supports OStransparent quick startup of bare-metal instances. This approach, which we think is very novel and useful for many use cases, takes longer to provision then network mounting, and lacks the potential benefits of image management a network mounted approach can offer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Bare-metal on-demand BigData platforms are becoming increasingly important with a number of commercial and research offerings. Enormous effort has gone on in these systems to reduce the delay to install software into the local disks. While previous work acknowledged that network booting is faster than a local installation, they rejected this approach because of the assumption that it would incur an ongoing unacceptable overhead. We hypothesized that if we separate boot and data disks, using local storage for data, this overhead would be substantially reduced. We demonstrate with a simple prototype that this very simple strategy preserves all the advantages of network booting while incurring negligible runtime overhead.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of our network mounted BigData provisioning environment and cluster provisioning flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Provisioning time comparison of local disk installation and network (iSCSI) mounting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scalability analysis for network (iSCSI) mounted BigData cluster provisioning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Cumulative writes (MB) made by servers on the iSCSI gateway.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance comparison of Hadoop WordCount, Sort, and Grep applications on local disk based and iSCSI mounted systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>shows that only around 250MBytes of the Boot disk are read over 10 hours. In contrast, out of the 8GB boot image image, 2.9GB were actually used. In other words, any image distribution service would need</figDesc><table>Initial 
Provisioning 

Data 
Generation 1 
Sort 1 

Data 
Generation 2 
Sort 2 

Data 
Generation 3 
Sort 3 

Data 
Generation 4 
Sort 4 

Data 
Generation 5 
Sort 5 

0 

100 

200 

300 

Cumulative iSCSI reads per node (MB) 

iSCSI Reads: Runs with 256GB Data 
iSCSI Reads: Runs with 128GB Data 

Figure 4: Per-node cumulative iSCSI read volume (MB). 

Initial 
Provisioning 

Data 
Generation 1 
Sort 1 

Data 
Generation 2 
Sort 2 

Data 
Generation 3 
Sort 3 

Data 
Generation 4 
Sort 4 

Data 
Generation 5 
Sort 5 

0 

100 

200 

300 

400 

500 

600 

700 

Cumulative iSCSI writes per node (MB) 

iSCSI Writes -Runs with 256GB Data 
iSCSI Writes -Runs with 128GB Data 

</table></figure>

			<note place="foot" n="1"> We should note that, in our deployments, /tmp and /swap are configured to reside on the local disk of servers.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Dan Shatzberg for his early suggestions in supporting a network mounted imaging service, and the MOC team in general for support and understanding while performing the experiments. We also thank Cisco and Fujitsu for their generous donations of server hardware and Ceph storage, respectively.</p><p>This research was supported in part by the MassTech Collaborative Research Matching Grant Program, NSF awards 1347525 and 1414119 and several commercial partners of the Massachusetts Open Cloud who may be found at http://www.massopencloud.org.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Amazon elastic mapreduce (amazon emr)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/elasticmapreduce/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ibm biginsights for apache hadoop</title>
		<ptr target="www.ibm.com/software/products/en/ibm-biginsights-for-apache-hadoop" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hadoop-nosql software and services market forecast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kelly</surname></persName>
		</author>
		<ptr target="http://wikibon.com/hadoop-nosql-software-and-services-market-forecast-" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Facebook: Virtualisation does not scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdnet</surname></persName>
		</author>
		<ptr target="http://www.zdnet.com/article/facebook-virtualisation-does-not-scale/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance analysis of cloud computing services for many-tasks scientific computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iosup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ostermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yigitbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fahringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="931" to="945" />
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hey, you, get off of my cloud: Exploring information leakage in third-party compute clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tromer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shacham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM Conference on Computer and Communications Security, ser. CCS &apos;09</title>
		<meeting>the 16th ACM Conference on Computer and Communications Security, ser. CCS &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="199" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Big data solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Softlayer</surname></persName>
		</author>
		<ptr target="http" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rackspace cloud big data onmetal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rackspace</surname></persName>
		</author>
		<ptr target="http://go.rackspace.com/baremetalbigdata/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bare-metal agileserver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Internap</surname></persName>
		</author>
		<ptr target="http" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Ironic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openstack</surname></persName>
		</author>
		<ptr target="http://docs.openstack.org/developer/ironic/deploy/user-guide.html" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Metal as a service (maas)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Canonical</surname></persName>
		</author>
		<ptr target="http://maas.ubuntu.com/docs/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic Online Validation of Network Configuration in the Emulab Network Testbed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lepreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Autonomic Computing, 2006. ICAC &apos;06</title>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="134" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving Agility and Elasticity in Bare-metal Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Omote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shinagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;15</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ser. ASPLOS &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="145" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hardware as a service -enabling dynamic, user-level bare metal provisioning of pools of data center resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hennessey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Denhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venugopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Silvis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Desnoyers</surname></persName>
		</author>
		<ptr target="https://open.bu.edu/handle/2144/11221" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE High Performance Extreme Computing Conference</title>
		<meeting><address><addrLine>Waltham, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Analysis of Diskless Workstation Traffic on an Ethernet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gusella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-11" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating the Shared Root File System Approach for Diskless High-Performance Computing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th LCI International Conference on High-Performance Clustered Computing (LCI-09)</title>
		<meeting>the 10th LCI International Conference on High-Performance Clustered Computing (LCI-09)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Gemini System Interconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alverson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roweth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE 18th Annual Symposium on High Performance Interconnects (HOTI)</title>
		<imprint>
			<date type="published" when="2010-08" />
			<biblScope unit="page" from="83" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gordon: design, performance, and experiences deploying and supporting a data intensive supercomputer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Strande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cicotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sinkovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Extreme Science and Engineering Discovery Environment: Bridging from the eXtreme to the campus and beyond</title>
		<meeting>the 1st Conference of the Extreme Science and Engineering Discovery Environment: Bridging from the eXtreme to the campus and beyond<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A comparative study of baremetal provisioning frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandrasekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<idno>CMU-PDL-14-109</idno>
	</analytic>
	<monogr>
		<title level="m">Parallel Data Laboratory</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The crowbar project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Opencrowbar</surname></persName>
		</author>
		<ptr target="https://opencrowbar.github.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Provisioning with razor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Puppetlabs</surname></persName>
		</author>
		<ptr target="https://docs.puppetlabs.com/pe/latest/razorintro.html" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;quot;</forename><surname>Cobbler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cobbler</surname></persName>
		</author>
		<ptr target="https://cobbler.github.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Google cloud dataproc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/dataproc/overview" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Hortonworks data platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hortonworks</surname></persName>
		</author>
		<ptr target="http://hortonworks.com/hdp/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cloudera enterprise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cloudera</surname></persName>
		</author>
		<ptr target="http" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mapr converged data platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mapr</surname></persName>
		</author>
		<ptr target="https://www.mapr.com/products/mapr-converged-data-platform" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High performance parallel computing with clouds and cloud technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ekanayake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="20" to="38" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
