<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding and Tackling the Hidden Memory Latency for Edge-based Heterogeneous Platform</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Dallas</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding and Tackling the Hidden Memory Latency for Edge-based Heterogeneous Platform</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With the burgeoning of autonomous driving, the edge-deployed integrated CPU/GPU (iGPU) platform gains significant attention from both academia and industries. NVIDIA issues a series of Jetson iGPU platforms that perform well in terms of computation capability, power consumption, and mobile size. However, these iGPU platforms typically contain very limited physical memory, which could be the bottleneck of these autonomous driving and edge computing applications. Although the introduction of the Unified Memory (UM) model in GPU programming can reduce the memory footprint , the programming legacy of the UM model initializes data on the CPU side by default as the conventional copy-and-execute model does, which causes significant latency of application execution. In this paper, we propose an enhanced unified memory management model (eUMM), which delivers a prefetch-enhanced data initialization method on the GPU side of the iGPU platform. We evaluate eUMM on the representative Jetson TX2 and Xavier AGX platforms and demonstrate that eUMM not only reduces the initialization latency significantly but also benefits the following kernel computation and the entire application execution latency.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The emergence of heterogeneous SoCs has pushed the computing platforms of many edge-intelligence applications (i.e., driving automation system and drone) to shift from highperformance, energy-consuming discrete CPU and GPUequipped platforms (dGPU) to effective, energy-efficient integrated CPU and GPU platform (iGPU). As the major game player, NVIDIA has developed a series of heterogeneous iGPU platforms, such as Jetson series and Drive AGX, for autonomous embedded systems and driving automation systems, respectively <ref type="bibr" target="#b16">[17]</ref>.</p><p>Exploiting iGPU for edge autonomous intelligent workloads processing can bring better size, weight, and power (SWaP) trade-off compared to traditional dGPU-based solutions. However, building future embedded autonomous systems based on this hardware is further stymied by the un-DNN YOLO2 <ref type="bibr" target="#b20">[21]</ref> YOLO3 SSD <ref type="bibr" target="#b13">[14]</ref> DAVE-2 <ref type="bibr" target="#b3">[4]</ref> M.O.S. 49K 81K 10K 250K precedented challenges that are specifically imposed by the memory footprint and stringent latency requirements of autonomous systems, and the intrinsic hardware restrictions of iGPU-enabled heterogeneous SoCs. For example, NVIDIA TX2 possesses an on-chip memory of 8GB, which is physically shared by both CPUs and GPUs. In practice, modern autonomous systems such as driving automation and drone systems need a vast amount of perception data for decisionmaking guidance. A typical automated vehicle equips with 8 to 12 cameras to provide 360-degree visibility around the vehicle, with 1440 Mbs of data generated every second. If a poorly-managed memory allocation method (e.g.,the traditional copy-and-execute model) is used, such a large amount of raw data can easily exhaust the memory space of iGPUbased heterogeneous platform. Secondly, modern autonomous systems consider the processing latency as one of the most important tenets for safety and functionality. The latency from image capture to recognition completion is critical since the response time of the control operations depends on it. Failing to execute actuation in time may cause catastrophic consequences. Typically, modern autonomous driving systems adopt a variety of DNN models to achieve complete functionalities. However, the DNN functions typically involve large-scale matrix operations. <ref type="table">Table.</ref> 1 shows the matrix operations scale involved in several representative DNNs models adopted by the mainstream autonomous driving solutions. We observe that the existing CPU-initiated matrix initialization can cause great latency for GPU tasks with large-scale matrix operations.</p><p>To meet these rigorous requirements of memory footprint and processing latency for the iGPU-based heterogeneous platforms, tapping into the emerging unified memory (UM) model is considered a promising solution. Recent NVIDIA GPU architectures support UM to ease the explicit programming efforts to handle data movement and address mapping <ref type="bibr" target="#b5">[6]</ref>. We observe that the UM is memory footprint-friendly to iGPU platform to process autonomous workloads since it replaces the explicit data copy in the traditional copy-andexecute model with implicit data initialization and addresses mapping, which significantly saves the memory footprint. However, we observe that the current UM model fails to provide efficient memory management on iGPU-based heterogeneous platforms, such as NVIDIA Jetson TX2. Our characterization shows that the inherited routine of data initialization on the CPU side provided by existing UM typically results in significant data initialization latency, which is caused by the limited computation capability of CPU, and overwhelms the execution time of the following GPU kernel. These findings suggest a potential opportunity to optimize the data initialization and thus significantly reduce the GPU-based task processing.</p><p>In this paper we propose an enhanced unified memory management model (eUMM) that delivers a prefetch-enhanced GPU-initialization method. The eUMM innovatively implements data initialization on GPU (GPU-Init.), which leverages GPU's advantages in large-scale computation to reduce the latency of data initialization. Furthermore, the eUMM integrates prefetch technique in the process to reduce the latency caused by the data address mapping, which is brought by the inherent limits of the kernel launch in pristine UM model. We conduct a series of experiments on emerging iGPU platforms and demonstrate that eUMM can reduce the latency of initialization up to 99.4% and 97.3% for assorted benchmarks on Jetson TX2 and AGX, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Limits of Unified Memory Management</head><p>Data Initialization Latency. In default copy-and-execute model, application program typically initializes data on CPU side. On iGPU platform, UM model inherits the legacy programming style, though it can reduce the memory footprint. To explore how this CPU-side initialization brings the latency, we breakdown the execution time of applications for both copy-and-execute model (Def.) and unified memory model (UM), as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Specifically, in Def. model, we divide the entire benchmark time into two parts: data initialization time (init.) and other time which covers the data migration between CPU and GPU as well as the kernel execution (others). In UM model, the benchmark time includes the init. time and others which covers the page mapping and kernel execution. Note that there is no data copy under UM for iGPU platform. We utilize two representative benchmarks, Matrix Add (MatAdd) and Matrix Multiplication (MatMul) to illustrate the init. latency and its ratio in application execution time, as is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We repeat the benchmarking five times and report the average latency.</p><p>For MatAdd in <ref type="figure" target="#fig_0">Fig. 1a</ref>, we observe that the init. latency  is non-trivial in Def. model, which can compete with the latency of others latency. With input data size increasing, the init. latency increases drastically. Intuitively, the CPU has to process larger amount of data, thus leading to greater latency. In UM model, the init. latency is significant as well, which even dominates the entire benchmark execution time. For MatMul in <ref type="figure" target="#fig_0">Fig. 1b</ref>, in either Def. model or UM model, the init. latency almost follows the same principle. Besides, we calculate the init. ratio in the entire benchmark execution time under the two models, as indicated by the dots in <ref type="figure" target="#fig_0">Fig.  1</ref>. For both benchmarks, the ratio of init. latency is around 50% in Def. model, and around 90% in UM model, indicating that the init. latency is even exacerbated under UM model. In fact, UM model on iGPU platform doesn't require explicit copy of initialized data from CPU as Def. model does, the initialization may not be necessarily performed on CPU side. Kernel Launch Latency. As we stated above, under UM model, such iGPU platform as Jetson TX2 doesn't support on-demand paging as well as concurrent access from CPU and GPU when the code is running. For each kernel launch, the GPU tries to access the data that is originally populated in the CPU side. The driver has to perform address translation and page mapping such that the GPU can know the pages reside on its own space, which inevitably introduces extra latency in kernel execution.</p><p>To find the hidden latency caused by the address translation and pages mapping during the kernel launch process, we compare the kernel execution time of the two benchmarks, MatAdd and MatMul, under Def. and UM models on TX2 platform. <ref type="figure" target="#fig_1">Fig. 2a</ref> shows the result of MatAdd, where the x-axis indicates increasing input data size and the y-axis indicates kernel execution time. We can observe that the kernel latency under UM model is significantly larger than the latency under Def. model. With data size increasing, the differ- ence is even exacerbated. Under Def. model, data is explicitly copied to the GPU side before kernel launches, while under UM model, there is no explicit data copy and the data has to be mapped to the GPU side when kernel launches. Compared to the real kernel computation time under Def. model, the data pages mapping really contributes a lot to the kernel execution time under UM model. <ref type="figure" target="#fig_1">Fig. 2b</ref> shows the result of MatMul, where the kernel latency under two models almost follows the same pattern. The latency caused by the mapping process is significant as well. Therefore, if the data pages mapping can be removed from the kernel execution under the UM model, the real kernel execution time will be reduced. In fact, chances are that the data allocation and initialization can be well managed such that the data can be pre-mapped and populated on the GPU side, thus benefiting the kernel execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Enhanced Unified Memory Management</head><p>In this section, we propose an enhanced unified memory management (eUMM) model to optimize the memory management for iGPU and reduce the latency of the time-critical workloads. Initializing Data on GPU Side. As the programming legacy in the copy-and-execute model, conventional UM still implements data initialization on CPU side, which significantly contributes to the application's latency. However, UM allows CPU and GPU to share a pointer to access the data in the allocated memory region, providing an opportunity to perform data initialization on the GPU instead of the CPU side. Furthermore, data initialization is typically a process of assigning values to variables, and thus is well structured and paralleled. GPU-Init. may gain significant benefit due to GPU's advantages in parallel computing and further reduce the application's latency. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the design of eUMM which initializes data on GPU side. The figure on the top shows the process of data being initialized on CPU in conventional UM model: (1-2) data allocation and initialization on CPU; (3) page fault handling indicates pages covering the data allocation are unmapped from CPU page table and remapped to GPU page table; (4) kernel execution on the GPU. The figure on the bottom shows the process of eUMM: (1) data allocation on CPU; (2) page fault handling; (3-4) data initialization and kernel execution on GPU. As shown in the bar figures, the GPU-Init. latency can be significantly reduced compared to the latency on the CPU side due to the powerful GPU capability in parallel computation. On the other hand, upon GPU-Init., all pages covering the data will populate in GPU prior to the following kernel execution. Therefore, there is no need for GPU to wait for page faulting and mapping any more when it implements the kernel, which consequently reduces the kernel execution latency. Prefetch-enhanced GPU-Init. Typically, the prefetch technique and NVIDIA CUDA streams <ref type="bibr" target="#b15">[16]</ref> can be combined to benefit the kernel execution. In CUDA 8.0, the API, cudaMemPre f etchAsync() is provided to hint where and how data are to be used. On dGPU platform, the data pages can be speculatively prefetched across PCI-e connection with negligible opportunity cost. If the data page to be used is successfully prefetched, only translation lookaside buffer (TLB) miss is incurred upon GPU accessing the data, which can be resolved locally instead of being converted into a far page fault. Therefore, GPU kernel execution latency can be reduced.</p><p>On an iGPU platform, such as Jetson TX2, address translation and mapping are required in the UM model when a GPU kernel tries to touch the data in the allocated region for the first time due to the fact the data populates in CPU side in the beginning. As indicated in the step (2) of eUMM in <ref type="figure" target="#fig_2">Fig. 3</ref>, page mapping has to be implemented in bulk after CPU's allocating data completes, which inevitably causes extra latency to the following GPU-Init. kernel, though GPU-Init. is typically faster than CPU initialization. Therefore, we propose to apply the prefetch technique in eUMM to further optimize the GPU-Init. performance, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. <ref type="figure" target="#fig_3">Fig. 4a</ref> indicates the pristine GPU-Init. case, where the GPU-Init. kernel in eUMM doesn't execute until all mappings complete. While <ref type="figure" target="#fig_3">Fig. 4b</ref> indicates the prefetch enhanced GPUInit. case, where prefetch combined with CUDA streams is introduced. As a result, the page mapping for a tile of initialized data can be overlapped with the GPU-Init. kernel of the data in different streams. Theoretically, the summation latency of serialized mapping and GPU-Init. can be reduced. Specifically, the 1st tile of data is prefetched for address mapping and subsequently initialized by the following gpu âˆ’ init kernel. Then, the 2nd tile of data is prefetched and initialized. Based on our measurement, the prefetch operation can cause around 5us latency by itself. By adapting the tile numbers, we can control the total number of prefetch and further manage the overall latency caused by prefetch. Meanwhile, by adapting the grid and block size of the GPU-Init. kernel, we can manage the latency of one-tile GPU-Init. kernel. Therefore, the pages mapping latency of (i + 1)th tile data in one stream can be overlapped with the latency of the ith GPU-Init. kernel in another stream. Instead of serializing all pages mapping and GPU-Init. kernel, the overlapping of the mapping and initialization operations can effectively reduce the overall latency of the process in eUMM model further. Basically, in the ith loop, the (i + 1)th tile of data is prefetched and is sequentially initialized by the kernel in the (i + 1)th loop. Besides, by adapting the initialization kernel size (i.e., the grid dimension and block dimension of the kernel) and the number of prefetch, the prefetch and initialization can be well overlapped in various streams, thus reducing latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation of eUMM</head><p>Effectiveness of Prefetch-enhanced GPU-Init. We implement our eUMM model on Jetson TX2 platform, which installs NVIDIA L4T 28.2.1. In addition to MatAdd and MatMul, we utilize another two applications, needle (NW ), and random access (RA), to test the irregular memory access scenario. We execute all benchmarks under conventional UM model and eUMM model to demonstrate the effectiveness of prefetch-enhanced GPU-Init. under eUMM model and its improvement over the conventional UM model. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the results, where x-axis represents the increasing input data size for each benchmark and y-axis represents data initialization latency. We can observe that for MatAdd, MatMul and RA, the data initialization latency under eUMM (i.e., prefetch-enhanced GPU-Init.) is higher than conventional UM (i.e., pristine CPU initialization) when the input data size is small (50-500). The eUMM significantly outperforms the UM when input data size exceeds the thresholds. Specifically, the initialization latency under eUMM can be reduced up to 76.2%, 99.4%, and 59.4% for MatAdd, MatMul, and NW, respectively. Intuitively, GPU has a large number of paralleling cores and can be scaled up well to handle the increasing amount of data initialization requests. Furthermore, with prefetch applied, the data address mapping and GPU-Init. kernel can be overlapped to further reduce the latency. In contrast, CPU has limited cores and can be easily saturated by the increasing amount of data initialization requests. However, we also observe that there are inherent overheads for GPUInit. mechanism. As a kernel, GPU-Init. has to be launched from the host CPU. Then it will be scheduled and dispatched to the GPU processing cores to execute. Notice that the kernel launch, schedule, and dispatch are unavoidable overheads for GPU-Init. When the input data size is small and there is not enough tiles of data mapping to overlap the GPU-Init. kernel, the benefit of data initialization acceleration could be offset.</p><p>Besides, it appears that the GPU-Init. latency under eUMM has been smaller than latency under UM for NW even in the case of small input data size. This is due to the input size of NW cannot be set as less than 16 * 16, which has exceeded the threshold. <ref type="figure" target="#fig_4">Fig.5c</ref> indicates that the latency under eUMM  In addition, we analyze the kernel latency (i.e., the kernel execution time) of all benchmarks under UM and eUMM models to demonstrate that GPU-Init. in eUMM can also benefit the execution time of following kernel. <ref type="figure" target="#fig_5">Fig. 6</ref> shows the results, where x-axis indicates the increasing data size corresponding to the size of each benchmark in <ref type="figure" target="#fig_4">Fig.5</ref> and y-axis indicates the latency. Obviously, the kernel execution latency under eUMM is significantly reduced than the latency under UM, though for NW both latency increases drastically. Specifically, the kernel latency under eUMM can be reduced up to 61.0%, 63.0%, 10.1% and 76.9% for MatAdd, MatMul, NW and RA, respectively. Under eUMM model, the GPUInit. enables much page fault handling and mapping to be processed prior to the real kernel computation. The data to be used in the kernel computation has populated in GPU locally (i.e., the page tables covering the data is set up well on GPU side) before the kernel launches. Therefore, kernel latency can be reduced as well. Therefore, eUMM not only reduces data initialization significantly but also benefits the following kernel execution as well as the entire benchmark execution latency. It's critical to the safety of autonomous driving system. GPU-side Initialization on Xavier AGX. We implement our eUMM model on the Xavier AGX <ref type="bibr" target="#b0">[1]</ref> as well, which installs NVIDIA L4T 32.3.1. The result is shown in <ref type="figure" target="#fig_6">Fig. 7</ref>, where (ad) indicates the initialization latency of the four benchmarks under UM and eUMM models, respectively. Basically, the initialization latency follows the same pattern as on TX2. The eUMM can significantly reduce the initialisation latency, which can be up to 93.4%, 97.3%, and 87.4% for MatAdd, MatMul, and NW respectively, though the UM outperforms eUMM in the beginning input data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Prospects and Challenges</head><p>Extending eUMM to a Broad Spectrum of Workloads. Our current characterizations and design only cover representative GPU benchmarks, MatAdd, MatMul, and part of the Rodinia benchmarks <ref type="bibr" target="#b4">[5]</ref>. We will continue to explore if the acceleration opportunity also apply to a broader spectrum of edge-based workloads by leveraging the GPU initialization and prefetch, such as the full Rodinia benchmark suite and PolyBench <ref type="bibr" target="#b7">[8]</ref>, which exhibits diverse computation and memory access patterns. Also, we will extend eUMM to accommodate emerging autonomous driving workloads, such as YOLO for objective detection, and GoTurn <ref type="bibr" target="#b10">[11]</ref> for mobile objectives tracking. It's anticipated that eUMM can reduce the latency of these DNN models execution considering that the MatAdd and MatMul are the basic operations in DNN models. We plan to provide transparent API, which can ease users to apply eUMM in practice. Generalizing eUMM to Other Platforms. Our current implementations of eUMM are based on Jetson TX2 and Xavier AGX. To further proof the generalization of eUMM, we will also validate eUMM on high-end edge heterogeneous platforms such as PX2 and AGX Drive <ref type="bibr" target="#b1">[2]</ref>. Also, we expect eUMM can apply to discrete GPU platforms to achieve a broader impact to datacenter computing. Reduce the Inherent Overheads of GPU-Init. In our preliminary investigation, we observe that the GPU-Init. inevitably incurs overheads that are caused by the GPU kernels scheduling and dispatching. Such overheads can even offset the performance benefits of GPU-Init. when input data size is small. Simply applying high-level programming optimizations (e.g., fine-tiled prefetch, scheduling) cannot thoroughly eliminate the overhead. We plan to mitigate the inherent overhead of GPU-Init. in eUMM through low-level design and optimization. For example, GPU persistent model can start up GPU faster without warming up and the data can directly utilized without being returned back to CPU. A specified hardware/model could be proposed to implement the initialization function considering that the process cannot be omitted by almost all applications.</p><p>6 Related work UM model and iGPU Platform for Autonomous Applications. UM model provides an illusion of CPU-GPU unified virtual memory to avoid explicit data copy and ease memory management. NVIDIA's UM model is introduced in CUDA 6 <ref type="bibr" target="#b8">[9]</ref>. and enhanced in CUDA 8 to support on-demand paging and migration mechanism <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. However, on-demand paging is only supported on dGPU <ref type="bibr" target="#b22">[23]</ref>, which hurts its benefit. On the other hand, NVIDIA proposes its Jetson line of iGPU platforms targeting at autonomous systems, which mainly includes a series of Tegra SoCs, such as Parker TX2 and Xavier AGX. Actually, much effort has been dedicated to the UM model performance on iGPU platform. <ref type="bibr" target="#b17">[18]</ref> summarizes the main characteristics of Def. and UM models on Tegra platforms. <ref type="bibr" target="#b12">[13]</ref> measures the performance loss of UM in CUDA on both iGPU and dGPU platforms and explores the underlying reasons. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref> compare applications performance under default and UM models on TK1 platform. <ref type="bibr" target="#b2">[3]</ref> comprehensively compares applications performance under three memory management models and co-optimize memory footprint and performance on Jetson iGPU platforms. Even so, these works don't deep excavate the latency hidden in UM model, especially the significant initialization latency and mostly optimize the latency in existing UM model. Prefetch in GPU Programming. <ref type="bibr" target="#b11">[12]</ref> demonstrate that prefetch may degrade the application's performance if being not judiciously used. <ref type="bibr" target="#b21">[22]</ref> designs an adaptive GPU prefetch technique to dynamically select the appropriate time step to achieve timely prefetch. <ref type="bibr" target="#b18">[19]</ref> proposes the warp-aware selective prefetch to monitor the progress of the warps. Besides, several other works are dedicated to the hardware prefetcher under UM model. <ref type="bibr" target="#b23">[24]</ref> classifies prefetchers into different types based on different selection ways. <ref type="bibr" target="#b6">[7]</ref> analyzes the relationship between hardware prefetcher and page eviction policy. However, these works mainly utilize prefetch to optimize the data migration overhead on dGPU platfrms and don't apply the technique to optimize the hidden latency under UM model, especially on iGPU platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we explore the hidden latency issues when the UM model is applied to the emerging edge-based iGPU platform for autonomous driving. Through comprehensive characterizations, we observe that the existing CPU-side data initialization mechanism incurs significant latency under the conventional UM model and can hurt the response time of real-time tasks on iGPU platforms. We propose eUMM model, which delivers a prefetch-enhanced GPU-Init. method, to significantly mitigate the latency and benefit the entire application execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Topics</head><p>Desired feedback. We are looking forward to hearing more opinions from edge computing and GPU experts whether the GPU-Init. delivered by eUMM is a promising method for GPU programming and applications. Since we are not aware of similar work in this area, we would like to learn from experts and peers' comments about whether eUMM seems to be pervasive to GPU computing applications. We also appreciate it if any advice is provided to pinpoint the key challenges that we may face in developing the full framework for eUMM. We also welcome the experts' comments from the autonomous driving area and would like to learn their willingness and doubts to deploy eUMM in real practice.</p><p>Open issues. Firstly, we have observed that the benefits of GPU utilization may diminish due to the increasing complexity of memory access patterns of workloads. Therefore, we have to comprehensively evaluate the performance gains of eUMM for a variety of workloads in practice. Based on these characterizations, we may determine if it is beneficial to further optimize the GPU initialization process or designing an adaptive memory initialization controller to help us with the decision. Concerning the complex DNN workload suites in autonomous driving applications, they typically capture sensed data in real-time besides initializing the large amount of weight data. Extending eUMM model to this scenario could be a non-trivial job. Therefore, we consider to leverage GPUDirect <ref type="bibr" target="#b14">[15]</ref> to further enhance the eUMM model for the purpose of efficient data transfer. Secondly, when we apply eUMM in such high-end heterogeneous platforms such as PX2, which incorporates dGPUs besides iGPUs, the benefits of eUMM may be negatively impacted due to the complicated architecture. Therefore, techniques such as smart scheduling and CPU-GPU cooperative initialization can be considered on the platforms to guarantee the benefit of eUMM model. Besides, we wonder how eUMM is well compatible with other discrete GPUs or other vendors products.</p><p>Depreciating Circumstances. As we stated, we target at autonomous driving and other real-time tasks, which care about the latency on the iGPU platforms. Therefore, the UM model can be enhanced to mitigate the large data initialization latency and benefit the entire application execution time. However, if non-time-critical applications are implemented, it may be not necessarily to apply eUMM model to ease the situation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CPU-side initialization latency and the ratio under Def. and UM models. (a) MatAdd. (b) MatMul. The bar indicates latency corresponding to left-y axis, and triangle indicates the ratio of init. in the entire benchmark execution time corresponding to the right-y axis. x-axis indicates increasing input size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hidden latency during kernel launch under Def. and UM models. (a) MatAdd. (b) MatMul.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GPU-side data initialization in eUMM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Prefetch-enhanced GPU-Init. in eUMM model.(a) pristine GPU-Init. (b) prefetch-enhanced GPU-Init.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Initialization latency comparison under UM and eUMM models on TX2. y-axis is logarithmic coordinate to indicate latency in millisecond and x-axis indicates increasing data size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Kernel execution time under UM and eUMM models on TX2. The data sizes are chosen as the same sizes in Figure 5. is quite close to the latency under UM in 16*16 case. In addition, we analyze the kernel latency (i.e., the kernel execution time) of all benchmarks under UM and eUMM models to demonstrate that GPU-Init. in eUMM can also benefit the execution time of following kernel. Fig. 6 shows the results, where x-axis indicates the increasing data size corresponding to the size of each benchmark in Fig.5 and y-axis indicates the latency. Obviously, the kernel execution latency under eUMM is significantly reduced than the latency under UM, though for NW both latency increases drastically. Specifically, the kernel latency under eUMM can be reduced up to 61.0%, 63.0%, 10.1% and 76.9% for MatAdd, MatMul, NW and RA, respectively. Under eUMM model, the GPUInit. enables much page fault handling and mapping to be processed prior to the real kernel computation. The data to be used in the kernel computation has populated in GPU locally (i.e., the page tables covering the data is set up well on GPU side) before the kernel launches. Therefore, kernel latency can be reduced as well. Therefore, eUMM not only reduces data initialization significantly but also benefits the following kernel execution as well as the entire benchmark execution latency. It's critical to the safety of autonomous driving system. GPU-side Initialization on Xavier AGX. We implement our eUMM model on the Xavier AGX [1] as well, which installs NVIDIA L4T 32.3.1. The result is shown in Fig. 7, where (ad) indicates the initialization latency of the four benchmarks under UM and eUMM models, respectively. Basically, the initialization latency follows the same pattern as on TX2. The eUMM can significantly reduce the initialisation latency, which can be up to 93.4%, 97.3%, and 87.4% for MatAdd, MatMul, and NW respectively, though the UM outperforms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Initialization latency comparison under UM and eUMM models on AGX. Y-axis is logarithmic coordinate to indicate latency in millisecond and x-axis indicates increasing data size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The matrix operation scale (M.O.S.) of typical DNN 
models applied in autonomous driving, including matrix mul-
tiplication and addition. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Jetson agx xavier developer kit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Nvidia drive -autonomous vehicle development platforms</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Co-optimizing performance and memory footprint via integrated cpu/gpu memory management, an implementation on autonomous driving platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time and Embedded Technology and Applications Symposium (RTAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariusz</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><forename type="middle">Del</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beat</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasoon</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathew</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakai</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rodinia: A benchmark suite for heterogeneous computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">W</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang-Ha</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE international symposium on workload characterization (IISWC)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Analyzing memory management methods on integrated cpu-gpu systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="59" to="69" />
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Interplay between hardware prefetcher and page eviction policy in cpu-gpu unified virtual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debashis</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Melhem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Auto-tuning a highlevel language targeted to gpu codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Grauer-Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Searles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhee</forename><surname>Ayalasomayajula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cavazos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 Innovative Parallel Computing (InPar)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unified memory in cuda 6</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cuda 8 features revealed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Harris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Many-thread aware prefetching mechanisms for gpgpu applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nagesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyesoon</forename><surname>Lakshminarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vuduc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An evaluation of unified memory technology on nvidia gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuewen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>See</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cluster, Cloud and Grid Computing (CCGrid)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1092" to="1098" />
		</imprint>
	</monogr>
	<note>15th IEEE/ACM International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Gpudirect</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Gpu pro tip: Cuda 7 streams simplify concurrency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Hardware for every situation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cuda for tegra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wasp: Selective data prefetching with monitoring runtime warp progress on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunho</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung</forename><forename type="middle">Kuk</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won</forename><surname>Woo Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1366" to="1373" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An evaluation of the nvidia tx1 for supporting real-time computer-vision workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Otterness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunbyung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shige</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Real-Time and Embedded Technology and Applications Symposium (RTAS), 2017 IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="353" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Apogee: Adaptive prefetching on gpus for energy efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Sethia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrzad</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Parallel architectures and compilation techniques</title>
		<meeting>the 22nd international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">On demand paging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trinayan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards high performance paged memory for gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Zulfiqar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="345" to="357" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
