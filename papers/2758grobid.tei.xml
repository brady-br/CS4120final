<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spotnik: Designing Distributed Machine Learning for Transient Cloud Resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Wagenl√§nder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Mai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pietzuch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spotnik: Designing Distributed Machine Learning for Transient Cloud Resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To achieve higher utilisation, cloud providers offer VMs with GPUs as lower-cost transient cloud resources. Transient VMs can be revoked at short notice and vary in their availability. This poses challenges to distributed machine learning (ML) jobs, which perform long-running stateful computation in which many workers maintain and synchronise model replicas. With transient VMs, existing systems either require a fixed number of reserved VMs or degrade performance when recovering from revoked transient VMs. We believe that future distributed ML systems must be designed from the ground up for transient cloud resources. This paper describes SPOTNIK, a system for training ML models that features a more adaptive design to accommodate transient VMs: (i) SPOTNIK uses an adaptive implementation of the all-reduce collective communication operation. As workers on transient VMs are revoked, SPOTNIK updates its membership and uses the all-reduce ring to recover; and (ii) SPOTNIK supports the adaptation of the synchronisation strategy between workers. This allows a training job to switch between different strategies in response to the revocation of transient VMs. Our experiments show that, after VM revocation, SPOTNIK recovers training within 300 ms for ResNet/ImageNet.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cloud providers offer transient cloud resources, e.g., transient virtual machines (VMs) with GPUs, which are periodically auctioned off to the highest bidder and therefore can be revoked at short notice. Amazon AWS supports EC2 Spot Instances since 2009 <ref type="bibr" target="#b2">[3]</ref>, which are revoked with a two-minutes notice; Microsoft announced Azure Spot VMs in 2019 <ref type="bibr" target="#b4">[5]</ref>, which have a 30-second revocation warning. From an economic point-of-view, the pricing of transient VMs follows the law of free markets. A free market is supposed to find the optimal price between the cloud providers and its customers through supply and demand. Transient VMs therefore allow providers to increase the utilisation of expensive hardware.</p><p>The distributed training of machine learning (ML) models, in particular through deep learning, is resource-hungry.</p><p>In a distributed ML job, workers in a cluster train independent model replicas in parallel and synchronise at the end of each training step. For synchronisation, workers either use a central parameter server <ref type="bibr" target="#b24">[26]</ref> or decentralised collective communication, e.g., following an all-reduce pattern <ref type="bibr" target="#b40">[43]</ref>.</p><p>In many cloud settings, large-scale ML jobs thus have become a dominant workload <ref type="bibr" target="#b10">[11]</ref>. Large models and datasets yield higher accuracies at the cost of using more cloud resources (e.g., GPUs or TPUs) and having longer training times. Therefore, there is an economic incentive for distributed training on transient VMs, which can yield cost reductions of up to 90% compared to regular reserved VMs <ref type="bibr" target="#b0">[1]</ref>.</p><p>We argue that it should be possible for distributed ML jobs to run exclusively on transient VMs. Systems, such as PyTorch <ref type="bibr" target="#b35">[38]</ref> and TensorFlow <ref type="bibr" target="#b46">[49]</ref>, however, face challenges when running on a transient cloud cluster: (i) workers may be added and removed from the cluster when transient VMs become available and are revoked, respectively. The system must be designed to support such dynamic changes, and these changes must not prolong the training process; and (ii) as a cluster changes based on the available transient VMs, configuration parameters of the ML job are affected. For example, a synchronisation strategy that is effective with few workers, such as synchronous stochastic gradient descent (S-SGD) <ref type="bibr" target="#b21">[23]</ref>, may suffer from network bottlenecks with many workers. Therefore, it is hard to deploy a distributed ML job in current systems with one configuration that achieves the best performance irrespective of the available transient VMs.</p><p>Existing work that attempts to address the above problems through a hybrid approach, in which a system utilises both transient and reserved VMs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">36]</ref>. Since reserved VMs cannot be revoked, systems use them e.g., to run parameter servers. This, however, increases the number of reserved VMs proportionally with more transient VMs, reducing cost benefits. Alternative approaches rely on expensive recovery mechanisms when transient VMs are revoked. For example, workers may periodically checkpoint their state to external storage, pausing the training during checkpointing <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b30">33]</ref>. When transient VMs are revoked, the state of the lost worker is recovered from the checkpoint, which increases job runtime.</p><p>In contrast, we want to design a distributed ML system that runs exclusively and efficiently on dynamic transient VMs. We want the design to be future-proof: even when revocation times are reduced from minutes to seconds, a distributed ML job should achieve fast convergence. In general, the less time is spent on handling revocations, the more time can be used by training tasks. This becomes especially important in serverless cloud environments <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> in which functions are shortrunning and there is no concept of revocation notifications.</p><p>The paper introduces SPOTNIK, a distributed ML system for transient VMs that explores two ideas: (1) Adaptive collective communication. SPOTNIK uses a novel adaptive collective communication layer that handles dynamic changes to membership of the cluster. The layer (i) detects worker failure due to revoked transient VMs, (ii) negotiates a new membership among all running workers; and (iii) notifies the workers to continue training with the new membership. To handle revocation, this layer embeds control messages as part of an all-reduce operation so that workers can mutually detect revocation and recover the system by exchanging local states. To reduce performance overheads, the control messages reuse the scalable all-reduce graph, and their number sent by each worker is bounded. (2) Adaptive worker synchronisation. To synchronise workers efficiently irrespective of the cluster size, SPOTNIK adapts its synchronisation strategy at runtime with low overhead. It can shift synchronisation among synchronous stochastic gradient descent (S-SGD) <ref type="bibr" target="#b21">[23]</ref>, synchronous model averaging (S-MA) <ref type="bibr" target="#b20">[22]</ref> and asynchronous decentralised parallel SGD (AD-PSGD) <ref type="bibr" target="#b26">[28]</ref>. For this, SPOTNIK relies on highlevel synchronisation operators that cover both collective operations, such as computing global averaged gradients, and point-to-point operations, such as pair-wise model averaging. Changing strategies has low overhead because it does not require the allocation of new system resources. SPOTNIK makes a decision to change the synchronisation strategy by monitoring workers.</p><p>2 ML training with transient cloud resources</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributed ML training in cloud</head><p>Distributed ML systems <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b46">49]</ref> use parallel workers to split the training workload for large ML models. Each worker samples a mini-batch of data from the training dataset and trains its model replica using the mini-batch stochastic gradient descent (SGD) algorithm <ref type="bibr" target="#b39">[42]</ref>. As replicas are trained with different mini-batches, their parameters (i.e., weights) diverge after each training iteration. The system therefore must use a synchronisation strategy to control the divergence of model replicas. A number of such strategies exist, e.g., synchronous SGD (S-SGD) <ref type="bibr" target="#b21">[23]</ref> updates model replicas using averaged gradients; model averaging <ref type="bibr" target="#b20">[22]</ref> coordinates model replicas using a central model. A synchronisation strategy can be implemented in different ways, and there are two major approaches used today: all-reduce organises the workers in a ring topology, which is used to compute aggregated gradients <ref type="bibr" target="#b40">[43]</ref> or a parameter server allows workers to send their local gradients and responds with aggregated gradients <ref type="bibr" target="#b24">[26]</ref>.</p><p>When users deploy a ML job in the cloud, they can scale it using a cluster of VMs, which typically includes accelerators such as GPUs for tensor computation. In practice, users face a high monetary costs when using dedicated VMs for training. For example, training a BERT-like model with Megatron-LM <ref type="bibr" target="#b42">[45]</ref> on 512 NVIDIA V100 GPUs on Azure costs $0.56 per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exploiting transient cloud resources</head><p>Transient cloud resources, such as Google Preemptible VMs <ref type="bibr" target="#b14">[15]</ref>, Azure low-priority/spot VMs <ref type="bibr" target="#b5">[6]</ref>, AWS Spot Instances <ref type="bibr" target="#b0">[1]</ref> and IBM Transient VMs <ref type="bibr" target="#b19">[20]</ref> have emerged as a means to reduce the cost of cloud computing. Transient VMs offer weak service level agreements (SLAs) and can be revoked when the provider needs the capacity back. For example, AWS and Azure set a dynamic market price for VMs and revoke VMs if the spot price goes above the price declared by users; Google and Azure also provide preemptible VMs, which are purchased at fixed discount prices but can be reclaimed after a 30-second notification.</p><p>Cloud providers offer transient VMs because they have benefits for both cloud providers and users. In data centres, cloud providers must maintain redundant resources for planned growth, which can be sold as transient VMs before they are reserved by future customers. Furthermore, data centres show a diurnal pattern <ref type="bibr" target="#b13">[14]</ref>. In off-peak times, they may have substantial idle resources, which can be monetised as transient resources to improve utilisation.</p><p>Cloud users can significantly reduce their costs of using cloud GPUs by saving up to 90% compared to reserved instances <ref type="bibr" target="#b0">[1]</ref>. Training with transient VMs offers users substantially lower GPU costs: with the same budget, a user can use 10 times more GPUs, reducing training times by 80% <ref type="bibr" target="#b25">[27]</ref>. In the future, transient cloud resources are likely to grow in popularity and thus become the dominant resource type for distributed ML systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges in using transient resources</head><p>In practice, performing distributed ML training with transient cloud resources is challenging: (1) Workers may lose resources at any time. The revocation of transient VMs adversely affects distributed training. Workers maintain large intermediate training state, and revoking a worker requires this state to be either discarded, compromising consistency and accuracy of the trained model <ref type="bibr" target="#b6">[7]</ref>, or migrated to another worker, adding I/O load and affecting training performance <ref type="bibr" target="#b24">[26]</ref>.</p><p>In addition, distributed ML systems require global computation barriers across workers for the synchronisation of model replicas. When a worker is revoked due to the loss of a transient VM, it may cause the entire computation to block <ref type="bibr">[32,</ref><ref type="bibr" target="#b32">35]</ref>. In such a case, the system must restarted manually from a checkpoint, losing performance <ref type="bibr" target="#b23">[25]</ref>.</p><p>(2) Resource availability may change substantially. Depending on the magnitude of a resource bid and the ratio of the transient resource price to the on-demand one <ref type="bibr" target="#b41">[44]</ref>, the probability of revocation increases. VMs with GPUs in AWS are among the ones with the highest revocation probability <ref type="bibr" target="#b44">[47]</ref>. As a result, the cluster size may vary substantially over time if a user's spending remains fixed.</p><p>Such dynamic changes to the cluster size shift the performance bottlenecks during the training process. This makes it hard for users to choose a single ML job configuration that achieves the best training performance. For example, the overhead of maintaining a global barrier is smaller with tens of workers compared to hundreds. In addition, larger clusters have a higher probability of stragglers <ref type="bibr" target="#b9">[10]</ref>, and thus require extra mechanisms for their mitigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Existing approaches and their limitations</head><p>We observe several reasons why existing distributed ML training systems are ill-suited for using transient cloud resources.</p><p>(1) Inefficient recovery with transient resources. When faced with the revocation of transient VMs, existing systems broadly follow two approaches: MPI-based distributed systems such as Horovod <ref type="bibr" target="#b40">[43]</ref> treat revocations as failures. They must collect periodic checkpoints and recover from the last checkpoint when the state of a worker was lost, In practice, this results in high performance overheads (see ¬ß4).</p><p>Systems that use parameter servers <ref type="bibr" target="#b16">[17]</ref> place them typically on expensive reserved VMs. In a hybrid model, a user's budget is split across transient and reserved VMs for workers and parameter servers, respectively. The split is workloadspecific, making it hard to find a good compromise <ref type="bibr" target="#b48">[52]</ref>.</p><p>Systems that scale elastically assume that scaling is possible at any time <ref type="bibr" target="#b38">[41]</ref>. When training with transient resources, however, the environment controls the available resources, and new resources may be unavailable. Prior work <ref type="bibr" target="#b38">[41]</ref> searches for the optimal cluster configuration for a training job; we want to do the opposite, i.e., optimise the training job for a given cloud environment. (2) No mechanisms to adapt to resource changes. Current distributed ML systems require users to select parameters of the training process, such as the synchronisation strategy, at job deployment time. The choice of S-SGD, the de-facto standard, is hard-coded as part of most training programs. This makes it hard for systems to scale when more VMs become available. Synchronisation strategies for large clusters, such as AD-PSGD <ref type="bibr" target="#b26">[28]</ref>, are poor general choices because they exhibit lower training performance than S-SGD on small clusters. This puts users in a dilemma: they prefer to use S-SGD for high training accuracy with few transient VMs, but then they sacrifice scalability when more VMs become available.   <ref type="bibr">[50]</ref> and BytePS <ref type="bibr" target="#b34">[37]</ref>. To use SPOTNIK, users embed a Spotnik distributed optimiser (shown in <ref type="figure" target="#fig_0">Fig. 1)</ref> into their TensorFlow programs. This makes the use of transient VMs transparent: the optimiser allocates/deallocates resources, and replicates the training programs on workers.</p><p>Workers in SPOTNIK use an adaptive collective communication layer (see <ref type="figure" target="#fig_0">Fig. 1</ref>) to synchronise model replicas. A unique feature of this layer is that it can transparently recover from VM revocations, avoiding the need to restart the training job as in existing collective communication libraries, such as OpenMPI <ref type="bibr" target="#b32">[35]</ref> and NCCL <ref type="bibr">[32]</ref>. The layer detects revocations and recovers training state, as part of the regular synchronisation communication between workers. This removes the need for the user to handle revocations explicitly.</p><p>In addition, SPOTNIK supports adaptive synchronisation strategies. It shifts to a different synchronisation strategy in response to changes in transient VM availability. Instead of hard-coding a synchronisation strategy in a training job, the SPOTNIK distributed optimiser contains a set of candidate synchronisation strategies (see <ref type="figure" target="#fig_0">Fig. 1</ref>). It then dynamically selects one according to monitored cluster metrics, such as the current number of transient VMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive collective communication</head><p>We explore a design for SPOTNIK that allows it to recover efficiently from the revocation of transient VMs. SPOTNIK must coordinate potentially hundreds of workers. Each worker computes model gradients and launches parallel collective communication operations such as ring all-reduce to aggregate gradients. If a worker is revoked in the middle of this To address this issue, SPOTNIK exploits the following idea: since workers have already constructed a collective communication topology (all-reduce) and have replicated training states, they can detect revocation and coordinate among themselves to continue training by reusing existing state. Note that the design of SPOTNIK does not rely on workers receiving revocation notifications, making this approach applicable irrespective of the notification interval.</p><p>SPOTNIK realises this approach as follows: workers exchange control messages to handle revocation in a collective communication operation. Here, each worker must ensure that (i) control messages do not incur a performance overhead; and (ii) worker states, which are asynchronously updated by collective communication operations, can be reused safely. SPOTNIK achieves this through an adaptive collective communication layer, which has two parts: (1) Revocation recovery algorithm. We design an algorithm that can handle revocations within a ring topology for collective communication topology, such as all-reduce. It reuses the existing data connections in the topology to pass control messages. The number of messages sent by each worker is bound by O(K), where K is the number of concurrent revocations, which allows it to scale to many transient VMs.</p><p>We explain the algorithm in <ref type="figure" target="#fig_1">Fig. 2</ref>. This algorithm has three phases: reconcile, accept and restart. We assume that workers exchange messages on a ring, a representative collective communication topology, to synchronise model replicas. On the ring, each worker monitors the health of its neighbours. Assuming worker W 0 detects that W 1 has been revoked, W 0 removes W 1 (step 1 ) from the membership, and propagates the new membership through the ring (step 2 ). During this process, W 4 finds that W 5 has been revoked (i.e., there are multiple revocations). It removes W 5 from the membership and continue the propagation process (step 3 ). The process terminates at a worker (i.e., W 6 in this example), which receives a membership list that is equal to the local one, indicating that all workers have reconciled the new membership. This worker initiates another propagation to have workers accept the new membership (accept phase). Finally, it propagates a message to restart training on all workers (restart phase).</p><p>(2) Atomic worker state update. SPOTNIK must ensure atomicity when modifying worker state. Distributed ML systems such as TensorFlow launch asynchronous all-reduce op- erations that overlap training and synchronisation. SPOTNIK must protect the integrity of worker state while it is modified concurrently, otherwise workers would observe a dirty state that has been partially updated.</p><p>Instead of directly updating parameters, SPOTNIK waits for all averaged gradients to be ready and applies them in an atomic fashion. The caching process has non-negligible performance overheads: SPOTNIK coordinates all-reduce operations in the background without blocking training. This is achieved through an all-reduce scheduler embedded in the dataflow graph that asynchronously receives notifications regarding events in the communication layer. If any all-reduce operation fails due to a worker revocation, the scheduler aborts the remaining all-reduce operations and discards cached results. The remaining workers then continue with the new membership state and re-try the failed training iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adapting synchronisation strategies</head><p>Past work has proposed a wide spectrum of synchronisation strategies for clusters of different sizes, which vary in terms of network efficiency and model accuracy. <ref type="figure" target="#fig_2">Fig. 3</ref> compares several synchronisation strategies: on one end, synchronous strategies such as S-SGD <ref type="bibr" target="#b21">[23]</ref> and SMA <ref type="bibr" target="#b20">[22]</ref> incorporate model updates from all workers for improved model accuracy; while HogWild! <ref type="bibr" target="#b37">[40]</ref>, AD-PSGD <ref type="bibr" target="#b26">[28]</ref> and EA-SGD <ref type="bibr" target="#b50">[54]</ref> remove expensive barriers to avoid network bottlenecks, assuming that training can reach a targeted accuracy sooner.</p><p>Since the number of transient VMs, and thus the network load, can change substantially during training, SPOTNIK adapts the choice of synchronisation strategy. This is challenging to achieve in existing synchronisation libraries such as Horovod <ref type="bibr" target="#b40">[43]</ref>, which hide low-level communication primitives and expose only an S-SGD implementation.</p><p>To address this, SPOTNIK provides high-level synchronisation primitives implemented on top of the SPOTNIK communication layer. Users can combine these primitives to implement various synchronisation strategies. Since these strategies are implemented as part of a unified communication layer, we can thus efficiently switch between strategies during training.</p><p>SPOTNIK supports a range of synchronisation primitives, which can be broadly classified into two groups: (i) collective synchronisation operators (e.g., global gradient sum and model averaging), and (ii) point-to-point synchronisation operators (e.g., pair-wise model averaging and model caching). These operators are general enough to implement many synchronisation strategies, including S-SGD, SMA and Recovery with 32 workers </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Our evaluation asks: (i) what is the recovery latency after a VM revocation? (ii) what is the overhead of supporting VM revocations during all-reduce? (iii) what are the benefits of adaptive synchronisation in a real-world training scenario?</p><p>We run experiments on Azure with 32 NC-6 VMs. Each VM has 6 vCPUs, 56 GB of memory and a NVIDIA K80 GPU. Additional experiments use the Huawei ModelArts Cloud <ref type="bibr" target="#b29">[31]</ref> with two nodes that have 8 V100 GPUs each, linked via 100 Gbps InfiniBand. We implement SPOTNIK on top of KungFu, a distributed training library <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b28">30]</ref>, which scales out TensorFlow programs and achieves performance similar to Horovod. We use KungFu because of its ability to add and remove workers during training. We run SPOTNIK with KungFu 0.2.1 and TensorFlow 1.15.0. As models, we use ResNet-50 <ref type="bibr" target="#b17">[18]</ref>, VGG-16 <ref type="bibr" target="#b43">[46]</ref>, and Inception-V3 <ref type="bibr" target="#b45">[48]</ref>, designed for the ImageNet task, and BERT-base <ref type="bibr" target="#b11">[12]</ref>, designed for the SQuAD question-and-answering task <ref type="bibr" target="#b36">[39]</ref>. Recovery latency. First we explore the latency after revocation. There are two factors that affect latency: the total number of workers, which determines the number of control messages, and the number of concurrently revoked VMs, which impacts the cost of the recovery operation. We therefore consider different numbers of workers (16 and 32) training a ResNet-50 model for ImageNet. During training, we revoke 1-3 workers concurrently and measure the recovery latency. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the recovery latency of workers, as we increase the number of concurrent revocations. The whisker plot indicates the 25th, median and 75th latency percentiles, respectively; the markers show maxima. For both 16 and 32 workers, the maximum latency remains below 300 ms due to SPOTNIK's fast adaptation. This allows SPOTNIK to work effectively even in highly dynamic transient clusters. Performance overhead. Next we measure the performance overhead when enabling revocation recovery during allreduce operations. We compare four representative deep learning models (VGG-16, ResNet-50, Inception-V3, BERT-base), which cover a large spectrum of model sizes used in practice and have different all-reduce workloads. BERT-base runs on ModelArts; the others run on Azure. gradients. The atomic updates have only a small impact on throughput. This reflects the benefits of using control messages for handling revocation inside all-reduce operations. The difference becomes negligible for the BERT-base model due to the higher bandwidth network on ModelArts.</p><p>Adaptive synchronisation. Now we investigate how SPOT-NIK handles dynamic cluster changes. We activate the (accuracy-friendly) S-SGD and (network-friendly) AD-PSGD strategies in SPOTNIK's optimiser. We begin with one worker training ResNet-50, adding a new worker every 2 mins. <ref type="figure">Fig. 6</ref> shows how the per-worker throughput changes with more workers. For S-SGD, it decreases because the communication overhead of an all-reduce grows substantially with the number of workers; for AD-PSGD, it roughly stays constant because communication is asynchronous and it occurs only between pairs of workers. SPOTNIK switches from S-SGD to AD-PSGD after 26 mins of training time when the throughput falls below 40%. It therefore removes the communication bottleneck and maintains high throughput. We speculate that SPOTNIK is faster than AD-PSGD due to the performance variability of cloud resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>SPOTNIK enables efficient distributed machine learning on transient cloud resources through adaptive collective communication and synchronisation. Its adaptive collective communication recovers from VM revocations by reusing the existing communication as part of the all-reduce ring. It also updates worker state atomically to ensure that gradient updates remain consistent during recovery. To account for the dynamic nature of transient clusters, SPOTNIK switches among different synchronisation strategies during training depending on network bottlenecks due to different cluster sizes.</p><p>We consider SPOTNIK an initial design for a distributed ML system specifically created to work effectively in cloud environments with transient VMs. Beyond adaptive collective communication and synchronisation, there are, however, other system aspects that are affected by transient resources. For example, hyper-parameter tuning <ref type="bibr" target="#b27">[29]</ref>, which is important for current ML models to achieve high accuracy and performance, must be rethought when the cloud execution environment itself is highly dynamic. Here, the systems community would benefit from feedback by ML practitioners on other aspects of ML systems that are affected by transient resources.</p><p>In addition, there are interesting design questions for cloud applications when finer-grained resources, beyond individual VMs, become transient. In the future, cloud providers may decide to auction off memory and network bandwidth as transient resources. In such fully disaggregated cloud environments <ref type="bibr" target="#b12">[13]</ref>, we could imagine that the mix of available transient resources changes over time. Future distributed ML systems must therefore be designed to work in such dynamic disaggregated cloud environments, e.g., by being able to shift quickly between different resource types and accounting for the associated trade-offs. As part of the discussion, we would ask for input by cloud providers whether the above is how they believe future cloud environments will evolve.</p><p>With the highly-disaggregated dynamic clouds of the future, the execution model may resemble today's serverless offerings, such as AWS Lambda <ref type="bibr" target="#b1">[2]</ref> and Azure Cloud Functions <ref type="bibr" target="#b3">[4]</ref>. Early work exists that explores how distributed ML jobs can be supported effectively in serverless clouds <ref type="bibr" target="#b7">[8]</ref>. The techniques to split the ML training computation into many short-running, potentially stateless, functions is applicable to transient resources. Especially when transient resources are allocated and revoked at a fine granularity, with short revocations times on the order of seconds, the models of transient cloud resources and serverless computing begin to unify.</p><p>All of this will require cloud pricing models that are intuitive and predictable for users. Users are often charged for resources in fixed-sized allocation blocks. We believe that future clouds will support more fine-grained resource accounting, e.g., at the level of CPU cycles <ref type="bibr" target="#b8">[9]</ref>. With the rise of ML accelerators such as GPUs <ref type="bibr" target="#b31">[34]</ref>, TPUs <ref type="bibr" target="#b47">[51]</ref>, Intel NNP <ref type="bibr">[21]</ref> and IPUs <ref type="bibr" target="#b15">[16]</ref>, simple pricing models that clearly capture trade-offs between cost and performance of technologies will be necessary for adoption by the ML community. We are interested in hearing about innovative cloud pricing models that will impact the design of next-generation ML systems.</p><p>Nore that SPOTNIK can only handle revocations of a part of the cluster. Since no node is left when the whole cluster is revoked, SPOTNIK cannot entirely avoid checkpointing. We want to consider options for supporting this scenario better. To decrease the probability of losing all transient VMs, users may adopt bidding schemes such as heterogenous bids <ref type="bibr" target="#b51">[55]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SPOTNIK design 3 SPOTNIK design 3.1 Architecture overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of repairing a broken all-reduce ring operation, its downstream workers in the all-reduce topology cannot proceed, potentially blocking the training job. To address this issue, SPOTNIK exploits the following idea: since workers have already constructed a collective communication topology (all-reduce) and have replicated training states, they can detect revocation and coordinate among themselves to continue training by reusing existing state. Note that the design of SPOTNIK does not rely on workers receiving revocation notifications, making this approach applicable irrespective of the notification interval. SPOTNIK realises this approach as follows: workers exchange control messages to handle revocation in a collective communication operation. Here, each worker must ensure that (i) control messages do not incur a performance overhead; and (ii) worker states, which are asynchronously updated by collective communication operations, can be reused safely. SPOTNIK achieves this through an adaptive collective communication layer, which has two parts: (1) Revocation recovery algorithm. We design an algorithm that can handle revocations within a ring topology for collective communication topology, such as all-reduce. It reuses the existing data connections in the topology to pass control messages. The number of messages sent by each worker is bound by O(K), where K is the number of concurrent revocations, which allows it to scale to many transient VMs. We explain the algorithm in Fig. 2. This algorithm has three phases: reconcile, accept and restart. We assume that workers exchange messages on a ring, a representative collective communication topology, to synchronise model replicas. On the ring, each worker monitors the health of its neighbours. Assuming worker W 0 detects that W 1 has been revoked, W 0 removes W 1 (step 1 ) from the membership, and propagates the new membership through the ring (step 2 ). During this process, W 4 finds that W 5 has been revoked (i.e., there are multiple revocations). It removes W 5 from the membership and continue the propagation process (step 3 ). The process terminates at a worker (i.e., W 6 in this example), which receives a membership list that is equal to the local one, indicating that all workers have reconciled the new membership. This worker initiates another propagation to have workers accept the new membership (accept phase). Finally, it propagates a message to restart training on all workers (restart phase). (2) Atomic worker state update. SPOTNIK must ensure atomicity when modifying worker state. Distributed ML systems such as TensorFlow launch asynchronous all-reduce op-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Landscape of synchronisation strategies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Recovery latency with concurrent revocations AD-PSGD. We are currently implementing more sophisticated strategies, such as the recent Lookahead strategy [53].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 Figure 5 :</head><label>55</label><figDesc>Figure 5: Revocation overhead during all-reduce</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spot</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ec2/spot/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Lambda</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/lambda/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Announcing Amazon EC2 Spot Instances</title>
		<ptr target="https://aws.amazon.com/about-aws/whats-new/2009/12/14/announcing-amazon-ec2-spot-instances/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Azure Functions</title>
		<ptr target="https://azure.microsoft.com/en-us/services/functions/,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Announcing the preview of Azure Spot Virtual Machines</title>
		<ptr target="https://azure.microsoft.com/en-gb/blog/announcing-the-preview-of-azure-spot-virtual-machines/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Azure Spot Virtual Machines</title>
		<ptr target="https://azure.microsoft.com/en-us/pricing/spot/,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Machine learning with adversaries: Byzantine tolerant gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peva</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Stainer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cirrus: a Serverless Framework for End-to-end ML Workflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Cloud Computing (SOCC)</title>
		<meeting>Symposium on Cloud Computing (SOCC)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Amit Vasudevan, and Vyas Sekar. Towards verifiable resource accounting for outsourced computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Perrig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Virtual Execution Environments (VEE)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Solving the straggler problem with bounded staleness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Kyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Hot Topics in Operating Systems (HotOS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A new golden age in computer architecture: Empowering the machine-learning revolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network requirements for resource disaggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Peter X Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagar</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangjin</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachit</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analyzing AWS spot instance pricing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Krintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Brevik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cloud Engineering (IC2E)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Google Preemptible Virtual Machines</title>
		<ptr target="https://cloud.google.com/preemptible-vms/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ipu</forename><surname>Graphcore</surname></persName>
		</author>
		<ptr target="https://www.graphcore.ai/products/ipu,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Proteus: agile ML elasticity through tiered reliability in dynamic resource markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Systems (EuroSys)</title>
		<meeting>European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVRP)</title>
		<meeting>Computer Vision and Pattern Recognition (CVRP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ibm Cloud Functions</surname></persName>
		</author>
		<ptr target="https://cloud.ibm.com/functions/,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<ptr target="https://www.ibm.com/cloud/blog/transient-virtual-servers?mhsrc=ibmsearch_a&amp;mhq=transient" />
	</analytic>
	<monogr>
		<title level="j">IBM Transient Virtual Servers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CROSSBOW: scaling deep learning with small batch sizes on multi-GPU servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Koliousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pijika</forename><surname>Watcharapichat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Weidlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pietzuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Very Large Data Bases (VLDB)</title>
		<meeting>Very Large Data Bases (VLDB)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kungfu</surname></persName>
		</author>
		<ptr target="https://github.com/lsds/KungFu,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepspotcloud: leveraging cross-region GPU spot instances for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungyong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myungjun</forename><surname>Son</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cloud Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>CLOUD</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling distributed machine learning with the parameter server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><forename type="middle">Woo</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amr</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanja</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Josifovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bor-Yiing</forename><surname>Shekita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speeding up deep learning with transient servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Autonomic Computing (ICAC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Asynchronous decentralized parallel stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06952</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05118</idno>
		<title level="m">Tune: A Research Platform for Distributed Model Selection and Training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Taming Hyperparameters in Deep Learning Systems. SIGOPS Operating Systems Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Koliousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreioctavian</forename><surname>Brabete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pietzuch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Modelarts</surname></persName>
		</author>
		<ptr target="https://www.huaweicloud.com/en-us/product/modelarts.html" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A prediction approach to define checkpoint intervals in spot instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Pergentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">M</forename><surname>Pianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C√©lia Ghedini</forename><surname>Ralha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Cloud Computing (CLOUD)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Tesla</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-gb/data-center/tesla/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">High Performance Message Passing Library</title>
		<ptr target="https://www.open-mpi.org" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimus: an efficient dynamic resource scheduler for deep learning clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Systems (EuroSys)</title>
		<meeting>European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A generic communication scheduler for distributed DNN training acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hogwild: A lock-free approach to parallelizing stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Resource elasticity in distributed deep learning</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems (MLSys)</title>
		<meeting>Machine Learning and Systems (MLSys)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sutton</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleaxander</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Del</forename><surname>Balso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05799</idno>
		<title level="m">Horovod: fast and easy distributed deep learning in TensorFlow</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">HotSpot: automated server hopping in cloud spot markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supreeth</forename><surname>Shastri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Cloud Computing (SOCC)</title>
		<meeting>Symposium on Cloud Computing (SOCC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Megatron-lm: Training multi-billion parameter language models using gpu model parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Advisor</forename><surname>Spot Instance</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/ec2/spot/instance-advisor/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Computer Vision and Pattern Recognition (CVRP)</title>
		<meeting>Computer Vision and Pattern Recognition (CVRP)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tensorflow</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Cloud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tpu</forename></persName>
		</author>
		<ptr target="https://cloud.google.com/tpu/,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ako: Decentralised deep learning with partial gradient exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pijika</forename><surname>Watcharapichat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victoria</forename><forename type="middle">Lopez</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pietzuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Symposium on Cloud Computing (SOCC)</title>
		<meeting>Symposium on Cloud Computing (SOCC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Lookahead Optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep learning with elastic averaging SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">E</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gauri</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlee</forename><surname>Joewong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05649</idno>
		<title level="m">Machine learning on volatile instances</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
