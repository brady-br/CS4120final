<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-01T14:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy Preserving OLAP</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
							<email>ragrawal@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Almaden</orgName>
								<orgName type="institution" key="instit2">IBM Almaden</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishnan</forename><surname>Srikant</surname></persName>
							<email>srikant@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Almaden</orgName>
								<orgName type="institution" key="instit2">IBM Almaden</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilys</forename><surname>Thomas</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Almaden</orgName>
								<orgName type="institution" key="instit2">IBM Almaden</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Privacy Preserving OLAP</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present techniques for privacy-preserving computation of mul-tidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches.We develop and analyze algorithms for reconstructing counts of sub-cubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the prac-ticality of our approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>On-line analytical processing (OLAP) is a key technology employed in business-intelligence systems. The computation of multidimensional aggregates is the essence of on-line analytical processing. We present techniques for computing multidimensional count aggregates in a privacy-preserving way.</p><p>We consider a setting in which clients C 1 , C 2 , . . . C n are connected to a server S . The server has a We take the randomization approach to preserving privacy. The basic idea is that every client C i perturbs its row r i before sending it to the server S . The randomness used in perturbing the values ensures information-theoretic row-level privacy. <ref type="figure" target="#fig_0">Figure 1</ref> gives the * Supported in part by NSF Grant ITR-0331640</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. <ref type="bibr">SIGMOD 2005</ref><ref type="bibr">June 14-16, 2005</ref>  schematic of our approach. S runs queries on the resultant perturbed table T ′ . The query meant for the original table T is translated into a set of queries on the perturbed table T ′ . The answers to these queries are then reconstructed to obtain the result to the original query with bounded error. We show that our techniques are safe against privacy breaches. The perturbation algorithm is publicly known; the actual random numbers used in the perturbation, however, are hidden. To allow clients to operate independently, we use local perturbations so that the perturbed value of a data element depends only on its initial value and not on those of the other data elements. Different columns of a row are perturbed independently. We use retention replacement schemes where an element is decided to be retained with probability p or replaced with an element selected from a probability distribution function (p.d.f.) on the domain of elements.</p><p>The proposed techniques can also be used for database tables in which some of the columns are categorical. They are also applicable in the settings in which the database tables are partitioned horizontally or vertically.</p><p>The organization of the rest of the paper is as follows. We start off with a discussion of related work in Section 2. Section 3 formally defines the retention replacement perturbation. Section 4 presents the reconstruction algorithms. Section 5 presents the guarantees against privacy breaches offered by our techniques. In Section 6, we discuss how our techniques can be extended to categorical data. We also discuss some additional perturbation techniques and describe how our techniques can be used in data mining by showing how to build a decision tree classifier. Section 7 presents an empirical evaluation of our techniques. We conclude with a summary and directions for future work in Section 8. The proofs of our results have been collected in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>The techniques for preserving privacy while answering statistical queries developed in the statistical database literature can be classified into query restriction, input perturbation and output perturbation <ref type="bibr" target="#b0">[1]</ref>. Both query restriction and output perturbation are applicable when the entire original unperturbed data is available in a single central repository, which is not true in our setting, where clients randomize their data before providing it to the server. Our scenario fits in the framework of input perturbation, where the goal is to create a version of the database that can be publicly released (e.g. census data), yet the individual rows should not be recoverable. Local perturbation for a single column has been studied in <ref type="bibr" target="#b23">[24]</ref>. However most previous work (e.g., <ref type="bibr" target="#b19">[20]</ref>) assume that during perturbation the entire database is available at a single site, while we require local perturbations at each client.</p><p>The use of local perturbation techniques to preserve privacy of individual rows while allowing the computation of data mining models at the aggregate level was proposed in <ref type="bibr" target="#b3">[4]</ref>. They used an additive perturbation technique, in which a random perturbation is added to the original value of the row, where the perturbation is picked from another probability distribution function (e.g. Gaussian). They showed that it was possible to build accurate decision tree classification models on the perturbed data.</p><p>However, it is difficult to provide guarantees against privacy breaches when using additive perturbation. For instance, if we add a Gaussian random variable with a mean 0 and variance 20 to age, and for a specific row the randomized value happens to be −60, one can estimate with high confidence that the original value of age was (say) less than 20. Additive schemes are also restricted to numeric data. Finally, the algorithms in <ref type="bibr" target="#b3">[4]</ref> reconstruct each column independently. Since OLAP requires queries over multiple columns, it is essential to be able to reconstruct them together.</p><p>The problem of privacy-preserving association-rule mining was studied in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>. The randomization schemes used in these works are similar to the retention replacement schemes we consider. However these studies are restricted to boolean data.</p><p>Formal definitions of privacy breaches were proposed in <ref type="bibr" target="#b8">[9]</ref>, and an alternate approach to defining privacy guarantees was proposed in <ref type="bibr" target="#b5">[6]</ref>. We adapt the definitions from <ref type="bibr" target="#b8">[9]</ref> to allow more accurate reconstruction while still providing strong privacy guarantees. As our notion of privacy encompasses multiple correlated columns over vertically partitioned tables, it extends to privacy breaches (called disclosure risk) considering row linkage, studied in statistical disclosure control methods and <ref type="bibr" target="#b10">[11]</ref>.</p><p>There has been recent work <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> to specify authorization and control inferences for OLAP data cubes. However the model assumes that the data resides at a single server, unlike our problem, where private data is integrated from multiple clients.</p><p>Another related area is that of secure multiparty computation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>, that allows any function, whose inputs are shared between multiple clients to be evaluated, such that nothing other than the result is revealed. Since the general protocols are expensive, efficient protocols have been proposed for specific database and data mining operations, e.g. <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. However, these protocols are designed for a small number of clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATA PERTURBATION</head><p>A single record of the table is referred to as a row, while an attribute is referred to as a column. A single column from a single row is the granularity of perturbation and is referred to as a data element. </p><formula xml:id="formula_0">so that | f ′ − f | &lt; max(ǫ, ǫ f ) with</formula><p>probability greater than (1 − δ) whenever the table T has more than n rows. The probability is over the random choices made by α.</p><p>For boolean functions, (n, δ) reconstructability needs f and f ′ to agree exactly with probability greater than (1 − δ).</p><p>Referring to <ref type="figure" target="#fig_0">Figure 1</ref>, to answer the aggregate query count(P 1 ∧ P 2 ∧. . . P k ) on k columns of the original table, T , a set of 2 k queries,</p><formula xml:id="formula_1">count(P 1 ∧ P 2 ∧ . . . P k ), count(¬P 1 ∧ P 2 ∧ . . . P k ), count(P 1 ∧ ¬P 2 ∧ . . . P k ), count(¬P 1 ∧¬P 2 ∧. . . P k ) . . . count(¬P 1 ∧¬P 2 ∧. . . ¬P k ) are generated.</formula><p>These queries are evaluated on the perturbed table T ′ .</p><p>The answers on T ′ are reconstructed into estimated answers for the same queries on T , which include the answer to the original query. Without loss of generality, assume that the predicates are only over perturbed columns. We present reconstruction algorithms for numeric columns. These algorithms can be extended to categorical columns too as shown in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reconstructing Single Column Aggregates</head><p>Consider the uniform retention replacement perturbation with retention probability p applied on a database with n rows and a single column, C, with domain <ref type="bibr">[min, max]</ref>. Consider the predicate P = C <ref type="bibr">[low, high]</ref>. Given the perturbed </p><formula xml:id="formula_2">n o = 1 p (n r − n(1 − p)b) , where b = high − low max − min .</formula><p>The intuition is that out of the n rows in table T , the expected number of rows that get perturbed is n(1 − p). For uniform perturbation, a b fraction of these rows, i.e. n(1 − p)b rows, will be expected to lie within the <ref type="bibr">[low, high]</ref> range. The total number of rows observed in range <ref type="bibr">[low, high]</ref> in T ′ , n r , can be seen as the sum of those rows that were decided to be perturbed into <ref type="bibr">[low, high]</ref> (from outside, or perturbed and retained within the interval) and those rows that were unperturbed in the original interval. Subtracting the n(1 − p)b perturbed rows from n r , we get an estimate for the number of unperturbed rows, with values in <ref type="bibr">[low, high]</ref> in T . This is scaled up by 1/p to get the total number of original rows in T in <ref type="bibr">[low, high]</ref>, as only a p fraction of rows were retained. The fraction f of rows originally in <ref type="bibr">[low, high]</ref> is therefore estimated as</p><formula xml:id="formula_3">f ′ = n o n = n r pn − (1 − p)(high − low) p(max − min) .</formula><p>Not only is the above estimator a Maximum Likelihood Estimator (MLE) as shown in Section 4.2, it reconstructs an approximate answer with high probability. THEOREM 1. Let the fraction of rows in <ref type="bibr">[low, high]</ref> in the original table f be estimated by f</p><formula xml:id="formula_4">′ , then f ′ is a (n, ǫ, δ) estimator for f if n ≥ 4 log( 2 δ )(pǫ) −2 .</formula><p>We now formalize the above reconstruction procedure. This formalization provides the basis for the reconstruction of multiple columns in Section 4.2.  <ref type="table" target="#tab_0">As only table T</ref> ′ is available, x is estimated using the constraint xA = y, which gives the estimator x = yA −1 . Here A is the following transition matrix</p><formula xml:id="formula_5">Let vector y = [y 0 , y 1 ] = [count(¬P), count(P)]</formula><formula xml:id="formula_6">(1 − p)a + p (1 − p)b (1 − p)a (1 − p)b + p .</formula><p>The element in the first row and first column of A, a 00 = (1− p)a+ p is the probability that an element originally satisfying ¬P in T after perturbation satisfies ¬P in T ′ . This probability was calculated as the sum of the probabilities of two disjoint events. The first being that the element is retained, which occurs with probability p. The second being that the element is perturbed and after perturbation satisfies ¬P, which together has probability (1 − p)a. The element a 01 is the probability that an element satisfying ¬P in T after perturbation satisfies P in T ′ . The element a 10 is the probability that an element satisfying P in T after perturbation satisfies ¬P in T ′ .</p><p>The element a 11 is the probability that an element satisfying P in T after perturbation satisfies P in T ′ . Their values were similarly derived.</p><p>If y = [n − n r , n r ] and x = [n − n o , n o ], the solution to the equation below gives the same estimator as derived earlier:</p><formula xml:id="formula_7">n − n o n o (1 − p)a + p (1 − p)b (1 − p)a (1 − p)b + p = n − n r n r .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Reconstructing Multiple Column Aggregates</head><p>Assume now that the uniform retention replacement perturbation, with retention probability p, has been applied to each of k columns of a table, T . Consider the aggregate query count(P 1 ∧ P 2 ∧ ...P k ) on table T . In practice k is small.</p><p>We create a k × 2 matrix, R, with k rows and 2 columns, having 1 row for each query column. R i,1 gives the probability that a number randomly selected from the replacing p.d.f. for column i will satisfy predicate P i , while R i,0 is the probability of the complementary event, that a number selected from the replacing p.d.f. will satisfy ¬P i .</p><p>Take for instance the query, Q=count(age <ref type="bibr">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr">[32]</ref><ref type="bibr">[33]</ref><ref type="bibr">[34]</ref><ref type="bibr">[35]</ref><ref type="bibr">[36]</ref><ref type="bibr">[37]</ref><ref type="bibr">[38]</ref><ref type="bibr">[39]</ref><ref type="bibr">[40]</ref><ref type="bibr">[41]</ref><ref type="bibr">[42]</ref><ref type="bibr">[43]</ref><ref type="bibr">[44]</ref><ref type="bibr">[45]</ref> ∧ salary[50k-120k] ∧ house-rent ) with the domains for age, salary and house-rent being </p><formula xml:id="formula_8">, [25k-200k], [500-2500]. Then R will be [[0.85, 0.15], [0.6, 0.4], [0.65, 0.35]]</formula><p>, since the first column being age <ref type="bibr">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr">[32]</ref><ref type="bibr">[33]</ref><ref type="bibr">[34]</ref><ref type="bibr">[35]</ref><ref type="bibr">[36]</ref><ref type="bibr">[37]</ref><ref type="bibr">[38]</ref><ref type="bibr">[39]</ref><ref type="bibr">[40]</ref><ref type="bibr">[41]</ref><ref type="bibr">[42]</ref><ref type="bibr">[43]</ref><ref type="bibr">[44]</ref><ref type="bibr">[45]</ref> </p><formula xml:id="formula_9">implies R 1,1 = (45 − 30)/(100 − 0) = 0.15, while R 1,0 = 1 − 0.15 = 0.85, etc.</formula><p>As stated earlier, to answer the query count(P 1 ∧ P 2 . . . P k ), we ask 2 k aggregate queries on the perturbed table, T ′ . The 2 k answers on perturbed table T ′ are converted into estimated answers to these 2 k aggregate queries on the original table T, which includes the estimated answer to the original query.</p><p>Let y be a row vector of size 2 k that has the answers to the above queries on perturbed table T ′ , and let x be a row vector of size 2 k that has the reconstructed estimated answers to the queries on original table T . We order the answers to the 2 k queries in vectors x, y using the bit representation of the vector index as shown in <ref type="figure">Figure 2</ref>. Let Q(r, 1) denote the predicate(P r ) on the r th column of query Q, and Q(r, 0) its negation (¬P r ). Let bit(i, r) denote the r th</p><formula xml:id="formula_10">Query Estimated on T Evaluated on T ′ count(¬P 1 ∧ ¬P 2 ) x 0 y 0 count(¬P 1 ∧ P 2 ) x 1 y 1 count(P 1 ∧ ¬P 2 ) x 2 y 2 count(P 1 ∧ P 2 ) x 3 y 3</formula><p>Figure 2: Answering query count(P 1 ∧ P 2 )</p><p>bit from the left in the binary representation of the number i using k bits. Then,</p><formula xml:id="formula_11">x i = count(∧ k r=1 Q(r, bit(i, r))) in T , for 0 ≤ i ≤ 2 k − 1; y i = count(∧ k r=1 Q(r, bit(i, r))) in T ′ , for 0 ≤ i ≤ 2 k − 1.</formula><p>For example, for the query count(age <ref type="bibr">[30]</ref><ref type="bibr">[31]</ref><ref type="bibr">[32]</ref><ref type="bibr">[33]</ref><ref type="bibr">[34]</ref><ref type="bibr">[35]</ref><ref type="bibr">[36]</ref><ref type="bibr">[37]</ref><ref type="bibr">[38]</ref><ref type="bibr">[39]</ref><ref type="bibr">[40]</ref><ref type="bibr">[41]</ref><ref type="bibr">[42]</ref><ref type="bibr">[43]</ref><ref type="bibr">[44]</ref><ref type="bibr">[45]</ref> </p><formula xml:id="formula_12">∧ salary[50k-120k] ∧ house-rent[700-1400]), y[6 10 ] = y[110 2 ] = count(age[30-45] ∧ salary[50k-120k] ∧ ¬ house-rent[700 − 1400])</formula><p>By a single scan through the perturbed table T ′ vector y can be calculated. Vector x is reconstructed from vector y using the matrix inversion technique or the iterative Bayesian technique described below. The data analyst may either be interested only in the component x 2 k −1 , which is the answer to the count(∧ k r=1 P r ) query on T , or she may be interested in the entire vector x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Matrix Inversion technique</head><p>If p r is the retention probability for the r th column, we calculate vector x from vector y as x = yA −1 . The transition matrix, A, with 2 k rows and 2 k columns, can be calculated as the tensor product <ref type="bibr" target="#b14">[15]</ref> of matrices</p><formula xml:id="formula_13">A = A 1 ⊗ A 2 ⊗ A 3 .... ⊗ A k</formula><p>where the matrix A r , for 1 ≤ r ≤ k is the transition matrix for column r (see Section 4.1).</p><formula xml:id="formula_14">A r = (1 − p r )a r + p r (1 − p r )b r (1 − p r )a r (1 − p r )b r + p r</formula><p>where b r = R r,1 and a r = R r,0 = 1 − R r,1 . The entries of the tensor product matrix, A, can be explicitly calculated to be</p><formula xml:id="formula_15">a i j = k r=1 ((1 − p r ) × R r,bit( j,r) + p r × δ (bit(i,r),bit( j,r)) ), ∀0 ≤ i &lt; 2 k , 0 ≤ j &lt; 2 k where δ (c,d) = 1 if c = d, and 0 if c d, for c, d ∈ {0, 1}.</formula><p>We split the space of possible evaluations of a row into 2 k states, according to which of the 2 k mutually exclusive predicate combinations the row satisfies. We say a row is said to belong to state i if it satisfies the predicate ∧ k r=1 Q ( r, bit(i, r)). For example, from <ref type="figure">Figure 2</ref>, a row in state 0 satisfies ¬P 1 ∧ ¬P 2 while a row in state 1 satisfies ¬P 1 ∧ P 2 etc.</p><p>The entry a i j of matrix A above represents the probability that a row belonging to state i in T , after perturbation belongs to state j in T ′ . As each column was independently perturbed the probability of transition from state i to state j is the product of the probabilities for the transitions on all columns. The contribution from the r th column to the transition probability is the sum of (1 − p r ) × R r,bit( j,r) , if the element was decided to be perturbed, and p r × δ (bit(i,r),bit( j,r)) , if the element was decided to be retained. The term δ (bit(i,r),bit( j,r)) ensures that the retention probability p r adds up only if the source and destination predicates on the r th column are the same for states i and j. Thus the probability of transition from state i to state j on the r th column is (1− p r )×R r,bit( j,r) + p r ×δ (bit(i,r),bit( j,r)) . The product of this probability over all columns gives the probability of transition from state i to state j, a i j .</p><p>THEOREM 2. The vector x calculated as A −1 y is the maximum likelihood estimator (MLE) of the relaxed a priori distribution ( i x i = n and 0 ≤ x i ≤ n are the exact constraints, the relaxed constraint only ensures i x i = n) on the states that generated the perturbed table.</p><p>The multiple column aggregate is (n, ǫ, δ) reconstructible, is shown by applying the Chernoff bound, to bound the error in y, and then bounding the error added during inversion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Iterative Bayesian technique</head><p>Let vectors x and y of size 2 k be the a priori distribution on states of the original rows, and posteriori distribution on states of perturbed rows, as introduced above. Let the original states of rows in T selected from the a priori distribution be given by random variables U 1 , U 2 , ....U n , while the states of the n perturbed rows in T ′ be given by the random variables V 1 , V 2 , ...V n . Then for 0 ≤ p, q ≤ t = (2 k − 1) and 1 ≤ i ≤ n, we have Pr(V i = q) = y q /n, and Pr(U i = p) = x p /n. Also Pr(V i = q|U i = p) = a pq is the transition probability from state p to q. From Bayes rule, we get We iteratively update x using the equation</p><formula xml:id="formula_16">Pr(U i = p|V i = q) = P(V i = q|U i = p)P(U i = p) P(V i = q) = P(V i = q|U i = p)P(U i = p) t r=0 P(V i = q|U i = r)P(U i = r) = a</formula><formula xml:id="formula_17">Pr(U i = p) = t q=0 Pr(V i = q)Pr(U i = p|V i = q).</formula><p>This gives us the update rule,</p><formula xml:id="formula_18">x T +1 p = t q=0 y q a pq x T p t r=0 a rq x T r ,</formula><p>where vector x T denotes the iterate at step T , and vector x T +1 the iterate at step T + 1. We initialize the vector, x 0 = y, and iterate until two consecutive x iterates do not differ much. This fixed point is the estimated a priori distribution. This algorithm is similar to the iterative procedure proposed in <ref type="bibr" target="#b3">[4]</ref> for additive perturbation and shown in <ref type="bibr" target="#b1">[2]</ref> to be the Expectation Maximization (EM) algorithm converging to the Maximum Likelihood Estimator (MLE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Error in Reconstruction</head><p>We provide here a brief analysis of the error in the reconstruction procedures. A quantitative analysis of the magnitude of error is easy for the inversion method, but such an analysis is much harder for the iterative method. Due to the randomization in the perturbation algorithm there are errors in the transition probabilities in matrix A. This causes y, the posteriori distribution after perturbation calculated from T ′ , to have errors. Hence the reconstructed x will have errors.</p><p>The error decreases as the number of rows, n, increases. Let a ′ i j denote the actual fraction of original rows of state i that were converted to state j. Then as n increases, a i j will be a closer approximation to a ′ i j . The error decreases as n −0.5 as indicated by Theorem 1, and verified empirically in Section 7.</p><p>The error in reconstruction increases as the number of reconstructed columns, k, increases, and the probability of retention, p, decreases. The largest and smallest eigenvalues of A can be shown to be 1 and p k respectively and the condition number of the matrix A grows roughly as p −k (see Section 7). The condition number of a matrix is a good indicator of the error introduced during inversion <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">GUARANTEES AGAINST PRIVACY BREACHES</head><p>Private data from multiple clients is perturbed before being integrated at the server. In this section, we formalize the privacy obtained by this perturbation.</p><p>The notion of a (ρ 1 , ρ 2 ) privacy breach was introduced in <ref type="bibr" target="#b8">[9]</ref>. We extend this to introduce a new privacy metric, called the (s, ρ 1 , ρ 2 ) privacy breach. Consider a database of purchases made by individuals. It is quite likely that many people buy bread, but not many buy the same prescription medicine. The new metric is more concerned about whether an adversary can infer from the randomized row which medicine a person bought, and is less concerned about the adversary determining with high probability that the original row had bread, as most individuals buy bread and it does not distinguish the individual from the rest of the crowd.</p><p>Assume that the adversary has access to the entire perturbed table T ′ at the server, and the exact a priori distribution on the unperturbed data (which can be reconstructed <ref type="bibr" target="#b3">[4]</ref>) <ref type="bibr" target="#b0">1</ref> . Also assume that any external information is already incorporated into the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Review of (ρ 1 , ρ 2 ) Privacy Breach</head><p>Consider a data element of domain V X perturbed by a perturbation algorithm into another domain V Y . DEFINITION 4. (ρ 1 , ρ 2 ) Privacy Breach <ref type="bibr" target="#b8">[9]</ref>: Let Y denote the random variable corresponding to the perturbed value and X that corresponding to the original value obtained from the a priori distribution. We say that there is a (ρ 1 , ρ 2 ) privacy breach with re-</p><formula xml:id="formula_19">spect to Q ⊆ V X if for some S ⊆ V Y P[X ∈ Q] ≤ ρ 1 and P[X ∈ Q|Y ∈ S ] ≥ ρ 2 where 0 &lt; ρ 1 &lt; ρ 2 &lt; 1 and P[Y ∈ S ] &gt; 0.</formula><p>In retention replacement perturbations, which are of interest to us, the column is perturbed back into the same domain, and hence V X = V Y . Let S ⊆ V X , with P[X ∈ S ] = p s , for X ∈ o V X where ∈ o represents selecting an element from V X according to the a priori distribution on V X . Let P[Y ∈ S ] = m s , for Y ∈ r V X , where ∈ r represents selecting an element from V X according to the replacing distribution, which is different from the distribution of the perturbed table. The ratio p s /m s is called the relative a priori probability of the set S .</p><p>The relative a priori probability is a dimensionless quantity that represents how frequent a set is according to its a priori probability as compared to the replacing p.d.f. (the uniform p.d.f.). In a database of purchases, medicines will have low relative a priori probability since different people take different medicines, while bread will have high relative a priori probability. Let S ⊆ V X , we say that there is a (s, ρ 1 , ρ 2 ) privacy breach with respect to S if the relative a priori probability of S , p s /m s &lt; s, and if P[X ∈ S ] = p s ≤ ρ 1 and P[X ∈ S |Y ∈ S ] ≥ ρ 2 where 0 &lt; ρ 1 &lt; ρ 2 &lt; 1 and P[Y ∈ S ] &gt; 0.</p><p>The value of s in the privacy breach is addressed by the next result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>THEOREM 3. The median value of relative a priori probability, over all subsets S , S ⊆ V X , is 1.</head><p>We define rare sets as those that have relative a priori probability smaller than 1. We next show that privacy breaches do not happen for rare sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Single Column Perturbation</head><p>THEOREM 4. Let p be the probability of retention, then uniform perturbation applied to a single column is secure against a (s, ρ 1 , ρ 2 ) breach, if</p><formula xml:id="formula_20">s &lt; (ρ 2 − ρ 1 )(1 − p) (1 − ρ 2 )p .</formula><p>As a concrete example, for uniform perturbation, with p=0.2, there are no (68, 0.1, 0.95) breaches. This means for any set S , if ρ 2 &gt; 0.95 with uniform perturbation, ρ 1 will be large (&gt; 0.1) when p s /m s &lt; 68. In fact, for a rare set, with s &lt; 1, there will be no (0.937, 0.95) privacy breaches in the original (ρ 1 , ρ 2 ) model for this perturbation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multiple</head><formula xml:id="formula_21">≤ i ≤ k. Let S = S 1 × S 2 × . . . S k , then S ⊆ D. Let P[S ] = p S 1 ×S 2 ×...S k = p s (say) be the a priori probability of S . Let P[Y i ∈ S i ] = m S i , for Y i ∈ α i D i ,</formula><formula xml:id="formula_22">≤ i ≤ k. Then P[Y ∈ S ] = m S 1 m S 2 ..m S k = m s (say) for Y = (Y 1 , Y 2 , . . . Y k ) ∈ α D,</formula><p>where ∈ α denotes selecting randomly from the replacing p.d.f. for each column independently. p s /m s , the relative a priori probability, is the ratio of the a priori probability to the replacing probability, of the combination of values for the columns together. Correlated columns with higher a priori probabilities have larger values of p s /m s . THEOREM 5. There will not be a (ρ 1 , ρ 2 ) privacy breach with respect to (S 1 × S 2 × . . .</p><formula xml:id="formula_23">S k ) = S ⊆ D, if p s m s &lt; ρ 2 (1 − ρ 1 )(1 − p) k (1 − ρ 2 ) k i=1 ((1 − p)m S i + p)</formula><p>. S i denotes the subset on column i within which the original value must be identified for the privacy breach. In the case, S i denotes a single value or a small range within the domain of a continuous column, hence (1 − p)m S i ≪ p. We approximate (1 − p)m S i + p by p to get</p><formula xml:id="formula_24">ρ 2 (1 − ρ 1 )(1 − p) k (1 − ρ 2 ) k i=1 ((1 − p)m S i + p) ≥ ρ 2 (1 − ρ 1 )(1 − p) k (1 − ρ 2 )p k (1 − ǫ)</formula><p>for some small constant ǫ. Thus for some small constant ǫ, uniform perturbation applied individually to k columns is secure against (s, ρ 1 , ρ 2 ) breaches for</p><formula xml:id="formula_25">s &lt; ρ 2 (1 − ρ 1 )(1 − p) k (1 − ρ 2 )p k (1 − ǫ).</formula><p>As an example, for uniform perturbation with p=0.2 applied independently to two columns, there are no (273,0.1,0.95) breaches for joint events on the columns (when m S i are small).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXTENSIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Categorical Data</head><p>Consider a categorical column, C, having discrete domain D. Let S ⊆ D. A predicate P, on column C, using S is defined as</p><formula xml:id="formula_26">P(x) = true if x ∈ S f alse otherwise.</formula><p>Given the a priori and replacing p.d.f. on D, the reconstruction algorithms in Section 4 and the privacy guarantees in Section 5 can be directly applied to the categorical data by computing the probability of the predicate, P, being true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Alternative Retention Replacement Schemes</head><p>Our analysis so far considered retention replacement perturbations where the replacing p.d.f is the uniform distribution. We now discuss some other interesting retention replacement schemes:</p><p>1 Identity perturbation: If the original data element is decided to be perturbed, the data element is replaced by a random element selected uniformly among all data elements <ref type="bibr" target="#b17">[18]</ref> (i.e. the replacing p.d.f. is the same as the a priori distribution).</p><p>2 Swapping: Swapping is closely related to identity perturbation. In swapping with probability p we retain a data element, and with probability (1 − p) we decide to replace it. Numbers decided to be replaced are then randomly permuted amongst themselves. Identity perturbation and swapping are different from uniform perturbation which is a local perturbation. Identity perturbation can be local if there is knowledge of the a priori distribution before perturbation. Swapping is not a local perturbation and requires multiple rows at the client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Reconstructing Aggregates</head><p>Identity perturbation and swapping do not affect the answers to single column aggregate queries, i.e. answers to single column aggregate queries on the perturbed table, T ′ , are returned directly as answers to those queries on the original table, T . The difference in multi-column reconstruction for identity perturbation and swapping as compared to uniform perturbation is in the evaluation of vector R in Section 4.2. Recall that R i,1 is the probability that an element selected from the replacing p.d.f. on column i satisfies the predicate on the i th column, P i . The replacing p.d.f. (which is the original p.d.f. for identity perturbation and swapping) is required for reconstruction. This requires the server to have the original p.d.f. for each column. This requirement is however obviated by the observation in the previous paragraph, that the fraction of elements satisfying P i in T is the same as the fraction of elements satisfying P i in T ′ . Hence R i,1 can be calculated from T ′ .</p><p>R i,0 as before is calculated as 1 − R i,1 . The reconstruction error after identity perturbation and swapping will be smaller than that compared to uniform perturbation for sets, S , with small relative a priori probability. This is because in uniform perturbation the noise due to the perturbed data elements that now belong to S , but did not before perturbation, exceeds significantly the number of data elements that were in S originally and retained during perturbation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Guarantees against Privacy Breaches</head><p>The guarantees for identity perturbation and swapping can be obtained using m S i = p S i in Theorems 4 and 5. As an example we restate Theorem 4 for identity perturbation. LEMMA 1. For a single column, identity perturbation is secure against (s, ρ 1 , ρ 2 ) privacy breaches for</p><formula xml:id="formula_27">ρ 1 &lt; ρ 2 − p 1 − p .</formula><p>PROOF: For identity perturbation, m s = p s , hence p s /m s = 1 ∀S . Repeating the argument in Theorem 4 we get (ρ 2 − ρ 1 )(1 − p) &gt; (1 − ρ 2 )p, which implies the result. The above (ρ 1 , ρ 2 ) guarantee for identity perturbation is independent of the subset S . Uniform perturbation gives better (ρ 1 , ρ 2 ) guarantees for a set of rare data elements, i.e. a set with p s /m s &lt; 1 and worse for sets with p s /m s &gt; 1. Identity perturbation and swapping have a privacy breach in the presence of external knowledge about rare values (eg. the largest or smallest value). Rare values need to be suppressed (i.e. blanked out) <ref type="bibr" target="#b16">[17]</ref> for privacy with these perturbations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Application to Classification</head><p>We show how aggregate queries on multiple columns can be used for privacy preserving construction of decision trees <ref type="bibr" target="#b3">[4]</ref>. Consider the tree in <ref type="figure" target="#fig_3">Figure 3</ref>  Now consider the third split, on age once again, but this time (age &lt; 21), is decided after the queries count(age  </p><formula xml:id="formula_28">∧ salary[25k- 100k] ∧ ¬ Q ), count(age[21-30] ∧ salary[25k-100k] ∧ ¬ Q ) count(age[0-21] ∧ salary[25k-100k] ∧ Q ) and count(age[21-30] ∧ salary[25k-100k] ∧ Q ) are reconstructed for T .</formula><p>The number of columns in the count query did not increase at this split on age, which was already present among the original set of queried columns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">EXPERIMENTS</head><p>We next present an empirical evaluation of our algorithms on real as well as synthetic data. For real data, we used the Adult dataset, from the UCI Machine Learning Repository <ref type="bibr" target="#b4">[5]</ref>, which has census information. The Adult dataset contains about 32,000 rows with 4 numerical columns. The columns and their ranges are: age <ref type="bibr">[17 - 90]</ref>, fnlwgt <ref type="bibr">[10000 -1500000]</ref>, hrsweek <ref type="bibr">[1 -100]</ref> and edunum <ref type="bibr">[1 - 16]</ref>.</p><p>For synthetic data, we used uncorrelated columns of data having Zipfian distribution with zipf parameter 0.5. We create three such tables with different number of rows. The number of rows is varied in factors of 10 from 10 3 to 10 5 . The frequencies of occurrences are such that the least frequent element occurs 5 times. This results in the number of distinct values to be approximately one tenth of the number of rows in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Randomization and Reconstruction</head><p>In this Section we assume that the vectors, x, y described in Section 4.2 have been normalized, i.e. all elements have been divided by n, the number of rows, so that the sum of the elements of each vector is 1. These vectors will also be referred to as probability density function <ref type="formula">(</ref>    Reconstruction algorithms: We first study the reconstruction error while reconstructing multiple columns of the Adult dataset for varying retention probabilities. The predicates being reconstructed are age , fnlwgt  and hrsweek <ref type="bibr">[30- 60]</ref>. <ref type="figure" target="#fig_6">Figure 4</ref> shows the errors on first two among the above predicates while <ref type="figure" target="#fig_2">Figure 5</ref> shows the errors on all three predicates. The retention probability, p, plotted on the x-axis, is the same for all columns. The reconstruction error is plotted on the y axis. There are three curves in each figure. The curve randomized, shows the l 1 norm of the difference between the perturbed p.d.f. vector y and the original p.d.f. vector x ′ . It serves as a baseline to study the reduction in error after reconstruction of y to x. The other two curves represent the reconstruction errors after the iterative and the inversion algorithms. The iterative procedure gives smaller errors than the inversion procedure, especially when a larger number of columns are reconstructed together, and the probability of retention, p, is small. This is reconfirmed later by <ref type="figure">Figures 7 and 8</ref>, and similar experiments on synthetic data (which we do not show for the lack of space). This may seem unintuitive as the inversion algorithm was shown to give the MLE estimator for x, satisfying i x i = 1 (after normalization). This can be explained by noting that the iterative algorithm gives the MLE estimator in the constrained space, i.e. for the subspace of i x i = 1 that satisfies 0 ≤ x i ≤ 1 ∀i. Since the number of rows are always non-negative, this is the subspace that contains the exact original p.d.f. vector x ′ . When the retention probability decreases, and the number of columns to be reconstructed increases, the error during randomization and reconstruction increases, and the inversion algorithm may return a point outside the constrained space. The reconstruction error by the inversion method can grow arbitrarily. However, the iterative algorithm being constrained, will have a reconstruction error of at most two. Condition number: <ref type="figure" target="#fig_9">Figure 6</ref> shows the condition number <ref type="bibr" target="#b13">[14]</ref> of the transition matrix using a logarithmic scale on the y axis, and the number of columns reconstructed on the x axis, for different retention probabilities (p= 0.2, 0.5 etc.). The selectivity of each predicate is set to 0.5. The condition number (which is independent of the dataset) increases as the retention probability decreases and increases exponentially as the number of columns reconstructed increase. The condition number is a good indicator of the reconstruction error by the inversion algorithm <ref type="bibr" target="#b13">[14]</ref>, and by the iterative Bayesian algorithm at small error values. Unlike the continuous exponential growth in error as the number of reconstructed columns increases for the inversion algorithm, the error flattens out for the iterative algorithm, as it is bounded above by two as discussed earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Scalability</head><p>Next we study, how the reconstruction error varies as the number of columns reconstructed, retention probability, number of rows, and selectivity of the predicates vary. Number of columns and retention probability: We study the reconstruction errors for varying number of columns and retention probabilities on the Adult dataset by the iterative and inversion algorithms. The predicates being reconstructed are age <ref type="bibr">[ 25 -45]</ref>, fnlwgt <ref type="bibr">[ 100000 -1000000]</ref>, hrsweek <ref type="bibr">[ 30 -60]</ref> and edulevel <ref type="bibr">[ 5 - 10]</ref>. For the i ( 1 ≤ i ≤ 4 ) column experiment, the first i among the above predicates are selected in the query. <ref type="figure">Figure 7</ref> shows the reconstruction errors with the iterative algorithm, while <ref type="figure">Figure 8</ref> shows the reconstruction errors with the inversion algorithm. Both iterative and inversion algorithms show an exponential increase in the error as the number of columns increases and as the probability of retention decreases. For smaller number of columns and higher retention probabilities both algorithms give comparable reconstruction errors. However for larger number of columns and lower retention probabilities the iterative algorithm gives smaller errors than the inversion algorithm. As explained in Section 7.1, unlike the iterative method, the reconstruction error by the inversion method can grow arbitrarily, whereas the error by the iterative method flattens out after an initial exponential increase. For all experiments on the Zipfian dataset, the predicate on each column has an independent selectivity of 0.5. <ref type="figure" target="#fig_11">Figure 9</ref> shows the reconstruction error after the iterative algorithm is applied to the perturbed Zipfian dataset of size 10 5 . The figure shows the increase in the reconstruction error, plotted on the y axis, for increasing number of columns, plotted on the x axis, for different retention probabilities. After an initial exponential increase, the reconstruction error flattens out. Number of rows in the table: <ref type="figure" target="#fig_0">Figure 10</ref> shows how the reconstruction error decreases as the number of perturbed rows available for reconstruction increase, for the the iterative reconstruction algorithm. In <ref type="figure" target="#fig_0">Figure 10</ref> the retention probabilities are varied while the number of columns remains fixed at 8. For large values of n the reconstruction error decreases as n −0.5 as suggested by Theorem 1. This is also ratified by the factor 10 displacement between the reconstruction error lines for 10 3 and 10 5 rows in <ref type="figure" target="#fig_0">Figures 11 and 12</ref>. As the number of rows increases, it is possible to reconstruct more columns together at smaller retention probabilities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Privacy Breach Guarantees</head><p>We study privacy breaches possible after perturbation on the Adult dataset. <ref type="figure" target="#fig_0">Figure 13</ref> and <ref type="figure" target="#fig_0">Figure 14</ref> show the maximum retention probability that avoids breaches for varying values of ρ 1 for fixed ρ 2 = 0.95, according to Theorem 5. To compute the values of s for sample predicates (subsets) of this dataset, we divide each column into 10 equiwidth intervals and consider predicates  that are subsets formed by the cross product of the intervals. Thus for two columns we consider 10 2 subsets and for three columns we consider 10 3 subsets. The maximum values of s were observed to be 15 and 30 for two and three columns respectively. The median value of s has been shown to be one in Theorem 3. The two figures plot the maximum retention probability, p, that would avoid a (s, ρ 1 , ρ 2 ) breach, on the y axis against the a priori probability, ρ 1 , on the x axis for different values of relative a priori probability, s. The values of s used are the maximum value of s, the median value s = 1, and s = 0.1 for a rare set. Both figures show that if it suffices to just hide rare properties (i.e., with s ≤ 0.1), then for ρ 1 &gt; 0.5, the retention probability p can be as high as 0.8. If we need to hide all the above properties, i.e. even for the largest s (the most common property), then for ρ 1 &gt; 0.5 the retention probability can be selected to be as high as p = 0.3. For p = 0.3 both <ref type="figure" target="#fig_6">Figure 4</ref> and <ref type="figure" target="#fig_2">Figure 5</ref> show low reconstruction error. Thus reconstructability of 2 and 3 aggregates together, and privacy of data elements, are both achieved by perturbation for the Adult dataset, with p = 0.3. Thus our experiments indicate (s, ρ 1 , ρ 2 )-privacy as well as multi-column aggregate reconstructability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">SUMMARY AND FUTURE WORK</head><p>The contributions of the paper are:</p><p>• We introduce the problem of privacy preserving OLAP in a distributed environment.</p><p>• We introduce the formalism for reconstructible functions on a perturbed table, and develop algorithms to reconstruct multiple columns together. We provide privacy guarantees that take into account correlations between any combination of categorical and numeric columns.</p><p>• We provide two reconstruction algorithms to work with retention replacement perturbation: an iterative Bayesian algorithm, and a matrix inversion algorithm that also yields the maximum likelihood estimator. These algorithms can reconstruct count aggregates over subcubes without assuming independence between columns.</p><p>• We evaluate proposed reconstruction algorithms both analytically and empirically. We study the privacy guarantees we get for different levels of reconstruction accuracy and show the practicality of our techniques.</p><p>• We show the use of our techniques to related applications like classification. Future work includes extending this work to other aggregates over subcubes.</p><p>by analyzing the <ref type="bibr">Hessian matrix, H, of l(x, λ)</ref>   Theorem 3. The median value of relative a priori probability, over all subsets S , S ⊆ V X , is 1. PROOF: Consider, any subset S ⊆ V X , and S = V X − S . Using notation as in Definition 5 we have p s + p s = 1 and m s + m s = 1. Hence if p s /m s ≥ 1 , we have p s /m s ≤ 1 and if p s /m s &lt; 1 we have p s /m s &gt; 1 Since this is true for any pair of complementary subsets, among all subsets of V X , half the subsets have relative a priori probability ≥ 1 and half ≤ 1. Hence the median value of s over all subsets of V X will be 1, if the median is not constrained to be one of the values attained. Let R denote the event that X was replaced and R c it being retained. For a (ρ 1 , ρ 2 ) privacy breach with respect to S we need P[X ∈ S ] ≤ ρ 1 . Also P[(X ∈ S )|(Y ∈ S )]</p><formula xml:id="formula_29">= P[(X ∈ S ) ∩ (Y ∈ S ) ∩ R] + P[(X ∈ S ) ∩ (Y ∈ S ) ∩ R c ] P[Y ∈ S ] ≤ ρ 1 (1 − p)m s + pp s (1 − p)m s + pp s</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Privacy preserving computation of multidimensional count aggregates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>be the answers on table T ′ , and let vector x = [x 0 , x 1 ] = [count(¬P), count(P)] denote the estimates for table T . Let b be defined as before and a = 1 − b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>DEFINITION 5 .</head><label>5</label><figDesc>(s, ρ 1 , ρ 2 ) Privacy Breach: Let Y denote the random variable corresponding to the perturbed value and X that corresponding to the original value obtained from the a priori dis- tribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Decision Tree Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>built on randomized table T ′ with schema (age, salary, house-rent, class-variable) to predict the column class- variable. The column class-variable can take two values: + and − representing high and low credit-risk (say). The private columns among age, salary, house-rent and class-variable, are each inde- pendently perturbed by a retention replacement perturbation. Let Q denote the predicate (class-variable = '+') while ¬Q denote the predicate (class-variable='-'). For the first split, say on (age &lt; 30), the gini index is calcu- lated using the estimated answers of the four queries: count(age[0- 30] ∧ ¬ Q), count(¬ age[0,30] ∧ ¬ Q), count(age[0-30]∧ Q ) and count(¬ age[0,30]∧ Q ) on T . Now consider the left sub- tree of elements having (age &lt; 30) using the predicate (salary &lt; 100k). We do not partition the randomized rows at any level in the decision tree. Previously with additive perturbation, random- ized rows were partitioned, and the columns were reconstructed independently [4]. With multi-column reconstruction the queries count(age[0-30] ∧ salary[25k-100k] ∧ ¬ Q), count(age[0,30] ∧ salary[100k-200k] ∧ ¬ Q ), count(age[0-30] ∧ salary[25k-100k] ∧ Q ) and count(age[0,30] ∧ salary[100k-200k] ∧ Q ) are recon- structed for T , to calculate the gini index or another split criterion at this level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>p.d.f.) vectors. x is the reconstructed p.d.f. vector, obtained by the inversion or iterative method in Section 4.2, while y is the p.d.f. vector on the perturbed table before reconstruction. Let the exact original value of the p.d.f. vector calculated directly on the unperturbed table, T , be x ′ . The l 1 norm of the difference between the estimated (x) and actual (x ′ ) p.d.f. vectors is used as the metric of error, and is referred to as the reconstruction error. The results of the reconstruction algorithm are quite accurate when the reconstruction error is much smaller than 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Reconstruction errors for conjunction of 2 predicates for Adult data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reconstruction errors for conjunction of 3 predicates for Adult data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Condition number of the transition matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Reconstruction errors for the Adult dataset for varying retention probabilities, p, by the iterative algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Reconstruction error by iterative method on Zipfian dataset with 10 5 rows varying number of columns</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Reconstruction error by iterative method on Zipfian dataset varying number of rows for 8 columns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Selectivity of the predicates:Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Absolute Error for the Zipfian dataset for p=0.2 for varying interval sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Privacy for two columns for Adult data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Privacy for three columns for Adult data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>x T Hx = − t h=0 φ h t i=0 t s=0 a ih a sh x i x s = − t h=0 φ h t i=0 a ih x i ( t s=0 a sh x s ) = − t h=0 φ h ( t s=0 a sh x s ) t i=0 a ih x i = − t h=0 φ h ( t s=0 a sh x s )( t i=0 a ih x i ) = − t h=0 φ h ( t i=0 a ih x i ) 2 ≤ 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Theorem 4 .</head><label>4</label><figDesc>Let p be the probability of retention, then uniform perturbation applied to a single column is secure against a (s, ρ 1 , ρ 2 ) breach, if s &lt; (ρ 2 − ρ 1 )(1 − p) (1 − ρ 2 )p PROOF: Let S ⊆ V X with P[S ] = p s according to the a priori distribution and P[S ] = m s according to the replacing p.d.f. Let X and Y denote the random variables for the original and perturbed value respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table T (A 1 , A 2 , . . . , A m ), where each column A i comes from a numeric domain. Each client C i contributes a row r i (a i 1 , a i 2 , . . . , a im ) to T . The server runs aggre- gate queries of the form select count( * ) from T where P j 1 and P j 2 . . . and P j k . Here P j i is a range predicate of the form a l i ≤ A j i ≤ a h i , denoted as A j i [a l i , a h i ]. We use count(P j 1 ∧ P j 2 . . . ∧ P j k ) to succinctly represent the above aggregate query.</head><label>T</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>DEFINITION 1 . Perturbation Algorithm: A perturbation al- gorithm α is a randomized algorithm that given a table T creates a table T ′ having the same number of rows and columns. We will denote the unperturbed table as T and the perturbed table as T ′ .and t ′ i j denote the value of the element in the i th row of the j th column in tables T and T ′ respectively. The perturbation algorithm is said to be local if t ′ i j depends only on t i j , while it is said to be global if t ′ i j depends on other elements in the j th column of T . Let D j denote the domain of elements in the j th column of T . D j is said to be continuous for numeric columnson D j . Both D j as well as the replacing p.d.f. on D j are public.(n, ǫ, δ) reconstructible by a function f ′ , if f ′ can be evalu- ated on the perturbed table T ′</head><label>1</label><figDesc></figDesc><table>The perturbation algorithm is public. However, the actual 

random numbers used by it are hidden. 
Let t i j , and discrete for 
categorical columns. For the class of perturbation algorithms we 
study, for every column being perturbed, we require the perturba-
tion algorithm to select a fixed probability density function (p.d.f.) 
on the column's domain. For the j th column we call this p.d.f. the 
replacing p.d.f. DEFINITION 2. Retention Replacement Perturbation: Re-
tention replacement perturbation is a perturbation algorithm, 
where each element in column j is retained with probability p j , and 
with probability (1 − p j ) replaced with an element selected from the 
replacing p.d.f. on D j . That is, 

t 

′ 

i j = 

t i j with probability p j 
element from replacing p.d.f. on D j with probability (1-p j ). 

If column j of the table can be revealed without perturbation we 
set p j = 1. 
Retention replacement perturbation, where the replacing p.d.f. is 
the uniform p.d.f. is called uniform perturbation. We assume that 
each column of the table T 
′ has been perturbed independently using 

uniform perturbation. In Section 6.2, we show that uniform pertur-
bation provides better privacy guarantees for rare events. Other 
alternatives and comparisons are also given in the same section. 

4. RECONSTRUCTION 

An aggregate function on the original table T , must be recon-
structed by accessing the perturbed table T 
′ . The accuracy of the 

reconstruction algorithm is formalized below by the notion of ap-
proximate probabilistic reconstructability. 

DEFINITION 3. Reconstructible Function: Given a perturba-
tion α converting table T to T 
′ , a numeric function f on T is said 

to be </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>table T ′ , we show how to estimate an answer to the query count(P) on T . Let tables T , T ′ each have n rows. Let n r = count(P) evaluated on table T ′ , while n o = count(P) estimated for table T . Given n r we estimate n o as</head><label>T</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Independently Perturbed Columns Let D i be the domain for column i in a k column table. Then the domain of the table, D = D 1 × D 2 ×. . . D k . Each. Let S i ⊆ D i be a subset of the domain of the i th column for 1</head><label></label><figDesc>column of the table is perturbed independently by a retention replacement perturbation scheme. There is an a priori probability distribution of the rows in ta- ble T</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>where ∈ α i denotes selecting ran- domly from the replacing p.d.f. on D i , for all 1</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>and showing x T Hx ≤ 0, for all x ∈ R t . Elements of H are given by, h si = ∂l ∂x s ∂x i = − m h=1 y h a ih a sh ( t j=0 x j a jh ) 2 ∀0 ≤ s, i ≤ m . x T Hx = t i=0 t s=0 h si x s x i = t i=0 t s=0 −</head><label></label><figDesc></figDesc><table>t 

h=0 

y h 
a ih a sh 
( 
t 
j=0 x j a jh ) 2 x s x i 

= − 

t 

i=0 

t 

s=0 

t 

h=0 

φ h a ih a sh x i x s 

where 
φ h = 
y h 
( 
t 
j=0 x j a jh ) 2 ≥ 0 
Thus 

</table></figure>

			<note place="foot">Intuitively suppose the probability of an event, (age ≤ 10) (say), according to the a priori probability is ≤ ρ 1 = 0.1 (say). After observing the perturbed value, if the posteriori probability of the same event increases to ≥ ρ 2 = 0.95 (say), then there is a (0.1,0.95) privacy breach with respect to the event (age ≤ 10). 5.2 (s, ρ 1 , ρ 2 ) Privacy Breach 1 From Section 4.1, the error in the reconstructed a priori distribution for very selective predicates is large. This adds to the privacy of the perturbed rows.</note>

			<note place="foot" n="2"> i.e. i x i = n and 0 ≤ x i ≤ n are the exact constraints, the relaxed constraint only ensures i x i = n</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Rajeev Motwani and Rajat Raina for discussions on identity perturbation and maximum likelihood estimators. We also thank Alexandre Evfimievski and an anonymous reviewer for insightful comments on the paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">APPENDIX</head><p>Theorem 1. Let the fraction of rows in <ref type="bibr">[low, high]</ref> in the original table, f be estimated by f ′ , then f ′ is a (n, ǫ, δ) estimator for f if n ≥ 4 log( 2 δ )(pǫ) −2 PROOF: Let Y i be the indicator variable for the event that the i th row (1 ≤ i ≤ n) is perturbed and the perturbed value falls within <ref type="bibr">[low, high]</ref>. </p><p>Let Z i be the indicator variable for the event that the i th randomized row falls in <ref type="bibr">[low, high]</ref>. We have Z i = X i +Y i , and Pr[Z i = 1] = q+r = t (say), and Pr[Z i = 0] = 1−t. Let Z = n i=1 Z i = n r , the number of randomized values in range <ref type="bibr">[low, high]</ref>. Since Z i 's are independent Bernoulli random variables, 0 ≤ t ≤ 1 and n ≥ 4 log( 2 δ )(pǫ) −2 × t, applying Chernoff bounds <ref type="bibr" target="#b6">[7]</ref> we get</p><p>Thus with probability &gt; 1 − δ, we have −npǫ</p><p>Theorem 2. The vector x calculated as A −1 y is the maximum likelihood estimator (MLE) of the relaxed a priori distribution 2 on the states that generated the perturbed table.</p><p>is the likelihood of the observations, V, given a probability distribution on the states,</p><p>x j a ji )) subject to the constraint t j=0 x j = n. This is equivalent to</p><p>where λ is the Lagrangian multiplier. If t j=0 x j −n &gt; 0 then setting λ to arbitrarily large positive value, you can minimize the term −λ( t j=0 x j − n) to an arbitrarily small negative number, similarly when t j=0 x j − n &lt; 0, as λ tends to −∞ the term becomes arbitrarily small. So the optimum ensures that the constraint t j=0 x j = n is satisfied. To maximize the expression, setting the partial derivatives to be zero we get, Thus at x, given by x = yA −1 , and λ = 1 we get a local maximum of l(x, λ). We show that the local maximum is the global maximum,</p><p>Hence for a (ρ 1 , ρ 2 ) privacy breach with respect to S , we need</p><p>Theorem 5. There will not be a (ρ 1 , ρ 2 ) privacy breach for (</p><p>.., X k ) be the random variable corresponding to the original value of the k column row from the a priori distribution on table T , and Y = (Y 1 , Y 2 , ....Y k ) that corresponding to the perturbed row, where each column is perturbed independently by a retention replacement perturbation. For</p><p>Suppose there is a (ρ 1 , ρ 2 ) privacy breach with respect to S , we need P[X ∈ S ] ≤ ρ 1 , and P[(X ∈ S )|(Y ∈ S )] ≥ ρ 2 Thus</p><p>Substituting values of U S , L S and noting that p s ≤ ρ 1 hence 1− p s ≥ 1 − ρ 1 , we get</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Security control methods for statistical databases: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wortmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Computing Surveys</title>
		<imprint>
			<date type="published" when="1989-12" />
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the design and quantification of privacy preserving datamining algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2001 ACM Symp. on Principles of Database Systems</title>
		<meeting>of the 2001 ACM Symp. on Principles of Database Systems</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information sharing across private databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2003 ACM SIGMOD Intl. Conf. on Management of Data</title>
		<meeting>of the 2003 ACM SIGMOD Intl. Conf. on Management of Data</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Privacy-preserving data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2000 ACM SIGMOD Intl. Conf. on Management of Data</title>
		<meeting>of the 2000 ACM SIGMOD Intl. Conf. on Management of Data</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Merz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards privacy in public databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Asymptotic efficiency for tests based on the sums of observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chernoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tools for privacy preserving distributed data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clifton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="28" to="34" />
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Limiting privacy breaches in privacy preserving data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2003 ACM Symp. on Principles of Database Systems</title>
		<meeting>of the 2003 ACM Symp. on Principles of Database Systems</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy preserving mining of association rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Evfimievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2002 ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining</title>
		<meeting>of the 2002 ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Disclosure risk assesment in statistical microdata protection via advanced record linkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Torra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="343" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient private matching and set intersection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pinkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Cryptology -EUROCRYPT</title>
		<meeting>Advances in Cryptology -EUROCRYPT</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">How to play any mental game -a completeness theorem for protocols with a honest majority</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Goldreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Micali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wigderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1987 Annual ACM Symp. on Theory of Computing</title>
		<meeting>of the 1987 Annual ACM Symp. on Theory of Computing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Loan</surname></persName>
		</author>
		<title level="m">Matrix computations. John Hopkins Series in the Mathematical Sciences</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Linear algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kunze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>Prentice-Hall Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing privacy and trust in electronic communities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st ACM Conference on Electronic Commerce</title>
		<meeting>of the 1st ACM Conference on Electronic Commerce</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model and methods for the microdata protection problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A J</forename><surname>Hurkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Tiourine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Official Statistics</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A data distortion by probability distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Privacy preserving data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pinkas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CRYPTO</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Controlled data-swapping techniques for masking public use microdata sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A J</forename><surname>Moore</surname></persName>
		</author>
		<idno>RR 96-04</idno>
	</analytic>
	<monogr>
		<title level="m">SRD Report</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>US Bereau of Census</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Maintaining data privacy in association rule mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rizvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Haritsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2002 Intl. Conf. on Very Large Data Bases</title>
		<meeting>of the 2002 Intl. Conf. on Very Large Data Bases</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Securing OLAP data cubes against privacy breaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jajodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wijesekera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2004 IEEE Symposium on Security and Privacy</title>
		<meeting>of the 2004 IEEE Symposium on Security and Privacy</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cardinality-based inference control in data cubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wijesekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jajodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Computer Security</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Randomized response: A survey technique for eliminating evasive answer bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Warner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Assoc</title>
		<imprint>
			<biblScope unit="issue">309</biblScope>
			<biblScope unit="page">60</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How to generate and exchange secrets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1986 Annual IEEE Symp. on Foundations of Computer Science</title>
		<meeting>of the 1986 Annual IEEE Symp. on Foundations of Computer Science</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
