<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Leveraging Cognitive Factors in Securing WWW with CAPTCHA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amalia</forename><surname>Rusu</surname></persName>
							<email>arusu@fairfield.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Fairfield University</orgName>
								<address>
									<settlement>Fairfield</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Docimo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Fairfield University</orgName>
								<address>
									<settlement>Fairfield</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Rusu</surname></persName>
							<email>rusu@rowan.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rowan University</orgName>
								<address>
									<settlement>Glassboro</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Leveraging Cognitive Factors in Securing WWW with CAPTCHA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Human Interactive Proofs systems using CAPTCHA help protect services on the World Wide Web (WWW) from widespread abuse by verifying that a human, not an automated program, is making a request. To authenticate a user as human, a test must be passable by virtually all humans, but not by computer programs. For a CAPTCHA to be useful online, it must be easy to interpret by humans. In this paper, we present a new method to combine handwritten CAPTCHAs with a random tree structure and random test questions to create a novel and more robust implementation that leverages unique features of human cognition, including the superior ability over machines in recognizing graphics and reading unconstrained handwriting text that has been transformed in precise ways. This combined CAPTCHA protects against advances in recognition systems to ensure it remains viable in the future without causing additional difficulties for humans. We present motivation for our approach, algorithm development, and experimental results that support our CAPTCHA in protecting web services while providing important insights into human cognitive factors at play during human-computer interaction.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most users of the WWW today are familiar with CAPTCHAs, which are presented to them as machineprinted text or sound samples to be interpreted. CAPTCHAs are typically used to prevent automated programs from gaining access to various Web resources for the purpose of spamming or other illegitimate use. CAPTCHA is needed because of the sheer volume of spam crossing the Internet and the agility and tenacity of spammers <ref type="bibr" target="#b11">[12]</ref>.</p><p>Artificial Intelligence (AI) experts consider CAPTCHA a win-win situation and point out that CAPTCHAs are useful even when broken for the insights provided to the field of AI <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b30">30]</ref>. While breaking CAPTCHAs can be useful for advancing the field of AI as well as Image Processing, Pattern Recognition, etc., the current usefulness of CAPTCHAs in protecting Web resources from widespread illegitimate use by automated programs must not be overlooked.</p><p>In this paper, we present the development of a new CAPTCHA-based Human Interactive Proofs (HIP)</p><p>authentication system to protect services on the WWW. In our system, users are authenticated as humans to gain access to Web services by correctly interpreting a tree structure with handwriting samples transformed according to specific principles of cognitive psychology, explained in greater detail in the next sections ( <ref type="figure" target="#fig_0">Figure 1</ref>). To correctly solve the challenge, the tree structure and handwriting samples must be segmented out and interpreted, a task that presents much difficulty for machines, while being trivial for humans. With this CAPTCHA, we further the work begun on handwritten CAPTCHAs <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>. As CAPTCHAs are currently a readily available, relatively low cost and easy to administer solution to protect Web resources, our goal is to provide a useful CAPTCHA to overcome the security and usability difficulties present in other CAPTCHAs <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34]</ref>. Such a CAPTCHA can also offer extremely valuable insights not only related to the parsing of handwriting. By using a tree structure with handwritten images, both of which must be parsed to pass the CAPTCHA test, we can also offer important insights for the fields of AI, Image Analysis, Graphics Recognition and others.</p><p>Tree drawings and handwriting are used in our CAPTCHA rather than the typical machine-printed text, both for the important advances to be gained in graphics and handwriting recognition fields if our CAPTCHA is broken, as well as for the security provided due to the extra challenges posed to machines. Human skill at interpreting basic drawings and handwriting, no matter the condition (i.e., rotated, occluded, or deformed) <ref type="bibr" target="#b20">[20]</ref>, is gained from an early age, while consistent machine recognition of graphics in general, and handwriting in particular, continues to be problematic mostly due to unconstrained writing style and segmentation, especially in the absence of a context <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26]</ref>. Moreover, when applying certain transformations to the handwriting and rendering it on a tree structure, the recognition drastically decreases. While humans are able to make use of certain aspects of perception and cognition to interpret transformed samples, this remains a difficult open problem for machines <ref type="bibr" target="#b9">[9]</ref>.</p><p>We begin by reviewing the concepts of both HIPs and CAPTCHAs. We then discuss advances in handwriting recognition and present human cognitive factors that relate to handwriting interpretation. In this context we introduce and discuss the Gestalt laws of perception and Geon theory related to human perception and reading skills to motivate the transformations we have applied to the images. The technical approach and methodology is then presented, as well as the findings of user studies and machine testing of our CAPTCHA to validate the usefulness of our system in protecting Web services. We conclude with important insights gained from our work and discuss possible future enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Overview of HIPs and CAPTCHA</head><p>The purpose of HIPs is to distinguish one class of users from another, most commonly humans from computer programs (also referred to as "machines") <ref type="bibr" target="#b0">[1]</ref>. CAPTCHA is the test used by HIPs to distinguish a human user from a machine by presenting a challenge that can only be passed by the human. CAPTCHAs leverage AI factors and similarly to the tests of Alan Turing <ref type="bibr" target="#b28">[28]</ref>, they determine whether a user is human or machine. CAPTCHAs differ from Turing Tests, however, by making the computer the judge and administrator, though the computer itself should not be able to pass the test. In a CAPTCHA, if the individual completing the challenge presented passes correctly, they gain access to the service they are requesting. Otherwise, they are deemed to be an illegitimate program and are not allowed access. For a CAPTCHA to be useful, it must be easily passable by virtually all human users but not by machines <ref type="bibr" target="#b30">[30]</ref>. If a CAPTCHA presents difficulty to machines, but also to humans, it has failed in its function <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b21">21]</ref>.</p><p>Primitive use of a commercial text-based riddle dates back to 1998 on the AltaVista search engine Web site (at altavista.com). Approximately two years later CAPTCHA was defined, along with the first commercial implementation by Carnegie Mellon University researchers. They set forth the basic properties of CAPTCHA: it must be automated so it can be administered by a computer program without human intervention, it must be public in that the test should not be unsolvable by machines simply because it is novel or the method or code is hidden, and it must be passable by virtually all humans but not by computer programs <ref type="bibr" target="#b30">[30]</ref>. Many text-based visual CAPTCHAs have been created from Gimpy <ref type="bibr">[10]</ref> to Baffletext <ref type="bibr" target="#b8">[8]</ref> to reCAPTCHA <ref type="bibr" target="#b19">[19]</ref>. An example text-based visual CAPTCHA is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Non-text visual CAPTCHAs have also been created, including those that leverage human cognitive abilities beyond word recognition, such as ARTiFACIAL <ref type="bibr" target="#b21">[21]</ref> where users are asked to identify facial features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivation for a Tree-based Handwritten CAPTCHA</head><p>While many CAPTCHAs have been created, more secure CAPTCHAs are needed to help secure the WWW against widespread abuse by programs posing as humans on the WWW. While a typical response to foil machine recognition to maintain the usefulness of CAPTCHAs is to make them harder for machines, care must be taken to ensure that human ease of use does not suffer. Accordingly, many current CAPTCHAs, both text and image based suffer from usability issues <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b34">34]</ref>. Intrinsic security flaws of various CAPTCHAs have also been found and various text-based and image-based CAPTCHAs have been broken <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">33]</ref>. For example, Mori and Malik from the University of California at Berkeley demonstrate how they were able to break the EZ-Gimpy CAPTCHA with a 92% rate of success and Gimpy with a 33% rate of success <ref type="bibr" target="#b16">[17]</ref>.</p><p>It is the need for a more efficient CAPTCHA that is both usable for humans while secure against machines, along with the insights to be gained from persistent problems in computer recognition of handwritten text and graphics <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b26">26]</ref> that motivates our approach. We combine a randomly generated tree structure with random test questions and mandatory interpretation of handwritten words transformed according to the Gestalt and Geon principles <ref type="figure" target="#fig_0">(Figure 1</ref>). This new challenge meets the criteria of being a CAPTCHA in that large quantities of human-like handwritten images can be automatically created via a synthetic handwriting tool <ref type="bibr" target="#b25">[25]</ref> and then transformed and rendered on a tree structure. Our stringent adherence to the Gestalt and Geon principles related to human perception of objects, including letters and words, to create our transformations ensures that our new CAPTCHA is still easily solvable by humans while presenting challenges to computer programs. The tree-based CAPTCHA featuring deformed handwritten images ( <ref type="figure" target="#fig_0">Figure 1</ref>) described in this paper addresses both security and usability aspects to create a viable alternative CAPTCHA to those in existence, while at the same time having the potential to add insights into the overlooked area of real time handwriting recognition and interpretation of complex multi-layer image based documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Technical Background</head><p>Challenge-response tests using visual recognition have been the most widely used type of CAPTCHA employed online to protect web services from abuse by automated programs. The purpose of this study and the approach developed is to expand the use of visual CAPTCHAs by inserting additional complexity for computers, while keeping the tests easy for humans to pass (again, if success rates for machines decrease but success rates for humans also decrease for a CAPTCHA, it is not viable <ref type="bibr" target="#b6">[6]</ref>). In this context we discuss several factors that we have leveraged in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Gestalt Principles and Geon Theory Factor</head><p>We have studied the Gestalt laws of perception and Geon theory and have used guiding principles of each to determine which very specific transformations can be applied to our handwriting samples to both assist human interpretation and present unique challenges to machine recognition. According to Gestalt principles, humans have a unique ability to make sense of pictures, even those that are incomplete or are marred in some way <ref type="bibr" target="#b14">[15]</ref>. Humans are able to make sense of images presented to them by relying on their senses, past experience, which shapes how they view data currently, and what they are expecting to see. Humans are able to filter out irrelevant data such as noise or extra pieces in an image in order to interpret it. Gestalt principles are based on the fact that humans typically experience things that are outside of the range of simple perception. Humans tend to group information and interpret the whole rather than looking at individual pieces and then combining them. This is similar to the theory of holistic word recognition where the word is seen as an indivisible unit rather than as a series of individual parts which can be interpreted separately and then reassembled for recognition <ref type="bibr" target="#b15">[16]</ref>.</p><p>The Gestalt laws that aid human recognition of objects with transformations applied include proximity, similarity, symmetry, continuity, closure, familiarity and figure-ground as follows:</p><p>Proximity: how objects are grouped together by distance from or location to each other.</p><p>Similarity: how elements that are similar to each other tend to be viewed as part of a singular group.</p><p>Symmetry: how objects are grouped into figures according to symmetry and meaning.</p><p>Continuity: how objects are grouped according to flow of lines or alignment.</p><p>Closure: how elements are grouped together if they tend to complete some pattern, allowing perception of objects that are visually absent.</p><p>Familiarity: how elements are more likely to be interpreted as part of a group if they appear familiar to the viewer.</p><p>Figure-ground distinction: how a scene is broken up into foreground (the object of interest) and background (the rest of the scene) which is what allows an object to be distinguished from its surroundings.</p><p>Other human cognitive factors at play in recognition are memory, internal metrics, familiarity of letters and letter orientation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>.</p><p>Human perception relies, in the end, on all of the Gestalt principles working together. In addition to using the Gestalt laws of perception to determine which transformations may be applied to CAPTCHAs to capitalize on machine recognition weaknesses and simultaneous human strengths, the Geon theory of pattern recognition is also useful to determine which core components must be present in a transformed image so that it is still interpretable by humans. Two key aspects of geons are edges and intersections. The importance of these has been tested on images where various parts were deleted <ref type="bibr" target="#b3">[3]</ref>. Recognition for humans is easy if an object"s geons can be recognized and edges and intersections are a critical part in recognition. We have made use of the Gestalt principles and Geon theory in development of our CAPTCHA through specific handwriting image transformations to ensure human legibility while foiling machines. Similar transformations can successfully be applied to the tree structure as well as any shape or object in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Handwriting Recognition Factor</head><p>Recognition of unconstrained handwriting, especially when it has certain transformations applied to it, continues to be a challenge for automatic recognition systems <ref type="bibr" target="#b26">[26]</ref> while humans maintain a superior ability due to the Gestalt laws of perception <ref type="bibr" target="#b14">[15]</ref> and Geon theory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3]</ref>. Part of the problem for machines is that natural variability in handwriting exists at a level that does not exist in machine-printed text <ref type="bibr" target="#b26">[26]</ref>. Our tests show that natural handwriting variability, as well as defects applied such as occlusions or fragmentations <ref type="figure" target="#fig_3">(Figure 4b</ref>), can currently be overcome by humans due to cognitive factors, but not by machines. Segmentation, or the ability for a machine to determine character and word boundaries, continues to be a problem <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b26">26]</ref>. Handwriting presents more segmentation issues for machines than machine-printed text making handwriting arguably superior to machineprinted text for use in a secure CAPTCHA.</p><p>While advances have been made in handwriting recognition and applications have found their place in certain contexts such as the US postal services <ref type="bibr" target="#b27">[27]</ref>, or bank check reading, these contexts are usually well known in the sense that a relatively small set of words is being used in a familiar and narrow context. Existing handwriting recognition approaches require a lexicon (as a dictionary or pre-determined list of words and expressions in a particular language used by a particular application), for high recognition accuracy <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b32">32]</ref>. In our application to CAPTCHA, the use of words is infinite with no specific context, thus the required lexicon would have to be extremely large with consequently extremely poor accuracy by recognizers. Moreover, by applying very specific transformations that exploit the weaknesses of state-ofthe-art recognition systems to our image samples on purpose, we add extra difficulty for machine recognition. One key open issue is the lack of a general purpose cross-domain recognition tool. Most tools are very domain specific and require domain context <ref type="bibr" target="#b5">[5]</ref> or a case-based approach <ref type="bibr" target="#b35">[35]</ref> to interpret a particular graphic. For example, tools used to recognize domain-specific graphics such as electrical diagrams rely on primitives in the graphic that have intrinsic meaning. In musical scores, for example, the musical notes comprise a finite set of primitives that can be extracted and interpreted because they have a meaning apart from the whole graphic <ref type="bibr" target="#b5">[5]</ref>. Once the primitives are interpreted, the graphic as a whole can be interpreted. We will discuss later how our tree drawing does not rely on any particular domain context and thus would be hard for machines to interpret. No parts of our tree have any intrinsic meaning and are always interpreted in the context of reading of deformed handwritten images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graphics Recognition Factor</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generation of Tree-based Handwritten CAPTCHA</head><p>The development of our CAPTCHA has focused on using transformed handwriting samples due to the aid provided by cognitive principles that humans can make use of and the challenges presented to machines. We have increased difficulty for machines as well as the potential for insights in the area of graphics recognition by arranging our handwritten images into a tree structure. It should be noted that in our CAPTCHA the interpretation of the tree structure will always be combined with interpreting our handwritten images. There are two main parts to the creation of our combined CAPTCHA. First, images featuring synthetic handwritten are generated using a handwriting generator <ref type="bibr" target="#b25">[25]</ref>. Gestalt and Geon-motivated transformations are then applied to the images to take advantage of human perception abilities and to create more difficulty for machines. Second, there is the random generation of a tree structure and tree elements and arrangement of the deformed handwritten images in the tree, making spatial recognition, which is a common task for humans, also part of the challenge. The overall architecture for our HIP system is shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformed Handwritten Images</head><p>For testing our approach we have used handwritten word image samples. We have also developed a method to generate virtually infinite quantities of synthetic handwritten images based on real character templates <ref type="bibr" target="#b25">[25]</ref> and to transform them on the fly. We note that a somewhat narrow set of words and their corresponding handwritten images was used for testing in order to provide machines with a lexicon to give them a fair chance at solving. Synthetic handwritten images that are generated and transformed on the fly for use in the CAPTCHA application are shown in <ref type="figure" target="#fig_3">Figure 4</ref>, before and after applying deformations that defeat state-of-theart handwriting recognizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a) b)</head><p>To create our CAPTCHA, the number, type and severity of transformations are randomly chosen and applied, with some basic rules applied to ensure the images remain at once unreadable to machines and understandable by humans related to both Gestalt principles and Geon theory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b14">15]</ref>. To the best of our knowledge, tying particular Gestalt principles to specific image transformations is novel and helps ensure that we maintain legibility for humans while foiling machine recognition.</p><p>Transformations that can be applied to handwritten images include horizontal or vertical overlaps with the principles guiding human interpretation proximity, symmetry, familiarity, continuity and figure-ground; occlusions such as by circles, rectangles or lines in the same color as the background pixels <ref type="figure" target="#fig_3">(Figure 4b</ref>, top) with the principles guiding human reconstruction closure, proximity, continuity and familiarity; occlusions by waves from left to right in the same color as the background with the principles guiding human reconstruction closure, proximity and continuity; occlusions using the same color pixels as the foreground with the principles guiding human reconstruction familiarity and figure-ground; adding extra strokes <ref type="figure" target="#fig_3">(Figure 4b, bottom)</ref> with the principles guiding human reconstruction familiarity and figureground; using empty letters, broken letters, rough contours, fragmentations, etc. with the principles guiding human reconstruction closure, proximity, continuity and figure-ground; splitting the image into parts and offsetting them from each other or splitting the image into parts and spreading the parts out (mosaic effect) with the principles guiding human reconstruction closure, proximity, continuity and symmetry; changing word orientation or stretching or compressing with the principles guiding human reconstruction memory, internal metrics, familiarity of  objects and orientation. The synthetic handwriting generation <ref type="bibr" target="#b25">[25]</ref>, as well as the transformations applied, ensures that infinitely many variations in samples can be produced to prevent machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Creation of a Tree Structure with Transformed Handwritten Images</head><p>Trees can be used for a wide range of tasks such as manipulating hierarchical data, making data easy to search, or as in our case, for presenting visual elements. Trees are drawn from the top down. A node is an element of the tree while a branch is a line that connects elements. We have used a binary tree in our CAPTCHA <ref type="figure" target="#fig_0">(Figure 1</ref>) to ensure the drawing does not become too large or unnecessarily complex for human readers. Each node in this tree has at most two branches. Our use of tree structures to add complexity to a handwritten CAPTCHA is motivated by several factors. First, we leverage currently superior human skills not only in reading carefully transformed handwritten images but in interpreting them inside a graphic with additional elements. Our CAPTCHA in all cases requires the interpretation of handwriting and the tree merely adds more complexity. Second, we take into account the various open issues in graphics recognition and in document analysis and recognition generally. Our use of tree drawings is further encouraged by early user studies, which indicate that the trees do not present additional complexity to humans beyond handwritten CAPTCHAs alone.</p><p>The tree generation algorithm for our CAPTCHA uses randomness whenever possible to create a random tree overlaid with transformed synthetic handwritten images. The program begins with generating a random number of nodes. Once the number of nodes has been determined, the algorithm begins building a binary tree data structure. In addition to the random makeup of the tree, a randomly selected scaling and sizing algorithm ensures that the visual representation of the tree never looks the same. During the drawing phase, when each node is created, a randomly chosen generated image of handwritten text will be placed next to it. Also, for each tree branch that is created, a randomly selected symbol is placed in the middle of the branch. This could be extended to use almost any shape, drawing, or another type of symbols or pictures. As noted previously, deformations may be applied not only to the handwritten images but to the tree itself to further discourage machine learning.</p><p>To complete the generation of our CAPTCHA, the program selects a node, or set of nodes in the rendered tree at random about which to ask the question. Once the tree and proper placement of images has been completed, a question will be selected at random from a list of potential questions about the tree such as "Which {word} is connected to {some other word} by a {shape}? <ref type="figure" target="#fig_0">(Figure 1)</ref>. In all cases the user must quickly scan through and interpret all handwritten samples in the tree to correctly answer the question. Alternatively, the user may be asked to name two words that are connected. For machines to break our CAPTCHA, much more than one handwritten image would need to be interpreted. In addition, to solve our CAPTCHA machines would need to segment out the various objects in our CAPTCHA, including tree elements, shapes or other graphics, and handwritten images. Thus with our CAPTCHA we exploit the open problem in document image analysis of analyzing, segmenting and recognizing the various elements in a digital document or image <ref type="bibr" target="#b5">[5]</ref>. To complete the generation of the test, the handwritten images and their truth words (correct answers) are passed to the verifier. Upon challenge submittal, the user response is verified and the application determines whether the user passes or fails. If the user passes by interpreting all of the letters in the handwritten image correctly, they would then be given access to the Web resource in question, otherwise they would be given another different challenge. Since the difficulty level of recognition needed to answer the question is greater and also has a higher level of randomness, our tree-based handwritten CAPTCHA poses more difficulty for machines, while still remaining simple for human users based on their cognitive abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">HIP System Evaluation</head><p>We have designed the tree-based handwritten HIP system as a challenge-response protocol for Web security. Experimental tests on word legibility have been conducted with human subjects and state-of-theart handwriting recognizers. We have tested large sets of images on machines. To make it a fair test for machines, we have assisted the word recognizers with lexicons that contain all the truth words of the test images. For testing we used scanned handwritten image samples of US city names which we had readily available from postal applications, in order to provide samples corresponding to a known, finite lexicon (size 40,000, roughly the number of US city names) to help machine recognition, as well as synthetically generated samples. In reality, in actual applications such as our CAPTCHA having no context-specific dictionary, the number of entries in the lexicon will be much larger which will affect recognition accuracy drastically, as indicated by researchers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Machine Testing</head><p>The handwritten CAPTCHAs have been tested by several state-of-the-art handwriting recognizers (Word Model Recognizer (WMR), Character Model Recognizer (CMR), and Accuscript (HMM)) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">31]</ref>. We have tested several sets of images using humanwritten scanned samples, synthetically generated samples, and tree-based handwritten images.</p><p>Transformations were applied to human-written and scanned image samples based on the Gestalt principles and Geon theory. Several sets, each of them with over 4,000 handwritten city name images, were used, one set for each transformation. Parameter values for transformations were randomly chosen and successively applied to the handwritten word images. The individual transformations we were concerned with were less vs. more fragmentation, empty letters, displacement, mosaic effect, adding jaws, arcs, or extra strokes, occlusion by circles, occlusion by waves (white vs. black), vertical or horizontal overlap, and overlap of different words. We note that with the exception of occlusion by circle transformation, machine recognition rates were low for Gestalt-based transformations, even though the lexicon used to help machine accuracy was of a relatively small size. We observed that for occlusions by circle, machine recognition could be affected if the transformation did not adequately affect the foreground. We encountered overall accuracy of 5.74% for WMR, 1.21% for Accuscript and 3.8% for CMR, with accuracies approaching 0% for individual transformations such as letter fragmentations, overlaps, and adding extra strokes, when recognizers were aided by a relatively small lexicon of 4,000 words. On the other hand, these presented the least difficulty for human subjects, based on the Gestalt laws of closure and continuity <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>.</p><p>We also tested 300 synthetic handwriting samples that were generated corresponding to US city names, US states and world-wide countries. Similar Gestalt and Geon-motivated transformations were applied. For these images we saw an accuracy rate of only 1.00% for WMR, 0.7% for Accuscript, and 0.3% for CMR. This extremely low machine recognition rate even with a small word set and a provided small lexicon suggests that synthetic handwritten images are an excellent choice for generating infinitely many CAPTCHAs.</p><p>To the best of our knowledge, there is not yet any commercial program which can fully interact with or decipher our tree-based handwritten CAPTCHA. As we have noted previously, our use of tree structures is motivated by the fact that they can provide complexity for machine recognition beyond interpretation of handwritten images. In addition, we leverage open problems in graphics recognition as well as in the wider field of Document Analysis and Recognition. In our tree structure, symbols have no intrinsic meaning outside of our CAPTCHA. Thus, segmenting parts of our drawing would not assist in solving the combined CAPTCHA since the task of handwriting interpretation would still remain difficult. Our drawing only makes sense as a complete entity which requires the interpretation of symbols with no inherent meaning on top of interpreting handwritten transformed images with their previously mentioned difficulties for machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Usability Testing</head><p>Our usability testing focused on understanding the viability of our CAPTCHA both from a user experience perspective and based on how often users were able to interpret our CAPTCHA. A key area of focus was on determining whether our tree structure in combination with handwritten images presented any additional difficulty as compared to a handwriting only CAPTCHA. User tests were conducted both for handwritten images alone and for our tree-based handwritten CAPTCHA.</p><p>To test handwritten images alone that were transformed according to Gestalt and Geon principles, random sets of 90 images were given to be recognized by 9 volunteers. The test consisted of 10 handwritten word images for each of the 9 types of transformations. For the purposes of testing, to ensure human results could be fairly compared to machine results, images were chosen at random from transformed US city name images. The actual application will feature virtually infinite-many different synthetically generated and transformed word images with an unrestricted domain to foil machine recognition. We note that most of the human errors came from poor original images rather than being related to the transformations applied. Success rates for humans averaged 80%. As noted earlier, we believe that through a careful process of parameter selection and good quality starting samples, which can best be guaranteed by using synthetic samples, we can achieve a higher success rate for human recognition for our images. We have also compared human recognition on a set of human handwritten US city name images to a set containing 79 synthetic US city name, state or country name images automatically generated by our handwriting generator program. Similar high human recognition of 80% or better for both human-written and synthetic image sets suggests that synthetic images pose no additional problems to humans.</p><p>Our first round of user studies on our tree-based handwritten CAPTCHA included 15 volunteer graduate and undergraduate students. Approximately 30% of the volunteers were non-native English speakers which suggests that our CAPTCHA may be useful to a large audience. Subsequent tests will be run with a larger set of participants and tests featuring a larger set of words. During our initial test, 190 challenges were completed and a series of survey questions were administered on a volunteer basis to all participants. Each participant was asked to take 20 tree-based handwritten CAPTCHA challenges. The tests were self-administered and were taken at a testing Web site. Participants were given only very basic information on the concept of CAPTCHA and no prior knowledge of the field was assumed. Users were asked to solve the CAPTCHA presented ( <ref type="figure" target="#fig_0">Figure 1</ref>) and to rate each CAPTCHA on a scale of 1-5, with 1 being "least difficult" and 5 being "most difficult". At the end users were asked to provide general comments about our CAPTCHA as well as responses to specific questions about their Web usage and exposure to CAPTCHAs.</p><p>Users were able to interpret the tree-based handwritten CAPTCHA 80.6% of the time which was no less often than the 80% for handwritten CAPTCHA trials. The most common rating given for the tree-based handwritten CAPTCHA trials was 2, although the presence of trials rated as 4 or 5 made the average (2.8) slightly higher. We have observed that generally the samples given a higher rating were those with poor image quality from scanned samples. We feel that the average rating is acceptable given the current sample set and will only be improved with a cleaner, all synthetic set to be used in the next round of testing. One interesting observation is that in many cases where users rated an image as a 4 or 5, they were still able to correctly guess the image. We believe that recognizing additional elements in the tree allowed users to fill in the blanks and interpret images that they may not have otherwise been able to read. Several general comments collected in the survey support this assumption. The ability to guess words even when they were hard to read once again highlights the importance of human perception factors involved in reading, including those of local context and Gestalt and Geon principles. The same assistance provided to humans by the tree structure provided more difficulty for machines due to problems in graphics segmentation of complex multilayer images. Of the individual transformations, more correct answers were for word images transformed using overlaps, mosaic effect and extra strokes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>This paper furthers the work on handwritten CAPTCHA <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref> and provides insights into the fields of AI, Handwriting and Graphics Recognition. A new approach is presented for a HIP system that combines handwritten word images in a randomly generated tree-based structure with randomly selected test questions. Our approach leverages currently superior human ability over machines in interpreting graphics and reading unconstrained handwriting, especially when various transformations such as fragmentations or occlusions are applied. The Gestalt laws of perception and Geon theory and the weaknesses of handwriting recognizers were closely studied to determine the most effective transformations to apply to keep images legible for humans while making them difficult to read by machines. We add the novel element of a randomly drawn tree structure with randomly drawn node elements in addition to handwritten images to further leverage human cognitive strengths over machines.</p><p>Experimental results show a high degree of human success even with inconsistent quality scanned handwriting samples, and user feedback has also been largely positive indicating that our CAPTCHA is human-friendly. At the same time, our tests using state of the art recognizers prove that machine success rates with the same tests are low to non-existent. Testing both real human handwriting and synthetically generated samples allowed us to compare the results at machine and human level and conclude that synthetic handwriting is at least as good as real handwriting for CAPTCHA purposes. All these aspects indicate that our CAPTCHA can successfully be used to protect online services as a viable alternative solution for Cyber security. Additionally, our CAPTCHA provides fertile ground for work on important problems in other areas such as AI, Image Analysis, Machine Learning, Security, etc., and invite researchers to work on breaking our CAPTCHA and further advance knowledge in those fields.</p><p>We are considering several improvements to our application based on user feedback. We will run additional user tests using a larger set of non-domain specific words and synthetic samples generated and transformed on the fly by our handwriting generator. A wider range of participants will be considered and metrics on human time to solve the challenges by transformation type collected. More testing will be conducted to understand the processing load that our application might present during real-time use by many concurrent users. We also plan to have researchers create custom attacks to understand any potential vulnerabilities of our approach or will release our application to the public using some of our university online services in order to further understand how machines and a wider set of human users might interact with it. Combining handwritten text images with images of objects is another possible extension for our CAPTCHA. Last but not least, an alternative CAPTCHA for visually impaired users will be considered as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A tree-based handwritten CAPTCHA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . Example of commercial CAPTCHA at msn.com.</head><label>2</label><figDesc>Figure 2. Example of commercial CAPTCHA at msn.com.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>While advances have been made in the area of document image analysis, various open problems of interest remain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . Synthetically generated handwritten images: a) original; b) transformed.</head><label>4</label><figDesc>Figure 4. Synthetically generated handwritten images: a) original; b) transformed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 . Overall architecture for tree-based handwritten CAPTCHA.</head><label>3</label><figDesc>Figure 3. Overall architecture for tree-based handwritten CAPTCHA.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human Interactive Proofs and Document Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Popat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IAPR Workshop on Document Analysis Systems</title>
		<meeting>IAPR Workshop on Document Analysis Systems</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recognition-by-components: A theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The perception of objects with deleted contours. Unpublished manuscript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blickle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognizing Depth-Rotated Objects: Evidence and Conditions for Three-Dimensional Viewpoint Invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Gerhardstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1162" to="1182" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Digital Document Processing: Major Directions and Recent Advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chaudhuri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Designing Human Friendly Human Interaction Proofs (HIPs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Czerwinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CHI 2005</title>
		<meeting>CHI 2005</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="711" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using Machine Learning to Break Visual Human Interaction Proofs (HIPs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Human Interactive Proof</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baffletext</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th IS&amp;T/SPIE Document Recognition and Retrieval Conference</title>
		<meeting>10th IS&amp;T/SPIE Document Recognition and Retrieval Conference</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamic Mental Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Freyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="427" to="438" />
			<date type="published" when="1987" />
			<publisher>American Psychological Assoc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Machine Learning Attacks Against the Asirra CAPTCHA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Golle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCS &quot;08</title>
		<meeting>CCS &quot;08<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spam and the Ongoing Battle for the Inbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="33" />
			<date type="published" when="2007" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Use of Lexicon Density in Evaluating Word Recognizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slavik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="789" to="800" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Lexicon Driven Approach to Handwritten Word Recognition for Real-Time Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="366" to="379" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Principles of Gestalt Psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koffka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Harcourt, Brace and Company</title>
		<meeting><address><addrLine>New York; New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Role of Holistic Paradigms in Handwritten Word Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madhvanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="149" to="164" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Recognizing objects in adversarial clutter: breaking a visual CAPTCHA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computer Vision and Pattern Recognition, 1, I-134-I-141</title>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online and OffLine Handwriting Recognition: A Comprehensive Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plamondon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="63" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">reCAPTCHA project -a CAPTCHA implementation</title>
		<ptr target="http://www.recaptcha.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optical Character Recognition: An Illustrated Guide to the Frontier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nartker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<pubPlace>Kluwer, Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automated Reverse Turing Test Using Facial Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artifacial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MM &quot;03</title>
		<meeting>MM &quot;03<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="295" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using the difference in the abilities of humans and machines in reading handwritten words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Captcha</forename><surname>Handwritten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Ninth International Workshop on Frontiers in Handwriting Recognition</title>
		<meeting>Ninth International Workshop on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual CAPTCHA with Handwritten Image Analysis. Human Interactive Proofs: Second International Workshop</title>
		<meeting><address><addrLine>Bethlehem, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005-05-19" />
			<biblScope unit="page" from="42" to="51" />
		</imprint>
	</monogr>
	<note>HIP</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Human Interactive Proof Algorithm Using Handwriting Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth International Conference on Document Analysis and Recognition</title>
		<meeting>Eighth International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="967" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synthetic Handwriting Generator for Cyber Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Midic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13 th Conference of the International Graphonomics Society</title>
		<meeting>13 th Conference of the International Graphonomics Society</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An Off-Line Cursive Handwriting Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="309" to="321" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Integration of Hand-Written Address Interpretation Technology into the United States Postal Service Remote Computer Reader System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Kuebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth International Conference on Document Analysis and Recognition</title>
		<meeting>Fourth International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="892" to="896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Computing machinery and intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="433" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using Hard AI Problems for Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Captcha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">2656</biblScope>
			<biblScope unit="page" from="294" to="311" />
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Telling humans and computers apart automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Von Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications ACM</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="56" to="60" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A stochastic model combining discrete symbols and continuous attributes and its application to handwriting recognition. International Workshop of Document Analysis and Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="70" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the dependence of handwritten word recognizers on lexicons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Govindaraju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1553" to="1564" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Low-cost Attack on a Microsoft CAPTCHA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCS &quot;08</title>
		<meeting>CCS &quot;08<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="543" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Usability of CAPTCHAs Or usability issues in CAPTCHA design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc</title>
		<imprint>
			<biblScope unit="page" from="44" to="52" />
			<date type="published" when="2008" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Engineering Drawings Recognition Using a Case-based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wenyin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Int&quot;l Conf. on Document Analysis and Recognition</title>
		<meeting>Seventh Int&quot;l Conf. on Document Analysis and Recognition<address><addrLine>Washington, D.C</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
