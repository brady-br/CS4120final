<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cost-effective Hardware Accelerator Recommendation for Edge Computing *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zhou</surname></persName>
							<email>xingyu.zhou@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of EECS</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Canady</surname></persName>
							<email>robert.e.canady@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of EECS</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunxing</forename><surname>Bao</surname></persName>
							<email>shunxing.bao@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of EECS</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Gokhale</surname></persName>
							<email>a.gokhale@vanderbilt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept of EECS</orgName>
								<orgName type="institution">Vanderbilt University</orgName>
								<address>
									<settlement>Nashville</settlement>
									<region>TN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cost-effective Hardware Accelerator Recommendation for Edge Computing *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Hardware accelerator devices have emerged as an alternative to traditional CPUs since they not only help perform computations faster but also consume much less energy than a traditional CPU thereby helping to lower both capex (i.e., procurement costs) and opex (i.e., energy usage). However, since different accelerator technologies can illustrate different traits for different application types that run at the edge, there is a critical need for effective mechanisms that can help developers select the right technology (or a mix of) to use in their context, which is currently lacking. To address this critical need, we propose a recommender system to help users rapidly and cost-effectively select the right hardware accelerator technology for a given compute-intensive task. Our framework comprises the following workflow. First, we collect realistic execution traces of computations on real, single hardware accelerator devices. Second, we utilize these traces to deduce the achievable latencies and amor-tized costs for device deployments across the cloud-edge spectrum, which in turn provides guidance in selecting the right hardware.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An issue that is often encountered by developers planning to deploy their applications, particulary those involving data-driven machine learning elements, is what device hardware is best suited (performance and costwise) for a given task under varying data processing demands. This question is even more pronounced <ref type="bibr" target="#b25">[26]</ref> for resource-constrained edge computing/IoT. This problem is particularly acute in scenarios where cameras with video streams are involved <ref type="bibr" target="#b0">[1]</ref> because compute intensive tasks, especially deep learning based machine learning applications on these data, often re- * This work was supported in part by AFOSR DDDAS FA9550-18-1-0126 program. Any opinions, findings, and conclusions or recommendations expressed are those of the author(s) and do not necessarily reflect the views of the sponsor.</p><p>quire millions of operations for each inference <ref type="bibr" target="#b15">[16]</ref>. Thus, to maintain low latencies and compliance with the power sensitivity of edge deployment, smaller models <ref type="bibr" target="#b7">[8]</ref> operating on more efficient hardware are preferred <ref type="bibr" target="#b13">[14]</ref>. To that end, hardware acceleration technologies, such as field programmable gate arrays (FPGAs), graphical processing units (GPUs) and applicationspecific integrated circuits (ASICs) among others, have shown significant promise for edge computing <ref type="bibr" target="#b19">[20]</ref>.</p><p>With the proliferation of different accelerator technologies, developers are faced with a significant dilemma: which accelerator technology is best suited for their application needs such that response times are met under different workload variations, the overall cost of the deployment fits their budget and the energy consumption for these devices are below a threshold. This dilemma stems from the fact that different accelerator technologies illustrate different performance and energy consumption traits for different application scenarios. Contemporary techniques that evaluate acceleration technologies are either too specific to a device type and incorporate only a single inference instance level <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref> or comprise low-level circuit design optimization analysis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30]</ref>. Consequently, there remains a significant lack of understanding on the applicability of these accelerator technologies in at-scale, edge-based applications <ref type="bibr" target="#b32">[33]</ref> due to diversity of systems settings <ref type="bibr" target="#b23">[24]</ref>.</p><p>In a general sense, selecting a suitable hardware accelerator technology for a given computational task can be regarded as finding out a hardware device placement and optimization strategy for a given topology. Prior work has explored this general problem for service placement for edge computing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref>. From a hardware perspective, workload-specific accelerator placement frameworks for automobiles and unmanned aircraft have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29]</ref>. In these workloads, energy usage is dominated by vehicles rather than computational overhead. From a higher cloud-level point of view, research on device and service placement also exists <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">37]</ref>. Yet, widespread adoption of these different hardware platforms requires more systematic understanding of both spatial and temporal impacts in realistic deployments in a network with compute-intensive tasks.</p><p>To address these challenges, we present HARE (Hardware Accelerator Recommender for the Edge), which is a framework to help users rapidly and cost-effectively select the right hardware accelerator for a given computeintensive task. Our work focuses on a general framework to model compute latency and energy usage across accelerator devices, and makes the following contributions:</p><p>• We present a novel hardware-software coevaluation framework that formalizes the performance comparison involving training samples obtained on real hardware.</p><p>• We develop a simple approximation method that can be utilized to implement long-term cost analysis for hardware-based machine learning inference behavior at the edge.</p><p>• We show case studies involving two realistic machine learning application settings and demonstrate HARE in these contexts.</p><p>The remainder of the paper is organized as follows: Section 2 describes the design of the HARE hardware accelerator evaluation framework; Section 3 provides empirical experiment design and evaluation results of applications across different hardware platforms; and Section 4 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recommender System Methodology</head><p>We now present technical details about HARE. In realizing it, we faced several challenges. The broad range of hardware options and settings makes performance quantification on the different device types very difficult. There are numerous hardware-related metrics and we had to decide which metrics were most significant and how to depict hardware acceleration patterns in a compact manner. Finally, for at-scale deployment of devices, the overall system performance is not just a simple addition of individual performance cases.</p><p>Our approach to realizing HARE uses the following steps: (1) Conduct performance and energy benchmarking in isolation per device type, and (2) Use these insights to approximate the performance, energy and cost metrics for at-scale deployments, which is then used as the recommendation engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Benchmarking in Isolation</head><p>For our first step, we have used machine learning-based applications to drive our benchmarking efforts. This choice stems from the fact that state-of-art deep learning advances have enabled higher demands for compute intensive data stream processing tasks from edge sources, especially video and image processing using deep learning <ref type="bibr" target="#b0">[1]</ref>. We chose application cases belonging to classification (ResNet-50 <ref type="bibr" target="#b8">[9]</ref>) and detection (Tiny Yolo <ref type="bibr" target="#b24">[25]</ref>) as test functions to explore and compare their performance across different hardware platforms. Note that machine learning comprises many more techniques and deployment on different hardware, such as random forests <ref type="bibr" target="#b31">[32]</ref> or decision trees <ref type="bibr" target="#b20">[21]</ref>. However, there is a general lack of comparison standards due to heterogeneous platform workflows and limited deployment scale. Further, although it is desirable for the same model to illustrate similar accuracy across different hardware <ref type="bibr" target="#b21">[22]</ref> as is illustrated by <ref type="bibr">TensorFlow [28]</ref>, the performance across these hardware may be different.</p><p>To that end we consider four types of hardware including CPU, GPU, ASIC and FPGA. CPUs are the most general hardware platform. GPUs, particulary the NVIDIA products with CUDNN <ref type="bibr" target="#b3">[4]</ref> support both desktop and embedded training and inference. For ASICs, we used two currently available neural network acceleration chips on the market: the Intel Neural Compute Stick (NCS) <ref type="bibr" target="#b9">[10]</ref> and Google Coral with Tensor Processing Unit (TPU) <ref type="bibr" target="#b11">[12]</ref>. For FPGAs, High-level synthesis (HLS) is a technology that can translate behaviorallevel design descriptions using C/C++ into RTL descriptions <ref type="bibr" target="#b12">[13]</ref> and make them easier for application developers to test edge applications <ref type="bibr" target="#b10">[11]</ref>. For deep learning tasks, Xilinx provides a toolkit called DNNDK <ref type="bibr" target="#b6">[7]</ref> based on HLS that is aimed at providing users with an end-toend workflow to deploy a trained network on FPGA devices. The design flows for different hardware platforms that we have used are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Single device executions aim at collecting time and power consumption data for the task under consideration. Time experiment records are series of data points of execution time length for each inference. From these data records, the mean and standard deviation of response time and its inverse (inference frequency) are determined to document the capability of this device. That is, we use a normal distribution conforming to N(muT dev , stdT 2 dev ) to approximate time consumption per inference for a device and N(muFreq dev , stdFreq 2 dev ) to denote the inference frequency. We use random variables rather than static values of average or maximum to allow uncertainty quantification for overall system performance. As the computation system is assumed to be active throughout the deployment cycle, for power consumption data both static and dynamic inference consumption is recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inferring the Desired Metrics At-Scale</head><p>The aim of this step is to infer the performance, energy and cost metrics for large-scale deployments of ML applications that span the cloud-to-edge spectrum using insights from benchmarking of isolated use cases and solv- ing an optimization problem. Our method for inferring the desired metric inference can be thought of as an optimistic upper bound approximation for the hardware device selection for application deployment across the cloud and edge without considering the underlying model-related error metrics like classification accuracy. Under a linear speedup assumption with additional device parallelism, further speed up with multiple devices would increase total inference capability without increasing uncertainty in performance (variance) <ref type="bibr" target="#b5">[6]</ref>. To ensure nHW dev that a specific type of device with total inference capability R dev ∼ N(muFreq dev * nHW dev , stdFreq 2 dev ) can handle input data load L dev ∼ N(muFreq in , stdFreq 2 in ), there should be enough devices deployed for the given input pressure with a design confidence level con f :</p><formula xml:id="formula_0">Pr(R dev − L dev ) &gt; con f<label>(1)</label></formula><p>For latency constraint, we have the latency as the sum of hardware running time and communication time in the inference loop. The average latency can be compared in a straightforward way:</p><formula xml:id="formula_1">T app (dev) = T hw + T comm = muT dev + t e2 f + t f 2c (2)</formula><p>where t e2 f = S e2 f /B e2 f refers to the communication time from edge to fog (which is a layer between the edge and cloud) and t f 2c = S f ec /B f 2c refers to the communication time from edge to cloud. The communication time is computed using the transfer package size S divided by the bandwidth B, where bandwidths are assumed to be stable. For different application scenarios, communication bandwidths vary <ref type="bibr" target="#b18">[19]</ref>.</p><p>We set the unit cost of electricity costElec as a constant $0.1/kwh <ref type="bibr" target="#b30">[31]</ref>. Moreover, the total electricity cost can be denoted as:</p><formula xml:id="formula_2">costP(dev, T cycle ) = P app (dev, T cycle ) * costElec (3)</formula><p>Likewise, the total power consumption for a given deployment cycle length T cycle can be computed by the sum of idle power and working power:</p><formula xml:id="formula_3">P app (dev, T cycle ) = P idle (dev) * T cycle + P perinf (dev) * muFreq in * T cycle (4)</formula><p>Based on the definitions given above, we formulate the problem of hardware accelerator selection as minimizing the total deployment cost while lowering time and power consumption to a lower target level. We consider performance requirements at the application level using hardware commercial cost, latency T app for each inference loop and total working power consumption P app (dev, T cycle ) over the design deployment cycle T cycle .</p><formula xml:id="formula_4">min dev∈ListHW ∑ costHW dev * nHW dev + costP(dev, T cycle )</formula><p>subject to:</p><formula xml:id="formula_5">T app (dev) t target (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Validation of HARE</head><p>We now validate the effectiveness of the HARE framework. Based on the functional descriptions and hardware design workflows discussed above, in this part we describe experiments involving hardware deployment and other metric evaluations on test applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Testbed Configuration</head><p>The testbed and commercial price (costHW dev ) of deployment platforms at the time we conducted the experiments are presented below.</p><p>1. CPU: CPU is the most general option. Two options from both server and edge side are considered in our test framework.</p><p>1. Raspberry Pi 3B ($40): Edge deployment with a Broadcom BCM2837B0, Cortex-A53 (ARMv8) 64-bit SoC @ 1.4GHz 2. AMD FX-6300 CPU ($70): Server deployment with 6 cores, 6 threads @ 3.5-3.8GHz</p><p>3. GPU: For GPU, two options from both server and edge side are considered in our test framework. As a result, the list of potential hardware in our problem setting can be shown as: ListHW = {RPi, FX6300, JetsonNano, GT X1060, NCS,Ultra96}. In this set of devices, the Raspberry Pi (RPi) could be regarded as the baseline for acceleration comparisons due to its lowest performance (being a CPU). Some devices need to work with a host. NCS needs to be plugged into a USB port. And GTX1060 needs to be connected to a PCIE port with external power support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Per-Device Benchmarking Results</head><p>Experiments on classification tasks are conducted on a set of 500 images with a resolution of 640 * 480. Experiments on detection tasks are conducted on a road traffic video consisting of 874 frames with a resolution of 1280 * 720. We primarily gather power and time consumption on these high-dimensional data compatible with ImageNet <ref type="bibr" target="#b15">[16]</ref> to guarantee higher generalization than cifar-10 <ref type="bibr" target="#b14">[15]</ref> or other lower-dimensional datasets. The power consumption data is gathered using a specific power measurement instrument called Watts Up Pro. For time consumption, it is worth pointing out that the time metric we are using here is the total response time running on hardware for inference tasks. As a result, T hw = T preprocess + T inference . That is, the total time consumption includes both the input data preprocessing time and the inference time. <ref type="table" target="#tab_2">Tables 2 and 4</ref> show response times for each Resnet50 and Tiny Yolo inference, respectively, while <ref type="table" target="#tab_3">Tables 3 and 5</ref> show both their idle and execution power consumptions.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">At-Scale Approximation Results</head><p>We make evaluations for time, power and cost for atscale deployments using our approximation approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Application Topology</head><p>We present a prototypical smart application case <ref type="bibr" target="#b4">[5]</ref> comprising distributed camera networks that can be generalized to scenarios like unmanned shopping using object classification and surveillance using detection as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Computation burdens are conducted on edge nodes. There would be different design standards for various scenarios. Without loss of generality, we set the system deployment goal as computation resources should guarantee to handle no less than half (2 of 4) of input loads from every fog group (3 groups) with an overall confidence level of 99%. We set the input with relatively high uncertainty when stdFreq in = muFreq in . Edge nodes in each fog group closest to sensors and data needs to be processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Latency Estimation</head><p>A straightforward comparison for response times per inference can be conducted using inference time data from <ref type="table" target="#tab_2">Tables 2 and 4</ref>. From the time point of view, Rapsberry Pi 3 b+ consumes the most for each inference task. Data for bandwidth between edge/fog and cloud is retrieved from standard IEEE802.11n Wifi setting <ref type="bibr" target="#b18">[19]</ref> as the optimal upper bound of 135Mbps. The size of control signal is set as 1kb and data signal using JPEG <ref type="bibr" target="#b33">[34]</ref> 100% with 24bit/pixel. Based on the definitions from Section 2, we show latency estimations of T app (dev) in <ref type="table" target="#tab_6">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Power Consumption</head><p>Based on the method discussed, we can approximate the total power consumption holding application accelerations for a given cycle length. We use the idle power of one device to denote the total idle power of this type of devices in a network based on an ideal time-division setting. We set the deployment time length as 24 months for our simulated at-scale setup and the total power consumption for both classification and detection tasks is computed for this period as shown in <ref type="table" target="#tab_7">Table 7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4">Cost Evaluation</head><p>Figures 2a and 2b show results for devices with ResNet-50 classification and Tiny Yolo detection applications, respectively. Under increasing input pressure, the number of devices required steps up like stairs. For a given design cycle, the total cost consisting of device and power cost would be proportional to the number of devices. Traditional CPUs like Raspberry Pi on the edge and FX6300 on the cloud both show worst performances. This explains why hardware accelerators are necessary for such use cases involving compute-intensive tasks like ML inference. For long-term deployments, power consumption would play a significant role in the total cost. For classification and detection tasks, the Ultra96 (FPGA) and JetsonNano (embedded GPU) illustrate the two most cost-efficient options for the edge side at high input pressures. NCS (ASIC) could be easily deployed to existing network topology but relatively lower individual device performance becomes its limitation. Edge accelerators show more obvious advantage for video detection case; this indicates the necessity for offloading streaming data processing in the edge computing pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper presents a simple recommendation system to help users select an accelerator hardware device for their applications deployed across the cloud to edge spectrum. Our approach first measures realistic execution traces of computations on individual hardware accelerator devices. We then use these data to approximate latency, power consumption and long-term cost for at-scale deployments, which provides guidance on device selection. Summary: For the hardware devices used, FPGAs show both highest absolute computation power and highest energy efficiency. This makes it a good option for flexible acceleration tasks at the edge. However, for relatively low pressure scenarios, the device costs could be dominant and FPGAs are harder to program. Furthermore, the number of devices needed is based only on an ideal assumption and a large number of connected nodes might still lead to a larger number of devices needed and higher usage threshold for seeking the desired parallelism. For machine learning inference tasks, server-side GPUs can have the most computation power and higher energy efficiency than CPUs and some lightweight edge devices. Thus, embedded GPUs like Jetson Nano could have much potential for more general computation tasks, particularly those related to graphics and vision. ASIC devices like NCS do not show the highest time efficiency or power efficiency but considering its low cost and low threshold to use, it is still competitive for neural networkspecific and relatively low input frequency tasks. Limitations: So far we have only considered a device selection strategy where only one type of device is considered at one time. In addition, we only consider an ideal minimum number of devices needed for a given input pressure without accounting for the cost of task scheduling. Moreover, we only consider a relatively simple application scenario where only one type of acceleration task is executed at a time. More experiments beyond classification and detection would be needed. We plan to investigate the possibility of at-scale deployment of RNN <ref type="bibr" target="#b22">[23]</ref> and GAN <ref type="bibr" target="#b34">[35]</ref> in edge scenarios. Another potential issue is the cost and power requirements to keep switches and gateways operational, which are not negligible especially in cases where commercial cloud environments are involved. Finally, we have not taken interference effects between device executions into consideration. This will shed light on further research in design and optimization of fog/edge topologies involving heterogeneous hardware accelerators targeting heterogeneous tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion Topics</head><p>This paper describes a device selection problem that must be overcome for the use of hardware accelerators in edge computing. The main challenge lies in the heterogeneity of hardware devices and in contrast the lack of systematic research about their impact on application development, operational costs and performance. We hope that our ideas will stimulate discussion on several open issues as follows regarding the applicability of hardware acceleration techniques at the edge:</p><p>• How far can the hardware acceleration pattern reach in edge application scenarios? We discuss the stateof-art deep learning accelerations but what other applications can benefit from accelerator technologies where our framework will be useful? • Is our objective function correct or do we need to incorporate additional or different cost function and constraints? Some applications may need mixed accelerators. How does this change the approach? • Should hardware acceleration techniques just be used for inference computations for already learned models, or can it also be used for model re-learning? • Anonymous reviews pointed out that the power models are relatively straightforward and require a lot of profiling. Can we develop models that cut across accelerators? That is, can we find a mathematical function that subsumes everything from a GPU to an FPGA?</p><p>Some of our claims may be considered controversial, and we are looking forward to hearing different points of view on these issues:</p><p>• We have used simulations to scale hardware device deployments. This was based on parameters obtained using devices at hand and executing two deep learning tasks. We have not experimented with other lower level computations like matrix multiplications and do not know if their performance accelerations are similar.</p><p>• Our experiments used single processing functions.</p><p>This is based on a full computation task offloading which put all task burden on the target device. We need additional experimentation with more integrated complex applications like streaming processing applications.</p><p>• The edge is a highly dynamic environment and there may be a need to dynamically schedule and migrate applications. For multiple different types of tasks, how can this be made feasible given that the circuitry devices like FPGA and ASIC need to be programmed and how can this be achieved on-the-fly? • We call this a recommendation system. Is this appropriate or what more needs to be done?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Three-level Design Topology Layout: (1) Top:Cloud servers; (2) Intermediate:3 Fog groups include communication control and some computation power; (3) Bottom:4 Edge nodes in each fog group closest to sensors and data needs to be processed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Cost (Unit:k$) for a 24 Month Deployment Cycle Under Increasing Data Input Pressure (Frequency) and Medium Complexity(muFreq in = stdFreq in )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Device-level Acceleration Deployment Workflows for Different Hardware Platforms</head><label>1</label><figDesc></figDesc><table>Design Flow 
Edge CPU 
Embedded GPU 
FPGA 
ASIC 
Server GPU 
Server CPU 
Hardware 
Raspberry Pi 3 b+ NVIDIA Jetson Nano 
Avnet Ultra96 
Intel NCS 
NVIDIA GTX1060 6Gb 
AMD FX-6300 
ResNet-50 
Tensorflow/Keras 
TensorRT 
DNNDK 
OpenVINO 
Tensorflow/Keras/Cuda 
Tensorflow/Keras 
Tiny Yolo 
Darknet 
Darknet/TensorRT 
DNNDK 
OpenVINO 
Tensorflow/Keras/Cuda 
Tensorflow/Keras 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Response Time (T hw ) for Object classification Task using ResNet-50 (Unit: Second)</head><label>2</label><figDesc></figDesc><table>Time 
RPi 
JetsonNano 
Ultra96 
NCS 
GTX1060 
FX6300 
mean 
2.089 
0.133 
0.029 
0.218 
0.039 
0.268 
std 
0.058 
0.016 
0.001 
0.003 
0.005 
0.006 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Power Consumption for Object classification 
using ResNet-50 (Unit: Watt) 

Power 
RPi 
JetsonNano 
Ultra96 
NCS 
GTX1060 
FX6300 
Idle 
1.8 
2.2 
6.2 
0.4 
10 
72 
Infer 
4.8 
5.6 
7.6 
1.9 
122 
145 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Response Time (T hw ) for Traffic Detection Task using Tiny Yolo (Unit: Second)</head><label>4</label><figDesc></figDesc><table>Time 
RPi 
JetsonNano 
Ultra96 
NCS 
GTX1060 
FX6300 
mean 
2.874 
0.096 
0.023 
0.238 
0.059 
0.217 
std 
0.068 
0.008 
0.001 
0.003 
0.002 
0.076 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Power Consumption for Traffic Detection using Tiny Yolo (Unit: Watt)</head><label>5</label><figDesc></figDesc><table>Power 
RPi 
JetsonNano 
Ultra96 
NCS 
GTX1060 
FX6300 
Idle 
1.8 
2.3 
7.4 
0.4 
10 
72 
Infer 
4.8 
11.7 
9.2 
2.1 
122 
150 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : Inference Cycle Time T app (dev)(Unit:Second)</head><label>6</label><figDesc></figDesc><table>Time 
RPi 
JetNano 
Ultra96 
NCS 
GTX1060 
FX6300 
Loc 
Edge 
Edge 
Edge 
Edge 
Cloud 
Cloud 
Res 
2.088 
0.131 
0.029 
0.218 
0.046 
0.275 
Yolo 
2.869 
0.096 
0.023 
0.238 
0.081 
0.190 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Total Working Power Consumption (Unit:kWh) 

Power 
RPi 
JetNano 
Ultra96 
NCS 
GTX1060 
FX6300 
Res 
3720 
372.6 
87.2 
144.8 
2243 
14236 
Yolo 
5040 
485.7 
87.1 
173.9 
2660 
27685 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time video analytics: The killer app for edge computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bod´ikbod´ik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chintala-Pudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Philipose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravindranath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="58" to="67" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Managing edge resources for fully autonomous aerial systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boubin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chumley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/IEEE Symposium on Edge Computing</title>
		<meeting>the 4th ACM/IEEE Symposium on Edge Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="74" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enabling fpgas in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM Conference on Computing Frontiers</title>
		<meeting>the 11th ACM Conference on Computing Frontiers</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chetlur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Co-Hen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelhamer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cudnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Edge Hardware Accelerator Recommender Demo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doc-Group</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanderbilt</forename><surname>University</surname></persName>
		</author>
		<ptr target="https://github.com/dustinjoe" />
		<imprint>
			<date type="published" when="2020-05" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Online; accessed 01</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Leverage Multiple Intel® Neural Compute Sticks with Intel® Distribution of OpenVINO™ Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/leverage-multiple-intel-neural-compute-sticks-with-intel-distribution-of-openvino-toolkit" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Online; accessed 20-Feb-2020</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Software-hardware codesign for efficient neural network acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="18" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<ptr target="https://software.intel.com/en-us/movidius-ncs" />
		<title level="m">INTEL INCORPORATION. Intel® Movidius™ Neural Compute Stick</title>
		<imprint>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
	<note>Online; accessed 19</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating mobile applications at the network edge with software-programmable fpgas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IN-FOCOM 2018-IEEE Conference on Computer Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouppi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kastner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neuendorffer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03648</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">Parallel programming for fpgas. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Key Attributes of an Intelligent IIoT Edge Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khona</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<ptr target="http://aiweb.techfak.uni-bielefeld.de/content/bworld-robot-control-software" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krizhevsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html55" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krizhevsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The architectural implications of autonomous driving: Constraints and acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Skach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mars</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="751" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Profit-aware application placement for integrated fog-cloud computing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmud</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srirama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyya</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on mobile edge computing: The communication perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Letaief</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Surveys &amp; Tutorials</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2322" to="2358" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hardware acceleration landscape for distributed real-time analytics: Virtues and limitations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najafi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sadoghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacobsen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1938" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An fpga implementation of decision tree classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Honbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Memik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zambreno</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 Design, Automation &amp; Test in Europe Conference &amp; Exhibition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerating binarized neural networks: Comparison of fpga, cpu, gpu, and asic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nurvitadhi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sheffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Field-Programmable Technology (FPT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="77" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Accelerating recurrent neural networks in analytics servers: Comparison of fpga, cpu, gpu, and asic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nurvitadhi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sheffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kr-Ishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Field Programmable Logic and Applications (FPL)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reddi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schmuelling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Breughe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Charlebois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02549</idno>
		<title level="m">Mlperf inference benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Redmon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhadi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The emergence of edge computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards qos-aware fog service placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skarlat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustdar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 1st international conference on Fog and Edge Computing (ICFEC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="89" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient processing of deep neural networks: A tutorial and survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sze</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="2295" to="2329" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lopecs: A low-power edge computing system for real-time autonomous driving services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="30467" to="30479" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A framework for fast, scalable binarized neural network inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umuroglu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jahre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vissers</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Electric Power Monthly with Data for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Energy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Administration</surname></persName>
		</author>
		<ptr target="https://www.eia.gov/electricity/monthly/currentmonth/epm.pdf" />
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
	<note>Online; accessed 06-Feb-2020</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Accelerating a random forest classifier: Multicore, gp-gpu, or fpga?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macaraeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prenger</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 20th International Symposium on Field-Programmable Custom Computing Machines</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="232" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varghese</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bermbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lara</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>A survey on edge benchmarking</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The jpeg still picture compression standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallace</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on consumer electronics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="xviii" to=" xxxiv" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flexigan: An end-to-end solution for fpga acceleration of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazdanbakhsh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brzozowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khaleghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Es-Maeilzadeh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A realistic high-level synthesis benchmark suite for software programmable fpgas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Featherston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ve-Lasquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosetta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2018 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kelp: Qos for accelerated machine learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ran-Ganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="172" to="184" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
