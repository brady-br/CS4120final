<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST &apos;15). Open access to the Proceedings of the 13th USENIX Conference on File and Storage Technologies is sponsored by USENIX RIPQ: Advanced Photo Caching on Flash for Facebook USENIX Association 13th USENIX Conference on File and Storage Technologies (FAST &apos;15) 373 RIPQ: Advanced Photo Caching on Flash for Facebook</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 16-19,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linpeng</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wyatt</forename><surname>Lloyd</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linpeng</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Princeton University, † Cornell University, ‡ University of Southern California, 񮽙 Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wyatt</forename><surname>Lloyd</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Kumar 񮽙</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Princeton University, † Cornell University, ‡ University of Southern California, 񮽙 Facebook Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Qi Huang, Cornell University and Facebook</orgName>
								<orgName type="laboratory" key="lab2">University of Southern California and Facebook; Sanjeev Kumar</orgName>
								<orgName type="institution">Princeton University</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Facebook</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST &apos;15). Open access to the Proceedings of the 13th USENIX Conference on File and Storage Technologies is sponsored by USENIX RIPQ: Advanced Photo Caching on Flash for Facebook USENIX Association 13th USENIX Conference on File and Storage Technologies (FAST &apos;15) 373 RIPQ: Advanced Photo Caching on Flash for Facebook</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February 16-19,</date>
						</imprint>
					</monogr>
					<note>https://www.usenix.org/conference/fast15/technical-sessions/presentation/tang</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Facebook uses flash devices extensively in its photo-caching stack. The key design challenge for an efficient photo cache on flash at Facebook is its workload: many small random writes are generated by inserting cache-missed content, or updating cache-hit content for advanced caching algorithms. The Flash Translation Layer on flash devices performs poorly with such a workload, lowering throughput and decreasing device lifespan. Existing coping strategies under-utilize the space on flash devices, sacrificing cache capacity, or are limited to simple caching algorithms like FIFO, sacrificing hit ratios. We overcome these limitations with the novel Restricted Insertion Priority Queue (RIPQ) framework that supports advanced caching algorithms with large cache sizes, high throughput, and long device lifespan. RIPQ aggregates small random writes, co-locates similarly prioritized content, and lazily moves updated content to further reduce device overhead. We show that two families of advanced caching algorithms, Segmented-LRU and Greedy-Dual-Size-Frequency, can be easily implemented with RIPQ. Our evaluation on Facebook&apos;s photo trace shows that these algorithms running on RIPQ increase hit ratios up to ~20% over the current FIFO system , incur low overhead, and achieve high throughput.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Facebook has a deep and distributed photo-caching stack to decrease photo delivery latency and backend load. This stack uses flash for its capacity advantage over DRAM and higher I/O performance than magnetic disks.</p><p>A recent study <ref type="bibr" target="#b19">[20]</ref> shows that Facebook's photo caching hit ratios could be significantly improved with more advanced caching algorithms, i.e., the Segmented-LRU family of algorithms. However, naive implementations of these algorithms perform poorly on flash. For example, Quadruple-Segmented-LRU, which achieved ~70% hit ratio, generates a large number of small random writes for inserting missed content (~30% misses) and updating hit content (~70% hits). Such a random write heavy workload would cause frequent garbage collections at the Flash Translation Layer (FTL) inside modern NAND flash devices-especially when the write size is small-resulting in high write amplification, decreased throughput, and shortened device lifespan <ref type="bibr" target="#b36">[36]</ref>.</p><p>Existing approaches to mitigate this problem often reserve a significant portion of device space for the FTL (over-provisioning), hence reducing garbage collection frequency. However, over-provisioning also decreases available cache capacity. As a result, Facebook previously only used a FIFO caching policy that sacrifices the algorithmic advantages to maximize caching capacity and avoid small random writes.</p><p>Our goal is to design a flash cache that supports advanced caching algorithms for high hit ratios, uses most of the caching capacity of flash, and does not cause small random writes. To achieve this, we design and implement the novel Restricted Insertion Priority Queue (RIPQ) framework that efficiently approximates a priority queue on flash. RIPQ presents programmers with the interface of a priority queue, which our experience and prior work show to be a convenient abstraction for implementing advanced caching algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">45]</ref>.</p><p>The key challenge and novelty of RIPQ is how to translate and approximate updates to the (exact) priority queue into a flash-friendly workload. RIPQ aggregates small random writes in memory, and only issues aligned large writes through a restricted number of insertion points on flash to prevent FTL garbage collection and excessive memory buffering. Objects in cache with similar priorities are co-located among these insertion points. This largely preserves the fidelity of advanced caching algorithms on top of RIPQ. RIPQ also lazily moves content with an updated priority only when it is about to be evicted, further reducing overhead without harming the fidelity. As a result, RIPQ approximates the priority queue abstraction with high fidelity, and only performs consolidated large aligned writes on flash with low write amplification.</p><p>We also present the Single Insertion Priority Queue (SIPQ) framework that approximates a priority queue with a single insertion point. SIPQ is designed for memory-constrained environments and enables the use of simple algorithms like LRU, but is not suited to support more advanced algorithms.</p><p>RIPQ and SIPQ have applicability beyond Facebook's photo caches. They should enable the use of advanced caching algorithms for static-content caching-i.e., readonly caching-on flash in general, such as in Netflix's flash-based video caches <ref type="bibr" target="#b38">[38]</ref>.</p><p>We evaluate RIPQ and SIPQ by implementing two families of advanced caching algorithms, Segmented-LRU (SLRU) <ref type="bibr" target="#b26">[26]</ref> and Greedy-Dual-Size-Frequency (GDSF) <ref type="bibr" target="#b11">[12]</ref>, with them and testing their performance on traces obtained from two layers of Facebook's photo-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Motivation</head><p>Facebook's photo-serving stack, shown in <ref type="figure">Figure 1</ref>, includes two caching layers: an Edge cache layer and an Origin cache. At each cache site, individual photo objects are hashed to different caching machines according to their URI. Each caching machine then functions as an independent cache for its subset of objects. <ref type="bibr" target="#b0">1</ref> The Edge cache layer includes many independent caches spread around the globe at Internet Points of Presence (POP). The main objective of the Edge caching layer-in addition to decreasing latency for users-is decreasing the traffic sent to Facebook's datacenters, so the metric for evaluating its effectiveness is byte-wise hit ratio. The Origin cache is a single cache distributed across  <ref type="table">Table 1</ref>: Flash performance summary. Read and write sizes are 128KiB. Max-Throughput Write Size is the smallest power-of-2 size that achieves sustained maximum throughput at maximum capacity.</p><p>Facebook's datacenters that sits behind the Edge cache. Its main objective is decreasing requests to Facebook's disk-based storage backends, so the metric for its effectiveness is object-wise hit ratio. Facing high request rates for a large set of objects, both the Edge and Origin caches are equipped with flash drives. This work is motivated by the finding that SLRU, an advanced caching algorithm, can increase the byte-wise and object-wise hit ratios in the Facebook stack by up to 14% <ref type="bibr" target="#b19">[20]</ref>. However, two factors confound naive implementations of advanced caching algorithm on flash. First, the best algorithm for workloads at different cache sites varies. For example, since Huang et al. <ref type="bibr" target="#b19">[20]</ref>, we have found that GDSF achieves an even higher objectwise hit ratio than SLRU in the Origin cache by favoring smaller objects (see Section 6.2), but SLRU still achieves the highest byte-wise hit ratio at the Edge cache. Therefore, a unified framework for many caching algorithms can greatly reduce the engineering effort and hasten the deployment of new caching policies. Second, flashbased hardware has unique performance characteristics that often require software customization. In particular, a naive implementation of advanced caching algorithms may generate a large number of small random writes on flash, by inserting missed content or updating hit content. The next section demonstrates that modern flash devices perform poorly under such workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Flash Performance Study</head><p>This section presents a study of modern flash devices that motivates our designs. The study focuses on write workloads that stress the FTL on the devices because write throughput was the bottleneck that prevented Facebook from deploying advanced caching algorithms. Even for a read-only cache, writes are a significant part of the workload as missed content is inserted with a write. At Facebook, even with the benefits of advanced caching algorithms, the maximum hit ratio is ~70%, which results in at least 30% of accesses being writes.</p><p>Previous studies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">36]</ref> have shown that small random writes are harmful for flash. In particular, Min et  al. <ref type="bibr" target="#b36">[36]</ref> shows that at high space utilization, i.e., 90%, random write size must be larger than 16 MB or 32 MB to reach peak throughput on three representative SSDs in 2012, with capacities ranging between 32 GB and 64 GB. To update our understanding to current flash devices, we study the performance characteristics on three flash cards, and their specifications and major metrics are listed in <ref type="table">Table 1</ref>. All three devices are recent models from major vendors, 2 and A and C are currently deployed in Facebook photo caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Random Write Experiments</head><p>This subsection presents experiments that explore the trade-off space between write size and device overprovisioning on random write performance. In these experiments we used different sizes to partition the device and then perform aligned random writes of that size under varying space utilizations. We use the flash drive as a raw block device to avoid filesystem overheads. Before each run we use blkdiscard to clear the existing data, and then repeatedly pick a random aligned location to perform write/overwrite. We write to the device with 4 times the data of its total capacity before reporting the final stabilized throughput. In each experiment, the initial throughput is always high, but as the device becomes full, the garbage collector kicks in, causing FTL write amplification and dramatic drop in throughput. During garbage collection, the FTL often writes more data to the physical device than what is issued by the host, and the byte-wise ratio between these two write sizes is the FTL write amplification <ref type="bibr" target="#b18">[19]</ref>. <ref type="figure" target="#fig_0">Figure 2a</ref> and <ref type="figure" target="#fig_0">Figure 2b</ref> show the FTL write amplification and device throughput for the random write experiments conducted on the flash drive Model A. The figures illustrate that as writes become smaller or space utilization increases, write throughput dramatically decreases and FTL write amplification increases. For example, 8 MiB random writes at 90% device utilization achieve only 160 MiB/s, a ~3.7x reduction from the maximum 590 MiB/s. We also experimented with mixed read-write workloads and the same performance trend holds. Specifically, with a 50% read and 50% write workload, 8 MiB random writes 2 Vendor/model omitted due to confidentiality agreements. at 90% utilization lead to a ~2.3x throughput reduction. High FTL write amplification also reduces device lifespan, and as the erasure cycle continues to decrease for large capacity flash cards, the effects of small random writes become worse over time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b39">39]</ref>.</p><p>Similar throughput results on flash drive Model B are shown in <ref type="figure" target="#fig_0">Figure 2c</ref>. However, its FTL write amplification is not available due to the lack of monitoring tools for physical writes on the device. Our experiments on flash drive Model C (details elided due to space limitations) agree with Model A and B results as well. Because of the low throughput under high utilization with small write size, more than 1000 device hours are spent in total to produce the data points in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>While our findings agree with the previous study <ref type="bibr" target="#b36">[36]</ref> in general, we are surprised to find that under 90% device utilization, the minimum write size to achieve peak random write throughput has reached 256 MiB to 512 MiB. This large write size is necessary because modern flash hardware consists of many parallel NAND flash chips <ref type="bibr" target="#b2">[3]</ref> and the aggregated erase block size across all parallel chips can add up to hundreds of megabytes. Communications with vendor engineers confirmed this hypothesis. This constraint informs RIPQ's design, which only issues large aligned writes to achieve low write amplification and high throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sequential Write Experiment</head><p>A common method to achieve sustained high write throughput on flash is to issue sequential writes. The FTL can effectively aggregate sequential writes to parallel erase blocks <ref type="bibr" target="#b30">[30]</ref>, and on deletes and overwrites all the parallel blocks can be erased together without writing back any still-valid data. As a result, the FTL write amplification can be low or even avoided entirely. To confirm this, we also performed sequential write experiments to the same three flash devices. We observed sustained high performance for all write sizes above 128KiB as reported in <ref type="table">Table 1</ref>. <ref type="bibr" target="#b2">3</ref> This result motivates the design of SIPQ, which only issues sequential writes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RIPQ</head><p>This section describes the design and implementation of the RIPQ framework. We show how it approximates the priority queue abstraction on flash devices, present its implementation details, and then demonstrate that it efficiently supports advanced caching algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Priority Queue Abstraction</head><p>Our experience and previous studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">45]</ref> have shown that a Priority Queue is a general abstraction that naturally supports various advanced caching policies. RIPQ provides that abstraction by maintaining content in its internal approximate priority queue, and allowing cache operations through three primitives:</p><p>• insert(x, p): insert a new object x with priority value p.</p><p>• increase(x, p): increase the priority value of x to p.</p><p>• delete-min(): delete the object with the lowest priority.</p><p>The priority value of an object represents its utility to the caching algorithm. On a hit, increase is called to adjust the priority of the accessed object. As the name suggests, RIPQ limits priority adjustment to increase only. This constraint simplifies the design of RIPQ and still allows almost all caching algorithms to be implemented. On a miss, insert is called to add the accessed object. Delete-min is implicitly called to remove the object with the minimum priority value when a cache eviction is triggered by insertion. <ref type="figure">Figure 3</ref> shows the architecture of a caching solution implemented with the priority queue abstraction, where RIPQ's components are highlighted in gray. These components are crucial to avoid a smallrandom-writes workload, which can be generated by a naive implementation of priority queue. RIPQ's internal mechanisms are further discussed in Section 4.2.</p><p>Absolute/Relative Priority Queue Cache designers using RIPQ can specify the priority of their content based on access time, access frequency, size, and many other factors depending on the caching policy. Although traditional priority queues typically use absolute priority values that remain fixed over time, RIPQ operates on a different relative priority value interface. In a relative priority queue, an object's priority is a number in the <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> range representing the position of the object relative to the rest of the queue. For example, if an object i has a relative priority of 0.2, then 20% of the objects in queue have lower priority values than i and their positions are closer to the tail.</p><p>The relative priority of an object is explicitly changed when increase is called on it. The relative priority of an object is also implicitly decreased as other objects are inserted closer to the head of the queue. For instance, if an object j is inserted with a priority of 0.3, then all   objects with priorities ≤ 0.3 will be pushed towards the tail and their priority value implicitly decreased.</p><formula xml:id="formula_0">D V D V … D D V … D D V Head Tail Section Section񮽙 Section D D [1, p K-1 ] (p k , p k-1 ] (p 1 , 0</formula><p>Many algorithms, including the SLRU family, can be easily implemented with the relative priority queue interface. Others, including the GDSF family, require an absolute priority interface. To support these algorithms RIPQ translates from absolutes priorities to relative priorities, as we explain in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Design</head><p>RIPQ is a framework that converts priority queue operations into a flash-friendly workload with large writes. <ref type="figure" target="#fig_1">Figure 4</ref> gives a detailed illustration of the RIPQ components highlighted in <ref type="figure">Figure 3</ref>, excluding the Index Map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index Map</head><p>The Index Map is an in-memory hash table which associates all objects' keys with their metadata, including their locations in RAM or flash, sizes, and block IDs. The block structure is explained next.</p><p>In our system each entry is ~20 bytes, and RIPQ adds 2 bytes to store the virtual block ID of an object. Considering the capacity of the flash card and the average object size, there are about 50 million objects in one caching machine and the index is ~1GiB in total.</p><p>Queue Structure The major Queue Structure of RIPQ is composed of K sections that are in turn composed of blocks. Sections define the insertion points into the queue and a block is the unit of data written to flash. The relative priority value range is split <ref type="table">Table 2</ref>: SLRU and GDSF with the priority queue interface provided by RIPQ.</p><formula xml:id="formula_1">Algorithm Interface Used On Miss On Hit Segmented-L LRU Relative Priority Queue insert(x, 1 L ) increase(x, min(1,(1+�p·L�) L )) Greedy-Dual-Size-Frequency L Absolute Priority Queue insert(x, Lowest + c(x) s(x) ) increase(x, Lowest + c(x) min(L,n(x)) s(x) )</formula><p>into the K intervals corresponding to the sections:</p><formula xml:id="formula_2">[1, p K−1 ],...,(p k , p k−1 ],...,(p 1 , 0]. 4</formula><p>When an object is inserted into the queue with priority p, it is placed in the head of the section whose range contains p. For example, in a queue with sections corresponding to <ref type="bibr">[1, 0.7]</ref>, (0.7, 0.3] and (0.1, 0], an object with priority value 0.5 would be inserted to the head of second section. Similar to relative priority queues, when an object is inserted to a queue of N objects, any object in the same or lower sections with priority q is implicitly demoted from priority q to qN N+1 . Implicit demotion captures the dynamics of many caching algorithms, including SLRU and GDSF: as new objects are inserted to the queue, the priority of an old object gradually decreases and it is eventually evicted from the cache when its priority reaches 0.</p><p>RIPQ approximates the priority queue abstraction because its design restricts where data can be inserted. The insertion point count, K, represents the key design tradeoff in RIPQ between insertion accuracy and memory consumption. Each section has size O( 1 K ), so larger Ks result in smaller sections and thus higher insertion accuracy. However, because each active block is buffered in RAM until it is full and flushed to flash, the memory consumption of RIPQ is proportional to K. Our experiments show K = 8 ensures that that RIPQ achieves hit ratios similar to the exact algorithm, and we use this value in our experiments. With 256MiB device blocks, it translates to a moderate memory footprint of 2GiB.</p><p>Device and Virtual Blocks As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, each section includes one active device block, one active virtual block, and an ordered list of sealed device/virtual blocks. An active device block accepts insertions of new objects and buffers them in memory, i.e, the Block Buffer. When full it is sealed, flushed to flash, and transitions into a sealed device block. To avoid duplicating data on flash RIPQ lazily updates the location of an object when its priority is increased, and uses virtual blocks to track where an object would have been moved. The active virtual block at the head of each section accepts virtuallyupdated objects with increased priorities. When the active device block for a section is sealed, RIPQ also transitions the active virtual block into a sealed virtual block. Virtual update is an in-memory only operation, which sets the virtual block ID for the object in the Index Map, increases the size counter for the target virtual block, and decreases the size counter of the object's original block.</p><p>All objects associated with a sealed device block are stored in a contiguous space on flash. Within each block, a header records all object keys and their offsets in the data following the header. As mentioned earlier, an updated object is marked with its target virtual block ID within the Index Map. Upon eviction of a sealed device block, the block header is examined to determine all objects in the block. The objects are looked up in the Index Map to see if their virtual block ID is set, i.e., their priority was increased after insertion. If so, RIPQ reinserts the objects to the priorities represented by their virtual blocks. The objects move into active device blocks and their corresponding virtual objects are deleted. Because the updated object will not be written until the old object is about to be evicted, RIPQ maintains at most one copy of each object and duplication is avoided. In addition, lazy updates also allow RIPQ to coalesce all the priority updates to an object between its insertion and reinsertion.</p><p>Device blocks occupy a large buffer in RAM (active) or a large contiguous space on flash (sealed). In contrast, virtual blocks resides only in memory and are very small. Each virtual block includes only metadata, e.g., its unique ID, the count of objects in it, and the total byte size of those objects.</p><p>Naive Design One naive design of a priority queue on flash would be to fix an object's location on flash until it is evicted. This design avoids any writes to flash on priority update but does not align the location of an object with its priority. As a result the space of evicted objects on flash would be non-contiguous and the FTL would have to coalesce the scattered objects by copying them forward to reuse the space, resulting in significant FTL write amplification. RIPQ avoids this issue by grouping objects of similar priorities into large blocks and performing writes and evictions on the block level, and by using lazy updates to avoid writes on update.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementing Caching Algorithms</head><p>To demonstrate the flexibility of RIPQ, we implemented two families of advanced caching algorithms for evaluation: Segmented-LRU <ref type="bibr" target="#b26">[26]</ref>, and Greedy-Dual-SizeFrequency <ref type="bibr" target="#b11">[12]</ref>, both of which yield major caching performance improvement for Facebook photo workload. A summary of the implementation is shown in <ref type="table">Table 2</ref>.</p><formula xml:id="formula_3">Segmented-LRU Segmented-L LRU (S-L-LRU) maintains L LRU caches of equal size. On a miss, an Section񮽙 Section񮽙 Section … … … Active Sealed insert(x, p) Ac [1 , p K-1 ] (p k , p k-1 ] (p 1 , 0] Head Tail … … ti A S l d D V D V D V p k &gt; p ≥ p k-1 (a) Insertion … … D V D V Section񮽙 Section񮽙 … (p k , p k-1 ] (p i , p i-1 ] Head Tail …… D … D increase(x, p') p k &gt; p' ≥ p k-1 D D D …… T p i &gt; p ≥ p i-1 (b) Increase …… D V D V Section񮽙 Section񮽙 … (p k , p k-1 ] (p 1 , 0] Head Tail …… delete-min( ) D D D D Evicted1 E i t d D V Reinsert x delete d l t D (c) Delete-min</formula><p>Figure 5: Insertion, and increase, and delete-min operations in RIPQ.</p><p>object is inserted to the head of the L-th (the last) LRU cache. On a hit, an object is promoted to the head of the previous LRU cache, i.e., if it is in sub-cache l, it will be promoted to the head of the max(l − 1, 1)-th LRU cache. An object evicted from the l-th cache will go to the head of the (l + 1)-th cache, and objects evicted from the last cache are evicted from the whole cache. This algorithm was demonstrated to provide significant cache hit ratio improvements for the Facebook Edge and Origin caches <ref type="bibr" target="#b19">[20]</ref>. Implementing this family of caching algorithms is straightforward with the relative priority queue interface. On a miss, the object is inserted with priority value 1 L , equaling to the head of the L-th cache. On a hit, based on the existing priority p of the accessed object, RIPQ promotes it from the �(1 − p) · L�-th cache to the head of the previous cache with the new, higher priority min <ref type="bibr">(1, 1+�p·L� L</ref> ). With the relative priority queue abstraction, an object's priority is automatically decreased when another object is inserted/updated to a higher priority. When an object is inserted at the head of the l-th LRU cache, all objects in l-th to L-th caches are demoted, and ones at the tail of these caches will be either demoted to the next lower priority cache or evicted if they are in the last L-th cache-the dynamics of SLRU are exactly captured by relative priority queue interface.</p><p>Greedy-Dual-Size-Frequency The Greedy-Dual-Size algorithm <ref type="bibr" target="#b9">[10]</ref> provides a principled way to trade-off increased object-wise hit ratio with decreased byte-wise hit ratio by favoring smaller objects. It achieves an even higher object-wise hit ratio for the Origin cache than SLRU (Section 2), and is favored for that use case as the main purpose of Origin cache is to protect backend storage from excessive IO requests. Greedy-Dual-SizeFrequency <ref type="bibr" target="#b11">[12]</ref> (GDSF) improves GDS by taking frequency into consideration. In GDSF, we update the priority of an object x to be Lowest + c(x) · n s(x) upon its n-th access since it was inserted to the cache, where c(x) is the programmer-defined penalty for a miss on x, Lowest is the lowest priority value in the current priority queue, and s(x) is the size of the object. We use a variant of GDSF that caps the maximum value of the frequency of an object to L. L is similar to the number of segments in SLRU. It prevents the priority value of a frequently accessed object from blowing up and adapts better to dynamic workloads. The update rule of our variant of GDSF algorithm is thus</p><formula xml:id="formula_4">p(x) ← Lowest+c(x)· min(L,n)</formula><p>s(x) . Because we are maximizing object-wise hit ratio we set c(x) = 1 for all objects. GDSF uses the absolute priority queue interface.</p><p>Limitations RIPQ also supports many other advanced caching algorithms like LFU, LRFU <ref type="bibr" target="#b28">[28]</ref>, LRU-k <ref type="bibr" target="#b40">[40]</ref>, LIRS <ref type="bibr" target="#b23">[24]</ref>, SIZE <ref type="bibr" target="#b0">[1]</ref>, but there are a few notable exceptions that are not implementable with a single RIPQ, e.g., MQ <ref type="bibr" target="#b48">[48]</ref> and ARC <ref type="bibr" target="#b34">[34]</ref>. These algorithms involve multiple queues and thus cannot be implemented with one RIPQ. Extending our design to support them with multiple RIPQs coexisting on the same hardware is one of our future directions. A harder limitation comes from the update interface, which only allows increasing priority values. Algorithms that decrease the priority of an object on its access, such as MRU <ref type="bibr" target="#b12">[13]</ref>, cannot be implemented with RIPQ. MRU was designed to cope with scans over large data sets and does not apply to our use case.</p><p>RIPQ does not support delete/overwrite operation because such operations are not needed for static content such as photos. But, they are necessary for a generalpurpose read-write cache and adding support for them is also one of our future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation of Basic Operations</head><p>RIPQ implements the three operations of a regular priority queue with the data structures described above.</p><p>Insert(x, p) RIPQ inserts the object to the active device block of section k that contains p, i.e., p k &gt; p ≥ p k−1 . <ref type="bibr" target="#b4">5</ref> The write will be buffered until that active block is sealed. <ref type="figure">Figure 5a</ref> shows an insertion.</p><p>Increase(x, p) RIPQ avoids moving object x that is already resident in a device block in the queue. Instead, RIPQ virtually inserts x into the active virtual block of section k that contains p, i.e., p k &gt; p ≥ p k−1 , and logically removes it from its current location. Because we remember the virtual block ID in the object entry in the indexing hash table, these steps are simply implemented by setting/resetting the virtual block ID of the object entry, and updating the size counters of the blocks and sec-  tions accordingly. No read/write to flash is performed during this operation. <ref type="figure">Figure 5b</ref> shows an update.</p><p>Delete-min() We maintain a few reserved blocks on flash for flushing the RAM buffers of device blocks when they are sealed. <ref type="bibr" target="#b5">6</ref> When the number of reserved blocks falls below this threshold, the Delete-min() operation is called implicitly to free up the space on flash. As shown in <ref type="figure">Figure 5c</ref>, the lowest-priority block in queue is evicted from queue during the operation. However, because some of the objects in that blocks might have been updated to higher places in the queue, they need to be reinserted to maintain their correct priorities. The reinsertion (1) reads out all the keys of the objects in that block from the block header, (2) queries the index structure to find whether an object, x, has a virtual location, and if it has one, (3) finds the corresponding section, k, of that virtual block and copies the data to the active device block of that section in RAM, and (4) finally sets the virtual block field in the index entry to be empty. We call this whole process materialization of the virtual update. These reinsertions help preserve caching algorithm fidelity, but cause additional writes to flash. These additional writes cause implementation write amplification, which is the byte-wise ratio of host-issued writes to those required to inserted cache misses. RIPQ can explicitly trade lower caching algorithm fidelity for lower write amplification by skipping materialization of the virtual objects whose priority is smaller than a given threshold, e.g., in the last 5% of the queue. This threshold is the logical occupancy parameter θ (0 &lt; θ &lt; 1).</p><p>Internal operations RIPQ must have neither too many nor too few insertion points: too few leads to low accuracy, and too many leads to high memory usage. To avoid these situations RIPQ splits a section when it grows too large and merges consecutive sections when their total size is too small. This is similar to how B-tree <ref type="bibr" target="#b6">[7]</ref> splits/merges nodes to control the size of the nodes and the depth of the tree.</p><p>A parameter α controls the number of sections of RIPQ in a principled way. α is in (0, 1) and determines <ref type="bibr" target="#b5">6</ref> It is not a critical parameter and we used 10 in our evaluation. No data is moved on flash for a split or merge. Splitting a section creates a new active device block with a write buffer and a new active virtual block. Merging two sections combines their two active device blocks: the write buffer of one is copied into the write buffer of the other. Splitting happens often and is how new sections are added to queue as objects in the section at the tail are evicted block-by-block. Merging is rare because it requires the total size of two consecutive sections to shrink from 2α (α is the size of a new section after a split) to α to trigger a merge. The amortized complexity of a merge per operation provided by the priority queue API is only O( 1 αM ), where M is the number of blocks.</p><p>Supporting Absolute Priorities Caching algorithms such as LFU, SIZE <ref type="bibr" target="#b0">[1]</ref>, and Greedy-Dual-Size <ref type="bibr" target="#b9">[10]</ref> require the use of absolute priority values when performing insertion and update. RIPQ supports absolute priorities with a mapping data structure that translates them to relative priorities. The data structure maintains a dynamic histogram that supports insertion/deletion of absolute priority values, and when given an absolute priorities return approximate quantiles, which are used as the internal relative priority values. The histogram consists of a set of bins, and we merge/split bins dynamically based on their relative sizes, similar to the way we merge/split sections in RIPQ. We can afford to use more bins than sections for this dynamic histogram and achieve higher accuracy of the translation, e.g., κ = 100 bins while RIPQ only uses K = 8 sections, because the bins only contains absolute priority values and do not require a large dedicated RAM buffer as the sections do. Consistent sampling of keys to insert priority values to the histogram can be fur-  ther applied to reduce its memory consumption and insertion/update complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Other Design Considerations</head><p>Parameters <ref type="table" target="#tab_6">Table 3</ref> describes the parameters of RIPQ and the value chosen for our implementation. The block size B is chosen to surpass the threshold for a sustained high write throughput for random writes, and the number of blocks M is calculated directly based on cache capacity. The number of blocks affects the memory consumption of RIPQ, but this is dominated by the size of the write buffers for active blocks and the indexing structure. The number of active blocks equals the number of insertion points K in the queue. The average section size α is used by the split and merge operations to bound the memory consumption and approximation error of RIPQ.</p><p>Durability Durability is not a requirement for our static-content caching use case, but not having to refill the entire cache after a power loss is a plus. Fortunately, because the keys and locations of the objects are stored in the headers of the on-flash device blocks, all objects that have been saved to flash can be recovered, except for those in the RAM buffers. The ordering of blocks/sections can be periodically flushed to flash as well and then used to recover the priorities of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Theoretical Analysis</head><p>RIPQ is a practical approximate priority queue for implementing caching algorithms on flash, but enjoys some good theoretical properties as well. In the appendix of a longer technical report <ref type="bibr" target="#b44">[44]</ref> we show RIPQ can simulate a LRU cache faithfully with 4α of additional space: if α = 0.125, this would mean RIPQ-based LRU with 50% additional space would provably include all the objects in an exact LRU cache. In general RIPQ with adjusted insertion points can simulate a S-L-LRU cache with 4Lα of additional space. It is also easy to show the number of writes to the flash is ≤ I + U, where I is the number of inserts and U is the number of updates.</p><p>Using K sections/insertion points, the complexity of finding the approximate insertion/update point takes O(K), and the amortized complexity of split/merge internal operations is O(1), so the amortized complexity of RIPQ is only O(K). If we arrange the sections in a red-black tree, it can be further reduced to O(log K). In comparison to this, with N objects, an exact implementation of priority queues using red-black tree would take O(log N) per operation, and a Fibonacci heap takes O(log N) per delete-min operation. (K � N, K is typically 8, N is typically 50 million). The computational complexity of these exact, tree and heap based data structures are not ideal for a high performance system. In contrast, RIPQ hits the sweet spot with fast operations and high fidelity, in terms of both theoretical analysis and empirical hit ratios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SIPQ</head><p>RIPQ's buffering for large writes creates a moderate memory footprint, e.g., 2 GiB DRAM for 8 insertion points with 256 MiB block size in our implementation. This is not an issue for servers at Facebook, which are equipped with 144 GiB of RAM, but limits the use of RIPQ in memory-constrained environments. To cope with this issue, we propose the simpler Single Insertion Priority Queue (SIPQ) framework.</p><p>SIPQ uses flash as a cyclic queue and only sequentially writes to the device for high write throughput with minimal buffering. When the cache is full, SIPQ reclaims device space following the same sequential order. In contrast to RIPQ, SIPQ maintains an exact priority queue of the keys of the cached objects in memory and does not co-locate similarly prioritized objects physically due to the single insertion limit on flash. The drawback of this approach is that reclaiming device space may incur many reinsertions for SIPQ in order to preserve its priority accuracy. Similar to RIPQ, these reinsertions constitute the implementation write amplification of SIPQ.</p><p>To reduce the implementation write amplification, SIPQ only includes the keys of a portion of all the cached objects in the in-memory priority queue, referred to as the virtual cache, and will only reinsert evicted objects that are in this cache. All on-flash capacity is referred to as the physical cache and the ratio between the total byte size of objects in the virtual cache to the size of the physical cache is controlled by a logical occupancy parameter θ (0 &lt; θ &lt; 1). Because only objects in the virtual cache are reinserted when they are about to be evicted from the physical cache, θ provides a trade-off between priority fidelity and implementation write amplification: the larger θ , the more objects are in the virtual cache and the higher fidelity SIPQ has relative to the exact caching algorithm, and on the other hand the more likely evicted objects will need to be reinserted and thus higher write amplification caused by SIPQ. For θ = 1, SIPQ implements an exact priority queue for all cached data on flash, but incurs high write amplification for reinsertions. For θ = 0, SIPQ deteriorates to FIFO with no priority enforcement. For θ in between, SIPQ performs additional writes compared to FIFO but also delivers part of the improvement of more advanced caching algorithms. In our evaluation, we find that SIPQ provides a good trade-off point for Segmented-LRU algorithms with θ = 0.5, but does not perform well for more complex algorithms like GDSF. Therefore, with limited improvement at almost no additional device overhead, SIPQ can serve as a simple upgrade for FIFO when memory is tight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We compare RIPQ, SIPQ, and Facebook's current solution, FIFO, to answer three key questions:</p><p>1. What is the impact of RIPQ and SIPQ's approximations of caching algorithms on hit ratios, i.e., what is the effect on algorithm fidelity? 2. What is the write amplification caused by RIPQ and SIPQ versus FIFO? 3. What throughput can RIPQ and SIPQ achieve? 4. How does the hit-ratio of RIPQ change as we vary the number of insertion points?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>Implementation We implemented RIPQ and SIPQ with 1600 and 600 lines of C++ code, respectively, using the Intel TBB library <ref type="bibr" target="#b21">[22]</ref> for the object index and the C++11 thread library <ref type="bibr" target="#b8">[9]</ref> for the concurrency mechanisms. Both the relative and absolute priority interfaces (enabled by an adaptive histogram translation) are supported in our prototypes.</p><p>Hardware Environment Experiments are run on servers equipped with a Model A 670GiB flash device and 144GiB DRAM space. All flash devices are configured with 90% space utilization, leaving the remaining 10% for the FTL.</p><p>Framework Parameters RIPQ uses a 256MiB block size to achieve high write throughput based on our performance study of Model A flash in Section 3. It uses α = 0.125, i.e., 8 sections, to provide a good trade-off between the fidelity to the implemented algorithms and the total DRAM space RIPQ uses for buffering: 256MiB × 8 = 2GiB, which is moderate for a typical server. SIPQ also uses the 256MiB block size to keep the number of blocks on flash the same as RIPQ. Because SIPQ only issues sequential writes, its buffering size could be further shrunk without adverse effects. Two logical occupancy values for SIPQ are used in evaluation: 0.5, and 0.9, each representing a different trade-off between the approximation fidelity to the exact algorithm and implementation write amplification. These two settings are noted as SIPQ-0.5 and SIPQ-0.9, respectively.</p><p>Caching Algorithms Two families of advanced caching algorithms, Segmented-LRU (SLRU) <ref type="bibr" target="#b26">[26]</ref> and Greedy-Dual-Size-Frequency (GDSF) <ref type="bibr" target="#b11">[12]</ref>, are evaluated on RIPQ and SIPQ. For Segmented-LRU, we vary the number of segments from 1 to 3, and report their results as SLRU-1, SLRU-2, and SLRU-3, respectively. We similarly set L from 1 to 3 for GreedyDual-Size-Frequency, denoted as GDSF-1, GDSF-2, and GDSF-3. Description of these algorithms and their implementations on top of the priority queue interface are explained in Section 4.3. Results of 4 segments or more for SLRU and L ≥ 4 for GDSF are not included due to their marginal differences in the caching performance.</p><p>Facebook Photo Trace Two sets of 15-day sampled traces collected within the Facebook photo-serving stack are used for evaluation, one from the Origin cache, and the other from a large Edge cache facility. The Origin trace contains over 4 billion requests and 100TB worth of data, and the Edge trace contains over 600 million requests and 26TB worth of data. To emulate different total cache capacities in Origin/Edge with the same space utilization of the experiment device and thus controlling for the effect of FTL, both traces are further down sampled through hashing: we randomly sample 1 2 , 1 3 , and 1 4 of the cache key space of the original trace for each experiment to emulate the effect of increasing the total caching capacity to 2X, 3X, and 4X. We report experimental results at 2X because it closely matches our production configurations. For all evaluation runs, we use the first 10-day trace to warm up the cache and measure performance during the next 5 days. Because both the working set and the cache size are very large, it takes hours to fill up the cache and days for the hit ratio to stabilize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Results</head><p>This section presents our experimental results regarding the algorithm fidelity, write amplification, and throughput of RIPQ and SIPQ with the Facebook photo trace. We also include the hit ratio, write amplification and throughput achieved by Facebook's existing FIFO solution as a baseline. For different cache sites, only their target hit ratio metrics are reported, i.e., object-wise hit ratio for the Origin trace and byte-wise hit ratio for the Edge trace. Exact algorithm hit ratios are obtained via simulations as the baseline to judge the approximation fidelity of implementations on top of RIPQ and SIPQ.  Performance of Exact Algorithms We first investigate hit ratios achieved by the exact caching algorithms to determine the gains of a fully accurate implementation. Results are shown in <ref type="figure" target="#fig_5">Figure 7</ref>.</p><p>For object-wise hit ratio on the Origin trace, <ref type="figure" target="#fig_5">Figure 7a</ref> shows that GDSF family outperforms SLRU and FIFO by a large margin. At 2X cache size, GDSF-3 increases the hit ratio over FIFO by 17%, which translates a to a 23% reduction of backend IOPS. For byte-wise hit ratio on the Edge trace, <ref type="figure" target="#fig_5">Figure 7b</ref> shows that SLRU is the best option: at 2X cache size, SLRU-3 improves the hit ratio over FIFO by 4.5%, which results in a bandwidth reduction between Edge and Origin by 10%. GDSF performs poorly on the byte-wise metric because it down weights large photos. Because different algorithms perform best at different sites with different performance metrics, flexible frameworks such as RIPQ make it easy to optimize caching policies with minimal engineering effort.</p><p>Approximation Fidelity Exact algorithms yield considerable gains in our simulation, but are also challenging to implement on flash. RIPQ and SIPQ make it simple to implement the algorithms on flash, but do so by approximating the algorithms. To quantify the effects of this approximation we ran experiments presented in <ref type="figure">Fig- ures 8a</ref> and 8d. These figures present the hit ratios of different exact algorithms (in simulations) and their approximate implementations on flash with RIPQ, SIPQ-0.5, and SIPQ-0.9 (in experiments) at 2X cache size setup from <ref type="figure" target="#fig_5">Figure 7</ref>. The implementation of FIFO is the same as the exact algorithm, so we only report one number. In general, if the hit ratio of an implementation is similar to the exact algorithm the framework provides high fidelity.</p><p>RIPQ consistently achieves high approximation fidelities for the SLRU family, and its hit ratios are less than 0.2% different for object-wise/byte-wise metric compared to the exact algorithm results on Origin/Edge trace. For the GDSF family, RIPQ's algorithm fidelity becomes lower as the algorithm complexity increases. The greatest "infidelity" seen for RIPQ is a 5% difference on the Edge trace for GDSF-1. Interestingly, for the GDSF family, the infidelity generated by RIPQ improves byte-wise hit ratio-the largest infidelity was a 5% improvement on byte-wise hit-ratio compared to the exact algorithm. The large gain on byte-wise hit ratio can be explained by the fact that the exact GDSF algorithm is designed to trade byte-wise hit ratio for object-wise hit ratio through favoring small objects, and its RIPQ approximation shifts this trade-off back towards a better byte-wise hit-ratio. Not shown in the figures (due to space limitation) is that RIPQ-based GDSF family incurs about 1% reduction in object-wise hit ratio. Overall, RIPQ achieves high algorithm fidelity on both families of caching algorithms that perform the best in our evaluation.</p><p>SIPQ also has high fidelity when the occupancy parameter is set to 0.9, which means 90% of the caching capacity is managed by the exact algorithm. SIPQ-0.5, despite only half of the cache capacity being managed by the exact algorithm, still achieves a relatively high fidelity for SLRU algorithms: it creates a 0.24%-2.8% object-wise hit ratio reduction on Origin, and 0.3%-0.9% byte-wise hit ratio reduction on Edge. These algorithms tend to put new and recently accessed objects towards the head of the queue, which is similar to the way SIPQ inserts and reinserts objects at the head of the cyclic queue on flash. However, SIPQ-0.5 provides low fidelity for the GDSF family, causing object-wise hit ratio to decrease on Origin and byte-wise hit ratio to increase on Edge. Within these algorithms, objects may have diverse priority values due to their size differences even if they enter the cache at the same time, and SIPQ's single insertion point design results in a poor approximation.</p><p>Write Amplification <ref type="figure">Figure 8b</ref> and 8e further show the combined write amplification (i.e., FT L × implementation) of different frameworks. RIPQ consistently achieves the lowest write amplification, with an exception for SLRU-1 where SIPQ-0.5 has the lowest value for both traces. This is because SLRU-1 (LRU) only inserts to one location at the queue head, which works well with SIPQ, and the logical occupancy 0.5 further reduces the reinsertion overhead. Overall, the write amplification of RIPQ is largely stable regardless of the complexity of the caching algorithms, ranging from 1.17 to 1.24 for the SLRU family, and from 1.14 to 1.25 for the GDSF family.  <ref type="figure">Figure 8</ref>: Performance of RIPQ, SIPQ, and FIFO on Origin and Edge.</p><p>SIPQ-0.5 achieves moderately low write amplifications but with lower fidelity for complex algorithms. Its write amplification also increases with the algorithm complexity. For SLRU, the write implementation for SIPQ-0.5 rises from 1.08 for SLRU-1 to 1.52 for SLRU-3 on Origin, and from 1.11 to 1.50 on Edge. For GDSF, the value ranges from 1.33 for GDSF-1 to 1.37 to GDSF-3 on Origin, and from 1.36 to 1.39 on Edge. Results for SIPQ-0.9 observe a similar trend for each family of algorithms, but with a much higher write amplification value for GDSF around 5-6.</p><p>Cache Throughput Throughput results are shown in <ref type="figure">Figure 8c</ref> and 8f. RIPQ and SIPQ-0.5 consistently achieve over 20 000 requests per second (rps) on both traces, but SIPQ-0.9 has considerably lower throughput, especially for the GDSF family of algorithms. FIFO has slightly higher throughput than RIPQ based SLRU, although the latter has higher byte hit ratio and correspondingly fewer writes from misses. This performance is highly related to the write amplification results because in all three frameworks (1) workloads are write-heavy with below 63% hit ratios, and our experiments are mainly write-bounded with a sustained write-throughput around 530 MiB/sec, (2) write amplification proportionally consumes the write throughput, which further throttles the overall throughput. This is why SIPQ-0.9 often with the highest write amplification has the lowest throughput, and also why RIPQ based SLRU has lower throughput than FIFO. However, RIPQ/SIPQ-0.5 still provides high performance for our use case, with RIPQ paticularly achieving over 24 000 rps on both traces. The slightly lower throughput comparing to FIFO (less than 3 000 rps difference) is well worth the hit-ratio improvement which translates to a decrease of backend I/O load and a decrease of bandwidth between Edge and Origin. <ref type="figure">Figure 9</ref> shows the effect of varying the number of insertion points in RIPQ on approximation accuracy.  <ref type="figure">Figure 9</ref>: Object-wise hit ratios sensitivity on approximate number of insertion points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity Analysis on Number of Insertion Points</head><p>When K ≈ 2 (α = 1 2 ), a section in RIPQ can grow to the size of the entire queue before it splits. In this case RIPQ effectively degenerates to FIFO with equivalent hit-ratios. The SLRU-3 hit ratio saturates quickly when K 񮽙 4, while GDSF-3 reaches its highest performance only when K 񮽙 8. GDSF-3 uses many more insertion points in an exact priority queue than SLRU-3 and RIPQ thus needs more insertion points to effectively colocate content with similar priorities. Based on this analysis we have chosen α = 1 8 for RIPQ in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>To the best of our knowledge, no prior work provides a flexible framework for efficiently implementing advanced caching algorithms on flash. Yet, there is a large body of related work in several heavily-researched fields.</p><p>Flash-based Caching Solutions Flash devices have been applied in various caching solutions for their large capacities and high I/O performance <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b46">46]</ref>. To avoid their poor handling of small random write workloads, previous studies either use sequential eviction akin to FIFO <ref type="bibr" target="#b1">[2]</ref>, or only perform coarse-grained caching policies at the unit of large blocks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b46">46]</ref>. Similarly, SIPQ and RIPQ also achieve high write throughputs and low device overheads on flash through sequential writes and large aligned writes, respectively. In addition, they allow efficient implementations of advanced caching policies at a fine-grained object unit, and our experience show that photo caches built on top of RIPQ and SIPQ yield significant performance gains at Facebook. While our work mainly focuses on the support of eviction part of caching operations, techniques like selective insertions on misses <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">46]</ref> are orthogonal to RIPQ and can be applied to further reduce the data written to flash. 7</p><p>RAM-based Advanced Caching Caching has been an important research topic since the early days of computer science and many algorithms have been proposed to better capture the characteristics of different workloads. Some well-known features include recency (LRU, MRU <ref type="bibr" target="#b12">[13]</ref>), frequency (LFU <ref type="bibr" target="#b33">[33]</ref>), inter-reference time <ref type="bibr">(LIRS [24]</ref>), and size (SIZE <ref type="bibr" target="#b0">[1]</ref>). There have also been a plethora of more advanced algorithms that consider multiple features, such as Multi-Queue <ref type="bibr" target="#b48">[48]</ref> and Segmented LRU (SLRU) <ref type="bibr" target="#b26">[26]</ref> for both recency and frequency, Greedy-Dual <ref type="bibr" target="#b47">[47]</ref> and its variants like GreedyDual-Size <ref type="bibr" target="#b9">[10]</ref> and Greedy-Dual-Size-Frequency <ref type="bibr" target="#b11">[12]</ref> (GDSF) using a more general method to compose the expected miss penalty and minimize it. While more advanced algorithms can potentially yield significant performance improvements, such as SLRU and GDSF for Facebook photo workload, a gap still remains for efficient implementations on top of flash devices because most algorithms are hardware-agnostic: they implicitly assume data can be moved and overwritten with little overhead. Such assumptions do not hold on flash due to its asymmetric performance for reads and writes and the performance deterioration caused by its internal garbage collection.</p><p>Our work, RIPQ and SIPQ, bridges this gap. They provide a priority queue interface to allow easy imple- <ref type="bibr" target="#b6">7</ref> We tried such techniques on our traces, but found the hit ratio dropped because of the long-tail accesses for social network photos. mentation of many advanced caching algorithms, providing similar caching performance while generating flashfriendly workloads.</p><p>Flash-based Store Many flash-based storage systems, especially key-value stores have been recently proposed to work efficiently on flash hardware. Systems such as FAWN-KV <ref type="bibr" target="#b5">[6]</ref>, SILT <ref type="bibr" target="#b32">[32]</ref>, LevelDB <ref type="bibr" target="#b15">[16]</ref>, and RocksDB <ref type="bibr" target="#b13">[14]</ref> group write operations from an upper layer and only flush to the device using sequential writes. However, they are designed for read-heavy workloads and other performance/application metrics such as memory footprints and range-query efficiencies. As a result, these systems make trade-offs such as conducting onflash data sorting and merges, that yield high device overhead for write-heavy workloads. We have experimented with using RocksDB as an on-flash photo store for our application, but found it to have excessively high write amplification (~5 even when we allocated 50% of the flash space to garbage collection). In contrast, RIPQ and SIPQ are specifically optimized for a (random) writeheavy workload and only support caching-required interfaces, and as a result have low write amplification.</p><p>Studies on Flash Performance and Interface While flash hardware itself is also an important topic, works that study the application perceived performance and interface are more related to our work. For instance, previous research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b43">43]</ref> that reports the random write performance deterioration on flash helps verify our observations in the flash performance study.</p><p>Systematic approaches to mitigate this specific problem have also been previously proposed at different levels, such as separating the treatment of cold and hot data in the FTL by LAST <ref type="bibr" target="#b29">[29]</ref>, and the similar technique in filesystem by SFS <ref type="bibr" target="#b36">[36]</ref>. These approaches work well for skewed write workloads where only a small subset of the data is hot and updated often, and thus can be grouped together for garbage collection with lower overhead. In RIPQ, cached contents are explicitly tagged with priority values that indicate their hotness, and are co-located within the same device block if their priority values are close. In a sense, such priorities provide a prior for identifying content hotness.</p><p>While RIPQ (and SIPQ) runs on unmodified commercial flash hardware, recent studies <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b41">41]</ref> which co-design flash software/hardware could further benefit RIPQ by reducing its memory consumption.</p><p>Priority Queue Both RIPQ and SIPQ rely on the priority queue abstract data type and the design of priority queues with different performance characteristics have been a classic topic in theoretical computer science as well <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18]</ref>. Instead of building an exact priority queue, RIPQ uses an approximation to trade algorithm fidelity for flash-aware optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Flash memory, with its large capacity, high IOPS, and complex performance characteristics, poses new opportunities and challenges for caching. In this paper we present two frameworks, RIPQ and SIPQ, that implement approximate priority queues efficiently on flash. On top of them, advanced caching algorithms can be easily, flexibly, and efficiently implemented, as we demonstrate for the use case of a flash-based photo cache at Facebook. RIPQ achieves high fidelity and low write amplification for the SLRU and GDSF algorithms. SIPQ is a simpler design, requires less memory and still achieves good results for simple algorithms like LRU. Experiments on both the Facebook Edge and Origin traces show that RIPQ can improve hit ratios by up to ~20% over the current FIFO system, reducing bandwidth consumption between the Edge and Origin, and reducing I/O operations to backend storage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Random write experiment on Model A and Model B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overall structure of RIPQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: RIPQ internal operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>the average size of sections.</head><label></label><figDesc>RIPQ splits a section when its relative size-i.e., a ratio based on the object count or byte size-has reached 2α. For example, if α = 0.3 then a section of [0.4, 1.0] would be split to two sections of [0.4, 0.7) and [0.7, 1.0] respectively, shown in Figure 6a. RIPQ merges two consecutive sections if the sum of their sizes is smaller than α, shown in Figure 6b. These op- erations ensure there are at most 񮽙 2 α 񮽙 sections, and that each section is no larger than 2α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Byte-wise hit ratios on Edge trace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Exact algorithm hit ratios on Facebook trace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙񮽙񮽙񮽙 񮽙񮽙</head><label></label><figDesc></figDesc><table>񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙 

񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 

(a) Write amplification for Model A 

񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙 
񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙 

񮽙 
񮽙񮽙 
񮽙񮽙 
񮽙񮽙񮽙񮽙 

(b) Throughput for Model A 

񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 
񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙 

(c) Throughput for Model B 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Parameter Symbol Our Value Description and Goal Block Size B 256MiB To satisfy the sustained high random write throughput. Number of Blocks M 2400 Flash caching capacity divided by the block size. Average Section Size α 0.125 To bound the number of sections ≤ �2/α� and the size of each section ≤ 2α, trade-off parameter for insertion accuracy and RAM buffer usage. Insertion Points K 8 Same as the number of sections, controlled by α and proportional to RAM buffer usage. Logical Occupancy θ 0 Avoid reinsertion of items that will soon be permanently evicted.</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Key parameters of RIPQ for a 670GiB flash drive currently deployed in Facebook. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>The number of insertion points, K, is roughly inversely proportional to α, so we vary K to</head><label>number</label><figDesc></figDesc><table>be approximately 
2, 4, 8, 16, and 32, by varying α from 1 
2 , 1 
4 , 1 
8 , 1 
16 to 

1 

32 . We measure approximation accuracy empirically 
through the object-wise hit-ratios of RIPQ based SLRU-
3 and GDSF-3 on the origin trace with 2X cache size. 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙񮽙񮽙 

񮽙 
񮽙 
񮽙 
񮽙񮽙 
񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙 
񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

</table></figure>

			<note place="foot" n="1"> Though the stack was originally designed to serve photos, now it handles videos, attachments, and other static binary objects as well. We use &quot;objects&quot; to refer to all targets of the cache in the text.</note>

			<note place="foot" n="3"> Write amplification is low for tiny sequential writes, but they attain lower throughput as they are bound by IOPS instead of bandwidth.</note>

			<note place="foot" n="376"> 13th USENIX Conference on File and Storage Technologies (FAST &apos;15) USENIX Association</note>

			<note place="foot" n="4"> We have inverted the notation of intervals from [low, high) to (high, low] to make it consistent with the priority order in the figures.</note>

			<note place="foot" n="5"> A minor modification when k = K is 1 = p k ≥ p ≥ p k−1 .</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Removal policies in network caches for World-Wide Web documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Standridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Abdulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Log-structured cache: trading hit-rate for storage performance (and winning) in mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aghayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX INFLOW</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for SSD Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Janus: Optimal Flash Provisioning for Cloud Storage Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waliji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Coehlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rethinking flash in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">FAWN: A fast array of wimpy nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSP</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Organization and maintenance of large ordered indexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mccreight</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">uFLIP: Understanding Flash IO Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bouganim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jnsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://en.cppreference.com/w/cpp/thread" />
		<title level="m">C++11 Thread Support Library</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cost-Aware WWW Proxy Caching Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX USITS</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The soft heap: an approximate priority queue with optimal error rate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chazelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Role of aging, frequency, and size in web cache replacement policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cherkasova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ciardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer HPCN</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An evaluation of buffer management strategies for relational database systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Facebook Database Engineering Team. RocksDB, A persistent key-value store for fast storage environments</title>
		<ptr target="http://rocksdb.org" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fibonacci heaps and their uses in improved network optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Fredman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">LevelDB, A fast and lightweight key/value database library by Google</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://github.com/google/leveldb" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ASPLOS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Data structures and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
<note type="report_type">AddisonWeely</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Write amplification analysis in flashbased solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eleftheriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pletka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SYSTOR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An Analysis of Facebook Photo Caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving flash-based disk cache with Lazy Adaptive Replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MSST</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<ptr target="https://www.threadingbuildingblocks.org" />
	</analytic>
	<monogr>
		<title level="j">Intel Thread Building Blocks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">uCache: A Utility-Aware Multilevel SSD Cache Management Policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE HPCC EUC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LIRS: an efficient low inter-reference recency set replacement policy to improve buffer cache performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metrics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Data center evolution: A tutorial on state of the art, issues, and challenges. Computer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Caching strategies to improve disk system performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Karedla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Wherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving NAND flash based disk caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kgil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE ISCA</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LRFU: A Spectrum of Policies That Subsumes the Least Recently Used and Least Frequently Used Policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LAST: locality-aware sector translation for NAND flash memory-based storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A log buffer-based flash translation layer using fully-associative sector translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nitro: A Capacity-Optimized SSD Cache for Primary Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smaldone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SILT: A memory-efficient, high-performance keyvalue store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cache management algorithms for flexible filesystems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maffeis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ARC: A SelfTuning, Low Overhead Replacement Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated Server Flash Cache Space Management in a Virtualization Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uttamchandani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SFS: Random write considered harmful in solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Eom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Flashcache at Facebook: From 2010 to</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mituzas</surname></persName>
		</author>
		<ptr target="https://www.facebook.com/notes/10151725297413920" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netflix</surname></persName>
		</author>
		<ptr target="https://www.netflix.com/openconnect" />
	</analytic>
	<monogr>
		<title level="j">Netflix Open Connect</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving performance and lifetime of the SSD RAID-based host cache through a log-structured approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX INFLOW</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The LRU-K Page Replacement Algorithm for Database Disk Buffering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMOD</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SDF: software-defined flash for webscale internet storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM ASPLOS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Flashtier: a lightweight, consistent and durable storage cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM EuroSys</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Evaluating and repairing write performance on flash devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Athanassoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<editor>ACM DAMON</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">RIPQ Princeton Technical Report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://www.cs.princeton.edu/research/techreps/TR-977-15" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Proxy caching that estimates page load delays. Computer Networks and ISDN Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wooster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abrams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hec: improving endurance of high performance flashbased cache devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Plasson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Talagala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SYSTOR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The k-server dual and loose competitiveness for paging. Algorithmica</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The Multi-Queue Replacement Algorithm for Second Level Buffer Caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
