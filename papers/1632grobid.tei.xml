<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Victim Disk First: An Asymmetric Cache to Boost the Performance of Disk Arrays under Faulty Conditions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenggang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghui</forename><surname>Zhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology Virginia Commonwealth University Wuhan</orgName>
								<address>
									<postCode>430074, 23284</postCode>
									<settlement>Richmond</settlement>
									<region>VA</region>
									<country>China, USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Victim Disk First: An Asymmetric Cache to Boost the Performance of Disk Arrays under Faulty Conditions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The buffer cache plays an essential role in smoothing the gap between the upper-level computational components and the lower-level storage devices. A good buffer cache management scheme should be beneficial to not only the computational components, but also to the storage components by reducing disk I/Os. Existing cache replacement algorithms are well optimized for disks in normal mode, but inefficient under faulty scenarios, such as a parity-based disk array with faulty disk(s). To address this issue, we propose a novel asymmet-ric buffer cache replacement strategy, named Victim (or faulty) Disk(s) First (VDF) cache, to improve the reliability and performance of a storage system consisting of a buffer cache and disk arrays. The basic idea is to give higher priority to cache the blocks on the faulty disks when the disk array fails, thus reducing the I/Os directed to the faulty disks. To verify the effectiveness of the VDF cache, we have integrated VDF into two popular cache algorithms LFU and LRU, named VDF-LFU and VDF-LRU, respectively. We have conducted extensive simulations as well as a prototype implementation. The simulation results show that VDF-LFU can reduce disk I/Os to surviving disks by up to 42.3% and VDF-LRU can reduce those by up to 36.2%. Our measurement results also show that VDF-LFU can speed up the online recovery by up to 46.3% under a spare-rebuilding mode with online reconstruction, or improve the maximum system service rate by up to 47.7% under a degraded mode without a reconstruction workload. Similarly, VDF-LRU can speed up the online recovery by up to 34.6%, or improve the system service rate by up to 28.4%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To reduce the number of I/O requests to the low level storage device, such as disk arrays, a cache is widely used and many cache algorithms exist to hide the long disk latencies. These cache algorithms work well for disk arrays under normal fault-free mode. However, when some disks in a disk array fail, the RAID may still work under this faulty scenario, either in a spare-rebuilding mode with online reconstruction or in a degraded mode without online reconstruction. The cost of a miss to faulty disks might be dramatically different compared to the cost of a miss to surviving disks. Existing cache algorithms cannot capture this difference because they treat the underlying (faulty or surviving) disks the same.</p><p>We take an example as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, which illustrates two different cache miss situations in a storage subsystem composed of a parity-based RAID with one faulty disk in degraded mode. As shown in <ref type="bibr">Fig- ure 1(a)</ref>, the missed data resides in the faulty disk. The RAID controller accesses the surviving disks to fetch all data and parity in the same stripe to regenerate the lost data. Therefore, to service one cache miss, several read requests are needed depending on the RAID organization. However, if the missed data is in a surviving disk as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(b), only one read request to the corresponding surviving disk is generated. Similar situations are observed in spare-rebuilding mode. A simple analysis shows that in a RAID5 system consisting of n disks, when a disk fails, the cost to fetch data from a faulty disk might be n − 1 times higher than the cost to access data from a surviving disk. This extra disk I/O activity will in turn reduce the effective array bandwidth available for reconstruction or user access.</p><p>When a disk array starts online reconstruction, it uses up regular bandwidth. Compared to offline reconstruction, during the process of online reconstruction, the user workflow interferes with the reconstruction workflow. As a result, the online reconstruction duration grows significantly compared to offline reconstruction. Wu et al. <ref type="bibr" target="#b0">[1]</ref> point out that, in a heavy user workflow, the duration of online reconstruction would grow as much as 70 times as that of the offline reconstruction. In this (a) A read miss to a faulty disk might result in several additional read requests to the surviving disks.</p><p>(b) A read miss to a surviving disk would result in only one request to the corresponding surviving disk. case, more requests to the surviving disks, caused by user requests, reduce the available reconstruction bandwidth and lengthen the reconstruction duration, which reduces the reliability of the storage system.</p><p>On the other hand, in a degraded mode without a reconstruction workflow, a miss to faulty disks would cause all the surviving data in the same parity chain (stripe in RAID-5) to be read and add additional workflow to surviving disks. With a decreasing serviceability and an increasing user workflow caused by misses to faulty disks, the storage subsystem might be overloaded under a heavy user workflow.</p><p>Therefore, in parity-based disk arrays under faulty conditions, a miss to faulty disks is much more expensive than a miss to surviving disks. Based on this observation, we propose an asymmetric buffer cache replacement strategy, named Victim (or faulty) Disk(s) First cache, or VDF for short, to improve the performance of storage subsystem composed of a parity-based disk array and its buffer cache. The basic idea is to design a cache scheme to treat the faulty disks more favorably, or give higher priority to cache the data associated with the faulty disks. The goal of this scheme is to reduce the cache miss directed to the faulty disk, and thus to reduce the I/O requests to the surviving disks overall. Reduced disk I/O caused by the user workflow will (1) improve the performance of the disk array, and (2) allow more bandwidth for online reconstruction which in turn speeds up the recovery, and thus improves the reliability. We make the following four contributions in this paper:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">We proposed a new metric, Requests Generation</head><p>Ratio or RGR, to capture the disk I/O activities of user workflows on the surviving disks when a storage system is under faulty conditions. This would directly influence the maximum bandwidth for reconstruction in a spare-rebuilding mode and the bandwidth available to user workflows in a degraded mode.</p><p>2. We developed a novel cache-replacement scheme, VDF, by giving higher priority to cache the data associated with the faulty disks, to minimize the RGR. VDF is flexible and could be integrated into existing cache algorithms such as LRU and LFU.</p><p>3. We conducted extensive simulations to verify the effectiveness of VDF under different workloads. The simulation results show that VDF-LRU can reduce overall disk I/Os to surviving disks by up to 36.2% and VDF-LFU can reduce those by up to 42.3%.</p><p>4. We implemented VDF in the Linux software RAID system. As a result, VDF-LFU can speed up the online recovery by up to 46.3% under spare-rebuilding mode, or improve the maximum system service rate by up to 47.7% under degraded mode. Similarly, VDF-LRU can speed up the online recovery by up to 34.6%, or improve the system service rate by up to 28.4%.</p><p>The rest of the paper is organized as follows: Section 2 gives a brief overview of the background information and related work. In Section 3, we describe our new metric, RGR, and the design of VDF. A case study of VDF cache is given in Section 4; we describe integrating VDF into two typical cache-replacement algorithms, LRU and LFU, based on RAID-5. We provide our simulation results of VDF in Section 5, and prototyping and measurement results in Section 6. We conclude our paper and describe future work in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>In this section we briefly overview some background materials and related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Optimizations of Disk Arrays under Faulty Conditions</head><p>Redundant Arrays of Independent Disks RAID <ref type="bibr" target="#b1">[2]</ref> are popular solutions to provide high performance and reliability for today's storage systems. Depending on its organizations, RAID could prevent data loss incurred by disk failures and even offer online services under faulty conditions. With a faulty disk, these RAIDs would work in a spare-rebuilding mode to support online reconstruction, or in a degraded mode without reconstruction. RAID can offer continuous online services even in faulty mode. However, the recovery workload and user request can interfere with each other, and lead to longer recovery times. Many solutions are proposed to solve this problem, such as optimizations of data/parity/spare layout <ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref><ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref>, reconstruction workload <ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref><ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>, and user workload <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Menon et al. present a method to distribute spares to all disks, which would not only reduce the lost data per disk but also parallelize the reconstruction <ref type="bibr" target="#b4">[4]</ref>. Holland et al. <ref type="bibr" target="#b3">[3]</ref> propose a trade-off between RAID-1 (mirror) and RAID-5, named parity declustering, to balance the storage efficiency and the recovery performance. Xin et al. use a RUSH-like hash algorithm to evenly distribute data, parity, and spares among the nodes in a distributed environment <ref type="bibr" target="#b5">[5]</ref>.</p><p>The track-based recovery (TBR) <ref type="bibr" target="#b8">[8]</ref> algorithm provides a trade-off between block-based recovery and cylinder-based recovery, and balances the user response time and the recovery duration. However, TBR requires much more buffer space compared to block-based recovery. The pipelined recovery (PR) scheme <ref type="bibr" target="#b9">[9]</ref> addresses this problem, and significantly reduces the buffer requirements close to that of the block-based recovery algorithms. The disk-oriented recovery (DOR) algorithm <ref type="bibr" target="#b10">[10]</ref> rebuilds the array at the disk-level instead of the stripe-level. With this approach, DOR could absorb the bandwidth of the array as much as possible. The popularity-based recovery (PRO) algorithm <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>, builds upon the DOR algorithm, further improving the recovery performance by utilizing the spatial locality of user requests.</p><p>Two techniques named redirection of reads and piggybacking of writes <ref type="bibr" target="#b13">[13]</ref> are proposed to reduce the user workflow by employing the reconstructed spare disk to absorb parts of the requests to the faulty disk. However, they need to maintain a bitmap in the dedicated cache in the RAID device to record the reconstruction status; as the increasing of disk size, a fine-granularity bitmap would consume too much memory, and increase synchronization costs. For example, a bitmap with granularity of 4KB for a 2TB disk would require 64MB of memory, which limits the use of piggybacking of writes. During the reconstruction, at most a coarse-granularity bitmap could be used only to redirect reads. MICRO <ref type="bibr" target="#b14">[14]</ref> achieves improved recovery performance by writing back the in-memory surviving data of the faulty disks into a spare disk first and using a file popularity table to find the hotspot. MICRO treats all the blocks in the cache equally, which is similar to the general cachereplacement algorithms and has the same limitations. WorkOut <ref type="bibr" target="#b0">[1]</ref>, an array-cache-array method, offloads the write requests and popular read requests to another disk array. As a result, WorkOut speeds up the recovery process and improves the user response time. However, WorkOut requires another disk array to help with the reconstruction and need maintain an addressing translation map, which might be much larger than a fine-granularity bitmap, in the dedicated cache on the RAID device. This suffers from the same problem as redirection of reads and piggybacking of writes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Buffer Cache Replacement Algorithms</head><p>RAID-based storage systems usually work together with the buffer cache. To improve the efficiency of the buffer cache, researchers have proposed many cachereplacement algorithms, such as LRU <ref type="bibr" target="#b15">[15]</ref>, LFU, FBR <ref type="bibr" target="#b16">[16]</ref>, LRU-k <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>, 2Q <ref type="bibr" target="#b19">[19]</ref>, LRFU <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21]</ref>, MQ <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">23]</ref>, LIRS <ref type="bibr" target="#b25">[24,</ref><ref type="bibr" target="#b26">25]</ref>, ARC <ref type="bibr" target="#b27">[26]</ref>, DULO <ref type="bibr" target="#b28">[27]</ref>, DISKSEEN <ref type="bibr" target="#b29">[28]</ref> and more. Each cache-replacement algorithm weigh the cached blocks with a different method, such as access interval, access frequency and so on, then decide which cached blocks to evict. The LRU (Least-Recently-Used) algorithm is one of the most popular and effective policies for buffer cache management. When a block needs to be inserted into the cache, the candidate to be evicted is the block which is least recently used. That is to say, the weight of the cached blocks in LRU is its last access timestamp. The block with the smallest last access timestamp is evicted. The LFU (Least-Frequently-Used) algorithm replaces the least frequently used block. In other words, the weight of the cached blocks in LFU is its number of accesses. The block with the smallest number of accesses is evicted. Other algorithms, such as LRU-k, 2Q, LRFU, MQ, LIRS, and ARC, integrate LRU and LFU algorithm together and demonstrate good performance under various scenarios. DULO and DISKSEEN consider both temporal and spatial locality when a block needs to be replaced.</p><p>However, the above cache-replacement algorithms work well when the RAID system is under normal operating mode. When some disks in the RAID system fail, it runs under faulty condition, but the buffer cache layer is not aware of the underlying failures in RAID and thus the existing cache algorithms do not work well as explained in Section 1. This motivates us to propose VDF: a cache scheme to treat the faulty disks more favorably, or give a higher priority to cache the data associated with faulty disks. The goal is to reduce the cache misses directed to the faulty disk and thus to reduce the I/O requests to the surviving disks overall. As our VDF only increases the weight of blocks in the faulty disks, theoretically it could work with the above-mentioned general cache-replacement algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design of VDF</head><p>In this section, we propose a new metric to describe the cache efficiency of disk I/O activities. We show how to use it to evaluate disk arrays under faulty conditions, and then we describe our VDF scheme. Before our discussion, we summarize the symbols in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RGR: A New Metric to Evaluate Cache Performance with Various Miss Penalty</head><p>Traditional cache-replacement algorithms are essentially evaluations on access probability of cached blocks, based on the assumption that the penalty of each miss at the same level is the same. However, in parity-based RAID with faulty disk(s), the penalty of a miss to the lost data in the faulty disks might be much more expensive than that of a miss to surviving data. Therefore, from the aspect of a RAID device, the buffer cache performance should not be simply evaluated by the traditional metrics such as Hit Ratio or Miss Ratio, particularly when the RAID is under faulty conditions. To address this issue, we propose a new metric called Requests Generation Ratio or RGR. This is the ratio of the number of requested blocks to the surviving disks and the number of the requested blocks to buffer cache, to evaluate the cache performance from the view point of a faulty RAID device. RGR represents the disk activities to service an I/O request to the buffer cache. Ideally, if all I/O requests are serviced by the buffer cache, RGR will be 0 (no disk I/Os are generated). For missed I/O requests, RGR will be different depending on the penalty to each underlying disk. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>(a) the RGR of a miss to the faulty disk is 4 because 4 disk I/Os are generated to service the missed I/O request, and in <ref type="figure" target="#fig_0">Figure 1</ref>(b), the RGR of a miss to surviving disks is 1.</p><p>To calculate the RGR, we assume a parity-based RAID of T data blocks with a buffer cache of C blocks. The access probability of a block B i is p i , where 0 ≤ i ≤ T −1, with a miss penalty of M P i in terms of the total requested blocks to surviving disks caused by a miss. From the viewpoint of a certain workload, p i actually represents the ratio of the number of request on B i and the number of total block requests. If block B i is not referenced in this workload, p i should be 0. As we have mentioned above, different cache algorithms evaluate p i with different approaches in runtime environments. If a block is serviced by the cache, the corresponding miss penalty M P i = 0. Therefore, the RGR of the next block request can be described by the following Equation 1.</p><formula xml:id="formula_0">RGR = T −1 i=0 (p i × M P i )<label>(1)</label></formula><p>3.2 Using RGR to Evaluate the Cache Efficiency in Faulty Mode</p><p>Consider a system composed of a buffer cache and a RAID in faulty mode which service a certain user workload. We have the following symbols. First, the total serviceability of all surviving disks is BW in terms of I/O bandwidth. Second, the unfiltered user workload would take BW U bandwidth, which is the service rate of the system from a user's perspective. The average RGR of the buffer cache is RGR. Therefore, the filtered user workload should take about BW U × RGR bandwidth. Third, all the remaining bandwidth BW R of all surviving disks could be utilized for reconstruction. Lastly, the total amount of surviving data for reconstruction is Q. Equation 2 describes the relationships among BW , BW U , RGR, and BW R .</p><formula xml:id="formula_1">BW = BW U × RGR + BW R<label>(2)</label></formula><p>We first consider the spare-rebuilding mode. The surviving disks would suffer from more requests as explained in Section 1. It means that the I/O bandwidth available for reconstruction on the surviving disks would be less than the I/O bandwidth for reconstruction on the spare disk. The total amount of requested data for reconstruction on each disk (including the surviving disks and the spare disks) is the same. Therefore, to the online recovery process, the I/O bandwidth for reconstruction on the surviving disks is the bottleneck. The reconstruction duration RD could be described with Equation 3.</p><formula xml:id="formula_2">RD = Q BW − BW U × RGR<label>(3)</label></formula><p>From Equation 3, we can find that, if Q, BW , and BW U are fixed, with the decreasing RGR, the reconstruction duration RD (and thus MTTR) decreases. Therefore, to minimize the MTTR, we should minimize the RGR.</p><p>We next consider the degraded mode without reconstruction. Each surviving disk would suffer from the extra requests caused by the access to faulty disks. The filtered user workload should not exceed the total serviceability of all surviving disks. In another words, the maximum unfiltered user workload BW U should not exceed BW RGR . Therefore, we should minimize the RGR to maximize the system serviceability, which is described with a maximum BW U .</p><p>From the above discussion, we notice that compared to the traditional metrics on cache evaluation, such as miss ratio, RGR is a useful metric to demonstrate two important indicators of a faulty disk array more clearly and directly. One is the reconstruction time which is directly related to MTTR and affects the system reliability. The other is the throughput that indicates the performance of the storage system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">VDF Cache</head><p>Based on the above analysis, we propose our VDF cache aiming at reducing the RGR for parity-based RAID under faulty conditions, either to enhance the system reliability by speeding up the reconstruction process in sparerebuilding mode, or to improve the system performance by increasing the system serviceability in degraded mode without reconstruction workloads. As it operates at the buffer cache level, VDF is practical and does not suffer from the same problems of the small dedicated cache in a RAID controller.</p><p>Cache-replacement algorithms are essentially evaluations on access probability of cached blocks. Once a miss occurs, typically a block should be evicted from cache, and the missed block would be loaded to the free space. General replacement algorithms evict the block with the smallest access probability to reduce the total access probability of the remaining blocks out of buffer cache. However, to minimize the RGR, the eviction of a block should not only be determined by the access probability but also by the miss penalty. In our VDF cache, we adopt the same evaluation approach of access probability p i for each cached block as the general cache. Furthermore, the miss penalty M P i of each block is evaluated with the requested blocks to the surviving disks of this block; the block with the minimum product of p i × M P i is evicted from the cache to minimize the RGR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Case Study of VDF</head><p>To verify the effectiveness of VDF, we apply VDF in a RAID-5 system of n disks where one disk fails. We focus on read operations for two reasons: first, in many applications, users are typically sensitive to read latency, particularly in a disk array under faulty conditions; second, in many storage systems, independent non-volatile memory is deployed as a write cache to enhance the reliability and this cache uses a dedicated write cache algorithm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Integrating VDF into LRU and LFU</head><p>Although VDF cache can cooperate with caches at other levels by adjusting the miss penalty of blocks, for demonstration purposes we just consider a one-level buffer cache above the disk array. Therefore, the miss penalty of blocks on the faulty disk would be n − 1 in our following discussion, which means one cache miss to the faulty disk will result in n − 1 I/O requests to the surviving disks. To integrate VDF with any cache algorithm, the access probability should be evaluated with a quantitative approach. Different cache-replacement algorithms evaluate the access probability of blocks using different approaches. Most existing cache management algorithms can be categorized into LRU-like and LFUlike algorithms. In LRU-like algorithms, the weight of blocks is often evaluated by the access time interval. As it is costly to record the real access timestamp, a simple alternative is to record the access sequence number, and use the reciprocal of the interval access sequence number as the access probability. This approach is widely used in many LRU-like algorithms. In LFU-like algorithms, the weight of blocks is majorly evaluated by the access frequency. Thus, to integrate VDF into these LFU-like algorithms, it needs only to keep the original evaluation approach. Furthermore, different from the access sequence number, the access frequency of two blocks might be the same. Therefore, in VDF based LFU-like algorithms, the access sequence number is also employed for choosing which block to evict with the same access frequency. In VDF cache the access probability of a block would not be the absolute but the relative value, because both the reciprocal of interval of access sequence number and the access frequency are actually the relative values.</p><p>The conversion from the original cache algorithms to the VDF-based algorithms should be smooth, because Algorithm 1: VDF-LRU for RAID-5 with n disks Input: The request stream x1, x2, x3, ..., xi, ... VDF LRU Replace(xi){ /*For every i ≥ 1 and any xi, one and only one of the following cases must occur.*/ if xi is in LS k ,0 ≤ k &lt; n then /*A cache hit has occurred.*/ Update T S of xi, by T S = GT S; Move xi to the heads of LS k and GS. else /*A cache miss has occurred.*/ if Cache is full then foreach block at the bottom of LSj , 0 ≤ j &lt; n do if LSj is a corresponding stack to a faulty disk then Its weight W =GT S − T S; else Its weight W =(GT S − T S) * (n − 1);</p><p>Delete the block with maximum W to obtain a free block; else /*Cache is not full.*/ Get a free block. Load xi to the free block. Update T S of xi, by T S = GT S; Add xi to the heads of GS and the corresponding LS. Update GT S, by GT S = GT S + 1; } VDF takes effect in faulty mode. In other words, the buffer cache should be managed with the original algorithms in fault-free mode, and the VDF policy becomes effective when disk failures occur. Thus, a smooth runtime conversion between the original algorithm and the VDF-based algorithm is needed, which is quite different from the general cache algorithms. Therefore, in VDF-based algorithm, we employ two types of stacks to achieve the smooth runtime conversion: one is the global stack (GS) which is similar to the stack in a general algorithm such as global LRU stack, and the other is the local stack (LS) holding the blocks on the same disk in cache. All blocks should be in two types of stacks concurrently as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. When the system works in fault-free mode, it evicts the block with the smallest weight at the bottom of the GS stack. Once a disk array drops to a faulty mode, it evicts the block with the smallest weight at the bottom of each LS stack instead of evicting the block at the bottom of the GS stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detailed Description of VDF-LRU and VDF-LFU</head><p>Detailed descriptions of VDF-LRU and VDF-LFU for ndisk RAID-5 are given in Algorithm 1 and Algorithm 2, respectively, using the variables summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Simulation Results and Analysis</head><p>To evaluate the effectiveness of VDF, we conducted simulations under three typical workloads: SPC-1-web, LM-TBE, and DTRS. Delete the block with minimum W and GT S − T S to obtain a free block; else /*Cache is not full.*/ Get a free block. Load xi to the free block. Initialize the frequency F and T S of xi, by F = 1 and T S = GT S; Move xi to right place of LS k and GS according to F and T S. Update GT S, by GT S = GT S + 1; } SPC-1-web, a trace used in the SPC-1 benchmark suites, was collected in a search engine, which is widely used in the evaluation of storage systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b14">14]</ref>. LM-TBE and DTRS are provided by Microsoft Corporation collected in 2008. The LM-TBE trace was collected in back-end servers supporting a front-end Live Maps application. The DTRS trace was collected in a file server accessed by more than 3000 users to download various daily builds of Microsoft Visual Studio. Both traces were taken in a period of 24 hours and broken into pieces with 1-hour intervals <ref type="bibr" target="#b30">[29]</ref>. We choose only the piece with most intensive I/O activities. For fairness and simplicity, we consider only the read operations and all block sizes are 4KB. We report RGR of LRU, LFU, VDF-LFU, and VDF-LRU under these workloads as shown in <ref type="figure" target="#fig_2">Figures 3,  4</ref>, and 5, respectively.</p><p>Our simulator, named VDF-Sim, is written in C and the source code is approximately 3000 lines. It slices/splits the trace records into block requests as the input. Data blocks in a stack or blocks with the same hash values are linked via double circular lists. For a certain block in our simulator, we record its logical offset as the unique ID since the disk array is transparent to the upper level systems such as a file system. According to the data/parity distribution of the disk array and the logical offset of a block, it is easy to identify on which disk the block resides. The arriving timestamps of the requests are also recorded to evaluate p i as we mentioned in Section 4 and to generate the misses trace used in the prototype discussed in the next section.</p><p>The results show that, compared to the original LRU and LFU algorithms, VDF optimized algorithms achieved better performance consistently by reducing the RGR. Compared to LRU, VDF-LRU reduces the RGR by up to 31.4%, 36.2%, and 22.7% under SPC-1-web, LM-TBE, and DTRS traces, respectively. Compared to LFU, VDF-LFU reduces the RGR by up to 42.3%, 39.4%, and 24.4%, respectively.</p><p>We find that the efficiency of VDF grows with the increased number of disks under the same number of cache-resident blocks in most cases. The efficiency of VDF is more significant with a moderate number of cache-resident blocks than that with a too small or too large number of cache-resident blocks. This can be explained as follows. The cache-resident blocks of a faulty disk in the original algorithm would occupy 1/n cache space with total n disks. With the fixed cache-resident blocks and the increased n, the number of cache-resident blocks of the faulty disk would be smaller. From the aspect of cache management, the impact of the marginal utility of blocks on hit ratio tends to decrease with the increased cache size. For example, adding P 1 blocks to a cache with P 2 blocks might improve the hit ratio with a larger gain compared to adding P 1 blocks to cache with P 3 blocks when P 2 &lt; P 3. Thus, the marginal utility of blocks would be more obvious with more disks and thus the efficiency of VDF grows accordingly. However, if the number of cache-resident blocks is too small, it is hard to find hot blocks even with an extended period due   to the large access interval. On the other hand, if the number of cache-resident blocks is too large, most of the requested blocks from the faulty disks would be cached, and the marginal utility of blocks becomes insufficient.</p><p>We also find that the VDF strategy becomes more efficient with LFU than LRU under the three workloads. One possible reason is that the temporal locality of these traces is weak as they are server-end traces and already filtered by upper level caches. Thus, the Stack Depth Distribution property of these traces is weak. As a result, the Independent Reference Model property of these traces would be relatively improved. Although we improve the weight of the blocks on faulty disk to n − 1 times in both VDF-LRU and VDF-LFU, the efficiency is not the same. Mostly, the caching duration of blocks from a victim disk in VDF-LFU is longer than that in VDF-LRU, especially when the total number of cacheresident blocks is not large. Therefore, VDF-LFU works better than VDF-LRU in these traces in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Prototyping of VDF</head><p>To further evaluate VDF, we implemented a prototype of VDF in a software RAID system in Linux known as MD. In this section, we present our measurement results, including the efficiency of online recovery duration in full-bandwidth reconstruction mode and system service rate under the degraded mode without reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Methodology</head><p>Measurements on real world systems are welcome in research of computer systems. However, implementation in a real system is a lengthy process and always complex and challenging. Here, we use a straightforward and accurate measurement approach to evaluate the efficiency of VDF. The architecture of our prototype is shown in <ref type="figure" target="#fig_5">Figure 6</ref>. First, we collect the cache miss information during our simulation in Section 5, which includes not only the block ID but also the real access timestamps. Then, we treat the RAID as a file device, and use an application in user mode to play the traces we have collected from our simulations which is similar to RAIDmeter <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">11]</ref>. However, the difference is that our application uses direct I/O (available in Linux 2.6 and up) instead of buffered I/O to avoid the requests being recached by the file system buffer cache. All missed I/O requests sent to the MD layer directly. Thus our simulation and the application join together to exploit the buffer cache and replacement algorithm. The trace player is also written in C and the source code is approximately 500 lines.</p><p>In our experiment, we evaluated the effectiveness of VDF, including the online reconstruction duration in full- bandwidth reconstruction mode, and the system serviceability in the degraded mode without reconstruction. For online reconstruction in full-bandwidth reconstruction mode, an open-loop measurement approach is adopted, where all filtered traces are played according to their timestamps as recorded in the original trace file. For degraded mode, a closed-loop measurement approach is adopted, where all filtered traces are played only according to their original sequence one by one and without any interval, to find the system serviceability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experimental Environment</head><p>In our experiment, we employ a SuperMicro storage server with two Intel(R) Xeon(R) X5560 @2.67GHz (six cores) CPUs, 12GB DDR3 main memory. All disks are Western Digital WD10EALS Caviar Blue SATA2, which are connected by an Adaptec 31605 SAS/SATA RAID controller with a 256MB dedicated cache. We disabled the RAID function of the controller and only used the direct I/O mode to connect each disk. The operating system is Linux Fedora 12 x86 64 with the kernel version of 2.6.32.</p><p>In Linux, there is a software implementation of RAID called Multiple Devices MD, which is popular in verification of RAID optimization scheme <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">11]</ref>. To facilitate the analysis and verification of VDF cache, we also used MD as our experimental platform. We used the default settings of MD: the chunk size is 64KB, the number of stripe-heads is 256 and the data layout is left-symmetric. In our open-loop testing, the minimum </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disks</head><p>Blocks LRU (s) VDF-LRU (s) Improvement LFU (s) VDF-LFU (s) Improvement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">disks</head><p>reconstruction bandwidth is set to 100MBps to utilize all remaining bandwidth for reconstruction besides the bandwidth taken by user workloads.</p><p>As VDF targets the storage server consisting of disk arrays which run under faulty conditions, we chose the server-end trace SPC-1-web as our experimental material, which spans a 60GB dataset. The workload is filtered by 131,072 to 524,288 blocks in our simulation to generate the experimental inputs. In the open-loop measurement, we test VDF with 5 to 8 disks. The results are reported in terms of reconstruction speed. The improvements of VDF over the original LRU and LFU are calculated. In the close-loop measurement, we used a multi-threaded application to play the filtered trace to measure the service rate. We also evaluated the impact of thread number to service rate, in addition to the impact of the number of blocks and the number of disks. The results are reported as system service rate improvement by VDF cache compared to original LFU and LRU. We ran each test three times and report the average. The results are very stable and consistent as the difference among all three rounds was very small (less than 5%). is larger than BW U . Therefore, the change rate of improvement according with the number of total disks should only be determined by the changing rates of BW U × RGR ORI and BW . Obviously, BW linearly grows with the total number of disks. From the simulation result, we can find that the changing rate of BW U × RGR ORI is slower than that of BW with the number of disks from 5 to 8. Therefore, in most of the above cases, the improvement of reconstruction durations of VDF cache to original cache decreases with the increased number of disks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Open-loop Measurement Results and Analysis</head><formula xml:id="formula_3">RD Imprv = BW U * RGR ORI RGR V DF − BW U BW RGR V DF − BW U (4)</formula><p>With the same number of disks, we notice that the reconstruction duration of the trace filtered by 131,072 blocks is less than the reconstruction duration of trace filtered by 262,144 blocks in many cases. On one hand, as we use a number of blocks to warm up the cache, this part of the miss information is not recorded in our filtered trace file. The first 131,072 block misses in the  trace filtered by 262,144 blocks has a lower average arrival rate than the remaining part. On the other hand, when the number of blocks in the cache is 131,072 or 262,144, the cache is too small to find the hot blocks, which implies fewer hits in those cases and the RGRs are similar. Therefore, the reconstruction duration of the trace filtered by 131,072 blocks might be less than that of the trace filtered by 262,144 blocks in those cases. From the experimental results, we find that VDF is effective in improving the system service rate. Compared to LFU, VDF-LFU improves the system service rate up to 46.8% with 60 threads under 8 disks and 262,144 blocks. Compared to LRU, VDF-LRU improves the system service rate by up to 28.4% with 80 threads under 8 disks and 524,288 blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Close-loop Measurement Results and Analysis</head><p>With the increasing number of I/O threads, the service rate improvement increases accordingly and gets close to the theoretical value calculated by the simulation results. Although the whole trace would be evenly distributed on all disks due to the round-robin addressing in RAID, the incoming user requests might not be evenly distributed on all the disks during a short period. Therefore, with a larger number of threads, which implies a longer scheduling window, the distribution of incoming user requests is more balanced and the user service rate is closer to the maximum system service rate.</p><p>With the same number of blocks and a fixed number of threads, the service rate improvement of VDF-LRU to LRU is consistent with the trend of the simulation result. However, to our surprise, the results of VDF-LFU to LFU were just opposite with the trend of the simulation result. As per our analysis, this was primarily due to two reasons. First, from the simulation result, the relative RGR reduction of VDF-LFU to LFU with 524,288 blocks is in a small area from 33.7% to 35.6%. Second, the number of concurrent threads is fixed, which means that the number of threads per disk would increase with the decreased total number of disks. Thus, based on the above analysis, when the total number of disks is small, the improvement is closer to the theoretical value. Therefore, under close to theoretical peak service rates and more I/O threads per disk, the trend of service rate improvement of VDF-LFU to LFU is very possibly opposite with the trend of the simulation result. As a result, with the same number of disks and a fixed number of threads, which means a fixed number of I/O threads per disk, the service rate improvement is quite consistent with the trend of the simulation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Further Discussion</head><p>Several more issues deserve further discussion. The first issue is the implementation cost of VDF. As we mentioned in Section 4, to make the smooth conversion between the original cache algorithms and the VDF-based algorithms, two types of stacks should be employed to implement VDF cache. This adds both spatial and temporal overhead. The spatial overhead include the extra information in each block head such as the timestamp and the extra stack pointer of the local stack. Compared to the buffer cache size, this overhead is very small. The temporal overhead is the computation of the weight of block at the bottom of each LS stack. Due to the high computation ability of today's CPU, this should not influence the overall system performance.</p><p>Second, can we integrate VDF into other optimizations in faulty mode? As the VDF cache essentially reduces the user requests to the surviving disks, it can be integrated with other optimizations in faulty mode, such as optimization on data/parity/spare layout and reconstruction workloads. The approach of redirection of reads utilizes the reconstructed data in a spare disk to serve part of the reads to the faulty disk. Thus the miss penalty of these reconstructed data block is zero in terms of extra requests to the surviving disks. There still exist hot data with large miss penalty on faulty disks. Therefore, VDF can still help.</p><p>Third, could RGR be suitable to describe the status of write operation? From its definition, RGR is determined by M P i and p i . The calculation of p i in write operations is similar to read operations. However, the calculation of M P i in write operations is quite different from read operations, as they might be done with two approaches based on the parity distribution in RAID with faulty disk(s). One is the Read-Modify-Write, and the other is Parity-Reconstruction-Write. Here, we take an example of short writes on an n-disk RAID-5 with one faulty disk to demonstrate the M P i calculation for write operations. Once a short write is sent to the surviving disks and the corresponding parity is not on the faulty disk, the Read-Modify-Write should be performed which results in two reads and two writes on the surviving disks, and thus the M P i is 4. Otherwise, the ParityReconstruction-Write should be performed which results in n − 1 reads and one write on the surviving disks, and thus the M P i is n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Future Work</head><p>In this paper, we present an asymmetric buffer cache replacement strategy, named Victim (or faulty) Disk(s) First (VDF) cache, to improve the reliability and performance of a RAID-based storage system, particularly under faulty conditions. The basic idea of VDF is to treat the faulty disks more favorably, or give a higher priority to cache the data associated with the faulty disks. The benefit of this scheme is to reduce number of the cache miss directed to the faulty disk, and thus to reduce the I/O requests to the surviving disks overall. Less disk I/O activity caused by the user workflow will (1) improve the performance of the disk array, and (2) allow more bandwidth for online reconstruction which in turn speeds up the recovery, and thus improves the reliability. Our results based on both simulation and prototyping implementation has demonstrated the effectiveness of VDF in terms of reduced disk I/O activities and a faster recovery.</p><p>To further understand VDF, we have the following plans as our future work. First, we plan to build VDF into more general cache algorithms such as CLOCK <ref type="bibr" target="#b31">[30]</ref> and ARC <ref type="bibr" target="#b27">[26]</ref>. Second, we are working to implement VDF in the kernel level and thus to directly run real benchmarks to conduct more extensive measurements. Third, in addition to RAID-5, we will investigate the scheme to apply VDF to other RAID levels such as RAID-4 and RAID-6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two typical cache-miss situations in a storage subsystem composed of a parity-based RAID in a degraded mode.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: VDF implementation with two types of stacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Simulation results under the SPC-1-web trace. The number of disks ranges from 5 to 8, and number of cache blocks varies from 64K to 2M with the block size of 4KB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Simulation results under the LM-TBE trace with various numbers of disks and cache blocks. The block size is 4KB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Simulation results under the DTRS trace with various numbers of disks and cache blocks. The block size is 4KB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Architecture of VDF prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Service rate improvement of VDF in degraded mode of a RAID-5 of 8 disks. The number of blocks is 524,288 and the number of threads ranges from 20 to 80.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Service rate improvement of VDF in degraded mode of a RAID-5 of 8 disks. The number of blocks ranges from 131,072 to 524,288 and the number of threads is 60.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figures 7 ,</head><label>7</label><figDesc>Figures 7, 8, and 9 present the close-loop testing results under different scenarios with various numbers of threads, disks, and data blocks. The results are reported as service rate improvement, which is inversely proportional to the Play Duration (PD) of a whole filtered trace. For example, the corresponding service rate improvement of VDF-LRU to LRU should be calculated by P D LRU −P D V DF −LRU P D V DF −LRU .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Variables and Definitions</head><label>1</label><figDesc></figDesc><table>Symbols 
Definition 
C 
Total number of blocks in the buffer cache 
T 
Total number of data blocks in a disk array 
B i 
Data block i 
p i 
Access probability of each block B i 
M P i 
Miss penalty of each block B i 
BW 
Total serviceability of all surviving disks in terms of I/O bandwidth 
BW U 
I/O bandwidth available to user workload, or service rate of the system from the user's point of view 
BW R 
I/O bandwidth for an online reconstruction workload 
RGR 
The ratio of the # of requested blocks to surviving disks and the # of requested blocks to buffer cache 
Q 
Total amount of data from surviving disks to reconstruct faulty disks 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Variants in VDF and Explanation</head><label>2</label><figDesc></figDesc><table>Variants 
Explanation 
x 
A block request to buffer cache 
LS 
The local stack holding the blocks on one certain disk in the buffer cache 
GS 
The global stack holding all the blocks on all the disks in the buffer cache 
n 
The total number of disks including the faulty disk and surviving disks 
T S 
The timestamp of a block: records the access sequence number 
F 
The access frequency of a block 
GT S 
The global timestamp: it is equal to the timestamp of currently accessed block 
W 
The weight of a block 

Algorithm 2: VDF-LFU for RAID5 of n disks 

Input: The request stream x1, x2, x3, ..., xi, ... 
VDF LFU Replace(xi){ 
/*For every i ≥ 1 and any xi, one and only one of the following cases 
must occur.*/ 
if xi is in LS k ,0 ≤ k &lt; n then 
/*A cache hit has occurred.*/ 
Update F and T S of xi, by F = F + 1; 
Move xi to right place of LS k and GS according to F and T S. 
else 
/*A cache miss has occurred.*/ 
if Cache is full then 
foreach block at the bottom of LSj , 0 ≤ j &lt; n do 
if LSj is a corresponding stack to a faulty disk then 
Its weight W =F  *  (n − 1); 
else 
Its weight W =F ; 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Experimental Results of an Open-loop Testing Using 5 to 8 Disks</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 describes</head><label>3</label><figDesc></figDesc><table>the results under an open-loop testing 
using the SPC-1-web trace, where the number of disks 
ranges from 5 to 8 and the number of blocks ranges 
from 131,072 to 524,288. The experimental results of 
the open-loop testing show that compared to the origi-
nal LRU and LFU algorithms, the VDF-optimized algo-
rithm speeds up the online reconstruction process. The 
speedup of VDF-LFU is up to 46.3% compared to LFU. 
VDF-LRU speeds up the online reconstruction by up to 
34.6% compared to LRU. 
With the same number of cache-resident blocks, the 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We are very grateful to our shepherd Erez Zadok and anonymous reviewers for their helpful comments. This research is sponsored in part by the National Ba- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">WorkOut: I/O workload outsourcing for boosting RAID reconstruction performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 7th USENIX Conference on File and Storage Technologies<address><addrLine>San Francisco, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A case for redundant arrays of inexpensive disks (RAID)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD international conference</title>
		<imprint>
			<date type="published" when="1988-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parity declustering for continuous operation in redundant disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the 5th Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-10" />
			<biblScope unit="page" from="23" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distributed sparing in disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mattson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th international conference on COMPCON</title>
		<meeting>the 37th international conference on COMPCON<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-02" />
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Evaluation of distributed recovery in large-scale storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J E</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th IEEE International Symposium on High Performance Distributed Computing</title>
		<meeting>13th IEEE International Symposium on High Performance Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2004-06" />
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flat XOR-based erasure codes in storage systems: Constructions, efficient recovery, and tradeoffs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Wylie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th Symposium on Mass Storage Systems and Technologies</title>
		<meeting><address><addrLine>Incline Village, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Code-M: A non-MDS erasure code scheme to support fast recovery from up to two-disk failures in storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/IFIP International Conference on Dependable Systems and Networks</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Balancing I/O response time and disk rebuild time in a RAID5 disk array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Twenty-Sixth Hawaii International Conference on System Sciences</title>
		<meeting>eeding of the Twenty-Sixth Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="1993-01" />
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic recovery from disk failure in continuous-media servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C S</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Computer Journal</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="499" to="515" />
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast, on-line failure recovery in redundant disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Siewiorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third International Symposium on Fault-Tolerant Computing</title>
		<meeting><address><addrLine>Toulouse , France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-06" />
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">PRO: A popularity-based multi-threaded reconstruction optimization for RAID-structured storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 5th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2007-02" />
			<biblScope unit="page" from="277" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implementation and evaluation of a popularity-based reconstruction optimization algorithm in availability-oriented disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th IEEE Conference on Mass Storage Systems and Technologies</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-09" />
			<biblScope unit="page" from="233" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Performance analysis of disk arrays under failure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Muntz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C S</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Very Large Databases</title>
		<meeting>the 16th International Conference on Very Large Databases</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MICRO: A multilevel caching-based reconstruction optimization for mobile storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1386" to="1398" />
			<date type="published" when="2008-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An approximate analysis of the LRU and FIFO buffer replacement schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 1990 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems<address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-05" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data cache management using frequency-based replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Devarakonda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings ACM SIG-METRICS Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>ACM SIG-METRICS Conference on Measurement and Modeling of Computer Systems<address><addrLine>Boulder, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-05" />
			<biblScope unit="page" from="134" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The LRU-K page replacement algorithm for database disk buffering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD international Conference on Management of data</title>
		<meeting>the ACM SIGMOD international Conference on Management of data<address><addrLine>Washington, D.C., USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993-05" />
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An optimality proof of the LRU-K page replacement algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="92" to="112" />
			<date type="published" when="1999-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2Q: A low overhead high performance buffer management replacement algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Very Large Databases</title>
		<meeting>the Twentieth International Conference on Very Large Databases<address><addrLine>Santiago de Chile, Chile, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-09" />
			<biblScope unit="page" from="439" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the existence of a spectrum of policies that subsumes the least recently used (LRU) and least frequently used (LFU) policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994-05" />
			<biblScope unit="page" from="134" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LRFU: A spectrum of policies that subsumes the least recently used and least frequently used policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1352" to="1361" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The multi-queue replacement algorithm for second level buffer caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">Annual USENIX Technical Conference</title>
		<meeting><address><addrLine>Boston, Massachusetts, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06" />
			<biblScope unit="page" from="91" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Second-level buffer cache management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="505" to="519" />
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">LIRS: An efficient low inter-reference recency set replacement policy to improve buffer cache performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 2002 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems<address><addrLine>Marina Del Rey, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-06" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Making LRU friendly to weak locality workloads: a novel replacement algorithm to improve buffer cache performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="939" to="952" />
			<date type="published" when="2005-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ARC: A self-tuning, low overhead replacement cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2nd USENIX Conference on File and Storage Technologies</title>
		<meeting>2nd USENIX Conference on File and Storage Technologies<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-03" />
			<biblScope unit="page" from="115" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DULO: An effective buffer cache management scheme to exploit both temporal and spatial localities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 4th USENIX Conference on File and Storage Technologies<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-12" />
			<biblScope unit="page" from="14" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">DiskSeen: Exploiting disk layout and access history to enhance I/O prefetch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Characterization of storage workload traces from production windows servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kavalanekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A paging experiment with the Multics system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Corbato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIT Project MAC Report MAC-M-384</title>
		<imprint>
			<date type="published" when="1968-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
