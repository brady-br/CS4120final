<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX. SkillExplorer: Understanding the Behavior of Skills in Large Scale SkillExplorer: Understanding the Behavior of Skills in Large Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiu</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijin</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiu</forename><surname>Guo</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijin</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Engineering</orgName>
								<orgName type="department" key="dep2">School of Cyber Security</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX. SkillExplorer: Understanding the Behavior of Skills in Large Scale SkillExplorer: Understanding the Behavior of Skills in Large Scale</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-17-5</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Smart speakers have been popularly used around the world recently, mainly due to the convenience brought from the virtual personal assistant (VPA) which offers interactive actions through the convenient voice commands from users. Besides the built-in capabilities, VPA services can be further extended by third-party developers through skills. Similar to smart-phone applications on Android and iOS markets, skills are also available on markets (e.g., Amazon, Google), attracting users together with malicious developers. Recent researches discover that malicious developers are able to route users&apos; requests to malicious skills without users&apos; consent by creating skills with similar names of legitimate ones. However, to the best of our knowledge, there is no prior research that systematically explores the interaction behaviors of skills, mainly due to the challenges in handling skills&apos; inputs/outputs which are in the form of natural languages. In this paper, we propose the first systematic study on behaviors of skills, which is achieved by a suite of new grammar-based techniques including utterance extraction, question understanding, and answer generation specifically designed for skills. We build an interactive system called SkillExplorer and analyze 28,904 skills from the Amazon market and 1,897 actions from the Google market. Among these skills, we find that 1,141 skills request users&apos; private information without following developer specifications , which are actually demanded by markets. 68 skills continue to eavesdrop users&apos; private conversations, even after users have sent the command to stop them.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Smart speakers have been widely used around the world recently, mainly due to the convenience brought from the integrated virtual personal assistant (VPA) that offers interactive actions. Merely through voice commands of users, the VPA can be activated and respond to users' commands such as * Corresponding author.</p><p>providing information like weather and news, playing music, making phone calls, and even controlling other smart devices such as smart lights and thermostats. Besides the built-in capabilities, VPA services can be further enhanced through ecosystems offered by their providers, where third-party developers can teach VPAs new abilities (called skills by Amazon or actions by Google 1 ). Through such skills, users' activities can be extended such as placing orders, communicating in social networks, and playing games, which attract tens of millions of users, and in turn attract more developers. According to a recent report <ref type="bibr">[1]</ref>, over 100,000 skills are on the Amazon market, which is 20,000 more than the number at the beginning of 2019; and over 19,000 actions are on the Google market <ref type="bibr">[2]</ref>. However, with the rapid development of skills, dangerous skills also appear. According to recent studies <ref type="bibr" target="#b9">[26,</ref><ref type="bibr" target="#b18">35,</ref><ref type="bibr" target="#b19">36]</ref>, some skills can route users' requests to malicious applications without their consent by creating skills with similar names of legitimate ones (e.g., the same or similar pronunciation but different spellings of skill names, like "Full Moon" v.s. "Four Moon").</p><p>Although the invocation of skills is recently studied to locate dangerous skills, less is understood about the contents provided by skills, or the behaviors of a skill. Actually, dangerous skills may eavesdrop users' privacy or even monitor users' conversations infinitely <ref type="bibr" target="#b9">[26]</ref>. For example, according to a recent report <ref type="bibr">[3]</ref>, an attacker can create a malicious skill to read an unpronounceable sequence. During this period, the speaker remains silent but still active, which allows the malicious skill to fully capture users' conversations. Even more, the malicious skill can pass through the strict vetting process of Amazon and Google, and is ready on the store waiting for victim users. To the best of our knowledge, there is no prior research to systematically explore the behaviors of skills, mainly due to the following challenges.</p><p>Challenges. C1: Fully black-box. Different from exploring behaviors of an application (e.g., an x86 binary with or with-out source code, or an Android application), a skill is a kind of web services, which is fully black-box to the analyzer. What the analyzer can only do is to send inputs to the skill and observe its responses. No inner states of the skill could be gained to facilitate the analysis process. As a result, it is hard to determine whether the behaviors of a skill have been fully explored. Sometimes, even if an input is accepted by a skill and a valid answer is given, it seems difficult to tell whether another input can trigger different behaviors of the skill. Also, without the complete understanding of the inner states of a skill (e.g., branches), it seems impossible to optimize the strategy to explore a skill's behaviors.</p><p>C2: Inputs/outputs of skills are in the form of natural languages. To explore the behaviors of a skill, the analyzer should understand the questions from skills and sort out certain answers in natural languages. The validity of inputs (i.e., answers in natural languages) is self-designed by various developers, which means that the generated inputs should be consistent with the designs of specific skills. Even for similar questions from different skills, the generated answers may be quite diverse. A conversational system (e.g., a chatbot) could be one of solutions to explore the behaviors of skills. However, the questions may not be well understood by existing conversational systems. For example, "To check out our new features, try saying what's new or help.", the famous chatbot Mitsuku <ref type="bibr">[4]</ref> will answer "The obvious one". Besides the problem of understanding questions, generating valid answers is also highly challenging. Our approach. To understand the skills in the markets, we develop a novel technique called SkillExplorer to explore the behaviors of a given skill and identify the suspicious ones. A suite of grammar-based approaches are designed to solve the unique problems encountered where natural language is the sole way for communication, including generating the initial input, understanding the questions (i.e., outputs) from skills, and generating the valid inputs. Besides, to make the inputs be able to trigger various behaviors of skills, we build a knowledge database containing multiple personal profiles that are automatically collected from the Internet. The full process of exploration is recorded and further utilized to increase efficiency.</p><p>Specifically, to initialize the dialog with a given skill, the first input should be carefully chosen. Based on the observation that the developer usually gives sample inputs (called sample utterances) on the introduction page of the skill in the market, hoping her skill to be easy to use, SkillExplorer analyzes the introduction page and extracts suitable inputs to initialize the dialog. After the target skill receives the initial input and gives the outputs, SkillExplorer will parse the outputs (questions) and further classify them into five basic types including Yes/No questions, Instruction questions, Selection questions, Wh questions and Mix questions. For some types (e.g., the question like "To check out our new features, try saying what's new."), the afterward valid responses can be extracted directly from the questions (referred to as explicit questions); while for other questions like "What's your phone number?" (referred to as implicit questions), the answers cannot be directly extracted. In particular, for the explicit questions, SkillExplorer enumerates all the valid answers from the corresponding questions and feeds them to the skill; for the implicit questions, SkillExplorer identifies those related to privacy and chooses suitable answers from a knowledge database which is pre-built by collecting different users' profiles from the Internet. In this way, by continuously repeating the procedures of parsing questions and answering, SkillExplorer can communicate with the target skill, and further to explore its behaviors. After the behaviors are explored, we will further check whether the questions from the target skill can impact users' privacy. Note that, to increase the efficiency of behavior exploration, we design an i-tree to record the status of exploration and let SkillExplorer quickly execute a branch question.</p><p>Findings. Benefit from the automatic exploration, we are able to analyze the behaviors of 30,801 skills (28,904 from the Amazon market in America and 1,897 actions from the Google market), whose scale has never been achieved before. Such a large-scale analysis gives us a unique chance to understand the behaviors of skills and their developers. From the results, we find 1,141 skills request users to provide personal information (e.g., mobile phone number, name, address, etc.) without following developer specifications (e.g., different from their claims in privacy policy pages or without configuring permissions, etc.). We also find that 68 skills continue to eavesdrop user's private conversations after users send commands to stop them.</p><p>Contributions. The contributions of the paper are as follows:</p><p>• A systematic study on skills' behaviors on a large scale. We propose the first systematic study on the behaviors of skills, which is achieved by a suite of new grammar-based techniques including utterances extraction, question understanding, and answer generation specifically designed for skills. The techniques have evaluated 28,904 skills from the Amazon market and 1,897 actions from the Google market, a scale that has never been achieved before for analyzing skills' behaviors.</p><p>• New findings. Besides a good number of suspicious skills found in our study, we also have the unique chance to observe the suspicious behaviors of skills on a large scale, and together with the understanding of their developers. Such understandings could not only help the administrators of the markets for better vetting skills but also shed new lights to develop new techniques to efficiently detect malicious skills 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Skill and Restrictions in Development</head><p>Skill and the ecosystem. The VPA is a software agent that provides services for a human individual by following his voice commands. Especially, with the rapid development of IoT devices such as smart speakers (e.g., Alexa Echo, Google Home), VPAs are popularly integrated into these devices for better user experience in controlling. Besides the built-in functionalities offered by the VPAs, the capabilities can be further extended through the ecosystem offered by their providers, which are called skills by Amazon (or actions by Google). Actually, the providers encourage third-party developers to build their own skills, serving as add-on functionalities to VPAs, just like the ecosystem of mobile applications (e.g., Android markets and the Apple market). Similarly, developers publish their skills on the market, including the invocation names, authors, descriptions, etc. For users, they ask their smart speakers to request services from skills. For example, as shown in <ref type="figure">Figure 1</ref>, a user asks "Alexa, ask Plan My Trip to plan a trip from Seattle to Portland on Friday". Alexa will send the audio stream to its cloud server Amazon Web Services (AWS) to parse the audio and determine the most suitable skill to respond to the request. In this case, the skill "Plan My Trip" is explicitly invoked and will receive the user's request in texts parsed by AWS. Then it generates the answer and sends it back to Alexa, which will speak out the answer at the user's side. The user can also request services from skills in an implicit way. For example, he can say something like "Alexa, i want to visit Portland" and Alexa will choose the most suitable skill that fulfills the request.</p><p>Although skills are very close to mobile applications, they have essential differences. One main difference is the way to request the services: voice commands for skills and click operations for mobile apps. The second difference is that users do not need to install skills on smart speakers (instead, they use a combination of phrases and invocation name supported by the Alexa service such as saying "Alexa, open XXX" to automatically enable a skill).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1: Overall workflow of interacting with skills</head><p>Restrictions in development. When a developer publishes a skill, he must follow the rules provided by the markets (e.g., Amazon or Google), which is also similar to publishing mobile applications. For Amazon, the basic information which he should provide includes invocation name, a cloud-based service, intents, and sample utterances. Details are shown in Appendix A. For example, the skill "Plan My Trip" has an invocation name "plan my trip". It uses the AWS Lambda cloud to execute the user's requests. Intent "PlanMyTrip" is used to fulfill requests such as the utterance"Alexa, ask Plan My Trip to plan a trip from Seattle to Portland on Friday". Besides the basic information, the developer must also follow some restrictions from the markets. Especially, if a skill requests personal information, it should provide the privacy policy link to Amazon <ref type="bibr">[5]</ref>. The markets have their requirements for privacy policies which describe the outline of collected information from users and ways to use and share them. During the developing process, Amazon stipulates that if a skill wants to obtain users' information such as the name, phone number, email, home address, and so on, it must include a link to the privacy policy that applies to the skill. It also needs to configure permissions so that when users enable this skill, they can agree or deny authorization to provide such information to the skill <ref type="bibr">[6]</ref>. Such fulfillment will be carefully checked by the markets before releasing the skill to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Researches on the Security of Skill</head><p>Until very recently, only a few researches have been carried on skills, which are mainly limited to the invocation mechanism of skills. KUMAR et al. <ref type="bibr" target="#b9">[26]</ref> and <ref type="bibr">Zhang et al. [35]</ref> find that a malicious skill could be mistakenly invoked by a user without her consent due to similar pronunciations between the skill and the legitimate ones (e.g., "Boil an Egg" v.s. "Boyle an Egg"). Zhang et al. <ref type="bibr" target="#b19">[36]</ref> find that the natural language understanding's classifier of a VPA could divert a user's request to a malicious skill due to improper semantic interpretation of the request. In October 2019, researchers from SR Labs implement two attacks on VPAs <ref type="bibr">[3]</ref>. One is to develop a malicious skill to camouflage as the VPA, asking for users' private information such as their password. The other attack is to let a malicious skill eavesdrop users' conversation, even if it has received users' voice command to exit. We also identify such a situation and find that 68 skills having similar behaviors are still alive in the Amazon market, which has not been discovered before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Conversational System</head><p>To explore the behaviors of a skill, one may consider using conversational AI systems. However, current conversational AI systems are not suitable for this task. According to a recent survey <ref type="bibr" target="#b8">[25]</ref>, there are three types of existing conversational systems. QA agents are often used to answer domain-specific questions or to search for answers from open knowledge systems (e.g., Wikipedia). Task-oriented dialogue agents are used to perform a series of tasks or services for users such as business trip planning whose input content needs to meet a certain format to be understood. A chatbot's response content is usually a combination of statistical methods and manual components. There is no standard content format because it is for communication with people. However, the questions from skills are various. Simply seeking answers from open knowledge sources may not deliver understandable answers to skills. Actually, a skill is usually designed to make conversations easy for users. So the expected answers from human users are usually simple but limited to a certain range. We design an efficient approach to correctly answer the questions from skills and make the conversation continue for behavior exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Explore Skills' Behaviors</head><p>In this section, we first give an overview (Section 3.1) of SkillExplorer to explore behaviors of skills, followed by the detailed design of each component (Section 3.2 to 3.5). Then we give our implementation (Section 3.6) and evaluate SkillExplorer (Section 3.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As mentioned previously, different from a traditional conversational system, inputs for skills should be in specified forms expected by various developers. Thus, besides understanding the questions given by skills, the answers should also be carefully prepared to continue conversations. In this paper, we design an interactive framework called SkillExplorer to explore skills' behaviors. A suite of grammar-based approaches are designed to solve the unique problems encountered. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the main procedures include utterances extraction, question understanding, answers generation, and skill exploration.</p><p>Specifically, utterances extraction is designed to initialize the first input to a target skill. As there is no question given by the skill at this stage, to generate an acceptable input, extra information should be provided. After the first input is generated and fed to the skill, it will feed back the output. Then SkillExplorer parses the output and further classifies it into five types. Further, SkillExplorer generates answers according to these types. Note that, for some questions related to users' profiles, SkillExplorer prepares different answers according to a knowledge database which is prepared by collecting different users' profiles from the Internet. In the end, skill exploration continues to analyze questions and generate answers, exploring the behaviors of the target skill. The conversations are stored for SkillExplorer to check whether users' privacy is impacted.</p><p>Example. Below, we give an example to detail the process. Take the skill "The Washington Post" as an example. Firstly, we obtain the basic information from its web page, including 12 items such as the skill name "The Washington Post", author "Washington Post Company", invocation name "washington post", utterance corpus, etc. Particularly, the utterance corpus contains "Alexa, open Washington Post", "Alexa, ask Washington Post for politics" and "Alexa, ask Washington Post for Post Reports". SkillExplorer uses the three utterances to start the interaction with the skill. Here, suppose we feed "Alexa, open Washington Post" to the skill, which will further return an output "Welcome to The Washington Post. We have three daily shows. Just ask me for news, politics, or a story from history. What would you like to do?". Then, SkillExplorer parses the question and identify it as the type Mix question. Later, SkillExplorer generates corresponding answers "news", "politics", "a story" to explore the three possible behaviors. Note that the three answers should be fed back to the skill one by one. Here, the keyword "news" is given, and the conversation continues until the end. In this process, SkillExplorer records the position of the branches and restarts the conversation from the beginning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Utterances Extraction</head><p>In most cases, developers provide utterance samples in their skills' introduction pages in the markets. There is a standard place that the market requests developers to put utterance questions there for letting a human user understand how to use the skill. The position of the utterance questions can be located by using "a2s-utterance-box-inner" in the source code of the web-page, which is easy for SkillExplorer to extract. Besides the standard position, we also find that some developers give instructions to users in descriptions. To extract the utterance there is very complicated since descriptions of skills are written by different developers with different writing habits. After manually analyzing 100 skills, we find that the utterances usually appear in double-quotes or the form of lists. This is easy to understand since developers also want the users to quickly identify the utterances for using their skills. Based on our analysis, only very few utterances are out of the scope (less than 1%).</p><p>For the utterances in double-quotes, we can directly use regular expressions to identify them. However, for utterance in lists, this approach does not work well since some developers put their company information or other contents in the lists, which may cause false positives. Further to increase the accuracy, we consider the number of sentences (S n ) and the length of the sentence (S l ) in one bullet in the list. Since more than one sentence in an utterance will be interrupted by the smart speakers, S n will always be 1, which is also verified by analyzing over 200,000 utterances. Regarding S l , in most cases, developers will not use long utterances since too long sentences may make it difficult for users to understand or repeat. After analyzing over 200,000 utterances, we find the average length is 5 and the longest is 29. The distribution is shown in Appendix B. We select 15 as the threshold of S l to identify utterances (the possibility of utterance with more than 15 words is 0.82%). Also, considering that utterances are listed in parallel in the list, if one bullet is not an utterance, all the sentences in the list should not be utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Question Understanding</head><p>After the first utterance is sent to a skill, it will feed back an output, answering the question, or asking users for further commands. In this paper, we refer to the outputs given by the skill as "questions". SkillExplorer should understand these questions for continuing the conversation. Different from a traditional interactive system designed for interacting with users, skills are usually developed to finish some pre-defined tasks. As a consequence, the expected answers are in fixed forms specified by the developers so that the skills can precisely understand them before performing the tasks. However, the diversity of developers also makes their design in different forms. To understand diverse questions, we should divide questions into several types, and further generate answers according to their types.</p><p>One may use the classification of questions according to English linguistics, which divides questions into two types: Yes/No questions and Wh questions <ref type="bibr">[7]</ref>. However, such classification is too rough for our interactive system. We take the following two questions as examples: Q1: "...Just ask me for news, politics, or a story from history. What would you like to do?" and Q2: " What's your zip code?" Although they are both Wh questions, users will answer them in different ways. For Q1, users will extract answers directly from the question, which is not suitable for answering Q2. Thus, instead of using the traditional way to classify questions, we interact with 10,000 randomly selected skills using the extracted utterances and collect the replies as the Basic Corpus of Replies. Then we manually analyze 2,000 randomly selected replies (i.e., questions) from the corpus. We find that the questions can be divided into five types according to the ways to generate answers. Yes/No questions. The question of this type is an interrogative construction, and expects answers like "yes" or "no". A Yes/No question usually has an auxiliary verb in front of the subject, which is also called subject-auxiliary inversion (SAI). It has two subtypes: Inverted question (IQ), and Tag question (TQ). An example of IQ is "Are you going?", in which subject and the first verb in the verb phrase will be inverted if the verb is a modal or an auxiliary verb or with the verb be and have. TQ is a short question at the end of a sentence, which is often made up of a modal or helping verb and a subject pronoun. An example is "You're going, aren't you?". Note that there is an Inverted Alternative Question (IAQ) such as "Are you staying or going?", which looks quite like IQ, but it actually does not require a simple yes or no for an answer. We should exclude it from this type.</p><p>In order to identify Yes/No questions, we use constituentbased parsing which is popularly used in natural language processing to analyze questions. A constituency-based parse tree can represent a context-free grammatical structure of sentences. Non-terminals in the tree are types of phrases (tagged by part of speech labels), and leaves are words in the sentence. We focus on the tag "SQ", which represents either a Yes/No question, or the main clause of a wh-question, following the wh-phrase with tag "SBARQ" (direct question introduced by a wh-word or a wh-phrase, e.g., "How can I help you?") <ref type="bibr">[8]</ref>. Examples are shown in <ref type="figure" target="#fig_1">Figure 3</ref> (more examples are shown in Appendix C). Thus, to identify Yes/No question through using the constituency-based parse tree, we first locate the tag "SQ", and then filter out those questions if "SQ" follows a W-tag (representing wh-phrase or wh-word). We should also filter the IAQ by checking whether there is a "CC" tag (representing the word "or").</p><p>For TQ type, it is a statement followed by a mini-question which has the form of "auxiliary verb + subject". We judge this kind of structure in a parse tree and extract the auxiliary verb (be, do, have, or a modal verb like will) with a subject. Considering that decisive questions often appear at the end, we only judge the last sentence 3 . Instruction questions. The questions of this type give users direct guidance on how to answer them. They are essentially similar to imperative sentences which transfer the guiding or suggestive information to users. In order to guide users to reply with the correct answer, the skill usually tells the user what to say by using the directive keywords (e.g., "say", "ask") in the sentence. For example, "Welcome to the Reddit Notifier skill... just Say: Help me".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROOT</head><p>After manually analyzing questions in Basic Corpus of Replies, we find that over 96% of the instruction questions use "ask" and "say". One main reason is the samples given by Amazon <ref type="bibr">[9,</ref><ref type="bibr">10]</ref> for developers to build skills, where "ask" and "say" are used. The two words are in line with user habits. To identify such a type, we first find InstruTag in the constituencybased parse tree including "VB" (Verb, base form), "VBG" (Verb, gerund or present participle), and "VBP" (Verb, non3rd person singular present) and check whether there are some command words like "ask" and "say" (or their "-ing" form). In this way, we can determine whether the question is an Instruction question. Selection questions. We refer to the questions containing multiple parallel answers as "Selection questions". Some former studies have a similar category, referring to the questions as "choice" <ref type="bibr" target="#b13">[30]</ref> if the answers are connected by the keyword "or" (e.g., a question like "...To get started, you can get a quote, listen to the daily briefing, or get an account summary."). To identify such questions (referred to as Selection_CC ), we try to find similar patterns in the constituency-based parse tree. The patterns should be with tag "CC" (Coordinating conjunction) that indicates the existence of Paratactic Structure in a sentence. We also include questions that need to be answered with serial marks into the selection question (such as "1: high, 2: medium, 3: low. Choose one."), which has three choices but no coordinating conjunction. To identify such questions (referred to as Selection_SC), we extract all numbers and single characters from the constituency-based parse tree, and then judge whether these numbers and letters are continuous. Wh questions. Wh questions are also known as open questions <ref type="bibr">[7]</ref>. Users are supposed to answer such questions in a free way, instead of obtaining the answers directly from questions or making some judgments. Their knowledge or understanding is usually needed in this process. An example is "What is your name?". To identify questions of this type, we find those questions with WH-tag which include "WDT", "WHADJP", "WHADVP", "WHNP", "WHPP", "WP", "WP$", "WP-S", "WRB" in the constituency-based parse tree with wh-words. If it contains related tags, we classify the question as Wh question. Mix questions. Sometimes the output from skills contains more than one of the previous four question types. For example, the output "You can say repeat or stop." is the combination of an Instruction question and a Selection question. We refer to it as the Mix question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Answer Generation</head><p>After classifying the questions, we get 5 types of reply content: Yes/No, Instruction, Selection, Wh, and Mix. We can generate corresponding answers for different types of questions. For some types (i.e., Yes/No questions, Instruction questions, and Selection questions), we can directly extract answers from the question itself. For Wh questions, we generate a knowledge database to answer the question and explore the behaviors of skills. For Mix questions, we have strategies to answer it according to the question types it contains. We show some examples in <ref type="figure" target="#fig_2">Figure 4</ref>. Yes/No questions. We simply generate the answers as "yes" or "no" to the questions.   <ref type="bibr">[11]</ref>, and find that the two words ASK and SAY usually have five patterns, as shown in <ref type="table">Table 1</ref>. For example, the skill can "ask (sb.) to INS". The components in the brackets (e.g., "sb." here) are not necessary for a sentence.</p><p>INS is the instruction that we should extract. Sometimes, Wh questions are used as a component (e.g., ask (sb) Wh-Q). Wh-Q is the Wh question here. An example is "You can say what is the current sibor rates". To identify the five patterns in an instruction question, we first get the constituency-based parsing trees for the question. According to the five rules in <ref type="table">Table 1</ref>, we can specify the matching rules based on them and use regular expressions to identify which pattern is used in the question. Then according to the patterns, we extract the INS or Wh-Q as the answers to the questions. Selection questions. In this type, the expected answers are usually connected in parallel by conjunctions (e.g., "or", "and") (referred to as Selection_CC), or clearly marked by indicators such as the numbers or letters (referred to as Selection_SC). So users can directly speak out the answer itself or feed back the indicator. For example, the skill myTuner Radio says: "Ok, Here's myTuner Radio. I've found: 1: CHOI-FM Radio X 98.1 from Canada, 2: Ibiza X Radio from the United Kingdom, 3: Radio X London from the United King-dom. Choose a station.". Users can answer the question by saying "CHOI-FM Radio X 98.1 from Canada" or directly say the number "one". To automatically extract answers, for Selection_CC questions, SkillExplorer checks the constituent parsing tree to find parallel structures connected by CC (Coordinating conjunction) which may be corresponding to the words, phrases, and clauses. For Selection_SC questions, SkillExplorer checks the leaf nodes of the parallel structures in the constituent parsing tree to judge whether the serial indicators (i.e., numbers or letters) exist there and have the same format. If so, SkillExplorer will enumerate the found indicators to explore the behaviors.</p><p>Wh questions. As mentioned previously, users commonly answer Wh questions according to their knowledge or understanding, instead of obtaining answers directly from the questions. So our idea to answer Wh questions is to first build a knowledge database and then extract answers from the knowledge database. However, if the knowledge database is designed to include all kinds of knowledge, it will be too large to construct. Considering that our purpose is to detect whether a skill impacts users' privacy or conflicts with Amazon's development rules <ref type="bibr">[6]</ref> (also related to users' privacy), we design the knowledge database from the viewpoint of users' privacy.</p><p>After analyzing Amazon's development rules, we try to create some virtual users with different profiles for answering the privacy-related questions raised by skills. For other questions, although they are not our focus, we still try to answer them by constructing a noun database (common nouns in Wh questions) or feeding questions to online chatbots (e.g., Mitsuku <ref type="bibr">[4]</ref> and Cleverbot <ref type="bibr">[12]</ref>) and using their answers. For each virtual user, the private information of the user should be created to build the profile. Such information includes the full name, first name, gender, birthday, etc. Some fields are shown in <ref type="table" target="#tab_2">Table 2</ref>, and more details are shown in Appendix D. Note that such information cannot be randomly generated. Otherwise, the skill may identify the inconsistency or some illogical problems (e.g., an 8-year boy may not like to have a driver's license number), which will impact behavior exploration. So the profiles should be created to be close to real situations. Since some questions from skills may be related to the relationship between the users (e.g., ask a child the name of his mother), the knowledge database should also consider such a situation.  We first build several virtual users (VUs) according to decades of age since skills may react differently to different ages. For users of the same age, we also create two VUs: male and female. Then we continue to give them other private information. To make such information representative, we have searched on the Internet for other fields. Note that, some fields in the table have logic connections (e.g., address and zip code). So we search them together to find the logic connected data after the logic connections are manually marked. Also, some fields (e.g., phone number, credit card number) have a specific format. A randomly generated phone number could not be accepted by skills. Therefore, we use an online information generator which can generate such fake yet correct-format information <ref type="bibr">[13]</ref>. At last, after collecting all the information, we manually check whether there are some inconsistencies. Then we add the social connection between them including husband and wife, parent and children, etc.</p><p>Then by searching keywords in the knowledge database, SkillExplorer will return the answers from a random profile (for the first question) and use other fields in the profile for answering other future questions. Note that for the same question, to explore its different behaviors, SkillExplorer will use different profiles to answer and observe the behaviors of the skills. If a skill reacts differently, SkillExplorer will continue to explore the behaviors. Otherwise, it will stop using more profiles to respond to this question. For example, the skill "Preventive Health Care Services" will have different behaviors according to the age of 13 and 18. False negatives may happen since SkillExplorer cannot enumerate all possible profiles. One possible solution is to extend the possible values (e.g., different addresses) for each field in the profile, which at the same time brings extra time spent on communicating with skills. For some behaviors that are really hard to trigger, little impact will they bring on users.</p><p>Mix questions. Mix questions include more than one type of question in the output of skills, which are also very common since developers can organize the outputs from skills in any way. To deal with such questions, a simple idea is to mark the types, generate answers according to each type, and feed back all the answers together. However, it may waste lots of unnecessary time due to that many answers are unaccepted to the skills. We should select the questions to answer according to their types. First, we hope to generate the rules from studies on linguistics. However, we do not find any useful rules. So we have to generate the rules by ourselves. Considering that Mix questions are designed for users to answer, the question should be understood by the majority of users. So we authors play the role of users to understand the questions and try to generate the rules. We randomly sample 2,000 Mix questions from the </p><formula xml:id="formula_0">Rule Situation Type R1 ∃ Y Y R2 ∃ S_SC &amp; ∃ I S_SC&amp;I R3 (I&amp;S_CC) in Q * I&amp;SC_CC R4 ∃ I I R5 ∃ S S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Behavior Exploration</head><p>By leveraging the previous approaches, SkillExplorer can explore one execution path of the skill. To explore all its behaviors, SkillExplorer should further record the branches and explore those un-executed ones. We also introduce an approach to speed up the interaction. Record and traverse branches. For a given question, there could be multiple answers. For example, there are two answers for Yes/No questions and two or more answers for selection questions. For an answer, a further output will be responded from the skill, which serves as a new question expecting further answers or simply ending the conversation. Continuing this process will form a tree-like structure to record questions and answers. So we design an interactive tree (i-tree for short) to represent the status of exploration. Each node in the i-tree represents a single interaction (include an input and corresponding output). While SkillExplorer communicates with the skill, an i-tree is drawn simultaneously. The node will be marked as visited if it is explored. If an execution path in the i-tree is explored to the end, and there are unvisited nodes, SkillExplorer will re-start from the beginning to the unvisited nodes. Note that there could be more than one roots in the i-tree, due to several utterances extracted. SkillExplorer will end the execution path in the i-tree if one of the following situations happens. <ref type="formula">(1)</ref> No answer can be generated for a given output (e.g., "That's our information, bye."). (2) An exception happens (e.g., the operation needs to be performed on the mobile phone 5 ). (3) For the same node in different executions, the questions are different. For example, a skill can generate different quizzes. There is no need to enumerate all the quizzes. gives an example when SkillExplorer analyzes a skill. When the first utterance "open c. s. n. premier collision" in N_0 is sent to the skill, the returned output is "...To learn more, say, about us, services, mobile app, phone number, address, or website. You can say, repeat or stop, at any time". SkillExplorer parses the output as a question and generates eight answers: "about us", "services", "mobile app", "phone number", "address", "website", "repeat" and "stop" (in the nodes from N1_0 to N1_7, respectively). The first answer is fed to the skill to continue the process of exploration. When the last question is reached, the process of exploration will end. At this time, SkillExplorer check whether there are any unvisited nodes (white node). If so, SkillExplorer will find a path to the node in the i-tree and restart from the root. This process will continue until there is no unvisited node left. Speed up the interaction. In the process of exploration, the real execution could be very time-consuming due to the following reasons. Firstly, some skills raise questions which have already been asked. If the question is parsed again for further exploration, the i-tree may not end. In other words, in an i-tree, if the output in a new node (i.e., a leaf node) is the same as a visited one (which may or may not be in the same execution path), the node should not be explored again.</p><p>Secondly, when SkillExplorer restarts from the root of an i-tree, some paths in the i-tree are repeatedly executed, which further makes the speaker to read the outputs many times. When the outputs are long, it will be very time-consuming, especially when the output is at the beginning part of the i-tree. For example, the output in N_0 with 39 words will take 18 seconds to read. When the skill is explored, it will be executed at least 8 times. More than 2 minutes will be spent on the node. To solve this problem, SkillExplorer does not need to wait until the reading of the whole output is finished. For a node in the i-tree representing the output from a skill, if this node is visited, SkillExplorer can directly utilize the generated inputs in the last execution to answer the output. For the previous example in the second execution, SkillExplorer will directly feed the input stored in N1_0 to the skill before the reading of the whole output is finished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Implementation</head><p>We build a crawl to collect skills from Amazon and Google markets and build SkillExplorer to explore the behaviors of these skills. The project includes more than 7,000 lines of Python code.</p><p>In the process of analyzing questions and generating answers, SkillExplorer builds the parse tree using NLTK (natural language toolkit) <ref type="bibr" target="#b11">[28]</ref> and Stanford NLP Parser <ref type="bibr" target="#b12">[29]</ref>  <ref type="bibr">6</ref> . Both of the two tools are popularly used in the field of natural language processing. To build the interactive system, one possible approach is to feed the utterances and the answers directly to the smart speakers (e.g., Amazon Echo), then record the outputs and transform them into texts by using speech recognition tools (e.g., Google TTS). However, this approach is too time-consuming 7 . Instead, our idea is to use the simulators provided by markets, which are often used by developers to test their skills. Both Amazon and Google have their own simulators. In particular, the simulator allows developers to communicate with skills using texts. That is to say, developers can directly feed text inputs to a skill and observe its outputs also in texts, which does not need any tools to translate a voice question to the texts, saving the exploring time. Regarding the chat robots for answering Wh questions, after trying some famous chatbots, we choose to use Mitsuku <ref type="bibr">[4]</ref> and Cleverbot <ref type="bibr">[12]</ref> due to their better performance. In the process of acquiring outputs from skills, the outputs will be returned one by one. The time interval equals the time to read the previous output. So if the first output is long, we should wait for long for the second output. Here, we set up a timeout (10 minutes) for waiting. If the timeout is reached, SkillExplorer will stop the current exploration and start a new path in the i-tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Evaluation</head><p>Coverage. SkillExplorer is designed to traverse the behaviors of skills. So the coverage of behaviors is important to evaluate the effectiveness of SkillExplorer. The ideal way to evaluate behavior coverage is to analyze the source codes and compare them with the behaviors explored by SkillExplorer. However, it is very hard to find open-source projects of skills from the Internet. An alternative way is to manually communicate with the skills, and try to collect as many behaviors as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes/No Instruction Selection Wh Mix</head><p>0% 8% 8% 5% 9% We further look into the 23 outputs that are not covered by SkillExplorer and try to figure out the reasons for missing. One reason is due to the problem of NLP tools. 5 questions cannot be correctly parsed by the tools (e.g., wrongly marked part of speech). Also due to the carelessness of developers, some questions contain grammar errors which cannot be correctly parsed. We also find 11 questions require human expertise (e.g., "What SGLs do you want to look up") or use complex structures (e.g., "Okay, player one tell me a name, by saying player one is, followed by the name"), which are quite difficult to answer even for human users. More examples are shown in Appendix E. Accuracy of answer generation. Regarding the accuracy, we care about missed answers and incorrect answers. Missed answers impact the coverage, which has already been evaluated previously. So we evaluate incorrect answers here. Incorrect answers cannot be accepted by skills, which may let SkillExplorer waste time on unnecessary execution. We randomly select 200 questions from each of the five categories classified by SkillExplorer. In sum, 1,000 questions are analyzed manually. We compare the two sets of answers and give the error answer results in <ref type="table" target="#tab_4">Table 4</ref>. On average, 6% of the answers are wrong. Yes/no Question has the lowest ratio (0%), while Mix Question is higher (9%). Note that the incorrect answers impact neither the results of coverage nor the results of the measurement. They only impose unnecessary analysis time on the exploration of skills. The reasons for incorrect answers are similar to those mentioned previously. Performance. SkillExplorer has analyzed 28,904 skills within 5,270 hours 8 (using a machine with a 3.6GHz CPU, 16GB memory, 1TB hard driver, and the Windows 10 operating system). Each skill costs about 627 seconds on average, including the utterance question generation, question understanding, answer generation, and behavior exploration. The median time of skill exploration is 428.5 seconds, ranging from 36 seconds to 8,969 seconds. For different categories, the time varies. It depends on the function and the branches of the skill. A game skill always spends much more time than a weather forecast skill because the game skill has more branches for users to choose. By the way, the stability of network connection matters as well. We also analyze the time of Google actions, which is much smaller than Alexa's, because the test console of Google Assistant does not support all the actions well and its robustness is not so good as Amazons's, making many actions unable to respond as they do in reality. If we use the real smart speaker for evaluation, the time should be much more. We also evaluate how much time could be saved by our speedup mechanism (see Section 3.5). If this mechanism is not used, the average time for each skill will be 885 seconds, which means that 29.2% (=258/885) of the time could be saved by this mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Measurement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Landscape</head><p>Skills &amp; Authors. We crawl 68,066 skills from the Amazon market <ref type="bibr">9</ref> , and 10,899 actions from the Google market. Skills in Amazon have 21 different categories and the category Games &amp; Trivia has the largest number of skills (as shown in Appendix F). Among these, 19.4% of them do not have invocation names, which means that developers use the pre-built model to build the skills. In other words, the developers cannot design their own questions, but use pre-designed questions by the markets, which should not contain any malicious questions. Thus, we do not measure these skills. Among the rest 54,865 skills, we randomly sample 30,000 for the measurement. However, some skills cannot be invoked due to that Alexa only wakes up the more popular one or the previously waked one if two skills have similar invocation names. So in the end, 28,904 skills are measured. We also record the developer names for the skills. In sum, 12,376 different developer names are recorded (a developer can register for different accounts with different names). On average, one developer's name is in charge of 5.5 skills. Interestingly, the developer InfoByVoice owns the most skills (i.e., 2,577 skills). All of them are in the category Lifestyle. We check the interactive content with them and find that these skills provide organizations' information. The developer Rhall owns 1,401 skills, and most of them aim to explain some facts (e.g., a skill "California Facts" gives facts about California). Structure of i-trees. We make a statistical analysis of i-trees. We measure the number of branches in i-trees, depth of i-trees, and the number of answers to a question. <ref type="figure" target="#fig_5">Figure 6 (a)</ref> shows the distribution of the number of branches in i-trees. From the figure, we can see that 90% of the skills have less than 15 branches. The average number of branches is 7.9. Some skills have more than 50 branches (most of them are games or Selection_SC questions with multiple choices), which are not user-friendly to answer. <ref type="figure" target="#fig_5">Figure 6 (b)</ref> shows the distribution of the depth of skills. The average depth is 3.6. From the figure, the depth of 68% skills is less than 4, and the depth of 95% skills is less than 10. It indicates that most skills do not interact with users with deep conversation. We find some skills are with the depth of 40. They are story-tellers. We also look into questions related to privacy. They are usually Wh questions, with the depth less than 5. Skills can customize their services from the requested information (e.g., assessing the value of a house in a location). <ref type="figure" target="#fig_5">Figure 6</ref> (c) shows the number of answers to a question. The average number of answers is 2.9, which means that most questions only have about three choices for users to answer. If there are too many answers in a question, users may not be able to remember them to answer. An interesting skill is Encyclopedia of dinosaurs. It contains a question with 41 answers. Popular questions and popular words. After analyzing more than 160,000 questions in our measurement, we list the top 5 questions in Appendix G. These questions are mainly from the developers InfoByVoice and SkillSet. For example, the question "say, service times, location, phone number, or goodbye" is mentioned by 1,045 skills developed by InfoByVoice, and mentioned by 264 skills developed by SkillSet. We also check the description of the skills on the website of the two developers. Both of them mention VoiceApps.com. Maybe the two developers have some connections. We also count a question for only once if it appears in different skills by the same developer. The most 10 popular words (we only count nouns in the constituency-based parsing tree) are "skill","alexa", "number", "fact", "help", "information", "name", "phone", "location" and "service". Invocation names. Different from previous studies <ref type="bibr" target="#b9">[26,</ref><ref type="bibr" target="#b18">35]</ref> on invocation names which mainly focus on the security problems of abnormal diversion (e.g., skill squatting, voice squatting, voice masquerading), our study checks whether the invocation names can meet Amazon's requirements. As we know, Amazon has strict requirements for designing invocation names <ref type="bibr">[5]</ref>. Some sample rules are given in Appendix H.</p><p>We check whether all 57,139 skills having invocation names <ref type="bibr">10</ref> are against the rules. We find that 9,799 skills do not meet the requirements. Among them, 120 skills do not follow the rule (2): two-word names with article words (e.g., the, a,  The words related to privacy an). 377 invocation names are person or place names (e.g., bainbridge island), which violate the rule (3). 179 skills fail to comply with the rule (4): using launch words (e.g., "open," "tell," "load,", etc.) or connecting words (e.g., "to," "from," "in,"). Two invocation names contain "app" or "skill". We also find that 2591 invocation names are used by 9,128 skills. The most commonly used invocation name is how many days, which is used by 153 skills.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Skills Conflicting with the Developer Specifications</head><p>As we mentioned in Section 2, according to the rules of some markets (e.g., Amazon), for some kinds of personal information, developers are allowed to obtain them for better user experience. Such information (shown in <ref type="table" target="#tab_5">Table 5</ref>) includes a user's name, email address, phone number, etc. which should be obtained by using specific APIs (e.g., Alexa customer profile API) to configure permissions, and should be claimed in the privacy policy of the skills <ref type="bibr">[6]</ref>. For the permissions, they can be seen on skills' introduction pages. For the privacy policy, the developers should clearly include what kinds of personal information are collected, how and why to collect the information. However, we find some developers request such information but do not claim in the privacy policy or configure the corresponding permissions. Instead, they directly ask users for private information. To detect the illegal collection of information, we analyze them in the interactive content. Note that we cannot directly compare the privacy words in the <ref type="table" target="#tab_5">Table 5</ref> with the contents from skills. For example, a skill may say "Our phone number is xxx-xxx-xxxx". The skill does not request such information from users. Instead, it just gives information about the skill. To distinguish the two situations, we leverage the dependency-based parse tree where all the nodes are words. The links among words are labeled by the syntactic function grammar tree. Particularly, we use a two-step comparison. (1) We first check whether the words are used by skills with the correct part of speech. Usually, the words are used as nouns. Sometimes, a skill may use a different part of speech of the word. For example, in the question "I can address the problem", the word "address" is used as a verb. To filter out such situations, we check the part of speech of the privacy word and only identify those used as nouns. <ref type="formula">(2)</ref> Then we check the ownership of the privacy words. We also leverage the dependency-based parse tree, which shows the relationship of dependency between words. For example, in <ref type="figure">Figure 7</ref>, the word name belongs to your, whose connection can be extracted by the tags. In the figure, nmod is used for nominal modifiers of nouns or clausal predicates, while poss means possession modifier. Their combination nmod:poss is used for a nominal modifier. However, a counterexample is "Our phone number is xxx-xxx-xxxx", where the keyword phone number belongs to "our" (i.e., the owners of the skill). We only check the privacy words belonging to the users (e.g., using the word "your"). As developers <ref type="figure">Figure 7</ref>: An example of dependency-based parse structure may not directly request the privacy keywords to evade the vetting process of markets, besides checking the keywords themselves, we should also check their synonyms. So we expend the privacy keywords using their synonyms.</p><p>After obtaining the privacy keywords that a skill requests from users, we further determine whether the skill conflicts with the development specifications. We first check if these keywords (including their synonyms) are declared in the skill's privacy policy. If not, the skill conflicts with the developer specifications. Otherwise, we should further check whether the privacy keywords are clearly declared for requesting users' information. If no such declaration is found, the skill will be viewed as conflicting with the developer specifications. However, in real situations, it is hard to check whether the privacy keywords are used for requesting users' data. An example is "We collect users' private data including their name and email", where the general term "personal data" and the pronoun make the analysis difficult. Fortunately, PolicyLint <ref type="bibr" target="#b1">[18]</ref> handles such situations. So we leverage PolicyLint to solve this problem. Specifically, what makes the situation complex is the general declaration which usually contains three types of words to collect users' information, including a verb of collect information (e.g., collect, gather, check), a general term (e.g., personal information, personal data), and subsumptive relationships words (e.g., such as, include). Note that, due to the limited number of keywords used by PolicyLint, it may not be enough to characterize all the possible general declarations, especially for the diverse privacy policies given by various developers, and further causes false positives. Thus, in our implementation, if any two of the three types of words are in a declaration, we will view it as a general declaration. Such an approach is very effective to find the declaration requesting users' private data, which is then compared with the contents in the skills to ensure whether the skills conflict with privacy policies.</p><p>Results. We first validate the accuracy of our approach. After analyzing 30,801 skills (28,904 from Amazon and 1,897 from Google), SkillExploer finds 1,156 skills conflicting with the developer specifications. Among these skills, 632 skills neither state privacy keywords in privacy policies nor configure corresponding permissions. 183 skills just conflict with the claimed privacy policy, and 341 skills just do not configure corresponding permissions. We manually check the results of the comparison between the keywords and the privacy policies, and only find 15 false positives which do not conflict with privacy policies, mainly due to the following two reasons. Firstly, the NLP tool (i.e., Stanford NLP Parser) cannot correctly parse a sentence, for example, the tool will label "username" as an adjective in the sentence "to use our voice experiences users may provide us with their data such as email, username and password to your service". Secondly, some declarations that explicitly state to collect users' information are not correctly caught by PolicyLint. For example, "you may be asked to enter your zip code or other details to help you with your experience.".</p><p>After removing false positives, we find 1,141 skills that conflict with the developer specifications. We analyze the keywords of these privacy contents. The most frequently requested information is as follows: address, name, phone number, zip code , and email. Most of them are in the categories of Lifestyle and Education &amp; Reference. An interesting case is the skill "WifiPassword". It requests users' wifi name and password and also asks them to finish the request through a webpage popping up on users' smartphones when such intent is activated. Note that the skill never mentions this suspicious request in its privacy policy list. We also check its reviews on the market. Some users mentioned that "... after filling out the form it gave me someone else's network name and password. What's much worse is that that the name of the wifi network makes me believe that it's very likely someone near to my location, due to the name being a local reference...", "Do not download this app. ... It stores your info and password.". It seems the suspicious behaviors have already troubled users. Another skill Scare Text requests users' phone numbers. According to its description, this skill will send a randomly selected scary image via texting message to the given number. However, after we test several phone numbers, the skill never sends the message.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Skills Conflicting with "Stop"</head><p>After finishing using a skill, users will stop the skill. Otherwise, the skill will continue listening to the users' private conversations. However, some malicious skills may not stop even if they receive users' commands to stop. So we want to check the existence of skills with such behaviors in the wild markets. According to the survey <ref type="bibr" target="#b18">[35]</ref>, 91% of Alexa users use the command "stop" to terminate a skill, and 36% of users choose "cancel", and only 14% of them use "exit". So we send the command "stop" to the skills. Then we leverage some built-in functions of the virtual personal assistant (VPA) and check whether the VPA is activated. For example, we can ask the time using Alexa's own function "what time". If the response is the current time, we can verify that the skill has already exited. Otherwise, it is still on. Although this approach may be circumvented by hijacking the built-in functions, we can try other different functions to test for better accuracy. In our interaction experiments, we also find that a small number of skills behave differently in simulators and the real devices when receiving the commands to exit. We are not sure about the concrete reason. Thus, after some potentially harmful skills are automatically detected, we need to check them on real devices. Note that the different behaviors only happen when receiving the command "stop" to exit. For other voice commands, they behave consistently in both environments <ref type="bibr">11</ref> . Results. We evaluate 28,904 Amazon skills, and find 802 skills do not really stop after receiving the stop command on the simulator. Then we use Echo for further checking, and find that 68 skills have problems on the real smart speakers. In this process, we only need to open a skill and stop it, which takes about 15 seconds to finish (about 3 hours in total). Then we carefully analyze 68 suspicious skills. They achieve eavesdropping using one of the following three ways. (1) 32 skills change the default "stop" commands to others which users may not know. For example, the skill Millennial Money <ref type="bibr">11</ref> One may be worried about that some malicious developers can find the differences between simulators and real environments. However, to the best of our knowledge, no open materials mention whether simulators are used in the vetting. In the current stage, it seems there is no motivation for developers to distinguish the two environments. Although we observed that the simulator behaves differently when receiving the "stop" command. However, the command can be replaced by "exit", which will not let the simulator behave differently. Also, SkillExplorer can evaluate the "stop" command after vetting all other behaviors. If a skill behaves differently later (after identifying the simulator), it is highly suspicious. changes the default command to "I've done". (2) 29 skills ignore the stop command after correctly receiving it (we verify this from the communication history supported by Amazon). For example, the skill My birthday month always says that it cannot get the stop command (which actually indicates that it has received it) and continues its other functions. (3) 7 skills seem more strange. For example, one skill named Malignant Tweets always returns "cannot find this skill" to mimic Alexa no matter what commands it receives and continues to listen to users' conversation. Another skill named math-training replies "OK" after receiving the stop command. But it will continue to listen to users' conversation for 6 seconds. Regarding actions from Google, since Google has very strict requirements on the exit operation, it limits the developer's final response to a simple reply within 60 characters and must be the last dialogue in this interaction <ref type="bibr">[16]</ref>. We did not find any action that has such a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Skills Conflicting with Their Descriptions</head><p>We further want to check whether the information requested by a skill is corresponding to its description given by developers. However, this is very challenging due to diverse ways to describe the skills and the different functionalities given by the skills. So we do a preliminary study. Considering that skills with similar functionalities should behave in a similar way, we use some skills as the seeds and compare other skills with them. For example, two skills A and B both provide real estate information. It is normal that both of them request users' addresses. However, it would be very strange if a skill requests for the health status of users.</p><p>To achieve the differential analysis, we manually select 100 typical skills in 10 categories which request for various kinds of privacy information and view them as the seeds. Then we extract the keywords of the descriptions (i.e., nouns in the constituency-based parsing tree) from all the collected skills. In this way, we could find the skills with similar functionalities. Then we compare the behaviors of the skills, especially the privacy information they request. In this way, we can find skills with abnormal behaviors.</p><p>Results. After manual verification to filter out some reasonable cases, only a few abnormal skills are left. In this preliminary measurement, less than 10 skills request personal information that does not match their descriptions. For example, a skill named Ehrlich Pest Control is supposed to tell users about how to prevent common household pests (e.g., mice) according to the description. However, if a user asks some questions that cannot be understood by the skill, it will ask the user for her phone number and area code. We also find there is a low rating for the skill on the market. A user mentioned that "...it complied then asked me for my phone number so not going to happen, fix that again asking me for my phone number wrong move."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Defense Suggestions</head><p>Although SkillExplorer could serve as a supplement approach for the market administrators to vet skills, we still have some suggestions for them. Firstly, skills should be strictly reviewed before being put on the shelf, especially the contents related to privacy contents. Considering some technical challenges (e.g., the ownership of the privacy-related words) may impede the detection, NLP should be included in the automatic analysis (see Section 3 and Section 4). Secondly, besides the contents provided by skills, the privacy policy links of skills also need to be strictly checked. In this way, users can have a general understanding of what kinds of personal information that the skills will request before users use them. Currently, the markets do not request the privacy policy to be in a unified form. So developers can prepare the privacy policy in various forms (e.g., on a web page, a PDF file, or even missing), which makes the vetting process quite difficult. An official template could be provided to the developers to follow. Thirdly, the built-in intents should also be strictly checked if skills are using them. For example, the built-in stop intent should be carefully checked which might allow a skill to continue working even after receiving the stop command.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Limitations and Future Work</head><p>Firstly, the accuracy of SkillExplorer can be further increased. Current problems are mainly due to developers' irregular design of the questions. Sometimes, developers want to make their questions be clearly spoken out by smart speakers. So they usually add some marks or punctuation insides the questions. For example, there is a question "You can say News -or-Story". Developers add marks before reminding users of the words they need to say to highlight the key points when pronouncing. Although it does not impact user experience (or maybe make the user experience better), this will greatly impact the analysis since such combinations of words and punctuation seldom appear in real texts. Currently, NLP tools (e.g., Stanford NLP Parser) cannot handle this situation. Although in our study we have some techniques and special rules (e.g., removing the punctuation except the quotation mark immediately after the instruction words "say" and "ask"), our tool can be further improved. Also, current NLP tools have false positives (e.g., the extraction of the juxtaposition relationship is wrong, resulting in problems in generating answers).</p><p>Another limitation is from the simulator. Currently, it has restrictions on the interaction with mobile phones and the transmission of geographical location. Neither can it play the non-text audio. If a privacy-related question is played by audio, the simulator cannot correctly identify and return the texts in it. We will identify such a situation and further solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Attacks on skills. Recent studies have been carried out to understand the invocation mechanisms of skills. KUMAR et al. discover skill squatting <ref type="bibr" target="#b9">[26]</ref>, a kind of homo-phonic attacks to divert users' request to an undesired skill. Zhang et al. <ref type="bibr" target="#b18">[35]</ref> further find voice squatting and voice masquerading, which allows a similar pronounced skill to hijack the existing legitimate skills. They also perform a large-scale analysis on skills with similar names or pronunciations in the Amazon market and Google market. Recently in October 2019, researches from SR Labs <ref type="bibr">[3]</ref> demonstrate how a malicious skill can eavesdrop users' privacy after receiving the command to stop based on the research of <ref type="bibr" target="#b9">[26]</ref>, which is also found by us simultaneously. Our work differs from theirs. They design a skill to implement such attacks, while we perform a large-scale analysis on skills and find 68 skills in markets having such problems. <ref type="bibr">Zhang et al. [36]</ref> find the vulnerability of NLU's Intent Classifier and leverage it to let the classifier misunderstand users' request and route the request to a malicious skill. These studies mainly focus on the invocation mechanism of skills, while our work is to explore the behaviors of skills and analyze the contents of the conversation. Attacks on smart speakers. Researches <ref type="bibr" target="#b0">[17,</ref><ref type="bibr" target="#b7">24,</ref><ref type="bibr" target="#b10">27]</ref> find that the mechanism of what they imagine is very different from what the smart speakers actually do. However, some studies <ref type="bibr" target="#b5">[22,</ref><ref type="bibr" target="#b6">23,</ref><ref type="bibr" target="#b15">32]</ref> have already analyzed the security and privacy of general IoT devices, including smart speakers. Carlini et al. <ref type="bibr" target="#b3">[20]</ref> perform Hidden Voice attacks on Amazon Echo, which proves the feasibility of audio attack from two aspects of black box and white box. It's found that both attacks can successfully occur on physical devices. Based on this, the authors put forward some ideas of defense. DolphinAttack <ref type="bibr" target="#b17">[34]</ref> can modulate voice commands on ultrasonic carriers such as frequencies greater than 20 kHz so that people cannot hear them, while still attacking smart speakers. Yuan et al. <ref type="bibr" target="#b4">[21,</ref><ref type="bibr" target="#b16">33]</ref> integrate the voice commands into a song and let the commands be correctly identified by an audio speech recognition (ASR) system but not perceptual to human. <ref type="bibr">Bispham et al. [19]</ref> try to hack the ASR system of smart speakers by gaining covert access to them with nonsense or missense sounds. Sugawara et al. <ref type="bibr" target="#b14">[31]</ref> leverage the laser to remotely inject inaudible and invisible commands into voice assistants, taking advantages of the vulnerability of MEMS microphones. These studies mainly focus on how to inject commands into smart speakers or related ASR systems without being captured by human users, which are different from our study on skill behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose the first systematic study on the behaviors of skills. The key techniques enabling the exploration are a suite of grammar-based methods including utterance extraction, question understanding and answer generation. We develop a tool called SkillExplorer to automatically communicate with 28,904 skills from the Amazon market and 1,897 actions from the Google market, a scale that has never been achieved before. Based on our measurement, over 1,000 skills request users to provide personal information without following developer specifications; 68 skills continue to eavesdrop users' conversation even after receiving the command to stop. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Constituency-based Parse Tree Samples</head><p>We show two samples in <ref type="figure" target="#fig_7">Figure 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Examples of Virtual Users</head><p>We list the information of three virtual users in <ref type="table">Table 10</ref>. These three virtual users constitute the relationship of a father, a mother and a son.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Questions Cannot Be Handled</head><p>We show the questions which cannot be handled by SkillExplorer in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F The Skills in Amazon Market</head><p>We show the skill numbers of different categories in <ref type="table">Table 8. Elements   Description  Example  Invocation name</ref> Only needed for custom skills and can be used for identifying desired skills. Mentioning an invocation name explicitly can wake up the specific skill straightforward.</p><p>"Plan My Trip" has the invocation name "plan my trip" A cloud-based service To handle the structured JSON-format requests from Alexa, skill developers can choose either an AWS Lambda cloud or a custom web service (only suiting custom skills).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AWS Lambda cloud</head><p>Intents An intent represents an action that fulfills a user's spoken request. It can optionally have parameters which officially called slots.</p><p>Intent "PlanMyTrip" with slot "fromCity", "toCity", "travelDate". Sample utterances A set of likely spoken phrases mapped to the intents to help Alexa deal with response, which should include as many representative phrases as possible.</p><p>"i want to visit {toCity}" is mapped to intent "PlanMyTrip". </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework of SkillExplorer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An examples of constituency-based parse tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Q&amp;A samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For example, in the question "You can ask some- thing, such as what's your name.", the user's name (extracted as the answer to the Wh question) is not the answer to the Mix question. So we need to select the question types to answer from all the sentences in Mix questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>I: about us O: …Would you like to learn more? I: open c. s. n. premier collision O: … To learn more, say, about us, services, mobile app, phone number, address, or website. You can say, repeat or stop, at any time.Figure 5 :Figure 5</head><label>55</label><figDesc>Figure 5: An example of itree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of i-tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The distribution of the length of utterances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Examples of constituency-based parse tree</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : An example of the virtual user</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Rules to generate answers for Mix questions 

Basic Corpus of Replies and manually answer them. From 
the answers, we summarize the rules as shown in Table 3, 
and evaluate the accuracy. We randomly select the other 200 
Mix questions, utilize the rules to generate answers, and com-
pare them to human answers. The accuracy is 91%. The rest 
9% (misunderstandings) are mainly due to grammar errors 
or parsing errors from the NLP tool. In the table, Y means 
Yes/No type, I means Instruction type and S means Selection 
type. S_SC and S_CC are Selection_SC and Selection_CC, 
respectively. According to R1, if there is a Yes/No type in Mix 
questions, we just answer "yes" or "no". According to R2, if 
Selection_SC type and Instruction type exist at the same time, 
both types need to be processed. According to R3, if Selec-
tion_CC and Instruction are included in the same sentence, 
they should be replied together. For example, "say next or 
previous". R4 means that if there is an Instruction type, Skill-
Explorer will just answer this type. For example, the question 
"You can say what is the weather like today" contains the In-
struction question and Wh question. It should be marked as 
an Instruction question. Based on our evaluation, the rules are 
accurate to extract answers from Mix questions. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : The rate of incorrect answers</head><label>4</label><figDesc>evaluation, we randomly sample 50 skills from the 21 categories. Then we manually and extensively communicate with them, trying to collect as many behaviors as possible, which lasts for about 8 hours. 226 outputs from skills are collected, including 28 Yes/No questions, 16 Instruction ques- tions, 13 Selection questions, 17 Wh questions, and 53 Mix questions. Further, we let SkillExplorer communicate with the skills, and collect 203 different outputs. So the coverage is 90% (=203 / 226).</figDesc><table>Such collected behaviors can be used as the ground truth for 
comparison with the behaviors explored by SkillExplorer. For 
simplicity, each node in the i-tree can be viewed as a behavior. 
So we can compare the i-trees generated by human and Skill-
Explorer, and calculate the coverage c by c = |n h ∩ n s |/|n h |. 
|n h | indicates the number of nodes in the i-tree explored by 
humans. |n h ∩ n s | shows the number of nodes in both the i-tree 
explored by humans and the i-tree explored by SkillExplorer. 
In our </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 : Custom skill elementsplayer one tell me a name, by saying player one is, followed by the name. Q2 Ok, Here's FakeNBAFreeAgency. Welcome to Fake NNBA Free Agency Search. I can help you find the latest market news. Which team are you looking for? Q3 Here are some things you can say: Give me an attraction. Tell me about Hamilton Wenham. Tell me the top five things to do.</head><label>6</label><figDesc></figDesc><table>KEY 
VALUE 
VALUE 
VALUE 
Full Name 
James C Washington 
Anne J Rosenthal 
Jerome C Washington 
Gender 
male 
male 
male 
Race 
White 
White 
White 
Birthday 
6/19/1980 
5/5/1985 
12/8/2014 
Social Security Number 
066-80-6240 
104-22-6909 
056-40-0812 
Street 
357 Bottom Lane 
357 Bottom Lane 
357 Bottom Lane 
City 
Buffalo 
Buffalo 
Buffalo 
State 
NY 
NY 
NY 
State Full 
New York 
New York 
New York 
Zip Code 
14214 
14214 
14214 
Phone Number 
716-780-4085 
716-780-4085 
Mobile Number 
716-903-8835 
716-214-6493 
716-780-4085 
Temporary email 
7mcjmqil0l@payspun.com 
9vqay8t7p54@iffymedia.com 
Height 
6' 0" (183 centimeters) 
6' 2" (188 centimeters) 
3' 3" (100 centimeters) 
Weight 
200.2 pounds (91.0 kilograms) 
212.3 pounds (96.5 kilograms) 
84 pounds (30.0 kilograms) 
Hair Color 
Black 
Brown 
Black 
Blood Type 
A 
A+ 
A+ 
Mother's Maiden Name 
Brooks 
Osorio 
Rosenthal 
Civil Status 
Married, with children 
Married, with children 
Single 
Educational Background 
Bachelor's degree 
High school diploma or GED 
Kindergarten 
Driver License 
685 549 815 -issued in New York (NY) on qouzznnhu8@claimab.com 
Employment Status 
Full-time work 
Part-time work 
Monthly Salary 
$3,000 
$800 
Occupation(Job Title) 
Waiter and Waitresse 
Presser, Textile, Garment, and Related Material 
Company Name 
Personal &amp; Corporate Design 
The Royal Canadian Pancake Houses 
Company Size 
11-50 employees 
51-100 employees 
Industry 
Food Preparation and Serving Related Occ Production Occupations 
Credit Card Type 
MasterCard 
MasterCard 
Credit Card Number 
5417027168183647 
5427498774029755 
CVV2 
025 
789 
Expires 
10/2023 
11/2024 
Vehicle 
2012 Audi RS3 
2006 Mitsubishi Pajero 
Car License Plate 
2DJ F99 -issued in Maryland (MD) in year 5BMF858 -issued in California (CA) in year 2010 
Favorite Color 
Violet 
White 
Blue 
Favorite Movie 
The Big Lebowski(1998) 
The Truman Show(1998) 
Her(2013) 
Favorite Music 
Gospel music 
Popular music 
Trance music 
Favorite Song 
I'm An Albatraoz(by AronChupa) 
I Have Questions (by Camila Cabello) 
Hula Hula(by Robin) 
Favorite Book 
Divine Secrets of the Ya-Ya Sisterhood --b Frostbite (Vampire Academy) --by Richelle Les Misérables --by Victor Hugo 
Favorite Sports 
Diving 
Diving 
Cycling 
Favorite TV 
Limitless CBS 
The Real O'Neals ABC 
NFL Sunday Night Football NBC 
Favorite Movie Star 
Lauren Cohan 
Thora Birch 
Manu Bennett 
Favorite Singer 
Gyllene Tider 
Paul Weller 
The Lumineers 
Favorite Food 
Pasta 
Italian, Pasta 
Noodles, Fried chicken 
Personality 
Philosophic 
Unpleasant 
Artistic 
Personal Style 
Jeans and t-shirt 
Swimsuit 
Jeans and t-shirt 
Username 
arshia_karikator 
certes 
Windows 7 
Password 
iRaetuuf7ai 
xah8Quohm2 
8cb19fd7 

Figure 10: Examples of Virtual Users 

USENIX Association 
29th USENIX Security Symposium 2665 

</table></figure>

			<note place="foot" n="1"> In this paper, we use skills to describe the abilities including Google&apos;s actions.</note>

			<note place="foot" n="2"> We have sent our verified findings to the markets and are waiting for their response.</note>

			<note place="foot" n="3"> Sometimes, we meet very short questions without an auxiliary verb or a modal verb. We still classify it as Yes/No questions. Such as &quot;next news?&quot;.</note>

			<note place="foot" n="4"> To include more words in the future, we can quickly generate the rules for them using dictionaries [11].</note>

			<note place="foot" n="5"> A special exception is the network error. When this situation happens, SkillExplorer exits the current execution and restarts from the root node.</note>

			<note place="foot" n="6"> We download NLTK v3.4.5 from [14] and Stanford NLP Parser v3.9.2 from [15]. 7 It will also exceed the time limit given to the user for feedback. Usually, the time limit is 6 seconds [34]. If the time of waiting for the user&apos;s response is too long, the smart speaker will automatically turn off itself.</note>

			<note place="foot" n="8"> We registered 25 different Amazon developer accounts, and 2 Google developer accounts for testing. 27 simulators were utilized (25 from Amazon and 2 from Google).</note>

			<note place="foot" n="9"> We crawled the skills from the America market from October 25, 2019 to November 12, 2019, where the number of skills is the largest in the world. Different countries may access different numbers of skills according to the policy of Amazon.</note>

			<note place="foot" n="10"> Some skills may not have invocation name which can be invoked through implicit invocation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank anonymous reviewers for their insightful comments that have helped improve this paper substantially. Specifically, we thank our shepherd, Professor Adam Bates, for his constructive feedback on this paper. </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Custom Skill Elements</head><p>We show the elements needed for a custom skill in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Utterance Distribution</head><p>We list the length distribution of the sample utterances. As shown in the <ref type="figure">Figure 8</ref>, only 0.8% of them are longer than 15. So we select 15 as the threshold of S l .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Top 5 Questions</head><p>We show the top 5 most frequently questions mentioned by skills in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Rules of Invocation Names in Amazon</head><p>(1) Amazon does not allow one-word invocation name unless it is unique to the developer's brand/intelligent. (2) Two-word invocation names are not allowed if it contains definite article ("the"), indefinite article ("a", "an") or preposition ("for", "to", "of," "about," "up," "by," "at," "off," "with"). (3) Invocation names cannot be a person or a place name. (4) Invocation name cannot contain skill's launch word such as "open," "tell," etc. and connecting words. include "to," "from," "in," etc. <ref type="formula">(5)</ref> The invocation name cannot contain the wake words "Alexa," "Amazon," "Echo," or the words "skill" or "app". (6) The invocation name must to be lowercase, and other characters like numbers must be spelled out. <ref type="formula">(7)</ref> Invocation name should be distinctive to help users wake up accurately.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">More than smart speakers: Security and privacy perceptions of smart home personal assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kopo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Ramokapane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Such</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Symposium on Usable Privacy and Security, SOUPS 2019</title>
		<editor>Heather Richter Lipford</editor>
		<meeting><address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Policylint: Investigating internal privacy policy contradictions on google play</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Andow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaseer</forename><surname>Samin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Enck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Reaves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th USENIX Security Symposium, USENIX Security 2019</title>
		<editor>Nadia Heninger and Patrick Traynor</editor>
		<meeting><address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="585" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonsense attacks on google assistant and missense attacks on amazon alexa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">K</forename><surname>Bispham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Agrafiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Information Systems Security and Privacy, ICISSP 2019</title>
		<editor>Paolo Mori, Steven Furnell, and Olivier Camp</editor>
		<meeting>the 5th International Conference on Information Systems Security and Privacy, ICISSP 2019<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>SciTePress</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="75" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hidden voice commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavish</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Sherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clay</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th USENIX Security Symposium, USENIX Security 16</title>
		<editor>Thorsten Holz and Stefan Savage</editor>
		<meeting><address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="513" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devil&apos;s whisper: A general approach for physical adversarial attacks against commercial black-box speech recognition devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th USENIX Security Symposium</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>USENIX Security 20</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Computer security and the modern home</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Denning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="103" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Security analysis of emerging smart home applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyeon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-05-22" />
			<biblScope unit="page" from="636" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Consumer attitudes towards privacy and security in home assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Fruchter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Liccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems, CHI</title>
		<editor>Regan L. Mandryk, Mark Hancock, Mark Perry, and Anna L. Cox</editor>
		<meeting><address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-04-21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural approaches to conversational AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018</title>
		<editor>Yoav Artzi and Jacob Eisenstein</editor>
		<meeting>ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Skill squatting attacks on amazon alexa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Paccagnella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Murley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Hennenfent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th USENIX Security Symposium, USENIX Security</title>
		<editor>William Enck and Adrienne Porter Felt</editor>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-08-15" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Alexa, are you listening?: Privacy perceptions, concerns and privacy-seeking behaviors with smart speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schaub</surname></persName>
		</author>
		<idno>102:1-102:31</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CSCW</publisher>
			<biblScope unit="volume">PACMHCI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nltk: The natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno>ETMTNLP &apos;02</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics</title>
		<meeting>the ACL-02 Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, MD, USA, System Demonstrations</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-22" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>The Association for Computer Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Question categorization and classification using grammar based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaa</forename><surname>Mohasseb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Bader-El-Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Cocea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Manage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1228" to="1243" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Light commands: Laserbased audio injection on voice-controllable systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Cyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rampazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Charting the attack surface of trigger-action iot platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pubali</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
		<editor>Lorenzo Cavallaro, Johannes Kinder, XiaoFeng Wang, and Jonathan Katz, editors</editor>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-11" />
			<biblScope unit="page" from="1439" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Commandersong: A systematic approach for practical adversarial voice recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th USENIX Security Symposium, USENIX Security</title>
		<editor>William Enck and Adrienne Porter Felt</editor>
		<meeting><address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-08-15" />
			<biblScope unit="page" from="49" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dolphinattack: Inaudible voice commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<editor>Bhavani M. Thuraisingham, David Evans, Tal Malkin, and Dongyan Xu</editor>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-10-30" />
			<biblScope unit="page" from="103" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dangerous skills: Understanding and mitigating security risks of voicecontrolled third-party functions on virtual personal assistant systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghang</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1381" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Life after speech recognition: Fuzzing semantic misinterpretation for voice assistant applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abner</forename><surname>Mendoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phakpoom</forename><surname>Chinprutthiwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofei</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th Annual Network and Distributed System Security Symposium, NDSS 2019</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>The Internet Society</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
