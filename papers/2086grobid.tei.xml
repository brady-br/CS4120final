<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sampling-based Alignment and Hierarchical Sub-sentential Alignment in Chinese-Japanese Translation of Patents</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information, Production and Systems</orgName>
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information, Production and Systems</orgName>
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosong</forename><surname>Yang</surname></persName>
							<email>yangbaosong@fuji.waseda.jpyves.lepage@waseda.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information, Production and Systems</orgName>
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Lepage</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information, Production and Systems</orgName>
								<orgName type="institution">Waseda University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sampling-based Alignment and Hierarchical Sub-sentential Alignment in Chinese-Japanese Translation of Patents</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper describes Chinese-Japanese translation systems based on different alignment methods using the JPO corpus and our submission (ID: WASUIPS) to the subtask of the 2015 Workshop on Asian Translation. One of the alignment methods used is bilingual hierarchical sub-sentential alignment combined with sampling-based multilingual alignment. We also accelerated this method and in this paper, we evaluate the translation results and time spent on several machine translation tasks. The training time is much faster than the standard baseline pipeline (GIZA++/Moses) and MGIZA/Moses.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phrase-based Statistical Machine Translation (PB-SMT) as a data-oriented approach to machine translation has been widely used for over 10 years. The Moses ( <ref type="bibr">Koehn et al., 2007</ref>) open source statistical machine translation toolkit was developed by the Statistical Machine Translation Group at the University of Edinburgh. During the three processes (training, tuning and decoding) for building a phrase-based translation system using Moses, training is the most important step as it creates the core knowledge used in machine translation. Word or phrase alignment in the training step allows to obtain translation relationships among the words or phrases in a sentence-aligned bi-corpus. Word or phrase alignment affects the quality of translation. It is also one of the most time-consuming processing step.</p><p>The probabilistic approach attempts at determining the best set of alignment links between source and target words or phrases in parallel sentences. IBM models ( <ref type="bibr">Brown et al., 1993)</ref> and HMM alignment models ( <ref type="bibr">Vogel et al., 1996)</ref>, which are typical implementation of the EM algorithm <ref type="bibr">(Dempster et al., 1977)</ref>, are the most widely used representatives in this category. GIZA++ <ref type="bibr">(Och and Ney, 2003)</ref> implemented IBM Models, it aligns words based on statistical models. It is a global optimization process simultaneously considers all possible associations in the entire corpus and estimates the parameters of the parallel corpus. Several improvements were made: MGIZA ( <ref type="bibr">Gao and Vogel, 2008</ref>) is a parallel implementation of IBM models. However, the parallelization may lead to slightly different final alignment results, thus preventing reproduction of results to a certain extent.</p><p>The associative approaches, introduced in ( <ref type="bibr">Gale and Church, 1991)</ref>, do not rely on an alignment model, but on independence statistical measures. The Dice coefficient, mutual information ( <ref type="bibr">Gale and Church, 1991)</ref>, and likelihood ratio <ref type="bibr">(Dun- ning, 1993)</ref> are representative cases of this approach. The associative approaches use a local maximization process in which each sentence is processed independently. Sampling-based multilingual alignment (Anymalign) ( <ref type="bibr">Lardilleux et al., 2013)</ref> and hierarchical sub-sentential alignment (Cutnalign) ( <ref type="bibr">Lardilleux et al., 2012</ref>) are two associative approaches.</p><p>Anymalign <ref type="bibr">1</ref> is an open source multilingual associative aligner <ref type="bibr">(Lardilleux and Lepage, 2009;</ref><ref type="bibr">Lardilleux et al., 2013)</ref>. This method samples large numbers of sub-corpora randomly to obtain source and target word or phrase occurrence distributions. The more often two words or phrases have the same occurrence distribution over particular sub-corpora, the higher the association between them.</p><p>We can run Anymalign by setting with -t (running time) option and stop it at any time, and the option -i allows to to extract longer phrases by enforcing n-grams to be considered as tokens. For pre-segmented texts, option -i allows to group words into phrases more easily.</p><p>Cutnalign is a bilingual hierarchical subsentential alignment method ( <ref type="bibr">Lardilleux et al., 2012)</ref>. It is based on a recursive binary segmentation process of the alignment matrix between a source sentence and its corresponding target sentence. We make use of this method in combination with Anymalign.</p><p>In the experiments, reported in this paper, we extend the work to decrease time costs in the training step. We obtained comparable results in only one fifth of the training time required by the GIZA++/Moses baseline pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Chinese and Japanese data used</head><p>The data used in our systems are the ChineseJapanese JPO Patent Corpus (JPC) 2 provided by WAT 2015 for the patents subtask ( <ref type="bibr">Nakazawa et al., 2015)</ref>. It contains 1 million Chinese-Japanese parallel sentences in four domains in the training data. These are Chemistry, Electricity, Mechanical engineering, and Physics. We used sentences of 40 words or less than 40 words as our training data for the translation models, but use all of the Japanese sentences in the parallel corpus for training the language models. We used all of the development data for tuning. For Chinese and Japanese segmentation we used the Stanford Segmenter (version: 2014-01-04 with Chinese Penn Treebank (CTB) model) <ref type="bibr">3</ref> and Juman (version 7.0) <ref type="bibr">4</ref> . <ref type="table">Table 1</ref> shows some statistics on the data we used in our systems (after tokenization, lowercase and clean).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bilingual hierarchical sub-sentential alignment method</head><p>Cutnalign as a bilingual hierarchical subsentential alignment method based on a recursive binary segmentation process of the alignment matrix between a source sentence and its translation. It is a three-step approach:</p><p>• measure the strength of the translation link between any source and target pair of words;  <ref type="table">Table 1</ref>: Statistics of our baseline training data of JPC.</p><p>• compute the optimal joint clustering of a bipartite graph to search the best alignment;</p><p>• segment and align a pair of sentences.</p><p>When building alignment matrices, the strength between two words is evaluated using the following formula ( <ref type="bibr">Lardilleux et al., 2012</ref>).</p><formula xml:id="formula_0">w(s, t) = p(s|t) × p(t|s)<label>(1)</label></formula><p>(p(s|t) and p(t|s)) are translation probabilities estimated by Anymalign. An example of alignment matrix is shown in <ref type="table">Table 2</ref>.</p><p>The optimal joint clustering of a bipartite graph is computed recursively using the following formula for searching the best alignment between words in the source and target languages ( <ref type="bibr">Zha et al., 2001;</ref><ref type="bibr">Lardilleux et al., 2012)</ref>.</p><formula xml:id="formula_1">cut(X, Y ) = W (X, Y ) + W (X, Y )<label>(2)</label></formula><p>X, X, Y , Y denote the segmentation of the sentences. Here the block we start with is the entire matrix. Splitting horizontally and vertically into two parts gives four sub-blocks.</p><formula xml:id="formula_2">W (X, Y ) = 񮽙 s∈X,t∈Y w(s, t)<label>(3)</label></formula><p>W (X, Y ) is the sum of all translation strengths between all source and target words inside a subblock (X, Y ).</p><p>The point where to is found on the x and y which minimize N cut ( <ref type="bibr">Lardilleux et al., 2012)</ref>: <ref type="table" target="#tab_2">Table 3</ref> shows several segmentations out of all the possible segmentation in two blocks by computing the sub-sentential alignment between a Chinese and a Japanese sentences. For each word pair (x, y), we compute N cut <ref type="bibr">(x, y)</ref>. In this case, we start at word pair (根据, ), the search space is the rectangle area [(根据, ), (。, 񮽙)]. In <ref type="table" target="#tab_2">Table 3</ref>, only 7 out of all the possible segmentations in two blocks are shown. The number of possible segmentation is: the length of the Japanese sentence minus one, multiplied by the length of the Chinese sentence minus one, multiplied by two, as there are two possible direction for segmenting. After computing all N cut(x, y), we compare and find the minimal N cut(x, y). Table 4 shows the flow of recursive segmentation and alignment.</p><formula xml:id="formula_3">N cut(X, Y ) = cut(X, Y ) cut(X, Y ) + 2 × W (X, Y ) + cut(X, Y ) cut(X, Y ) + 2 × W (X, Y )<label>(4)</label></formula><p>In the our experiments, we introduced two types of improvements ( <ref type="bibr">Yang and Lepage, 2015)</ref> compared to the original implementation. The first one, introduces multi-processing in both the sampling-based alignment method and hierarchical sub-sentential alignment method so as to trivially accelerate the overall alignment process. We also re-implement the core of Cutnalign in C. The second one, approximations in the computation of N cut accelerate some decisions. Also a method to reduce the search space in hierarchical subsentential alignment has been introduced, so that important speed-ups are obtained. We refer the reader to ( <ref type="bibr">Yang and Lepage, 2015</ref>) for a detailed description of these improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments based on different alignment methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment settings</head><p>Here, we basically perform experiments with GIZA++ or MGIZA. The phrase tables are extracted from the alignments obtained using the grow-diag-final-and heuristic <ref type="bibr">(Ayan and Dorr, 2006</ref>) integrated in the Moses toolkit. Our sampling-based alignment method and hierarchical sub-sentential alignment method are also evaluated within a PB-SMT system built by using the Moses toolkit, the Ken Language Modeling toolkit (Heafield, 2011) and a lexicalized reordering model ( <ref type="bibr">Koehn et al., 2005</ref>). We built systems from Chinese to Japanese. Each experiment was run using the same data sets (see Section 2). Translations were evaluated using BLEU <ref type="bibr">(Pap- ineni et al., 2002</ref>) and RIBES ( <ref type="bibr">Isozaki et al., 2010)</ref>. We used Anymalign (i=2, two words can be considered as one token) and Cutnalign to build phrase tables. As a timeout (-t) should be given, we set two different timeouts (5400 sec. and 1200 sec.). We also use different Cutnalign versions where core components are implemented in C or Python. We passed word-to-word associations output by Anymalign (i=2) to Cutnalign which produces sub-sentential alignments, which are in turn passed to the grow-dial-final-and heuristic of the Moses toolkit to build phrase tables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Evaluation results using different alignment methods based on the same data sets are given in <ref type="table" target="#tab_4">Tables 5 and 7</ref>. The system built based on GIZA++/Moses pipeline as a baseline system is given in <ref type="table" target="#tab_4">Table 5</ref>. We also show the evaluation results obtained by the WAT 2015 automatic evaluation <ref type="bibr">5</ref> in <ref type="table">Table 6</ref> and 8. The results in <ref type="table">Table 7</ref> and 8 show that there are no significant differences among the evaluation results based on different versions of Moses, different Anymalign timeouts or different versions of Cutnalign. However, the training times changed considerably depending on the timeouts for Anymalign. The fastest training time is obtained with Moses version 2.1.1, a timeout of 1200 sec. for Anymalign and the C version of Cutnalign: 57 minutes, i.e., about one fifth of the time used by GIZA++ or MGIZA <ref type="table" target="#tab_4">(Table 5 and  6</ref>). We also checked the confidence intervals between using GIZA++ and our method (the fastest one): 37.24 ± 0.86 and 35.72 ± 0.90. The probability of actually getting them (p-value) is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have shown that it is possible to accelerate development of SMT systems following the work by <ref type="bibr">Lardilleux et al. (2012)</ref> and <ref type="bibr">Yang and Lepage (2015)</ref> on bilingual hierarchical sub-sentential alignment. We performed several machine translation experiments using different alignment methods and obtained a significant reduction of processing training time. Setting different timeouts for Anymalign did not change the translation quality. In other word, we get a relative steady translation quality even when less time is allotted to word-to-word association computation.</p><p>Here, the fastest training time was only 57 minutes, one fifth compared with the use of GIZA++ or MGIZA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The paper is part of the outcome of research performed under a Waseda University Grant for Special Research Project (Project number: 2015A-063).    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>References</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : 7 out of all the possible segmentation in two blocks are shown.</head><label>3</label><figDesc></figDesc><table>92 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Steps in recursive segmentation and alignment result using sampling-based alignment and hier-
archical sub-sentential alignment method. 

s→t 
Moses 
Aligner BLEU 
RIBES Training time 

zh→ja 
2.1.1 MGIZA 37.70 0.783000 
5:34:28 
2.1.1 GIZA++ 37.46 0.778914 
4:43:56 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Evaluation results by using different aligner (GIZA++ and MGIZA) based on the data of JPC 
given in Table 1. </table></figure>

			<note place="foot" n="2"> http://lotus.kuee.kyoto-u.ac.jp/WAT/ patent/index.html 3 http://nlp.stanford.edu/software/ segmenter.shtml 4 http://nlp.ist.i.kyoto-u.ac.jp/index. php?JUMAN</note>

			<note place="foot" n="5"> http://orchid.kuee.kyoto-u.ac.jp/WAT/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>94</head></div>			</div>
			<div type="references">

				<listBibl/>
			</div>
		</back>
	</text>
</TEI>
