<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-dataset Time Series Anomaly Detection for Cloud Systems Cross-dataset Time Series Anomaly Detection for Cloud Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 10-12, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingwei</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Qiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingnong</forename><surname>Dang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsheng</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murali</forename><surname>Chintalapati</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Microsoft;</roleName><forename type="first">Ken</forename><surname>Hsieh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Sui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Meng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Microsoft</roleName><forename type="first">Wenchi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingwei</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Qin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">The University of Newcastle</orgName>
								<orgName type="institution" key="instit2">NSW</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Qiao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingnong</forename><surname>Dang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Azure</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinsheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Azure</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Azure</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murali</forename><surname>Chintalapati</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Azure</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Wu</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Azure</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Hsieh</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Azure</orgName>
								<address>
									<settlement>Redmond</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Sui</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Meng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohai</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furao</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<orgName type="institution" key="instit3">Microsoft Research; Hongyu Zhang</orgName>
								<orgName type="institution" key="instit4">The University of Newcastle</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cross-dataset Time Series Anomaly Detection for Cloud Systems Cross-dataset Time Series Anomaly Detection for Cloud Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2019 USENIX Annual Technical Conference</title>
						<meeting>the 2019 USENIX Annual Technical Conference <address><addrLine>Renton, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 10-12, 2019</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2019 USENIX Annual Technical Conference is sponsored by USENIX. Research; Furao Shen, Nanjing University; Dongmei Zhang, Microsoft Research https://www.usenix.org/conference/atc19/presentation/zhang-xu</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity , volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments , we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1%-5% of unlabelled data and can still achieve significant performance improvement.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, we have witnessed increasing adoption of cloud service systems. Many software applications are now deployed on cloud computing platforms such as Microsoft Azure, Google Cloud Platform, and Amazon Web Services (AWS). As the cloud systems could be used by millions of users around the world on a 24/7 basis, high service reliability and availability are critical.</p><p>However, cloud systems, like other software systems, may exhibit some anomalous behaviors. These anomalies could seriously affect service availability and reliability and could even lead to huge financial loss. The anomalies could be caused by a variety of factors (such as software bugs, disk failures, memory leaks, network outage, etc.) and reflected by a variety of cloud monitoring data (such as KPI, performance counters, usage statistics, system metrics, logs, etc.). The cloud monitoring data are usually time series data, which have high velocity and enormous volume because of the scale and complexity of cloud systems. To maintain high service reliability and availability, it is important yet challenging to detect anomalies from a large amount of cloud monitoring data precisely and timely.</p><p>Specifically, anomaly detection in practice encounters several challenges due to the characteristics of cloud systems. A large cloud system is composed of a variety of services and each service is associated with some monitoring data. For some types of data, the characteristics of anomalies are common across many services. While for some other types of data, the characteristics of anomalies could differ from service to service. For example, 90% CPU utilization is normal for computation intensive services but anomalous for other services. Therefore, simple threshold-based anomaly detectors can hardly perform well for a variety of services.</p><p>Over the years, many machine-learning based anomaly detection methods have been proposed, including supervised <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref> and unsupervised methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b37">38]</ref>. However, it is not trivial to detect anomalies in a large and diversified set of time series data in real cloud environment where labelled data is scarce but a high detection performance is demanded. Unsupervised learning methods can deal with a large amount of data as they do not require labelled data. However, the performance achieved by these methods is rather low <ref type="bibr" target="#b12">[13]</ref>. Although supervised learning methods can achieve higher accuracy than the unsupervised counterparts, it is time-consuming and tedious to manually label the anomalies due to the volume and diversity of cloud monitoring data. Therefore, supervised-learning based methods are difficult to be applied to anomaly detection in practice.</p><p>Facing the above challenges, to build an accurate and efficient anomaly detection model, we propose ATAD, which enables cross-dataset anomaly detection for cloud systems. The main idea of cross-dataset is to perform anomaly detection on an unlabelled dataset (the target dataset) by learning from existing, labelled datasets (the source datasets). For example, a detector can be learned from a public dataset such as NAB <ref type="bibr" target="#b22">[23]</ref>, and then applied to an unlabelled dataset collected from a real-world system. ATAD consists of two major components: 1) Transfer Learning, which transfers the common anomalous behavior learned from a labelled time series data to a large volume of target unlabelled dataset. Through transfer learning, the commonalities across datasets could be leveraged and the labelling effort for the target dataset could be reduced. 2) Active Learning, which improves the detection performance by labelling only a small number of selected samples in the target dataset. Through active learning, the diversified data with specific characteristics can be addressed with a small amount of labelling effort.</p><p>In particular, in the Transfer Learning component, we identify multiple features of cloud monitoring data and perform clustering to select an appropriate subset of existing labelled data as the sub source domain. Then the CORAL algorithm <ref type="bibr" target="#b36">[37]</ref> is applied to narrow the feature difference between the source and target domain. In the Active Learning component, we utilize the UCD (Uncertainty-ContextDiversity) method to recommend informative data points to be labelled. The labelled points are used to retrain the classifier trained from the Transfer Learning component. In this way, we aim at minimizing the labelling effort and improving the performance of the detector as much as possible.</p><p>We have conducted experiments on public datasets to verify the effectiveness of our method. The experimental results show that using ATAD we can achieve cross-dataset anomaly detection with good accuracy. We test the effectiveness on both public datasets and real-world cloud monitoring data. On public datasets, ATAD shows higher accuracy than existing methods when performing anomaly detection on a target dataset (i.e. Yahoo) by the detector learned from a source dataset (Non-Yahoo, like AWS, Twitter and Artificial datasets). Furthermore, labelling about 1%-5% of unlabelled data could achieve much higher F1-score than the related methods. We also train an ATAD model using public datasets and apply the trained model to detect anomalies in real-world cloud monitoring data of Microsoft. ATAD achieves the best F1-Score, which is much higher than those achieved by other methods.</p><p>The contributions of this paper are as follows:</p><p>• We propose a new anomaly detection method called ATAD, which enables cross-dataset anomaly detection for cloud systems.</p><p>• To the best of our knowledge, we are among the first to detect anomalies in time series cloud data using a combination of transfer learning and active learning techniques.</p><p>• We have performed an extensive evaluation of the proposed approach using public and real-world datasets.</p><p>This paper is organized as follows: we first elaborate the background and motivation of our work in Section 2. In Section 3, the details of ATAD are described. Section 4 reports the experiments and corresponding results. Next, we discuss threats to validity in Section 5. We introduce the related work in Section 6, before concluding the paper in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>For cloud service vendors like Microsoft Azure, Google Cloud Platform, and Amazon Web Services (AWS), there are millions of servers and virtual machines providing a variety of services to users. Despite many quality assurance methods, it is difficult to avoid system failures in reality. A severe system failure can cause damage to user's operation and vendor's reputation. Recovering system from failures in time is of great importance. In order to do that, quick and accurate anomaly detection is essential.</p><p>Anomaly detection is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data <ref type="bibr" target="#b42">[43]</ref>. Anomaly detection in cloud is usually performed on Cloud Monitoring Data (such as KPI, performance counters, CPU utilization, VM downtime, system workload, etc.). The cloud monitoring data is often presented in time series, that is a series of numerical data points recorded in time order.</p><p>Unlike a general anomaly detection problem, it is much more difficult to detect anomalies in a large-scale cloud service system. We identify the following challenges:</p><p>• Diverse characteristics of anomalies: in a large-scale cloud service system, different usage scenarios and components have different levels of tolerance to anomalies. For example, a minor system deviation occurring in a certain key component, like storage cluster, may become an anomaly and lead to the failure of the whole system <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>. However, such a deviation may not cause serious problems in other components. It is difficult to set accurate thresholds of anomalies for each usage scenario and system component <ref type="bibr" target="#b10">[11]</ref>. Therefore simple threshold-based anomaly detection methods are not suitable for cloud service systems.</p><p>• Anomaly detection in time series data: cloud monitoring data is large-scale time series data that has temporal property. Many commonly-used machine learning algorithms cannot be directly applied because the time series data does not satisfy the independent and identically distributed (i.i.d) assumption. Although some deep learning models, like LSTM <ref type="bibr" target="#b16">[17]</ref>, could capture the temporal property, they require enormous labelled data to train an accurate model. Thus, an appropriate approach to incorporate the temporal property of time series data is important.</p><p>• Unsatisfactory performance of unsupervised learning: unsupervised machine learning techniques such as Isolation Forest <ref type="bibr" target="#b25">[26]</ref> or Seasonal Hybrid ESD <ref type="bibr" target="#b15">[16]</ref> can be applied to anomaly detection. These methods detect anomalies by checking outliers/deviations from the normal data distribution. However, the effectiveness of unsupervised anomaly detection algorithms is often unsatisfactory <ref type="bibr" target="#b12">[13]</ref>. The false alarm rate of unsupervised models is higher, which requires much more effort for engineers to check the status of the cloud system.</p><p>• Lacking labels for supervised learning: As mentioned above, if the temporal property of time series data can be well incorporated into the labelled data, supervised machine learning methods such as SVM or Random Forest are good to be used to learn and predict anomaly patterns <ref type="bibr" target="#b28">[29]</ref>. However, due to the scale and complexity of a cloud service system, labelling the whole dataset requires enormous human effort and is an almost impossible task. The problem of lacking labelled data limits the application of supervised anomaly detection methods to cloud service systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>In order to address the challenges mentioned above, in this paper, we propose a novel time series anomaly detection method called ATAD (Active Transfer Anomaly Detection), which combines transfer learning and active learning technologies for anomaly detection in cloud monitoring data. <ref type="figure" target="#fig_1">Fig. 1</ref> shows the overall workflow of ATAD.  There are two sets of input data, one is the unlabelled time series data T u on which anomaly detection will be conducted, the other is labelled time series data T l collected from the public domains or the other components of the cloud system. In T l , each point in the time series has been manually labelled as either anomaly or normal.</p><p>Our approach consists of two main components, namely Transfer Learning component and Active Learning component. In transfer learning, to incorporate temporal property of time series data, multiple general features are extracted from the raw dataset T l and form the feature dataset F l . Feature-based and instance-based transfer learning methods are then applied on F l to learn a base detection model. After that, the unlabelled time series T u goes through the same feature extraction process and forms the feature dataset F u . The active learning component recommends a small number of informative samples from F u for labelling through Uncertainty and Context Diversity (UCD) strategy. Then the labelled data is used to retrain the base detection model. After T rounds of active learning, we obtain the final anomaly detector.</p><p>We will describe more details about ATAD in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transfer Learning Component</head><p>Many machine learning methods assume that the distributions of labelled and unlabelled data are the same. However, transfer learning, in contrast, allows the domains, tasks, and distributions used in training and testing to be different <ref type="bibr" target="#b29">[30]</ref>. In order to achieve knowledge transfer among different time series datasets, we utilize an efficient and effective transfer method for large-scale cloud monitoring data. For the anomaly detection problem in cloud systems, it is non-trivial to perform transfer learning directly. We need to consider the following factors when we design our transfer learning component.</p><p>• Cloud monitoring data is usually presented in the form of time series. Time series is not independent data, but a set of data points with temporal dependence. Thus the anomaly patterns of time series have contextual relevance. How to incorporate this relevance into anomaly detection is a challenge. In our work, we extract time series related features at each data point. Each data point is transformed from the original singledimensional scalar into a high-dimensional feature vector, and the contextual information is preserved by these features. In ATAD, we not only take account of the simple descriptive features (statistical values), but also the order-aware features (forecasting error features and temporal features).</p><p>• For a time series, it is a problem what granularity transfer learning should be performed at. We can conduct transfer learning on entire time series, subseries, or discrete time points. If transfer learning is performed at a coarse-granularity (e.g. the entire time series or subseries) that contains several different anomalous patterns, it is not conducive to distinguish them. Further, coarse-granularity anomaly detection leads to the difficulty of locating and retrieving the cause of anomalies.</p><p>In this work, we aim to conduct anomaly detection at a fine granularity (i.e. for each data point), and so we perform transfer learning at the level of data point.</p><p>• Transfer learning requires that the source domain and the target domain have the underlying similarity. However, the time series generated from various components in a large-scale cloud system could be very different. We should guarantee that the source domain and the target domain come from similar services or have similar characteristics. Thus, during the transfer learning process, we need to filter out those source-domain samples that are not similar to the counterpart in the target domain. <ref type="figure" target="#fig_3">Fig. 2</ref> shows the workflow of the Transfer Learning Component. The following subsections describe our algorithm in detail.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Feature Identification</head><p>The feature engineering process of ATAD converts each data point in a time series (T l ) into a set of features (F l ), which can capture both contextual and temporal information around the point. These features can be categorized into three groups: statistical features, forecasting error features, and temporal features.</p><p>Before computing the value of these features, we use Discrete Fourier Transform (DFT) to estimate the period p of the most dominant frequency. Different periods determine different sizes of the sliding window used in the following process.</p><p>Statistical features: Statistical features describe some basic characteristics around each data point in time series. We hold a view that the statistical features are able to describe the basic characteristics of different time series generated from various sources in cloud systems, and it is conducive to detecting anomalies that violate the basic characteristics. For example, in an active-running computation intensive service, if the average CPU utilization over a time window tends to be low, it may be an indicator that part of the computation process on it halts unexpectedly.</p><p>We identify features for depicting statistical characteristics of time series data (as listed in <ref type="table" target="#tab_0">Table 1</ref>). These descriptive features are all calculated in a rolling window derived from the period p. They can represent short-period aspects of time series data such as mean, variance, autocorrelation, trend, remainder, and stationary test. Spectral entropy <ref type="bibr" target="#b11">[12]</ref>.</p><p>ARCHtest.p <ref type="bibr" target="#b8">[9]</ref> P value of Lagrange Multiplier (LM) test for ARCH model <ref type="bibr" target="#b7">[8]</ref>.</p><p>GARCHtest.p <ref type="bibr" target="#b23">[24]</ref> P value of Lagrange Multiplier (LM) test for GARCH model <ref type="bibr" target="#b7">[8]</ref>.</p><p>Forecasting error features: Following the prior work <ref type="bibr" target="#b21">[22]</ref>, we use a set of error metrics resulted from time series forecasting as features. The intuition is that if the value of current point deviates from the forecasting result, there is more likely to be an anomaly. We use ensemble models to carry out forecasting and apply different models based on the seasonality of the time series. The models leverage classical forecasting techniques, namely SARIMA <ref type="bibr" target="#b26">[27]</ref>, Holt <ref type="bibr" target="#b18">[19]</ref>, HoltWinters <ref type="bibr" target="#b18">[19]</ref>, and STL <ref type="bibr" target="#b4">[5]</ref> for seasonal data, and SARIMA, Holt, Holt-Winters, and Polynomial Regression <ref type="bibr" target="#b14">[15]</ref> for nonseasonal data, respectively. The metric RMSE (Root Mean Squared Error) is used to endow different forecasting methods with different weights at a fixed time. More weight should be assigned to more precise forecasting model. The weighted prediction result of the ensemble model from M models at time t is calculated by:</p><formula xml:id="formula_0">ˆ Y t = M ∑ m=1ˆY m=1ˆ m=1ˆY m,t M − 1 · (1 − RMSE m,t M ∑ n=1 RMSE n,t )<label>(1)</label></formula><p>wherê Y m,t is the prediction by model m at time t, RMSE m,t is the prediction error of model m at time t, ˆ Y t is the ensemble prediction at time t.</p><p>After gaining the ensemble predictionˆYpredictionˆ predictionˆY t , we calculate 5 metrics on 3 rolling time windows to measure the bias between predicted and actual values. The metrics are shown in <ref type="table" target="#tab_1">Table 2</ref>, wherê Y is the predicted value, Y represents the actual value, and N is the size of the time window. </p><formula xml:id="formula_1">ME ∑(Yi− ˆ Y i) N Mean Error. RMSE ∑(Yi− ˆ Y i) 2 N Root Mean Squared Error. MAE ∑|Yi− ˆ Y i| N Mean Absolute Error. MPE 1 N · ∑ Y i − ˆ Y i Y Mean Percentage Error. MAPE 1 N · ∑ |Yi− ˆ Y i| Y Mean Average Percentage Error.</formula><p>Temporal features: Generally speaking, the drastic changes of system metrics are likely to be anomalies. For example, the sharp decline of disk I/O traffic rate may be caused by the hardware failure in the disk array. To understand the changes of time series data over time, we identify temporal features (as shown in <ref type="table" target="#tab_2">Table 3</ref>) by comparing data in two consecutive windows. We also compute the difference between current values and previous w values (e.g., the difference between x n−w and x n ). In our implementation, we set w = p/2, p, 2p, w pr , w pr /2, respectively to get the corresponding different values (Diff-w). w pr is selected according to some prior knowledge. For example, if a time series is recorded by hours, we can set w pr = 24. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features Description</head><p>Max level shift Max trimmed mean between two consecutive windows.</p><p>Max var shift Max variance shift between two consecutive windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max KL shift</head><p>Max shift in Kullback-Leibler divergence between two consecutive windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lumpiness</head><p>Changing variance in remainder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flatspots</head><p>Discretize time series values into ten equal-sized intervals. Find maximum run length within the same bucket. <ref type="bibr" target="#b17">[18]</ref>.</p><p>Diff-w</p><p>The differences between the current value and the w-th previous value.</p><p>In summary, with original time series value, we extract total 37 features used to capture the characteristics of time series data. It is also worth mentioning that all those features are normalized in order to make them comparable among different time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">The Transfer between Source Domain and Target Domain</head><p>In order to transfer knowledge between the source and target domains, it is necessary to narrow the difference between the two domains. Considering the effectiveness and efficiency requirements in the anomaly detection task, we propose a transfer method combining the instance-based transfer learning and feature-based transfer learning.</p><p>In transfer learning, we should guarantee that the source domain and the target domain come from similar fields (such as similar monitoring data) or own similar characteristics (such as trend or period). However, the source domain may consist of various time series data. Thus, the first step of transfer learning is to collect the time series data from the source domain that are similar to the data in the target domain. In ATAD, we use instance-based method to filter out those source-domain samples which are not similar to the counterpart in the target domain.</p><p>The idea of instance-based transfer learning is to select the source-domain samples which are similar to samples in the target domain so that the difference between two domains can be reduced. For source domain, after identifying features and converting</p><formula xml:id="formula_2">T l into F l , we perform K-means [29] algorithm on F l to build K clusters. Each cluster F i l i ∈ [1, K]</formula><p>is a subset of F l without overlap, which can be regarded as a sub source domain. To select the similar samples, firstly, the same feature extraction process is applied to the unlabelled time series data T u to form feature dataset F u . Then we calculate the Euclidean distance between each unlabelled sample and the central point of each cluster. Each target-domain sample will be assigned to the nearest sub source domain F i l . We denote the testing samples which are assigned into the same cluster as F i u i ∈ <ref type="bibr">[1, K]</ref>. After the instance-based transfer, we need to further narrow the difference between target domains and corresponding sub source domains from the perspective of feature space, because the distribution of features may still remain different. In ATAD, we conduct CORrelation ALignment (CORAL) <ref type="bibr" target="#b36">[37]</ref> on each cluster, which is the idea of the feature-based transfer learning process. CORAL is a domain adaption algorithm, which can align the second-order statistics, namely, the co-variance of the source and target features in an unsupervised manner. Specifically, CORAL aims to minimize the Frobenius norm between co-variance matrices of the target and source domains, as shown in Eq. 2.</p><formula xml:id="formula_3">min A A T C i l A −C i u 2 F (2)</formula><p>where A is a linear transformation matrix, C i l is the covariance matrix of labelled data in cluster i, and C i u is the co-variance matrix of unlabelled data in cluster i. We can get the optimal solution of A by whitening source data and recoloring with target co-variance method, i.e. CORAL. More details can be referred in <ref type="bibr" target="#b36">[37]</ref>. Finally, we can get the new sub source domain features datâ F i l after transformation. In the last step, we train a base supervised model, like Random Forest (RF) or Support Vector Machine (SVM), on each sub source domainˆFdomainˆ domainˆF i l . In the end, we can get K independent base models.</p><p>Some notes about the proposed transfer learning component:</p><p>• Base model: we use Random Forest as the supervised machine learning model (i.e. the base model) in our implementation. Random Forest can be implemented in a parallel way thus it owns high efficiency.</p><p>• Computational framework: we emphasize that this component can be regarded as a computational framework. In fact, the choices of distance measurement, clustering method, and base model are flexible.</p><p>• Assignment complexity analysis: when assigning unlabelled samples, we need to calculate the distance between each sample in the unlabelled set and the center points of all clusters (sub source domains). This time complexity is O (m · K), where K is the number of clusters and m is the size of F u . K is generally much less than m and can be regarded as a constant, so</p><formula xml:id="formula_4">O (m · K) ≈ O (m)</formula><p>. Therefore, our assignment process has linear complexity.</p><p>• Parallel processing: it is worth noting that the sub source domains are completely independent to each other, which means that the follow-up processes for each sub source domain could be conducted in a parallel manner. This can help improve the efficiency of anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Active Learning Component</head><p>Due to the high complexity of the cloud service systems, the time series generated from different components are characterized by great diversity. Thus, transfer learning technique is not enough to achieve satisfactory results on various time series in cloud. In ATAD, leveraging active learning method, the diversified data with specific characteristics can be addressed with a small amount of labelling effort. Active learning focuses on minimizing the labeling effort of users and improving the accuracy of the prediction model. In this work, we utilize an active learning method that considers Uncertainty and Context Diversity of samples during sampling. We call it UCD for short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Uncertainty</head><p>Most active learning methods use uncertainty as the principle to select samples for labelling <ref type="bibr" target="#b32">[33]</ref> because it is believed that if a model is less certain about the classification results of some samples, labelling such samples would be more helpful to the base model. In our approach, we use the base model (Random Forest) to estimate the probability of an unlabelled data to be normal or anomalous. We then use the following formula to calculate the uncertainty for unlabelled samples:</p><formula xml:id="formula_5">Uncertainty = − |Prob (Normal) − Prob (Anomaly)| (3)</formula><p>where Prob represents the probability given by the base model.</p><p>We calculate the uncertainty according to Eq. 3 and sort them in descending order. The larger the uncertainty, the more it needs to be labelled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Context Diversity</head><p>To recommend samples to be labelled, diversity is also an important factor to be considered. Sometimes, two samples are very similar or may belong to the same anomaly pattern. It is unnecessary to label both of them.</p><p>Traditional diversity methods are generally based on clustering <ref type="bibr" target="#b6">[7]</ref>, which do not consider the context of samples in time series scenario. In cloud systems, time series of system metrics, such as CPU utilization or traffic load, are continuous without drastic breakpoints. Thus, samples that are adjacent in time series tend to be similar. Our active learning algorithm makes full use of this property in time series.</p><p>Specifically, we sort all samples by uncertainty and scan them sequentially. If a new sample we scanned is in the context of another sample in the candidate set, i.e. these two samples are adjacent to each other, we hold a view that the information embedded in them could also be similar. We thus ignore the new sample because it may contain redundant information. If the new sample does not appear in the context of all samples in the candidate set, it is added to the candidate set. In our work, the context of sample x t is controlled by a parameter α, which represents a range from x t−α to x t+α in a time series. <ref type="figure" target="#fig_4">Fig. 3</ref> illustrates the concept of context diversity in a time series. More details can be found in Algorithm 1. For each source domain, we perform active learning on its own testing data F i u . We recommend d diverse and uncertain samples to be labelled, and add the labelled samples into the training set to retrain the base model. After repeating T rounds of this process, we obtain the final detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Active Learning Component</head><p>Input: labelled feature data from source domain F l ; unlabelled feature data from target domain F u ; base model M base obtained from F l ; the number of samples at each round d; context parameter α; the number of rounds T ; Output:</p><formula xml:id="formula_6">Final model M f inal ; 1 M = M base 2 for i = 1 to T do 3 Candidate Set, S = ∅ 4 Prob (Normal) , Prob (Anomaly) = M (F u ) 5 Uncertainty = − |Prob (Normal) − Prob (Anomaly)| 6</formula><p>Uncertainty Candidate = argsort (Uncertainty) </p><formula xml:id="formula_7">F l = F l ∪ F f ed 21 M = M.train (F l ) 22 end 23 return M as M f inal</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Usage of ATAD</head><p>Transfer learning and active learning are only conducted in the training process. Once the training of ATAD finished, a classifier will be generated and further applied to anomaly detection task in practice. The detection process of ATAD is as follows: Firstly, we feed the time series data to be detected into the feature extraction component and extract features as the training process does. After that, the features are input to the trained classifier to get the anomaly probabilities. Finally, the points whose probabilities are higher than a pre-specified threshold are predicted as anomalies. This prespecified threshold can be treated as the sensitivity parameter for adapting to different requirements of various users and scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the effectiveness of our approach ATAD through a series of experiments. We aim to answer the following research questions in evaluation:</p><p>RQ1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Setup</head><p>We use two public time series anomaly detection datasets, NAB <ref type="bibr" target="#b22">[23]</ref> and Yahoo <ref type="bibr" target="#b21">[22]</ref>, to evaluate our proposed method. NAB is a novel benchmark for evaluating anomaly detection algorithms in streaming, real-time applications. It contains datasets collected from different fields, including AWS, Twitter, and Artificial, etc. Each dataset contains several time series of variable length. The AWS dataset contains different server metrics, such as CPU utilization, network traffic, disk write bytes, etc, collected by the Amazon CloudWatch service. The Artificial dataset contains artificially generated time series data with various types of anomalies, while the anomaly patterns are much simpler. The Twitter dataset is the collection of Twitter mentions of large publicly-traded companies such as Google and IBM. The Yahoo dataset consists of metrics of various Yahoo services, which reflects the status of Yahoo system. All datasets are given in time series form and every data point is manually labelled. These time series range in length from hundreds to thousands. The proportion of anomaly is about 1% ∼ 5%.</p><p>In the experiments, we use Yahoo, AWS, Artificial and Twitter datasets as the testing set (target domain). There are two reasons for this choice. First, these datasets are related to cloud monitoring data. Second, the scale of these datasets are relatively large, or the anomalous points are also much more than other datasets. More details about datasets are shown in <ref type="table" target="#tab_4">Table 4</ref>. The first column is the average length of time series. The second and third columns are the total number of data points and the number of anomalies, respectively. We also show the percentage of anomaly data points in the last column.</p><p>We perform cross-dataset anomaly detection according to our setup. The experiments are conducted on four pairs of datasets, including non-Yahoo→Yahoo, non-AWS→AWS, non-Twitter→Twitter, and non-Artificial→Artificial. The right side of the arrow represents the unlabelled testing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metric</head><p>We evaluate the accuracy of anomaly detection methods using F1-Score, which is defined as follows:</p><formula xml:id="formula_8">F1 = 2 · P · R P + R , P = TP TP + FP , R = TP TP + FN<label>(4)</label></formula><p>where P and R denote the precision and recall, respectively. In addition, T P, FP, FN, and T N are referred to as true positive, false positive, false negative, and true negative, respectively. We might fail to detect potential anomalies if only focus on the precision. On the other hand, a couple of false positives might be received when we solely pay attention to the recall. F1-Score builds up the balance of the precision and recall, is therefore used as the main evaluation metric in our experiments.</p><p>Under an acceptable recall, we expect the anomaly detector to achieve as high precision as possible. The more precise the detector is, the less amount of false alarms will be reported, and thus less human effort is required to investigate. Therefore, precision can be regarded as an important metric, which reflects the automation degree of anomaly detection systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">RQ1: How effective is ATAD?</head><p>In this section, we evaluate the effectiveness of our proposed approach, ATAD. First, we compare ATAD with some commonly-used anomaly detection algorithms to examine the superior performance of the proposed approach. Second, we present the advantages of ATAD in saving labelling cost, as a comparison with supervised learning based anomaly detectors.</p><p>The comparative anomaly detection algorithms are developed based on Isolation Forest (iForest) <ref type="bibr" target="#b25">[26]</ref>, K-Sigma <ref type="bibr" target="#b13">[14]</ref>, Seasonal Hybrid ESD (S-H-ESD) <ref type="bibr" target="#b15">[16]</ref> and Random Forest (RF). The iForest model ensembles random split tree models to identify which points are isolated. K-Sigma is a common statistics-based method, in which the samples are taken as anomalies whose values deviate more than k times of the variance of samples from the corresponding mean. S-H-ESD builds upon the Generalized ESD test <ref type="bibr" target="#b31">[32]</ref>, and is able to detect both global and local anomalies. This algorithm is incorporated in the well-known AnomalyDetection R package <ref type="bibr" target="#b38">[39]</ref> and thus is widely used. Because the RF is used in ATAD as the based learning classifier, we exploit a classical RF based supervised model as a comparison to demonstrate the superior performance of ATAD. The RF and iForest models use the same features as the ATAD. The K-Sigma and S-H-ESD are performed on the raw time series.</p><p>In the experiments, we evaluate the metrics under different settings and present the best results in terms of F1-Score in the following. Towards this end, the proportion of anomalies in iForest is set to 0.01, 0.05, 0.10 and 0.20, respectively, and the integer k varies from 1 to 3 in K-Sigma. In addition, the maximum proportion of anomalies in S-H-ESD is set to 0.01 and 0.05, respectively. We labelled the same proportion of samples for RF as the counterpart in ATAD used in the target domain. In ATAD, the K in the Transfer Learning component changes from 3 to 5, whereas the number of rounds T and the labelling ratio are fixed to 3 and 1%, respectively, in the Active Learning component. The probability threshold is set to 0.6 ∼ 0.8.</p><p>The results are shown in <ref type="table" target="#tab_5">Table 5</ref>. It is clear that the ATAD can achieve much higher F1-Scores than other approaches on all datasets. Particularly, though the supervised learning method (RF) achieves better performance than those unsupervised methods, the ATAD outperforms the RF when given the same number of labels.</p><p>In order to demonstrate the advantages of ATAD in saving labelling efforts, we compare the number of labelled samples of ATAD and RF under the similar F1-Scores. The relevant results are presented in <ref type="table" target="#tab_6">Table 6</ref>, whose first and third column depicted the F1-Scores of the supervised model (RF) and the ATAD, respectively. Their corresponding quantities of labelled data are included in the second and fourth column of <ref type="table" target="#tab_6">Table 6</ref>. It is evident that the supervised model generally takes 3∼10 times more labels than the ATAD to achieve comparable results. The superior performance benefits the transferred knowledge from the source domain in the Transfer Learning component. As a consequence, a small number of labels is required in the target domain. Moreover, the UCD method helps to further reduce the number of labels because the performance can be improved rapidly and significantly by the extraordinary informative recommendations in the Active Learning component.  We evaluate the effectiveness of our Transfer Learning Component from the following two aspects:</p><p>• The effectiveness of identified features, including forecasting error, statistical, and temporal features.</p><p>• The effectiveness of our proposed transfer method.</p><p>Effectiveness of the identified features Our proposed transfer learning is based on many time series features including forecasting error, statistical, and temporal features. The conventional transfer learning is only based on statistical features such as averages and variances <ref type="bibr" target="#b29">[30]</ref>. The statistical features are simple descriptive values that are independent of the context of time series. In order to evaluate the validity of features used in ATAD, we perform an experiment on four dataset pairs. We compare ATAD using statistical features alone and ATAD using order-aware features (forecasting error features and temporal features).</p><p>Experimental results are shown in <ref type="table" target="#tab_7">Table 7</ref>. It can be seen that using statistical features alone for ATAD leads to poor results, and when order-aware features are added, the performance becomes better. The reason is that the transfer learning needs to narrow the differences between the source domain and the target domain. If only statistical features are extracted and the characteristics of time series are ignored, the features do not reflect the context property in time series. Thus the resulting performance is less satisfactory. Random Forest also provides a popular approach to feature ranking. Here we use the Random Forest classifier to evaluate the importance of the features used in ATAD. The importance of a feature can be ranked according to the mean decrease impurity <ref type="bibr" target="#b2">[3]</ref>. In <ref type="table" target="#tab_8">Table 8</ref>, we show the top-10 most important features of the RF models in ATAD. The definitions of these features can be found in <ref type="table" target="#tab_0">Table 1, Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>. We can see that the forecasting error features and the original time series are much more informative for anomaly detection. In addition, some temporal features, such as Diff-w and Flatspots, also play an important role in the model. It is also worth noting that the important features of each dataset are different. It implies that we should consider all the features comprehensively when conducting transfer learning between different datasets.</p><p>Effectiveness of the proposed transfer method In our transfer learning, we create multiple independent sub source domains through clustering and conduct the CORAL algorithm to transform the features of sub source domains. We expect the transfer learning can reduce the labelling effort for users, because we can activate the Active Learning component directly based on the transfer model, without any manual labels in the testing set. We validate the effectiveness of our Transfer Learning component by comparing with Specifically, in the naive method, we pre-fetch a part of samples in the testing dataset to train a base model. After that, we use this base model to conduct the active learning process. This naive active learning approach needs no auxiliary public labelled data as source domain and transfer learning technique, but it needs more labelling effort for building the base model, as illustrated in <ref type="table">Table.</ref> 9. In <ref type="table" target="#tab_9">Table 9</ref>, we fix the labelling ratio of the active learning process to 1% in both Naive method and ATAD. For fairness, in the naive method, we pre-fetch samples from the testing set in stratified style to train the base model. From the table, it can be seen that the number of samples required for the naive method without transfer learning component is, in average, 1.56 times than that of ATAD, but its results are still slightly lower than those achieved by ATAD. We also illustrate the number of labels required by the supervised model, naive active learning model without transfer methods, and ATAD in   To evaluate the effectiveness of the Active Learning component, we use total labelled datasets F l to train the base model and do not apply transfer learning on them. We compare the UCD method with the conventional Uncertainty method (U) and the random selecting method (random). In all experiments, we conduct 3 rounds of active learning and select 60 samples for labelling at each round. In order to avoid the data leakage problem <ref type="bibr" target="#b19">[20]</ref>, all samples labelled by the active learning component are removed from the testing set. The experimental results are shown in <ref type="table" target="#tab_0">Table 10</ref>. From <ref type="table" target="#tab_0">Table 10</ref>, it can be seen that the UCD method achieves the best results on all datasets, confirming the usefulness of incorporating time series context diversity. We also find that as the number of rounds increases, the F1-Scores achieved by UCD are also steadily improved. However, the random selection method cannot guarantee such trend. The results confirm the validity of the active learning method. The hyper-parameters in the Active Learning component include the number of samples to be labelled per round d, the number of rounds T , and the context parameter α. Obviously, if T and d are much larger, the entire algorithm will be closer to supervised learning. Therefore the accuracy will be improved gradually.</p><p>We conduct an experiment to explore the impact of different α values on the final results. In <ref type="figure" target="#fig_8">Fig. 5</ref>, we can see that as the α value increases, the F1-Score rises rapidly first and then falls gradually. The larger α means the wider range of context and more attention to Context Diversity because the wider context will cause more uncertain candidate samples to be discarded. The smaller α means the narrower range of context and more attention to Uncertainty. If α = 1, UCD becomes purely U. Therefore, α can be treated as a parameter which trades off between Context Diversity and Uncertainty. Too large or too small α value will result in loss of accuracy. In conclusion, choosing an appropriate α can make full use of both kinds of information. Through our practice on all experimental datasets, we empirically set α = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">RQ4: How effective is ATAD in detecting anomalies in a company's local dataset based on public datasets?</head><p>In RQ1, we examined the performance of ATAD on public datasets. In this RQ, we evaluate its effectiveness by using practical industrial data from the large-scale cloud system in Microsoft. More specifically, we hourly record IOPS (I/O Operations Per Seconds) of the storage service in a cluster and collect the corresponding time series data in the past few months. The data is labelled by domain experts in the operation team. All public datasets are utilized as the source domain, and then the ATAD is applied to the target domain dataset, i.e., the IOPS data, for anomaly detection. The experiment is conducted as the similar process in RQ1. The results are shown in <ref type="table" target="#tab_0">Table 11</ref>. It is noted that the traditional unsupervised methods cannot work well due to the low recalls and the resulting F1-Scores. The extremely low recalls exhibit a high probability in failing to detect potential anomalies, which might cause a huge customer and financial loss. RF can achieve a higher recall but lower precision, which means lots of false alarms. In contrast, ATAD can achieve a higher F1-Score, which is beneficial to improving the service availability and detection automation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Threats to Validity</head><p>• Data quality: in this work, we use public datasets in evaluation. The labels about anomalies are provided with the datasets. Although these dataset are of high quality and are used by several other studies <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>, it is possible that they contain a small degree of noise. Furthermore, their data volume is also limited. We will experiment with larger scale datasets in our future work.</p><p>• Correctness of labelling: the Active Learning component in ATAD requires users to manually label a few percentages of data and assumes that the labels are correct. However, in reality, the quality of labeling may vary (i.e., users may incorrectly label a data point).</p><p>• Data leakage: in order to avoid the data leakage problem, we remove the samples labelled in the active learning process from the testing set. Since active learning may recommend different samples to be labelled at different rounds, the resulting testing set could be slightly different in each experiment, which may cause bias in comparisons. However, the proportion of samples to be labelled is very small, so we can ignore this influence on the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>In recent years, there have been many studies on anomaly detection problem for time series data. The proposed methods can be largely divided into supervised methods, semisupervised methods, unsupervised methods and statisticalbased methods <ref type="bibr" target="#b3">[4]</ref>. Unsupervised methods do not require manual labelling. These methods assume that normal instances are far more frequent than anomalies and anomalies deviate from the normal data distribution. For example, Ahmad, et al. proposed an unsupervised online sequence memory algorithm called Hierarchical Temporal Memory to detect anomaly in streaming data <ref type="bibr" target="#b1">[2]</ref>. Xu, et al. proposed Donut, an unsupervised anomaly detection algorithm based on Variational Auto Encoder (VAE) <ref type="bibr" target="#b40">[41]</ref>.</p><p>Supervised methods aim to build a classification model for normal and anomaly classes. General supervised machine learning models can be applied to this problem. Steinwart, et al. interpreted anomaly detection problem as a binary classification task and proposed a supervised framework <ref type="bibr" target="#b35">[36]</ref>. The most famous is the anomaly detection system of Yahoo, called EGADS <ref type="bibr" target="#b21">[22]</ref>, which used a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time series. Opperentice <ref type="bibr" target="#b24">[25]</ref> used operators' periodical labels on anomalies to train a random forest classifier and automatically select parameters and thresholds.</p><p>Semi-supervised methods assume that the training data has labelled instances for only the normal class. Malhotra, et al. used stacked LSTM networks trained on non-anomalous data as a predictor to detect anomaly <ref type="bibr" target="#b27">[28]</ref>. Erfani, et al. used a combination of one-class SVM model and deep learning model to detect anomaly <ref type="bibr" target="#b9">[10]</ref>. Daneshpazhouh, et al. presented an entropy-based method that consists of two phases, including reliable negative examples extraction and entropybased outlier detection <ref type="bibr" target="#b5">[6]</ref>.</p><p>Statistical-based methods are built up based on the statistical theory. The most famous method is K-Sigma method <ref type="bibr" target="#b13">[14]</ref>, in which the samples are taken as anomalies whose values deviate more than k times of the variance of samples from the corresponding average. Recently, more advanced methods using Extreme Value Theory <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref> were proposed. Compared with K-Sigma method, these methods do not require prior assumption on the data distribution.</p><p>From the view of effectiveness, supervised and semisupervised methods generally perform better than unsupervised methods and statistical-based methods. However, due to the high labelling cost, they are difficult to be applied in the real world when the scale of dataset is very large. More recently, transfer learning and active learning techniques are applied to deal with this important problem. Transfer learning has been widely applied in many fields like classification, regression, and forecasting <ref type="bibr" target="#b29">[30]</ref>. For example, Spiegel, et al. embeds a given set of labelled data into dissimilarity space, leading to enriched feature representations that facilitate statistical learning procedures <ref type="bibr" target="#b34">[35]</ref>. Vercruyssen, et al. transferred labelled examples from a source domain to a target domain where no labels are available and constructed a nearest-neighbor classifier in the target domain with DTW measure <ref type="bibr" target="#b39">[40]</ref>. Another technique to reduce the labelling effort is active learning. For example, Abe, et al. used a selective sampling mechanism based on active learning to the reduced classification problem of outlier detection <ref type="bibr" target="#b0">[1]</ref>. Pelleg, et al. proposed a novel active learning method to identify rare category records in an unlabelled noisy set with a small budget of data points that they are prepared to categorize <ref type="bibr" target="#b30">[31]</ref>.</p><p>In our work, we combine transfer learning and active learning methods so that we could achieve a balance between labelling effort and performance. On the one hand, unlike unsupervised models, ATAD could achieve a high F1-Score. On the other hand, we only need to label a few number of samples from the unlabelled dataset. These advantages are not possessed by the existing methods mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a novel anomaly detection method ATAD for cloud service systems. ATAD combines transfer learning and active learning techniques. In transfer learning, we use an existing labelled dataset as the source dataset. We extract multiple features, construct source domains, and use the labelled data in each source domain to train base models for a target, unlabelled dataset. In active learning, we use the UCD method to recommend informative samples in the target dataset to label and retrain the base models. Our experiments on cross-dataset anomaly detection show that we can achieve satisfactory detection accuracy by labeling only a small number of samples in the target dataset. We have also evaluated the effectiveness of ATAD using real-world data collected from a production cloud system in Microsoft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head><p>We thank Professor Mickey Gabel (University of Toronto) for the valuable and constructive suggestions on this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall workflow of ATAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Transfer Learning Component</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Context in time series</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>S</head><label></label><figDesc>= S ∪Uncertainty Candidate[ j] 10 continue 11 end 12 if S.size() &gt; d then 13 break 14 end 15 if Uncertainty Candidate[ j] / ∈ [x t−α , x t+α ] , ∀x t ∈ S then 16 S = S ∪Uncertainty Candidate[ j] 17 end 18 end 19 label the S as F f ed 20</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 4 .</head><label>4</label><figDesc>It is clear that active learning can signifi- cantly save labelling effort and ATAD can further reduce the labelling cost by introducing the transfer learning compo- nent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The number of labels required by Supervised Model, Naive Active Learning without transfer learning and ATAD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Experimental results with different α</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Statistical Features 
Feature 
Description 

Mean 
Mean. 
Var 
Variance. 
Crossingpoint[18] The number of crossing points. 
ACF1 
First order of autocorrelation. 
ACFremainder 
Autocorrelation of remainder. 
Trend 
Strength of trend. 

Linearity [18] 
Strength of linearity computed on 
trend of STL [5] decomposition. 

Curvature [18] 
Strength of curvature computed on 
trend of STL [5] decomposition. 
Entropy [18] 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 : Metrics used as forecasting error features</head><label>2</label><figDesc></figDesc><table>Features 
Formula 
Description 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 : Temporal Features</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>: How effective is the proposed ATAD approach? RQ2: How effective is the Transfer Learning component? RQ3: How effective is the Active Learning component? RQ4: How effective is ATAD in detecting anomalies in a company's local dataset based on public datasets?</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Summary of datasets</head><label>4</label><figDesc></figDesc><table>Dataset 
time series 
mean length 

#data 
points 

#anomaly 
points 
%anomaly 

Yahoo 
1415 
92016 
1617 
1.76% 
AWS 
3985 
67740 
3097 
4.57% 
Artificial 
4032 
16128 
624 
3.87% 
Twitter 
15862 
142765 
217 
0.15% 

dataset, i.e. target domain and the left side of the arrow repre-
sents the labelled dataset from other fields. The labels of the 
target domain are not used during the training and transfer 
learning process. They are only used during active learning 
and evaluation. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Results of Comparative Methods</head><label>5</label><figDesc></figDesc><table>Dataset 
Method Precision 
Recall 
F1-Score 

Non-Yahoo 
→ Yahoo 

iForest 
0.3832 
0.2183 
0.2781 
K-Sigma 0.6499 
0.3364 
0.4433 
S-H-ESD 0.2779 
0.6215 
0.3840 
RF 
0.8668 
0.2075 
0.3348 
ATAD 
0.8847 
0.4040 
0.5547 

Non-AWS 
→ AWS 

iForest 
0.1523 
0.0491 
0.0743 
K-Sigma 0.6899 
0.1992 
0.3091 
S-H-ESD 0.5382 
0.7100 
0.6123 
RF 
0.9999 
0.6226 
0.7674 
ATAD 
0.9195 
0.8142 
0.8637 

Non-Artificial 
→ Artificial 

iForest 
0.3477 
0.9006 
0.5017 
K-Sigma 1.000 
0.1730 
0.2950 
S-H-ESD 0.7888 
0.4568 
0.5785 
RF 
0.9182 
0.9301 
0.9241 
ATAD 
0.9990 
0.9850 
0.9924 

Non-Twitter 
→ Twitter 

iForest 
0.4685 
0.3087 
0.3722 
K-Sigma 0.2608 
1.0000 
0.2608 
S-H-ESD 0.7481 
0.4654 
0.5739 
RF 
0.7285 
0.4811 
0.5795 
ATAD 
0.8769 
0.6951 
0.7755 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Supervised Model (Random Forest) vs. ATAD 

RF 
#labels ATAD #labels 

Non-Yahoo 
→ Yahoo 
0.5440 
4600 
0.5547 
920 

Non-AWS 
→ AWS 
0.8403 
763 
0.8637 
254 

Non-Artificial 
→ Artificial 
0.9755 
1612 
0.9924 
161 

Non-Twitter 
→ Twitter 
0.7126 17131 0.7755 
1427 

4.3.2 RQ2: How effective is the Transfer Learning 
Component? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>The effectiveness of features (F1-Score) 

Features 
Yahoo 
AWS 
Artifical Twitter 

Statistical 
0.2956 0.7387 0.7441 
0.6937 
Order-aware 0.4200 0.8441 0.7569 
0.6622 
All features 
0.5781 0.8637 0.9924 
0.7755 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 8 : Feature Importance Evaluation</head><label>8</label><figDesc></figDesc><table>Dataset 
Important Features 

Yahoo 
Original data, RMSE, MAE, ME, Mean, 
MPE, Diff-p, Diff-w pr/2 , Diff-2p, MAPE 

AWS 
Original data, RMSE, MAE, ME, Mean, 
ACF1, Diff-2p, Curvature, Flatspots, Diff-p 

Artificial 
Original data, Mean, Diff-p, MPE, Flatspots, 
RMSE, MAE, Diff-2p, Diff-w pr , Lumpiness. 

Twitter 

Var, RMSE, MAE, ARCHtest.p, Mean, 
Flatspots, Original data, Max level shift, 
MPE, Original Data 

naive active learning applied directly on target domain with-
out transfer methods. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="true"><head>Table 9 : Comparative Experiment of ATAD and Naive Ac- tive Learning without Transfer Learning (F1-Score)</head><label>9</label><figDesc></figDesc><table>Naive 
ATAD 
F1-Score #labels F1-Score #labels 

Yahoo 
0.5691 
1380 
0.5697 
920 
AWS 
0.8589 
381 
0.8637 
254 
Artificial 
0.9815 
322 
0.9924 
161 
Twitter 
0.6164 
2855 
0.7755 
1427 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table>The experimental result with active learning alone 
(F1-Score) 

Dataset Method 
Base 
Round1 Round2 Round3 

Yahoo 

U 
0.2090 0.2437 
0.2839 
0.2695 
UCD 
0.2090 0.2383 
0.3032 
0.3814 
random 0.2090 0.2275 
0.2256 
0.2196 

AWS 

U 
0.0443 0.1932 
0.6838 
0.6799 
UCD 
0.0443 0.7550 
0.8092 
0.8879 
random 0.0443 0.3227 
0.5164 
0.6604 

Artificial 

U 
0.6239 0.7272 
0.8737 
0.9006 
UCD 
0.6239 0.7340 
0.8780 
0.9715 
random 0.6239 0.6446 
0.6275 
0.5000 

Twitter 

U 
0.1647 0.2916 
0.2959 
0.3298 
UCD 
0.1999 0.3070 
0.3703 
0.4232 
random 0.1647 0.1798 
0.1944 
0.1689 

4.3.3 RQ3: How effective is the Active Learning com-
ponent? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="true"><head>Table 11 : Experimental result on IOPS dataset of Microsoft</head><label>11</label><figDesc></figDesc><table>Precision 
Recall 
F1-Score 

iForest 
0.2886 
0.3988 
0.3349 
K-Sigma 
0.8170 
0.1882 
0.3059 
S-H-ESD 
0.9117 
0.1741 
0.2924 
RF 
0.5213 
0.6724 
0.5873 
ATAD 
0.8082 
0.6188 
0.7009 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Outlier detection by active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zadrozny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Langford</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="504" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised real-time anomaly detection for streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Purdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">262</biblScope>
			<biblScope unit="page" from="134" to="147" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breiman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandola</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stl: A seasonal-trend decomposition procedure based on loess</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleveland</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleveland</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ter-Penning</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Official Statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Entropy-based outlier detection using semi-supervised approach with few positive examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daneshpazhouh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="77" to="84" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical sampling for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dasgupta</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="208" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Garch 101: The use of arch/garch models in applied econometrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic perspectives</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="157" to="168" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoregressive conditional heteroscedasticity with estimates of the variance of united kingdom inflation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="987" to="1007" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-dimensional and large-scale anomaly detection using a linear one-class svm with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erfani</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Rajasegarar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karunasek-Era</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leckie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="121" to="134" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent fault detection in large scale services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjørner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Forecastable component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goreg</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G ¨ Ornitz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brefeld</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="235" to="262" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linear and nonlinear models: Fixed effects, random effects, and mixed models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grafarend</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Walter De Gruyter</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Polynomial Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiberger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neuwirth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="269" to="284" />
			<pubPlace>New York, New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automatic anomaly detection in the cloud via statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hochenbaum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejari-Wal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07706</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hochreiter</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmidhuber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale unusual time series detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laptev</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1616" to="1619" />
		</imprint>
	</monogr>
	<note>Data Mining Workshop</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time series forecasting using holtwinters exponential smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalekar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kanwal Rekhi School of Information Technology</title>
		<imprint>
			<biblScope unit="volume">4329008</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Leakage in data mining: Formulation, detection, and avoidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaufman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Perlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stitelman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistical diagnosis of chronic problems in large distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavulya</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hiltunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narasimhan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Draco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generic and scalable framework for automated time-series anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laptev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Amizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flint</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1939" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluating real-time anomaly detection algorithms-the numenta anomaly benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lavin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="38" to="44" />
		</imprint>
	</monogr>
	<note>IEEE 14th International Conference on</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A lagrange multiplier test for garch models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics Letters</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="265" to="271" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Opprentice: Towards practical and automatic anomaly detection through machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Internet Measurement Conference</title>
		<meeting>the 2015 Internet Measurement Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Isolation forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining, 2008. ICDM&apos;08. Eighth IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Arma models and the box-jenkins methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makridakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hibon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Long short term memory networks for anomaly detection in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malhotra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agar-Wal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presses universitaires de Louvain</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">89</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrabadi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of electronic imaging</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">49901</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Active learning for anomaly and rare-category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pelleg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moore</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Percentage points for a generalized esd many-outlier procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="165" to="172" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Anomaly detection in streams with extreme value theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siffer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fouque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Termier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Largouet</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1067" to="1075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfer learning for time series classification in dissimilarity spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spiegel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AALTD</title>
		<meeting>AALTD</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">78</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A classification framework for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steinwart</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scovel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="211" to="232" />
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saenko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Big data analytics for network anomaly detection from netflow data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terzi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagiroglu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Science and Engineering</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="592" to="597" />
		</imprint>
	</monogr>
	<note>2017 International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Twitter</forename><surname>Anomalydetection</surname></persName>
		</author>
		<ptr target="https://github.com/twitter/AnomalyDetection" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transfer learning for time series anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vercruyssen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Meert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAL@ ECML PKDD 2017</title>
		<imprint>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly detection via variational autoencoder for seasonal kpis in web applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03903</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data streams anomaly detection algorithm based on self-set threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xuehui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Communication and Information Processing</title>
		<meeting>the 4th International Conference on Communication and Information Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Outlier detection. Encyclopedia of Database Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schubert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
