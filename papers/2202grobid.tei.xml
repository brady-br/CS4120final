<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding the Impact of Cache Locations on Storage Performance and Energy Consumption of Virtualization Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
							<email>ming.zhang@emc.com</email>
							<affiliation key="aff1">
								<orgName type="institution">EMC Corporation</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Commonwealth University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding the Impact of Cache Locations on Storage Performance and Energy Consumption of Virtualization Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>As per-server CPU cores and DRAM capacity continuously increase, application density of virtualization platforms hikes. High application density imposes tremendous pressure on storage systems. Layers of caches are deployed to improve storage performance. Owing to its manageability and transparency advantages, hypervisor-side caching is widely employed. However, hypervisor-side caches locate at the lower layer of VM disk filesys-tems. Thus, the critical path of cache access involves the virtual I/O sub-path, which is expensive in operation latency and CPU cycles. The virtual I/O sub-path caps the throughput (IOPS) of the hypervisor-side cache and incurs additional energy consumption. It&apos;s viable to directly allocate spare cache resources such as DRAM of a host machine to a VM for building a VM-side cache so as to obviate the I/O virtualization overheads. In this work, we quantitatively compare the performance and energy efficiency of VM-side and hypervisor-side caches based on DRAM, SATA SSD, and PCIe SSD devices. Insights of this work can direct designs of high-performance and energy-efficient virtualization systems in the future.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Virtualization techniques are the backbone of production clouds. As CPU cores and memory capacities increase, virtual machine density of production clouds increases, disk storage becomes a salient performance bottleneck. DRAM or SSD-based caches <ref type="bibr">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> have been deployed in virtualization storage stacks for I/O acceleration of guest virtual machines. Currently, caches are commonly implemented in hypervisors due to several reasons. First, hypervisor-side caches are transparent to VMs, thus, can be naturally ported across many guest operating systems. Second, a hypervisor has complete control over system resources, therefore, can make the most informed cache management decisions such as cache capacity allocation and page replacement <ref type="bibr" target="#b4">[5]</ref>. Third, comparing with VMside caches, hypervisor-side caches can be shared, resulting in increased resource utilization <ref type="bibr" target="#b1">[2]</ref>.</p><p>Despite the management flexibilities, hypervisor-side cache access involves the costly I/O virtualization layers, which cause intensive activities of the userspace process such as qemu, and incur considerable interrupt deliveries <ref type="bibr" target="#b5">[6]</ref>. Study on virtio <ref type="bibr" target="#b6">[7]</ref> with network transactions shows that a busy virtualized web-server may consume 40% more energy, due to 5x more CPU cycles to deliver a packet, than its non-virtualized counterparts <ref type="bibr" target="#b7">[8]</ref>. Similarly, our tests on storage transactions show that I/O virtualization caps hypervisor-side cache performance and increases per I/O energy consumption. In contrast, VMside caches may obviate the I/O virtualization overheads. More specifically, for 4KB read requests at the IOPS of 5k, hypervisor-side DRAM caches consume about 3x the power and delivers 30x the per I/O latency of VM-side DRAM caches. For a server hosting four I/O intensive VMs, the hypervisor-side caching consumes 48 watts while the VM-side caching consumes only 13 watts. The idle power of the server is 121 watts. In other words, comparing with hypervisor-side caching, VMside caching can save about 25% of the entire server's active power. Unfortunately, benefits of VM-side caching have long been ignored. To rouse awareness of its benefits, as a first step, we conduct empirical comparisons between hypervisor-side (host-side) and VM-side (guestside) caches. We believe answering the following questions is helpful for future cache designs:</p><p>• How much are the performance and energy efficiency penalties for hypervisor-side caching? • What are the root causes of the penalties of hypervisorside caching? • Why do these penalties need to be mitigated?</p><p>• What are the potential approaches to reduce these penalties?</p><p>In this paper, we present empirical studies of these problems on a KVM <ref type="bibr" target="#b8">[9]</ref>  caches yield enticing performance and energy efficiency gains over their hypervisor-side counterparts. For a SATA SSD, either attached to a VM to build a VM-side cache, or used to build a hypervisor-side cache, virtio is involved in the device access path. As a result, VMside SATA SSD caches don't provide benefits over their hypervisor-side counterparts. However, PCI passthrough enables a VM bypassing the virtual I/O path and directly accessing a PCIe SSD in a dedicated way to achieve better performance and energy efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Analysis</head><p>In this section, we first present storage access for VMs, explain the involvement of storage components when cache hits at VM and hypervisor side. Then, we analyze where the virtual I/O overheads lie in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Storage Access for VMs</head><p>Block devices are commonly exposed to VMs via emulation. The host-side entity of a virtualized block device can be a file, an LVM logical volume, a device partition, or a whole device. Since device emulation incurs overheads, dedicated device allocation, also known as device passthrough, is implemented to enable a device being exclusively used by a VM without the involvement of I/O virtualization layers. However, not all devices can be allocated in the way of passthrough. For block devices, currently only the PCI-based devices such as PCIe SSDs can be assigned to VMs via PCI passthrough; SATA devices, however, cannot be allocated to VMs in the way of passthrough. In this paper we assume the persistent storage of VMs is backed by emulated block devices. But when we discuss the implementations of SSD-based guest-side caches, we will compare the performance and energy efficiency of SSDs connected to VMs via PCI passthrough and virtio, respectively.</p><p>For efficient emulation of block devices, paravirtualization is the standard solution. Two of the most widely used para-virtualized device drivers are virtio <ref type="bibr" target="#b6">[7]</ref> and Xen paravirtualization <ref type="bibr" target="#b9">[10]</ref>. The former is widely used in QEMU/KVM based virtualization platforms; the latter is from the Xen project. Xen PV and virtio are architecturally similar, we discuss the virtio based storage stack in details. As it's shown in <ref type="figure">Figure 1</ref>, assume a guest OS issued a read request on some disk file, the activities of the guest OS and the host OS components are as follows:</p><p>1. The read request activates a Virtual Filesystem (VFS) function, passing to it a file descriptor and an offset. 2. If the request doesn't indicate direct I/O, the VFS function determines whether the required data are available in the page cache 1a. If 1a hits, the data are returned from the cache and the request is completed. 3. Assuming the page cache 1a missed, the guest OS kernel must read the data from the block device. If the requested data reside in the flash cache, it's a flash cache hit at 1b. In this case, data are fetched from the flash device and HDD access is obviated. 4. Assuming it's a flash cache miss, the request has to go through the traditional generic block and I/O scheduler layer and then be served by the virtual I/O device driver such as virtio-blk. 5. Upon a read request, the virtio-blk frontend driver composes a request entry and places it into the descriptor table of the virtqueue. Then, the virtio-blk frontend driver will call virtqueue kick, which causes guest I/O exit and triggers a hardware register access called VIRTIO PCI QUEUE NOTIFY. 6. vhost is the host-side virtio component for completing the virtual I/O request. Once vhost is notified by KVM for the guest kick, it fetches the virtio request from the queue and calls QEMU, which works as a regular userspace process, to complete the data transfer. 7. To fetch the data, QEMU issues I/O requests which again traverse the host OS storage stack. Host-side page cache or the optional flash cache are successively checked. Once the data hit at the caches or have been fetched from the HDD, vhost updates the status bit of the virtio request and issues an irqfd interrupt to notify the guest that the request is completed.  the HDD access, thus, obviate the HDD latency. For hypervisor-side caching, virtio and qemu are always in the I/O critical path, thus, I/O virtualization overheads are inevitable. In contrast, for VM-side caching, if it's DRAM-based cache, virtio and qemu are not involved in the I/O path, because the DRAM of a VM is managed by KVM instead of qemu. KVM is much more efficient than qemu userspace emulation, especially with the support of hardware-assisted virtualization. If the cache is PCIe SSD-based, and the cache device is allocated to the VM via PCI passthrough, the I/O virtualization layers are also obviated. PCI passthrough is supported by IOMMU, which enables direct remapping of the guest physical address to host physical address, thus avoids I/O virtualization layers to apply the translations and obviates the I/O operation delay. However, if it's SATA SSD-based, even if the cache is logically VM-side, since the access to the cache device needs the involvement of virtio and qemu, I/O virtualization overheads still exist. As a result, VMside caching is not superior to hypervisor-side caching for SATA SSD devices. To understand the I/O virtualization penalties as well as the performance and energy efficiency characteristics of various cache deployments, we quantitatively compare different cache schemes. Insights from the evaluation can direct future cache designs and optimizations of virtualization systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">I/O Virtualization Overheads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Results</head><p>We choose random read as the I/O pattern of the fio benchmark to minimize the interference caused by potential data prefetching of operating systems. For each cache setting, we report the maximum throughput, latency distributions, and energy consumption of the benchmark. For latency distribution, we focus on the I/O size of 4KB, which is the default page management unit of most Linux operating systems.</p><p>Observation 1 (on DRAM cache): The performance of VM-side caches is very close to the native caches in throughput and per I/O response time; while the hypervisor-side caches have an up to 97% performance penalty.</p><p>Observation 2 (on DRAM cache): For 4KB small I/O requests, the maximum throughput of hypervisor-side caches is only about 3% of the VMside caches; for 1MB large I/O requests, the maximum throughput of hypervisor-side caches is nearly the same as the VM-side caches.</p><p>Observation 3 (on DRAM cache): For same I/O throughput, hypervisor-side caches consume about 3x the power of VM-side caches.</p><p>As it's shown in <ref type="figure" target="#fig_1">Figure 2</ref>  Native SSD denotes fio directly runs on the raw SSD of the host machine; Native flashcache denotes fio directly runs on the host machine and hits the host OS flashcache; VM side denotes fio runs on the VM and hits the guest OS flashcache; Hypervisor side denotes fio runs on the VM, misses the guest OS flashcache but hits the host OS flashcache. <ref type="table">Table 1</ref>: perf system-event statistics during a 30-second cache access period with an IOPS of 5k. We ensure the cache hit at VM-side and hypervisor-side, respectively. fio is running on the VM; perf is running on the host OS to probe the system events caused by the VM process. For a direct comparison, the percentages of system events are normalized to the total number of hypervisorside events. performance with a gap of less than 15%. In contrast, the hypervisor-side cache has a performance penalty of up to 97%. In <ref type="figure" target="#fig_1">Figure 2</ref>(b), the VM-side cache consistently achieves a near-native per I/O response time. In contrast, the hypervisor-side cache has a response time penalty of nearly 65 µs for a single 4KB read request. In <ref type="figure" target="#fig_1">Figure  2</ref>(c), for a same I/O throughput, the hypervisor-side cache consumes up to 3x the power of VM-side cache. For DRAM, VM-side caches perform better and consumes less power than hypervisor-side caches. We believe the main reason is that the VM-side cache hit bypasses the I/O virtualization layer. Memory virtualization is implemented in the KVM kernel module, which is efficient with the support of hardware-assisted virtualization techniques such as Intel VT-x and AMD-V. In contrast, the I/O virtualization, including virtual I/O operations and disk emulation, is mainly managed by QEMU, which is a userspace process. The execution of virtual I/O requires frequent CPU mode switches, such as switches between user and kernel as well as kernel to guest mode, which are expensive in CPU cycles. As it's shown in <ref type="figure">Figure 1</ref>, when cache hits at 1a (VM-side DRAM cache), virtual I/O and disk emulation are bypassed, the disk I/O operation is actually transformed to virtual memory access which is managed by KVM.</p><p>When cache hits at 2a (hypervisor-side DRAM cache), it implies a cache miss at 1a, although the disk access can be avoided, the virtual I/O and disk emulation operations are still involved, thus, longer response time is observed by applications running on the VM, as well as a higher system power consumption.</p><p>To further verify our explanation, we conduct system event statistics during cache access under different caching schemes. fio benchmark is running inside a VM, and perf utility is employed to monitor the system events caused by the VM process. The perf statistic results are shown in <ref type="table">Table 1</ref>. Generally, for a same amount of I/O requests, in hypervisor-side cache scheme, the total number of VM caused system events is 6x of the VM-side cache scheme. Specifically, in VM-side cache scheme, there are few userspace events caused, while, in hypervisor-side cache scheme, there are considerable user space system events such as events caused by qemu process and GLib library, etc.. This statistical analysis explains why hypervisor-side cache access is more costly than VM-side cache access.</p><p>From <ref type="figure" target="#fig_1">Figure 2</ref>(a) we can observe that as the I/O size increases, the throughput gap between VM side and hypervisor side cache schemes narrows down. We believe the reason is that for virtual I/O requests, the communication time between the front-end (VM) and the back-end (Hypervisor) is almost constant, thus for small requests the response time is dominated by the virtual I/O round trip time (RTT) between the VM and the hypervisor. When the request size increases, the real data transfer time dominates and the RTT becomes ignorable, thus, the throughput gap between VM side and hypervisor side cache narrows down.</p><p>Differing from <ref type="figure" target="#fig_1">Figure 2</ref>(a) and <ref type="figure" target="#fig_1">Figure 2</ref>(b) in which the VM side and Native lines are almost overlapping, in <ref type="figure" target="#fig_1">Figure 2</ref>(c) there is an obvious gap between the VM side and Native lines. The reason is that although VM-side memory access has a close-to-native performance, memory virtualization involves intensive activities of KVM module, which consumes extra power compared with native memory access.</p><p>Observation 4 (on SATA SSD cache): The VM-side cache and hypervisor-side cache have similar performance in throughput and per I/O response time; both VM-side and hypervisor-side cache have an up to 60% performance penalty compared with the native SSD device or flashcache.</p><p>Observation 5 (on SATA SSD cache): For 4KB small I/O requests, the maximum throughput of both VM-side and hypervisor-side caches is about 60% of the native SSD device or flashcache; for 1MB large I/O requests, the maximum throughput of VMside and hypervisor-side caches is nearly the same as the native SSD device or flashcache.</p><p>Observation 6 (on SATA SSD cache): For same I/O throughput, VM-side cache or hypervisorside cache consumes up to 4x the power of the native SSD device or flashcache access.</p><p>Specifically, as it's shown in <ref type="figure" target="#fig_3">Figure 3</ref>(a) and <ref type="figure" target="#fig_3">Figure  3(b)</ref>, in a wide range of I/O sizes, the hypervisor-side cache consistently delivers almost the same performance as the VM-side cache in throughput and per I/O response time. This observation differs from the observation made in the DRAM-based cache that VM-side caching has a much higher performance than hypervisor-side caching. We believe the main reason is that when the cache device is a SATA SSD, either the device is allocated to a VM to build a VM direct cache, or used by the hypervisor to build a hypervisor-side cache, a cache hit needs the involvement of virtio and qemu, thus, the applications on the VM observe similar response time. As it's shown in <ref type="figure">Figure 1</ref>, VM-side flash cache is denoted as 1b, and hypervisor-side flash cache is denoted as 2b. Although logically a cache hit at 1b has a shorter access path, since the SATA SSD device cannot be accessed by VMs in the hypervisor passthrough way, the real data transfer still requires the involvement of the virtio driver. Thus, a cache hit at 1b has similar overheads as a cache hit at 2b. Since virtio is involved in either case, in <ref type="figure" target="#fig_3">Figure  3</ref>(a) we observe that either the hypervisor-side cache or the VM-side cache can only achieve about 60% of the native SSD I/O throughput for small requests. In <ref type="figure" target="#fig_3">Figure 3</ref>(c), we observe that for the same throughput both the hypervisor-side cache and the VM-side cache consume about 4x the power of native SSD access. To our surprise, the VM-side cache even consumes a little bit higher power than the hypervisor-side cache. We use flashcache, which is built on top of the Linux kernel's device mapper, as our cache implementation. Since we observe in <ref type="figure" target="#fig_3">Figure 3</ref>  similar power consumption as Native SSD access, we believe the extra power consumed in VM-side caching is not caused by the flashcache implementation, but virtualization components. In the case of VM-side caching, flashcache is built in the guest OS, and frequent guest OS activities cause considerable KVM module events. In contrast, in the case of hypervisor-side caching, flashcache is built in the host OS, cache activities will not stress the KVM module, causing less system events and consuming lower power.</p><p>Observation 7 (on PCIe SSD cache): 1 The SSD allocated to the VM via PCI passthrough delivers near-native performance and energy efficiency. A PCIe SSD is attached to a VM via virtio and PCI passthrough, respectively. We compare the performance and energy consumption of the SSD access in these two cases. As it's shown in <ref type="figure" target="#fig_5">Figure 4</ref>(a) and 4(b), PCI passthrough delivers near-native performance and energy efficiency. In contrast, virtio has obvious penalties in either throughput or system energy efficiency. This is coincident with our previous observations. To our surprise, PCI passthrough performs slightly better and consumes less power than native PCIe access. Since we currently employ a black-box method to analyze our test results, a further explanation to this surprising observation is our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Potential Optimizations</head><p>Based on our tests, we believe the following considerations are practical for the design and implementation of virtualization systems in terms of cache performance and energy efficiency.</p><p>Allocating DRAM resources directly to VMs. Dynamic live VM memory allocation has been supported by Xen and KVM. Since VM-side caching delivers higher performance and energy efficiency than hypervisor-side caching, instead of using spare memory resource to build hypervisor-side caches <ref type="bibr">[1,</ref><ref type="bibr" target="#b1">2]</ref>, memory resources can be allocated directly to VMs with high storage pressure. But for public clouds, dynamic live memory allocation requires the changes of pricing policies which need further investigations. Although hypervisor-side caches can build a distributed global cache pools to support VM with large working sets, we believe reclaiming the memory resource of idle VMs and VM migration are practical to meet the big-cache requirement.</p><p>VM-side Block Device Read-ahead. Even in the case that caches are built at the hypervisor side (virtio backend), the virtio frontend bulk prefetching can mitigate the communication overheads between the frontend and backend. Our tests show that for large requests, instead of the communication overheads, data transfer will dominate the response time. Thus, for workloads with intensive small requests, front-end bulk prefetching enables the communication overheads to be amortized. For instance, block device read-ahead of the guest OS can be set to prefetch data upon each virtual I/O request. The result is that even the I/O request size is 4KB, a larger bulk of data such as 128KB can be read ahead into the VMside cache. This will benefit subsequent read requests targeting prefetched pages. A similar method has been employed by network file systems such as NFS, in which the default block size is 1MB, instead of 4KB, so as to reduce the network communication frequency between clients and servers.</p><p>PCI Passthrough. PCI passthrough is a practical way to allocate PCIe SSDs directly to VMs, so as to avoid the overheads of virtual I/O and disk emulation. However, PCI passthrough is limited to devices with PCI interfaces. SATA SSDs currently cannot be allocated in this way. This limits the deployment scope of PCI passthrough. Moreover, the exclusiveness of PCI passthrough limits each device being solely used by a single VM. A single PCIe SSD consumes a relatively high power of up to 20 watts. All these factors cripple the benefits of employing PCI passthrough for cache performance and energy efficiency purposes.</p><p>Reducing Virtual I/O Overheads. Linux kernel community continuously optimizes the virtio drivers. Virtio-blk <ref type="bibr" target="#b11">[12]</ref>, Virtio-blk-data-plane <ref type="bibr" target="#b12">[13]</ref>, and Virtio-blk Multi-queue <ref type="bibr" target="#b13">[14]</ref> have been successively implemented to improve the virtio performance. DID <ref type="bibr" target="#b5">[6]</ref> was proposed to reduce the I/O virtualization caused interrupt delivery overheads. Future congeneric optimizations on I/O virtualization will similarly benefit the performance and energy efficiency of hypervisor-side caches. Finally, container-based operating system level virtualization solutions such as docker can eliminate the overheads of hypervisor-based virtualization, thus, can be considered as an alternative in some cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper evaluates the impact of cache locations on storage performance and energy consumption of QEMU/KVM based virtualization systems. We present the performance and energy consumption results of VMside and hypervisor-side caching using DRAM and SSD devices. Tests show that for DRAM-based caches with 4KB read requests, comparing with VM-side caching, hypervisor-side caching consumes 3x power while only achieves 3% of its throughput. We also demonstrate that I/O virtualization overheads are the culprit of hypervisor-side caching penalties. For SATA SSD-based caches, VM-side caching doesn't have any performance or energy efficiency superiority, because VM-side SSD caching also requires the involvement of virtio. Our tests show that PCI passthrough can bypass virtio, thus, eliminate the I/O virtualization overheads. Finally, we propose possible choices which will be useful for building high-performance and energy-efficient virtualization systems. Using the insights of this paper to optimize practical systems is our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>In the virtual I/O path, there are two operations expen- sive in CPU cycles or request latency. The first is virtual I/O emulation, which requires intensive interactions be- tween the virtio frontend and backend. Emulation causes frequent I/O interrupts and guest I/O exits, which are ex- pensive in CPU cycles, as well as increase I/O latency. The second is the relatively slow HDD-based storage ac- cess, which costs milliseconds and has long been the bot- tleneck of cloud applications. Both VM-side and hypervisor-side cache hits avoidPower consumption (watts) Native VM_side Hypervisor_side (c) Power consumption of 4KB randread at various IOPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: fio benchmark on DRAM-based Caches. Native denotes fio directly runs on the host machine and hits the host OS page cache; VM side denotes fio runs on the VM and hits the guest OS page cache; Hypervisor side denotes fio runs on the VM, misses the guest OS page cache but hits the host OS page cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a), with various I/O sizes, the VM-side cache consistently achieves near-nativePower consumption (watts) Native_SSD Native_flashcache VM_side Hypervisor_side (c) Power consumption of 4KB randread at various IOPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: fio benchmark on SATA SSD-based Caches. Native SSD denotes fio directly runs on the raw SSD of the host machine; Native flashcache denotes fio directly runs on the host machine and hits the host OS flashcache; VM side denotes fio runs on the VM and hits the guest OS flashcache; Hypervisor side denotes fio runs on the VM, misses the guest OS flashcache but hits the host OS flashcache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PCI passthrough vs. virtio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>virtualization platform us- ing DRAM, SATA SSD, and PCIe SSD as cache de- vices. Our evaluation demonstrates that VM-side DRAM</figDesc><table>Mapping Layer 

App 
App 
App 

VFS 

Direct 
I/O 

Network FS 

Page Cache 

Flash Cache 
Network 

Disk 
Filesystem 

Generic Block Layer 

I/O Scheduler Layer 

Virtio-blk 
virtqueue 

Emulated 
Hard Disk 

Mapping Layer 
To 
Device Driver 

VM 
VM 
VM 

VFS 

Direct 
I/O 

Network FS 

Page Cache 

Flash Cache 
Network 

Local Disk 

QEMU 

kvm.ko 

Host Machine 

vhost-
scsi 

Image 
File 

Kick 
Guest 
I/O exit 

ioeventfd 
irqfd 

2b 

Block 
Device File 

2a 
1a 

1b 

Figure 1: System components affected by a VM block 
device operation. DRAM-based page caches are built-in 
modules of operating systems. Flash-based caches are 
optional but widely deployed in virtualization platforms 
to accelerate VM storage. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>benchmark performance such as IOPS and latency dis- tribution. We measure the power of the whole machine, because the cache access in virtualization platform in- volves intensive activities of multiple resources, includ- ing cache devices and CPUs. A Watts Up? Pro ES meter is used to measure the wall power of the machine. Experimental System. Our system is equipped with an AMD Phenom II X4 B95 Quad-core 3.0 GHz processor with AMD-V virtualization support. The host OS is a 64-bit Ubuntu 15.04 with Linux kernel version 3.19.0- 30-generic. QEMU emulator version 2.4.1 and KVM are used as the hypervisor. An official Ubuntu 15.04 64-bit Server Cloud Image is run on the VM as the guest oper- ating system with 2 VCPUs and 2GB memory.</figDesc><table>3 Evaluation of Cache Schemes 

3.1 Evaluation Setup 

Cache Devices. We use 4x2GB DDR2-800Mhz devices 
as the DRAM cache, one Samsung 850 Pro SATA SSD, 
and one 240GB OCZ RevoDrive 3 PCIe SSD as flash 
caches. 
Measurement and Characterization Tools. We use 
fio [11] as the I/O benchmark. fio enables various I/O 
workloads with optional parameters including read/write 
type, sequential/random access, I/O size, IOPS, and 
O DIRECT etc.. We run fio on VMs. Setting the di-
rect parameter enables I/O requests bypassing VM-side 
caches and hitting hypervisor-side caches. fio reports 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>The units of Count and Percent are million and %, respectively.</figDesc><table>Event Source 

Cache hit location 
Hypervisor 
VM 
Count 
Percent 
Count Percent 
kernel 
17545 
60.90 
1569 
5.45 
qemu 
3295 
11.44 
0 
0.00 
kvm 
2714 
9.42 
1478 
5.13 
kvm amd 
1811 
6.29 
1176 
4.08 
libglib 
1220 
4.23 
0 
0.00 
libpthread 
936 
3.25 
0 
0.00 
vdso 
675 
2.35 
0 
0.00 
libc 
609 
2.12 
0 
0.00 
Total 
28809 
100 
4225 
14.66 

</table></figure>

			<note place="foot" n="1"> This group of tests is conducted on another HP ProLiant DL370 G6 server for its IOMMU support.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Available</surname></persName>
		</author>
		<ptr target="http://www.infinio.com/sites/default/files/resources/Infinio-technical-brief-architecture-matters.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mortar: Filling the gaps in data center memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Uppal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEE&apos;14</title>
		<meeting><address><addrLine>Salt Lake City, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mercury: Host-side flash caching for the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Byan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lentini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Condict</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSST&apos;12</title>
		<meeting><address><addrLine>Pacific Grove, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">S-cave: Effective ssd caching to improve virtual machine storage performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT&apos;13</title>
		<meeting><address><addrLine>London, England, Septemebr</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geiger: Monitoring the buffer cache in a virtual machine environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS&apos;06</title>
		<meeting><address><addrLine>San Jose, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comprehensive implementation and evaluation of direct interrupt delivery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cker Chiueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEE&apos;15</title>
		<meeting><address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">virtio: towards a de-facto standard for virtual i/o devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Power consumption of virtual machines with network transactions: Measurement and improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM&apos;14</title>
		<meeting><address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">kvm: the linux virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kivity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kamay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Lublin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liguori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OLS&apos;07</title>
		<meeting><address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP&apos;03</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">) -linux man page. Jens Axboe and Aaron Carroll</title>
		<ptr target="http://linux.die.net/man/1/fio" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Virtio-blk performance improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Kvm virtualized i/o performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Theurer</surname></persName>
		</author>
		<ptr target="http://www.novell.com/docrep/2013/05/kvmvirtualizedioperformance.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Virtio-blk multi-queue conversion and qemu optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<ptr target="http://www.linux-kvm.org/images/6/63/02x06a-VirtioBlk.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
