<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dominant Resource Fairness: Fair Allocation of Multiple Resource Types</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ghodsi</surname></persName>
							<email>alig@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hindman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
							<email>andyk@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
							<email>shenker@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dominant Resource Fairness: Fair Allocation of Multiple Resource Types</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We consider the problem of fair resource allocation in a system containing different resource types, where each user may have different demands for each resource. To address this problem, we propose Dominant Resource Fairness (DRF), a generalization of max-min fairness to multiple resource types. We show that DRF, unlike other possible policies, satisfies several highly desirable properties. First, DRF incentivizes users to share resources , by ensuring that no user is better off if resources are equally partitioned among them. Second, DRF is strategy-proof, as a user cannot increase her allocation by lying about her requirements. Third, DRF is envy-free, as no user would want to trade her allocation with that of another user. Finally, DRF allocations are Pareto efficient, as it is not possible to improve the allocation of a user without decreasing the allocation of another user. We have implemented DRF in the Mesos cluster resource manager, and show that it leads to better throughput and fairness than the slot-based fair sharing schemes in current cluster schedulers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Resource allocation is a key building block of any shared computer system. One of the most popular allocation policies proposed so far has been max-min fairness, which maximizes the minimum allocation received by a user in the system. Assuming each user has enough demand, this policy gives each user an equal share of the resources. Max-min fairness has been generalized to include the concept of weight, where each user receives a share of the resources proportional to its weight.</p><p>The attractiveness of weighted max-min fairness stems from its generality and its ability to provide performance isolation. The weighted max-min fairness model can support a variety of other resource allocation policies, including priority, reservation, and deadline based allocation <ref type="bibr" target="#b30">[31]</ref>. In addition, weighted max-min fairness ensures isolation, in that a user is guaranteed to receive her share irrespective of the demand of the other users.</p><p>Given these features, it should come as no surprise that a large number of algorithms have been proposed to implement (weighted) max-min fairness with various degrees of accuracy, such as round-robin, proportional resource sharing <ref type="bibr" target="#b31">[32]</ref>, and weighted fair queueing <ref type="bibr" target="#b10">[12]</ref>. These algorithms have been applied to a variety of resources, including link bandwidth <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29]</ref>, CPU <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>, memory <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b30">31]</ref>, and storage <ref type="bibr">[5]</ref>.</p><p>Despite the vast amount of work on fair allocation, the focus has so far been primarily on a single resource type. Even in multi-resource environments, where users have heterogeneous resource demands, allocation is typically done using a single resource abstraction. For example, fair schedulers for Hadoop and Dryad <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>, two widely used cluster computing frameworks, allocate resources at the level of fixed-size partitions of the nodes, called slots. This is despite the fact that different jobs in these clusters can have widely different demands for CPU, memory, and I/O resources.</p><p>In this paper, we address the problem of fair allocation of multiple types of resources to users with heterogeneous demands. In particular, we propose Dominant Resource Fairness (DRF), a generalization of max-min fairness for multiple resources. The intuition behind DRF is that in a multi-resource environment, the allocation of a user should be determined by the user's dominant share, which is the maximum share that the user has been allocated of any resource. In a nutshell, DRF seeks to maximize the minimum dominant share across all users. For example, if user A runs CPU-heavy tasks and user B runs memory-heavy tasks, DRF attempts to equalize user A's share of CPUs with user B's share of memory. In the single resource case, DRF reduces to max-min fairness for that resource.</p><p>The strength of DRF lies in the properties it satisfies. These properties are trivially satisfied by max-min fairness for a single resource, but are non-trivial in the case of multiple resources. Four such properties are sharing incentive, strategy-proofness, Pareto efficiency, and envy-freeness. DRF provides incentives for users to share resources by guaranteeing that no user is better off in a system in which resources are statically and equally partitioned among users. Furthermore, DRF is strategyproof, as a user cannot get a better allocation by lying about her resource demands. DRF is Pareto-efficient as it allocates all available resources subject to satisfying the other properties, and without preempting existing allocations. Finally, DRF is envy-free, as no user prefers the allocation of another user. Other solutions violate at least one of the above properties. For example, the preferred <ref type="bibr">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b32">33]</ref> fair division mechanism in microeconomic theory, Competitive Equilibrium from Equal Incomes <ref type="bibr" target="#b29">[30]</ref>, is not strategy-proof.</p><p>We have implemented and evaluated DRF in Mesos <ref type="bibr" target="#b15">[16]</ref>, a resource manager over which multiple cluster computing frameworks, such as Hadoop and MPI, can run. We compare DRF with the slot-based fair sharing scheme used in Hadoop and Dryad and show that slot-based fair sharing can lead to poorer performance, unfairly punishing certain workloads, while providing weaker isolation guarantees.</p><p>While this paper focuses on resource allocation in datacenters, we believe that DRF is generally applicable to other multi-resource environments where users have heterogeneous demands, such as in multi-core machines.</p><p>The rest of this paper is organized as follows. Section 2 motivates the problem of multi-resource fairness. Section 3 lists fairness properties that we will consider in this paper. Section 4 introduces DRF. Section 5 presents alternative notions of fairness, while Section 6 analyzes the properties of DRF and other policies. Section 7 provides experimental results based on traces from a Facebook Hadoop cluster. We survey related work in Section 8 and conclude in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>While previous work on weighted max-min fairness has focused on single resources, the advent of cloud computing and multi-core processors has increased the need for allocation policies for environments with multiple resources and heterogeneous user demands. By multiple resources we mean resources of different types, instead of multiple instances of the same interchangeable resource.</p><p>To motivate the need for multi-resource allocation, we plot the resource usage profiles of tasks in a 2000-node Hadoop cluster at Facebook over one month (October 2010) in <ref type="figure" target="#fig_0">Figure 1</ref>. The placement of a circle in <ref type="figure" target="#fig_0">Figure 1</ref> indicates the memory and CPU resources consumed by tasks. The size of a circle is logarithmic to the number of tasks in the region of the circle. Though the majority of tasks are CPU-heavy, there exist tasks that are memory- heavy as well, especially for reduce operations.</p><p>Existing fair schedulers for clusters, such as Quincy <ref type="bibr" target="#b17">[18]</ref> and the Hadoop Fair Scheduler <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>, ignore the heterogeneity of user demands, and allocate resources at the granularity of slots, where a slot is a fixed fraction of a node. This leads to inefficient allocation as a slot is more often than not a poor match for the task demands. <ref type="figure" target="#fig_1">Figure 2</ref> quantifies the level of fairness and isolation provided by the Hadoop MapReduce fair scheduler <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>. The figure shows the CDFs of the ratio between the task CPU demand and the slot CPU share, and of the ratio between the task memory demand and the slot memory share. We compute the slot memory and CPU shares by simply dividing the total amount of memory and CPUs by the number of slots. A ratio of 1 corresponds to a perfect match between the task demands and slot resources, a ratio below 1 corresponds to tasks underutilizing their slot resources, and a ratio above 1 corresponds to tasks over-utilizing their slot resources, which may lead to thrashing. <ref type="figure" target="#fig_1">Figure 2</ref> shows that most of the tasks either underutilize or overutilize some of their slot resources. Modifying the number of slots per machine will not solve the problem as this may result either in a lower overall utilization or more tasks experiencing poor performance due to over-utilization (see Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Allocation Properties</head><p>We now turn our attention to designing a max-min fair allocation policy for multiple resources and heterogeneous requests. To illustrate the problem, consider a system consisting of 9 CPUs and 18 GB RAM, and two users: user A runs tasks that require 1 CPUs, 4 GB each, and user B runs tasks that require 3 CPUs, 1 GB each. What constitutes a fair allocation policy for this case?  <ref type="bibr">(October 2010)</ref>. A demand to slot ratio of 2.0 represents a task that requires twice as much CPU (or memory) than the slot CPU (or memory) size.</p><p>One possibility would be to allocate each user half of every resource. Another possibility would be to equalize the aggregate (i.e., CPU plus memory) allocations of each user. While it is relatively easy to come up with a variety of possible "fair" allocations, it is unclear how to evaluate and compare these allocations.</p><p>To address this challenge, we start with a set of desirable properties that we believe any resource allocation policy for multiple resources and heterogeneous demands should satisfy. We then let these properties guide the development of a fair allocation policy. We have found the following four properties to be important:</p><p>1. Sharing incentive: Each user should be better off sharing the cluster, than exclusively using her own partition of the cluster. Consider a cluster with identical nodes and n users. Then a user should not be able to allocate more tasks in a cluster partition consisting of 1 n of all resources. 2. Strategy-proofness: Users should not be able to benefit by lying about their resource demands. This provides incentive compatibility, as a user cannot improve her allocation by lying.</p><p>3. Envy-freeness: A user should not prefer the allocation of another user. This property embodies the notion of fairness <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>4. Pareto efficiency: It should not be possible to increase the allocation of a user without decreasing the allocation of at least another user. This property is important as it leads to maximizing system utilization subject to satisfying the other properties.</p><p>We briefly comment on the strategy-proofness and sharing incentive properties, which we believe are of special importance in datacenter environments. Anecdotal evidence from cloud operators that we have talked with indicates that strategy-proofness is important, as it is common for users to attempt to manipulate schedulers. For example, one of Yahoo!'s Hadoop MapReduce datacenters has different numbers of slots for map and reduce tasks. A user discovered that the map slots were contended, and therefore launched all his jobs as long reduce phases, which would manually do the work that MapReduce does in its map phase. Another big search company provided dedicated machines for jobs only if the users could guarantee high utilization. The company soon found that users would sprinkle their code with infinite loops to artificially inflate utilization levels.</p><p>Furthermore, any policy that satisfies the sharing incentive property also provides performance isolation, as it guarantees a minimum allocation to each user (i.e., a user cannot do worse than owning 1 n of the cluster) irrespective of the demands of the other users.</p><p>It can be easily shown that in the case of a single resource, max-min fairness satisfies all the above properties. However, achieving these properties in the case of multiple resources and heterogeneous user demands is not trivial. For example, the preferred fair division mechanism in microeconomic theory, Competitive Equilibrium from Equal Incomes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>, is not strategyproof (see Section 6.1.2).</p><p>In addition to the above properties, we consider four other nice-to-have properties:</p><p>• Single resource fairness: For a single resource, the solution should reduce to max-min fairness.</p><p>• Bottleneck fairness: If there is one resource that is percent-wise demanded most of by every user, then the solution should reduce to max-min fairness for that resource.</p><p>• Population monotonicity: When a user leaves the system and relinquishes her resources, none of the allocations of the remaining users should decrease.</p><p>• Resource monotonicity: If more resources are added to the system, none of the allocations of the existing users should decrease.  CPU, while the dominant resource of a user running an I/O-bound job is bandwidth. 1 DRF simply applies maxmin fairness across users' dominant shares. That is, DRF seeks to maximize the smallest dominant share in the system, then the second-smallest, and so on. We start by illustrating DRF with an example ( §4.1), then present an algorithm for DRF ( §4.2) and a definition of weighted DRF ( §4.3). In Section 5, we present two other allocation policies: asset fairness, a straightforward policy that aims to equalize the aggregate resources allocated to each user, and competitive equilibrium from equal incomes (CEEI), a popular fair allocation policy preferred in the micro-economic domain <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>In this section, we consider a computation model with n users and m resources. Each user runs individual tasks, and each task is characterized by a demand vector, which specifies the amount of resources required by the task, e.g., 1 CPU, 4 GB. In general, tasks (even the ones belonging to the same user) may have different demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">An Example</head><p>Consider a system with of 9 CPUs, 18 GB RAM, and two users, where user A runs tasks with demand vector 1 CPU, 4 GB, and user B runs tasks with demand vector 3 CPUs, 1 GB each.</p><p>In the above scenario, each task from user A consumes 1/9 of the total CPUs and 2/9 of the total memory, so user A's dominant resource is memory. Each task from user B consumes 1/3 of the total CPUs and 1/18 of the total memory, so user B's dominant resource is CPU. DRF will equalize users' dominant shares, giving the allocation in <ref type="figure" target="#fig_2">Figure 3</ref>: three tasks for user A, with a total of 3 CPUs, 12 GB, and two tasks for user B, with a total of 6 CPUs, 2 GB. With this allocation, each user ends up with the same dominant share, i.e., user A gets 2/3 of RAM, while user B gets 2/3 of the CPUs.</p><p>This allocation can be computed mathematically as follows. Let x and y be the number of tasks allocated</p><formula xml:id="formula_0">Algorithm 1 DRF pseudo-code R = r 1 , · · · , r m total resource capacities C = c 1 , · · · , c m consumed resources, initially 0 s i (i = 1..n) user i's dominant shares, initially 0 U i = u i,1 , · · · , u i,m (i = 1..n) resources given to user i, initially 0 pick user i with lowest dominant share s i D i ← demand of user i's next task if C + D i ≤ R then C = C + D i update consumed vector U i = U i + D i update i's allocation vector s i = max m j=1 {u i,j /r j } else return</formula><p>the cluster is full end if by DRF to users A and B, respectively. Then user A receives x CPU, 4x GB, while user B gets 3y CPU, y GB. The total amount of resources allocated to both users is (x + 3y) CPUs and (4x + y) GB. Also, the dominant shares of users A and B are 4x/18 = 2x/9 and 3y/9 = y/3, respectively (their corresponding shares of memory and CPU). The DRF allocation is then given by the solution to the following optimization problem:</p><formula xml:id="formula_1">max (x, y) (Maximize allocations) subject to x + 3y ≤ 9 (CPU constraint) 4x + y ≤ 18 (Memory constraint) 2x 9 = y 3 (Equalize dominant shares)</formula><p>Solving this problem yields 2 x = 3 and y = 2. Thus, user A gets 3 CPU, 12 GB and B gets 6 CPU, 2 GB.</p><p>Note that DRF need not always equalize users' dominant shares. When a user's total demand is met, that user will not need more tasks, so the excess resources will be split among the other users, much like in max-min fairness. In addition, if a resource gets exhausted, users that do not need that resource can still continue receiving higher shares of the other resources. We present an algorithm for DRF allocation in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DRF Scheduling Algorithm</head><p>Algorithm 1 shows pseudo-code for DRF scheduling. The algorithm tracks the total resources allocated to each user as well as the user's dominant share, s i . At each step, DRF picks the user with the lowest dominant share among those with tasks ready to run. If that user's task demand can be satisfied, i.e., there are enough resources  <ref type="table">Table 1</ref>: Example of DRF allocating resources in a system with 9 CPUs and 18 GB RAM to two users running tasks that require 1 CPU, 4 GB and 3 CPUs, 1 GB, respectively. Each row corresponds to DRF making a scheduling decision. A row shows the shares of each user for each resource, the user's dominant share, and the fraction of each resource allocated so far. DRF repeatedly selects the user with the lowest dominant share (indicated in bold) to launch a task, until no more tasks can be allocated. available in the system, one of her tasks is launched. We consider the general case in which a user can have tasks with different demand vectors, and we use variable D i to denote the demand vector of the next task user i wants to launch. For simplicity, the pseudo-code does not capture the event of a task finishing. In this case, the user releases the task's resources and DRF again selects the user with the smallest dominant share to run her task.</p><p>Consider the two-user example in Section 4.1. <ref type="table">Table 1</ref> illustrates the DRF allocation process for this example. DRF first picks B to run a task. As a result, the shares of B become 3/9, 1/18, and the dominant share becomes max(3/9, 1/18) = 1/3. Next, DRF picks A, as her dominant share is 0. The process continues until it is no longer possible to run new tasks. In this case, this happens as soon as CPU has been saturated.</p><p>At the end of the above allocation, user A gets 3 CPU, 12 GB, while user B gets 6 CPU, 2 GB, i.e., each user gets 2/3 of its dominant resource.</p><p>Note that in this example the allocation stops as soon as any resource is saturated. However, in the general case, it may be possible to continue to allocate tasks even after some resource has been saturated, as some tasks might not have any demand on the saturated resource.</p><p>The above algorithm can be implemented using a binary heap that stores each user's dominant share. Each scheduling decision then takes O(log n) time for n users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weighted DRF</head><p>In practice, there are many cases in which allocating resources equally across users is not the desirable policy. Instead, we may want to allocate more resources to users running more important jobs, or to users that have contributed more resources to the cluster. To achieve this goal, we propose Weighted DRF, a generalization of both DRF and weighted max-min fairness.</p><p>With Weighted DRF, each user i is associated a weight vector W i = w i,1 , . . . , w i,m , where w i,j represents the weight of user i for resource j. The definition of a dominant share for user i changes to s i = max j {u i,j /w i,j }, where u i,j is user i's share of resource j. A particular case of interest is when all the weights of user i are equal, i.e., w i,j = w i , (1 ≤ j ≤ m). In this case, the ratio between the dominant shares of users i and j will be simply w i /w j . If the weights of all users are set to 1, Weighted DRF reduces trivially to DRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Alternative Fair Allocation Policies</head><p>Defining a fair allocation in a multi-resource system is not an easy question, as the notion of "fairness" is itself open to discussion. In our efforts, we considered numerous allocation policies before settling on DRF as the only one that satisfies all four of the required properties in Section 3: sharing incentive, strategy-proofness, Pareto efficiency, and envy-freeness. In this section, we consider two of the alternatives we have investigated: Asset Fairness, a simple and intuitive policy that aims to equalize the aggregate resources allocated to each user, and Competitive Equilibrium from Equal Incomes (CEEI), the policy of choice for fairly allocating resources in the microeconomic domain <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. We compare these policies with DRF in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Asset Fairness</head><p>The idea behind Asset Fairness is that equal shares of different resources are worth the same, i.e., that 1% of all CPUs worth is the same as 1% of memory and 1% of I/O bandwidth. Asset Fairness then tries to equalize the aggregate resource value allocated to each user. In particular, Asset Fairness computes for each user i the aggregate share x i = j s i,j , where s i,j is the share of resource j given to user i. It then applies max-min across users' aggregate shares, i.e., it repeatedly launches tasks for the user with the minimum aggregate share.</p><p>Consider the example in Section 4.1. Since there are twice as many GB of RAM as CPUs (i.e., 9 CPUs and 18 GB RAM), one CPU is worth twice as much as one GB of RAM. Supposing that one GB is worth $1 and one CPU is worth $2, it follows that user A spends $6 for each task, while user B spends $7. Let x and y be the number of tasks allocated by Asset Fairness to users A and B, respectively. Then the asset-fair allocation is given by the solution to the following optimization problem:</p><p>max (x, y) (Maximize allocations) subject to x + 3y ≤ 9 (CPU constraint)</p><p>4x + y ≤ 18 (Memory constraint) 6x = 7y (Every user spends the same)</p><p>Solving the above problem yields x = 2.52 and y = 2.16. Thus, user A gets 2.5 CPUs, 10.1 GB, while user B gets 6.5 CPUs, 2.2 GB, respectively.</p><p>While this allocation policy seems compelling in its simplicity, it has a significant drawback: it violates the sharing incentive property. As we show in Section 6.1.1, asset fairness can result in one user getting less than 1/n of all resources, where n is the total number of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Competitive Equilibrium from Equal Incomes</head><p>In microeconomic theory, the preferred method to fairly divide resources is Competitive Equilibrium from Equal Incomes (CEEI) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33]</ref>. With CEEI, each user receives initially 1 n of every resource, and subsequently, each user trades her resources with other users in a perfectly competitive market. <ref type="bibr">3</ref> The outcome of CEEI is both envy-free and Pareto efficient <ref type="bibr" target="#b29">[30]</ref>.</p><p>More precisely, the CEEI allocation is given by the Nash bargaining solution 4 <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The Nash bargaining solution picks the feasible allocation that maximizes i u i (a i ), where u i (a i ) is the utility that user i gets from her allocation a i . To simplify the comparison, we assume that the utility that a user gets from her allocation is simply her dominant share, s i .</p><p>Consider again the two-user example in Section 4.1. Recall that the dominant share of user A is 4x/18 = 2x/9 while the dominant share of user B is 3y/9 = y/3, where x is the number of tasks given to A and y is the number of tasks given to B. Maximizing the product of the dominant shares is equivalent to maximizing the product x · y. Thus, CEEI aims to solve the following optimization problem:</p><formula xml:id="formula_2">max (x · y)</formula><p>(maximize Nash product) subject to x + 3y ≤ 9 (CPU constraint)</p><p>4x + y ≤ 18 (Memory constraint)</p><p>Solving the above problem yields x = 45/11 and y = 18/11. Thus, user A gets 4.1 CPUs, 16.4 GB, while user B gets 4.9 CPUs, 1.6 GB.</p><p>3 A perfect market satisfies the price-taking (i.e., no single user affects prices) and market-clearance (i.e., matching supply and demand via price adjustment) assumptions. <ref type="bibr" target="#b2">4</ref> For this to hold, utilities have to be homogeneous, i.e., u(α x) = α u(x) for α &gt; 0, which is true in our case. Unfortunately, while CEEI is envy-free and Pareto efficient, it turns out that it is not strategy-proof, as we will show in Section 6.1.2. Thus, users can increase their allocations by lying about their resource demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with DRF</head><p>To give the reader an intuitive understanding of Asset Fairness and CEEI, we compare their allocations for the example in Section 4.1 to that of DRF in <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>We see that DRF equalizes the dominant shares of the users, i.e., user A's memory share and user B's CPU share. In contrast, Asset Fairness equalizes the total fraction of resources allocated to each user, i.e., the areas of the rectangles for each user in the figure. Finally, because CEEI assumes a perfectly competitive market, it finds a solution satisfying market clearance, where every resource has been allocated. Unfortunately, this exact property makes it possible to cheat CEEI: a user can claim she needs more of some underutilized resource even when she does not, leading CEEI to give more tasks overall to this user to achieve market clearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section, we discuss which of the properties presented in Section 3 are satisfied by Asset Fairness, CEEI, and DRF. We also evaluate the accuracy of DRF when task sizes do not match the available resources exactly.  <ref type="bibr" target="#b13">[14]</ref> contains a more complete list of results for DRF and CEEI. In the remainder of this section, we discuss some of the interesting missing entries in the table, i.e., properties violated by each of these disciplines. In particular, we show through examples why Asset Fairness and CEEI lack the properties that they</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Fairness Properties</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Allocation Policy Property</head><p>Asset do, and we prove that no policy can provide resource monotonicity without violating either sharing incentive or Pareto efficiency to explain why DRF lacks resource monotonicity.</p><note type="other">CEEI DRF Sharing Incentive Strategy-proofness Envy-freeness Pareto efficiency Single Resource Fairness Bottleneck Fairness Population Monotonicity Resource Monotonicity</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Properties Violated by Asset Fairness</head><p>While being the simplest policy, Asset Fairness violates several important properties: sharing incentive, bottleneck fairness, and resource monotonicity. Next, we use examples to show the violation of these properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 1 Asset Fairness violates the sharing incentive property.</head><p>Proof Consider the following example, illustrated in <ref type="figure">Figure 5</ref>: two users in a system with 30, 30 total resources have demand vectors D 1 = 1, 3, and D 2 = 1, 1. Asset fairness will allocate the first user 6 tasks and the second user 12 tasks. The first user will receive 6, 18 resources, while the second will use 12, 12. While each user gets an equal aggregate share of <ref type="bibr" target="#b23">24</ref> 60 , the second user gets less than half (15) of both resources. This violates the sharing incentive property, as the second user would be better off to statically partition the cluster and own half of the nodes.</p><p>Theorem 2 Asset Fairness violates the bottleneck fairness property.</p><p>Proof Consider a scenario with a total resource vector of 21, 21 and two users with demand vectors D 1 = 3, 2 and D 2 = 4, 1, making resource 1 the bottleneck resource. Asset fairness will give each user 3 tasks, equalizing their aggregate usage to 15. However, this only gives the first user <ref type="bibr">3</ref> 7 of resource 1 (the contended bottleneck resource), violating bottleneck fairness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Properties Violated by CEEI</head><p>While CEEI is envy-free and Pareto efficient, it turns out that it is not strategy proof. Intuitively, this is because CEEI assumes a perfectly competitive market that achieves market clearance, i.e., matching of supply and demand and allocation of all the available resources. This can lead to CEEI giving much higher shares to users that use more of a less-contended resource in order to fully utilize that resource. Thus, a user can claim that she needs more of some underutilized resource to increase her overall share of resources. We illustrate this below.</p><p>Theorem 4 CEEI is not strategy-proof. Proof Consider the following example, shown in <ref type="figure">Figure  6</ref>. Assume a total resource vector of 100, 100, and two users with demands 16, 1 and 1, 2. In addition, for the same intuitive reason (market clearance), we have the following result:</p><p>Theorem 5 CEEI violates population monotonicity.</p><p>Proof Consider the total resource vector 100, 100 and three users with the following demand vectors D 1 = 4, 1, D 2 = 1, 16, and D 3 = 16, 1 (see <ref type="figure" target="#fig_4">Figure 7</ref>). CEEI will yield the allocation A 1 = 11.3, 5.4, 3.1, where the numbers in parenthesis represent the number of tasks allocated to each user. If user 3 leaves the system and relinquishes her resource, CEEI gives the new allocation A 2 = 23.8, 4.8, which has made user 2 worse off than in A 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Resource Monotonicity vs. Sharing Incentives and Pareto efficiency</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, DRF achieves all the properties except resource monotonicity. Rather than being a limitation of DRF, this is a consequence of the fact that sharing incentive, Pareto efficiency, and resource monotonicity cannot be achieved simultaneously. Since we consider the first two of these properties to be more important (see Section 3) and since adding new resources to a system is a relatively rare event, we chose to satisfy sharing incentive and Pareto efficiency, and give up resource monotonicity. In particular, we have the following result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 6</head><p>No allocation policy that satisfies the sharing incentive and Pareto efficiency properties can also satisfy resource monotonicity.</p><p>Proof We use a simple example to prove this property. Consider two users A and B with symmetric demands 2, 1, and 1, 2, respectively, and assume equal amounts of both resources. Sharing incentive requires that user A gets at least half of resource 1 and user B gets half of resource 2. By Pareto efficiency, we know that at least one of the two users must be allocated more resources. Without loss of generality, assume that user A is given more than half of resource 1 (a symmetric argument holds if user B is given more than half of resource 2). If the total amount of resource 2 is now increased by a factor of 4, user B is no longer getting its guaranteed share of half of resource 2. Now, the only feasible allocation that satisfies the sharing incentive is to give both users half of resource 1, which would require decreasing user 1's share of resource 1, thus violating resource monotonicity.</p><p>This theorem explains why both DRF and CEEI violate resource monotonicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Discrete Resource Allocation</head><p>So far, we have implicitly assumed one big resource pool whose resources can be allocated in arbitrarily small amounts. Of course, this is often not the case in practice. For example, clusters consist of many small machines, where resources are allocated to tasks in discrete amounts. In the reminder of this section, we refer to these two scenarios as the continuous, and the discrete scenario, respectively. We now turn our attention to how fairness is affected in the discrete scenario.</p><p>Assume a cluster consisting of K machines. Let max-task denote the maximum demand vector across all demand vectors, i.e., max-task = max i {d i,1 }, max i {d i,2 }, · · · , max i {d i,m }}. Assume further that any task can be scheduled on every machine, i.e., the total amount of resources on each machine is at least max-task. We only consider the case when each user has strictly positive demands. Given these assumptions, we have the following result.</p><p>Theorem 7 In the discrete scenario, it is possible to allocate resources such that the difference between the allocations of any two users is bounded by one max-task compared to the continuous allocation scenario.</p><p>Proof Assume we start allocating resources on one machine at a time, and that we always allocate a task to the user with the lowest dominant share. As long as there is at least a max-task available on the first machine, we continue to allocate a task to the next user with least dominant share. Once the available resources on the first machine become less than a max-task size, we move to the next machine and repeat the process. When the allocation completes, the difference between two user's allocations of their dominant resources compared to the continuous scenario is at most max-task. If this were not the case, then some user A would have more than max-task discrepancy w.r.t. to another user B. However, this cannot be the case, because the last time A was allocated a task, B should have been allocated a task instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental Results</head><p>This section evaluates DRF through micro-and macrobenchmarks. The former is done through experiments running an implementation of DRF in the Mesos cluster resource manager <ref type="bibr" target="#b15">[16]</ref>. The latter is done using tracedriven simulations. We start by showing how DRF dynamically adjusts the shares of jobs with different resource demands in Section 7.1. In Section 7.2, we compare DRF against slot-level fair sharing (as implemented by Hadoop Fair Scheduler <ref type="bibr" target="#b33">[34]</ref> and Quincy <ref type="bibr" target="#b17">[18]</ref>), and CPU-only fair sharing. Finally, in Section 7.3, we use Facebook traces to compare DRF and the Hadoop's Fair Scheduler in terms of utilization and job completion time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Dynamic Resource Sharing</head><p>In our first experiment, we show how DRF dynamically shares resources between jobs with different demands. We ran two jobs on a 48-node Mesos cluster on Amazon EC2, using "extra large" instances with 4 CPU cores and 15 GB of RAM. We configured Mesos to allocate up to 4 CPUs and 14 GB of RAM on each node, leaving 1 GB for the OS. We submitted two jobs that launched tasks with different resource demands at different times during a 6-minute interval.</p><p>Figures 8 (a) and 8 (b) show the CPU and memory allocations given to each job as a function of time, while <ref type="figure" target="#fig_5">Figure 8</ref> (c) shows their dominant shares. In the first 2 minutes, job 1 uses 1 CPU, 10 GB RAM per task and job 2 uses 1 CPU, 1 GB RAM per task. Job 1's dominant resource is RAM, while job 2's dominant resource is CPU. Note that DRF equalizes the jobs' shares of their dominant resources. In addition, because jobs have different dominant resources, their dominant shares exceed 50%, i.e., job 1 uses around 70% of the RAM while job 2 uses around 75% of the CPUs. Thus, the jobs benefit from running in a shared cluster as opposed to taking half the nodes each. This captures the essence of the sharing incentive property.</p><p>After 2 minutes, the task sizes of both jobs change, to 2 CPUs, 4 GB for job 1 and 1 CPU, 3 GB for job 2. Now, both jobs' dominant resource is CPU, so DRF equalizes their CPU shares. Note that DRF switches allocations dynamically by having Mesos offer resources to the job with the smallest dominant share as tasks finish.</p><p>Finally, after 2 more minutes, the task sizes of both jobs change again: 1 CPU, 7 GB for job 1 and 1 CPU, 4 GB for job 2. Both jobs' dominant resource is now memory, so DRF tries to equalize their memory shares. The reason the shares are not exactly equal is due to resource fragmentation (see Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">DRF vs. Alternative Allocation Policies</head><p>We next evaluate DRF with respect to two alternative schemes: slot-based fair scheduling (a common policy in current systems, such as the Hadoop Fair Scheduler <ref type="bibr" target="#b33">[34]</ref> and Quincy <ref type="bibr" target="#b17">[18]</ref>) and (max-min) fair sharing applied only to a single resource (CPU). For the experiment, we ran a 48-node Mesos cluster on EC2 instances with 8 CPU cores and 7 GB RAM each. We configured Mesos to allocate 8 CPUs and 6 GB RAM on each node, leaving 1 GB free for the OS. We implemented these three scheduling policies as Mesos allocation modules.</p><p>We ran a workload with two classes of users, representing two organizational entities with different workloads. One of the entities had four users submitting small jobs with task demands 1 CPU, 0.5 GB. The other en-DRF 3 slots 4 slots 5 slots 6 slots CPU-fair <ref type="table" target="#tab_2">0  5  10  15  20  25  30  35  40 35  33  30</ref> 17 8 13 tity had four users submitting large jobs with task demands 2 CPUs, 2 GB. Each job consisted of 80 tasks.</p><p>As soon as a job finished, the user would launch another job with similar demands. Each experiment ran for ten minutes. At the end, we computed the number of completed jobs of each type, as well as their response times. For the slot-based allocation scheme, we varied the number of slots per machine from 3 to 6 to see how it affected performance. <ref type="figure" target="#fig_0">Figures 9 through 12</ref> show our results. In <ref type="figure" target="#fig_0">Figures 9 and 10</ref>, we compare the number of jobs of each type completed for each scheduling scheme in ten minutes. In <ref type="figure" target="#fig_0">Figures 11 and 12</ref>, we compare average response times.</p><p>Several trends are apparent from the data. First, with slot-based scheduling, both the throughput and job response times are worse than with DRF, regardless of the number of slots. This is because with a low slot count, the scheduler can undersubscribe nodes (e.g.,, launch only 3 small tasks on a node), while with a large slot count, it can oversubscribe them (e.g., launch 4 large tasks on a node and cause swapping because each task needs 2 GB and the node only has 6 GB). Second, with fair sharing at the level of CPUs, the number of small jobs executed is similar to DRF, but there are much fewer large jobs executed, because memory is overcommitted on some machines and leads to poor performance for all the high-memory tasks running there. Overall, the DRFbased scheduler that is aware of both resources has the lowest response times and highest overall throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Simulations using Facebook Traces</head><p>Next we use log traces from a 2000-node cluster at Facebook, containing data for a one week period (October 2010). The data consists of Hadoop MapReduce jobs. We assume task duration, CPU usage, and memory consumption is identical as in the original trace. The traces are simulated on a smaller cluster of 400 nodes to reach higher utilization levels, such that fairness becomes relevant. Each node in the cluster consists of 12 slots, 16 cores, and 32 GB memory. <ref type="figure" target="#fig_0">Figure 13</ref> shows a short 300 second sub-sample to visualize how CPU and memory utilization looks for the same workload when using DRF compared to Hadoop's fair scheduler (slot). As shown in the figure, DRF provides higher utilization, as it is able to better match resource allocations with task demands. <ref type="figure" target="#fig_0">Figure 14</ref> shows the reduction of the average job completion times for DRF as compared to the Hadoop fair scheduler. The workload is quite heavy on small jobs, which experience no improvements (i.e., −3%). This is because small jobs typically consist of a single execution phase, and the completion time is dominated by the longest task. Thus completion time is hard to improve for such small jobs. In contrast, the completion times of the larger jobs reduce by as much as 66%. This is because these jobs consists of many phases, and thus they can benefit from the higher utilization achieved by DRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>We briefly review related work in computer science and economics.</p><p>While many papers in computer science focus on multi-resource fairness, they are only considering multiple instances of the same interchangeable resource, e.g., CPU <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b34">35]</ref>, and bandwidth <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. Unlike these approaches, we focus on the allocation of resources of different types. Quincy <ref type="bibr" target="#b17">[18]</ref> is a scheduler developed in the context of the Dryad cluster computing framework <ref type="bibr" target="#b16">[17]</ref>. Quincy achieves fairness by modeling the fair scheduling problem as a min-cost flow problem. Quincy does not currently support multi-resource fairness. In fact, as mentioned in the discussion section of the paper [18, pg. 17], it appears difficult to incorporate multi-resource requirements into the min-cost flow formulation.</p><p>Hadoop currently provides two fair sharing schedulers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b33">34]</ref>. Both these schedulers allocate resources at the slot granularity, where a slot is a fixed fraction of the resources on a machine. As a result, these schedulers cannot always match the resource allocations with the tasks' demands, especially when these demands are widely heterogeneous. As we have shown in Section 7, this mismatch may lead to either low cluster utilization or poor performance due to resource oversubscription.</p><p>In the microeconomic literature, the problem of equity has been studied within and outside of the framework of game theory. The books by <ref type="bibr">Young [33]</ref> and Moulin <ref type="bibr" target="#b21">[22]</ref> are entirely dedicated to these topics and provide good introductions. The preferred method of fair division in microeconomics is CEEI <ref type="bibr">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b21">22]</ref>, as introduced by Varian <ref type="bibr" target="#b29">[30]</ref>. We have therefore devoted considerable attention to it in Section 5.2. CEEI's main drawback compared to DRF is that it is not strategy-proof. As a result, users can manipulate the scheduler by lying about their demands.</p><p>Many of the fair division policies proposed in the microeconomics literature are based on the notion of utility and, hence, focus on the single metric of utility. In the economics literature, max-min fairness is known as the lexicographic ordering <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b24">25]</ref> (leximin) of utilities.</p><p>The question is what the user utilities are in the multiresource setting, and how to compare such utilities. One natural way is to define utility as the number of tasks allocated to a user. But modeling utilities this way, together with leximin, violates many of the fairness properties we proposed. Viewed in this light, DRF makes two contributions. First, it suggests using the dominant share as a proxy for utility, which is equalized using the standard leximin ordering. Second, we prove that this scheme is strategy-proof for such utility functions. Note that the leximin ordering is a lexicographic version of the KalaiSmorodinsky (KS) solution <ref type="bibr" target="#b18">[19]</ref>. Thus, our result shows that KS is strategy-proof for such utilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and Future Work</head><p>We have introduced Dominant Resource Fairness (DRF), a fair sharing model that generalizes max-min fairness to multiple resource types. DRF allows cluster schedulers to take into account the heterogeneous demands of datacenter applications, leading to both fairer allocation of resources and higher utilization than existing solutions that allocate identical resource slices (slots) to all tasks. DRF satisfies a number of desirable properties. In particular, DRF is strategy-proof, so that users are incentivized to report their demands accurately. DRF also incentivizes users to share resources by ensuring that users perform at least as well in a shared cluster as they would in smaller, separate clusters. Other schedulers that we investigated, as well as alternative notions of fairness from the microeconomic literature, fail to satisfy all of these properties.</p><p>We have evaluated DRF by implementing it in the Mesos resource manager, and shown that it can lead to better overall performance than the slot-based fair schedulers that are commonly in use today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Future Work</head><p>There are several interesting directions for future research. First, in cluster environments with discrete tasks, one interesting problem is to minimize resource fragmentation without compromising fairness. This problem is similar to bin-packing, but where one must pack as many items (tasks) as possible subject to meeting DRF. A second direction involves defining fairness when tasks have placement constraints, such as machine preferences. Given the current trend of multi-core machines, a third interesting research direction is to explore the use of DRF as an operating system scheduler. Finally, from a microeconomic perspective, a natural direction is to investigate whether DRF is the only possible strategyproof policy for multi-resource fairness, given other desirable properties such Pareto efficiency. a task of user i is selected, she is allocated an amount si d i,k = · r k of the dominant resource. This means that the share of the dominant resource of user i increases by ( · r k )/r k = , as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Allocation Properties</head><p>We start with a preliminary result.</p><p>Lemma 8 Every user in a DRF allocation has at least one saturated resource.</p><p>Proof Assume this is not the case, i.e., none of the resources used by user i is saturated. However, this contradicts the assumption that progressive filling has completed the computation of the DRF allocation. Indeed, as long as none of the resources of user i are saturated, progressive filling will continue to increase the allocations of user i (and of all the other users sharing only non-saturated resources).</p><p>Recall that progressive filling always allocates the resources to a user proportionally to the user's demand vector. More precisely, let</p><formula xml:id="formula_3">D i = d i,1 , d i,2 , . . . , d i,m</formula><p>be the demand vector of user i. Then, at any time t during the progressive filling process, the allocation of user i is proportional to the demand vector,</p><formula xml:id="formula_4">A i (t) = α i (t) · D i = α i (t) · d i,1 , d i,2 , . . . , d i,m (1)</formula><p>where α i (t) is a positive scalar. Now, we are in position to prove the DRF properties.</p><p>Theorem 9 DRF is Pareto efficient.</p><p>Proof Assume user i can increase her dominant share, s i , without decreasing the dominant share of anyone else. According to Lemma 8, user i has at least one saturated resource. If no other user is using the saturated resource, then we are done as it would be impossible to increase i's share of the saturated resource. If other users are using the saturated resource, then increasing the allocation of i would result in decreasing the allocation of at least another user j sharing the same saturated resource. Since under progressive filling, the resources allocated by any user are proportional to her demand vector (see Eq. 1), decreasing the allocation of any resource used by user i will also decrease i's dominant share. This contradicts our hypothesis, and therefore proves the result.</p><p>Theorem 10 DRF satisfies the sharing incentive and bottleneck fairness properties.</p><p>Proof Consider a system consisting of n users. Assume resource k is the first one being saturated by using progressive filling. Let i be the user allocating the largest share on resource k, and let t i,k denote her share of k.</p><p>Since resource k is saturated, we have trivially t i,k ≥ 1 n .</p><p>Furthermore, by the definition of the dominant share, we have s i ≥ t i,k ≥ 1 n . Since progressive filling increases the allocation of each user's dominant resource at the same rate, it follows that each user gets at least 1 n of her dominant resource. Thus, DRF satisfies the sharing incentive property. If all users have the same dominant resource, each user gets exactly 1 n of that resource. As a result, DRF satisfies the bottleneck fairness property as well.</p><p>Theorem 11 Every DRF allocation is envy-free.</p><p>Proof Assume by contradiction that user i envies another user j. For user i to envy another user j, user j must have a strictly higher share of every resource that i wants; otherwise i cannot run more tasks under j's allocation. This means that user j's dominant share is strictly larger than user i's dominant share. Since every resource allocated to user i is also allocated to user j, this means that user j cannot reach its saturated resource after user i, i.e., t j ≤ t i , where t k is the time that user k's allocation gets frozen due to saturation. However, if t j ≤ t i , under progressive filling, the dominant shares of users j and i will be equal at time t j , after which the dominant share of user i can only increase, violating the hypothesis.</p><p>Theorem 12 (Strategy-proofness) A user cannot increase her dominant share in DRF by altering her true demand vector.</p><p>Proof Assume user i can increase her dominant share by using a demand vectorˆdvectorˆ vectorˆd i = d i . Let a i,j andâandˆandâ i,j denote the amount of resource j user i is allocated using progressive filling when the user uses the vector d i andˆdandˆ andˆd i , respectively. For user i to be better off usingˆdusingˆ usingˆd i , we need thatâthatˆthatâ i,k &gt; a i,k for every resource k where d i,k &gt; 0. Let r denote the first resource that becomes saturated for user i when she uses the demand vector d i . If no other user is allocated resource r (a j,r = 0 for all j = i), this contradicts the hypothesis as user i is already allocated the entire resource r, and thus cannot increase her allocation of r using another demand vectorˆdvectorˆ vectorˆd i . Thus, assume there are other users that have been allocated r (a j,r &gt; 0 for some j = i). In this case, progressive filling will eventually saturate r at time t when using d i , and at time t when using demandˆddemandˆ demandˆd i . Recall that the dominant share is the maximum of a user's shares, thus i must have a higher dominant share in the allocationâallocationˆallocationâ than in a. Thus, t &gt; t, as progressive filling increases the dominant share at a constant rate. This implies that i-when usingˆdusingˆ usingˆd-does not saturate any resource before time t , and hence does not affect other user's allocation before time t . Thus, when i usesˆdusesˆ usesˆd, any user m using resource r has allocation a m,r at time t. Therefore, at time t, there is only a i,r amount of r left for user i, which contradicts the assumption thatâthatˆthatâ i,r &gt; a i,r .</p><p>The strategy-proofness of DRF shows that a user will not be better off by demanding resources that she does not need. The following example shows that excess demand can in fact hurt user's allocation, leading to a lower dominant share. Consider a cluster with two resources, and 10 users, the first with demand vector 1, 0 and the rest with demand vectors 0, 1. The first user gets the entire first resource, while the rest of the users each get 1 9 of the second resource. If user 1 instead changes her demand vector to 1, 1, she can only be allocated 1 10 of each resource and the rest of the users get 1 10 of the second resource.</p><p>In practice, the situation can be exacerbated as resources in datacenters are typically partitioned across different physical machines, leading to fragmentation. Increasing one's demand artificially might lead to a situation in which, while there are enough resources on the whole, there are not enough on any single machine to satisfy the new demand. See Section 6.2 for more information.</p><p>Next, for simplicity we assume strictly positive demand vectors, i.e., the demand of every user for every resource is non-zero.</p><p>Theorem 13 Given strictly positive demand vectors, DRF guarantees that every user gets the same dominant share, i.e., every DRF allocation ensures s i = s j , for all users i and j.</p><p>Proof Progressive filling will start increasing every users' dominant resource allocation at the same rate until one of the resources becomes saturated. At this point, no more resources can be allocated to any user as every user demands a positive amount of the saturated resource.</p><p>Theorem 14 Given strictly positive demands, DRF satisfies population monotonicity.</p><p>Proof Consider any DRF allocation. Non-zero demands imply that all users have the same saturated resource(s). Consider removing a user and relinquishing her currently allocated resources, which is some amount of every resource. Since all users have the same dominant share α, any new allocation which decreases any user i's dominant share below α would, due to Pareto efficiency, have to allocate another user j a dominant share of more than α. The resulting allocation would violate max-min fairness, as it would be possible to increase i's dominant share by decreasing the allocation of j, who already has a higher dominant share than i.</p><p>However, we note that in the absence of strictly positive demand vectors, DRF no longer satisfies the population monotonicity property <ref type="bibr" target="#b13">[14]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CPU and memory demands of tasks in a 2000-node Hadoop cluster at Facebook over one month (October 2010). Each bubble's size is logarithmic in the number of tasks in its region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: CDF of demand to slot ratio in a 2000-node cluster at Facebook over a one month period (October 2010). A demand to slot ratio of 2.0 represents a task that requires twice as much CPU (or memory) than the slot CPU (or memory) size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: DRF allocation for the example in Section 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Allocations given by DRF, Asset Fairness and CEEI in the example scenario in Section 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example showing that CEEI violates population monotonicity. When user 3 leaves, CEEI changes the allocation from a) to b), lowering the share of user 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: CPU, memory and dominant share for two jobs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: Number of large jobs completed for each allocation scheme in our comparison of DRF against slot-based fair sharing and CPU-only fair sharing. DRF 3 slots 4 slots 5 slots 6 slots CPU-fair 0 20 40 60 80 100 91 37 61 66 35</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Average response time (in seconds) of large jobs for each allocation scheme in our comparison of DRF against slot-based fair sharing and CPU-only fair sharing. DRF 3 slots 4 slots 5 slots 6 slots CPU-fair 0 10 20 30 40 50 60 70 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :Figure 14 :</head><label>1314</label><figDesc>Figure 13: CPU and memory utilization for DRF and slot fairness for a trace from a Facebook Hadoop cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 summarizes</head><label>2</label><figDesc>the fairness properties that are sat- isfied by Asset Fairness, CEEI, and DRF. The Appendix contains the proofs of the main properties of DRF, while our technical report</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Properties of Asset Fairness, CEEI and DRF.</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>In this case, CEEI allocates 100 31 and 1500 31 tasks to each user respectively (approximately 3.2 and 48.8 tasks). If user 1 changes her demand vector to 16, 8, asking for more of resource 2 than she actually needs, CEEI gives the the users 25 6 and 100 3 tasks respectively (approximately 4.2 and 33.3 tasks). Thus, user 1 improves her number of tasks from 3.2 to 4.2 by lying about her demand vector. User 2 suf- fers because of this, as her task allocation decreases.</figDesc><table></table></figure>

			<note place="foot" n="1"> A user may have the same share on multiple resources, and might therefore have multiple dominant resources.</note>

			<note place="foot" n="2"> Note that given last constraint (i.e., 2x/9 = y/3) allocations x and y are simultaneously maximized.</note>

			<note place="foot" n="6"> Recall that in this section we assume that all tasks of a user have the same demand vector.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgements</head><p>We thank Eric J. Friedman, Hervé Moulin, John Wilkes, and the anonymous reviewers for their invaluable feedback. We thank Facebook for making available their traces. This research was supported by California MI-CRO, California Discovery, the Swedish Research Council, the Natural Sciences and Engineering Research Council of Canada, a National Science Foundation Graduate Research Fellowship, <ref type="bibr">5</ref> and the RAD Lab sponsors: Google, Microsoft, Oracle, Amazon, Cisco, Cloudera, eBay, Facebook, Fujitsu, HP, Intel, NetApp, SAP, VMware, and Yahoo!.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix: DRF Properties</head><p>In this appendix, we present the main properties of DRF. The technical report <ref type="bibr" target="#b13">[14]</ref> contains a more complete list of results for DRF and CEEI. For context, the following table summarizes the properties satisfied by Asset Fairness, CEEI, and DRF, respectively.</p><p>In this section, we assume that all users have an unbounded number of tasks. In addition, we assume that all tasks of a user have the same demand vector, and we will refer to this vector as the user's demand vector.</p><p>Next, we present progressive filling <ref type="bibr" target="#b7">[9]</ref>, a simple technique to achieve DRF allocation when all resources are arbitrary divisible. This technique is instrumental in proving our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Progressive Filling for DRF</head><p>Progressive filling is an idealized algorithm to achieve max-min fairness in a system in which resources can be allocated in arbitrary small amounts <ref type="bibr">[9, pg 450]</ref>. It was originally used in a networking context, but we now adapt it to our problem domain. In the case of DRF, progressive filling increases all users' dominant shares at the same rate, while increasing their other resource allocations proportionally to their task demand vectors, until at least one resource is saturated. At this point, the allocations of all users using the saturated resource are frozen, and progressive filling continues recursively after eliminating these users. In this case, progressive filling terminates when there are no longer users whose dominant shares can be increased.</p><p>Progressive filling for DRF is equivalent to the scheduling algorithm presented in <ref type="figure">Figure 1</ref> after appropriately scaling the users' demand vectors. In particular, each user's demand vector is scaled such that allocating resources to a user according to her scaled demand vector will increase her dominant share by a fixed , which is the same for all users. Let </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadoop Capacity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scheduler</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/common/docs/r0.20.2/capacity_scheduler.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadoop Fair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scheduler</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/common/docs/r0.20.2/fair_scheduler.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Models of memory scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Bryant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;75</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linux Block IO -Present and Future (Completely Fair Queueing)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Axboe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ottawa Linux Symposium</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Proportionate progress: A notion of fairness in resource allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Plaxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Varvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="600" to="625" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast scheduling of periodic tasks on multiple resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Plaxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPPS &apos;95</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WF 2 Q: Worst-case fair weighted fair queueing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Data Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fair queuing for aggregated multiple links</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Blanquer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ozden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM &apos;01</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Group ratio round-robin: O(1) proportional share scheduling for uniprocessor and multiprocessor systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caprita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis and simulation of a fair queueing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Keshav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM &apos;89</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1989" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Any opinions, findings, conclusions, or recommendations expressed in this publication are those of the authors and do not necessarily reflect the views of the NSF</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Resource allocation and the public sector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Foley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yale Economic Essays</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="76" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dominant resource fairness: Fair allocation of multiple resource types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno>UCB/EECS-2011-18</idno>
		<imprint>
			<date type="published" when="2011-03" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Start-time fair queuing: A scheduling algorithm for integrated services packet switching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="690" to="704" />
			<date type="published" when="1997-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dryad: distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quincy: Fair scheduling for distributed computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;09</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Other Solutions to Nash&apos;s Bargaining Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smorodinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="513" to="518" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fairness in routing and load balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rabani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="20" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Opportunistic fair scheduling over multiple wireless channels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Knightly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fair Division and Collective Welfare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Moulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Bargaining Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="162" />
			<date type="published" when="1950-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A generalized processor sharing approach to flow control -the single node case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gallager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM/IEEE Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="344" to="357" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Egalitarian equivalent allocations: A new concept of economic equity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Pazner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schmeidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="671" to="687" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rawls Versus Bentham: An Axiomatic Examination of the Pure Distribution Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Decision</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="301" to="309" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient fair queuing using deficit round robin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shreedhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Net</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A proportional share resource allocation algorithm for real-time, time-shared systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdel-Wahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jeffay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baruah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Plaxton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE RTSS 96</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Core-stateless fair queueing: Achieving approximately fair bandwidth allocations in high speed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Equity, envy, and efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Theory</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="91" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lottery and Stride Scheduling: Flexible Proportional Share Resource Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
		<idno>MIT/LCS/TR-667</idno>
		<imprint>
			<date type="published" when="1995-09" />
		</imprint>
		<respStmt>
			<orgName>MIT, Laboratory of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lottery scheduling: flexible proportional-share resource management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Weihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;94</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Equity: in theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Delay Scheduling: A Simple Technique for Achieving Locality and Fairness in Cluster Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys 10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiple-Resource Periodic Scheduling Problem: how much fairness is necessary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mossé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Melhem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE RTSS</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
