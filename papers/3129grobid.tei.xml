<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-01T14:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IntelliLight: A Reinforcement Learning Approach for Intelligent Traffic Light Control</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-08-19">2018. August 19-23, 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
							<email>huaxiuyao@ist.psu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxiu</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhui</forename><forename type="middle">Li</forename></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The Pennsylvania State University University Park</orgName>
								<orgName type="institution" key="instit2">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA, PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The Pennsylvania State University</orgName>
								<address>
									<settlement>University Park</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IntelliLight: A Reinforcement Learning Approach for Intelligent Traffic Light Control</title>
					</analytic>
					<monogr>
						<title level="m">KDD &apos;18: The 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
						<meeting> <address><addrLine>London</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2018-08-19">2018. August 19-23, 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3219819.3220096</idno>
					<note>United Kingdom. ACM, New York, NY, USA, 10 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Control methods</term>
					<term>• Applied computing → Transportation</term>
					<term>KEYWORDS</term>
				</keywords>
			</textClass>
			<abstract>
				<p>The intelligent traffic light control is critical for an efficient transportation system. While existing traffic lights are mostly operated by hand-crafted rules, an intelligent traffic light control system should be dynamically adjusted to real-time traffic. There is an emerging trend of using deep reinforcement learning technique for traffic light control and recent studies have shown promising results. However, existing studies have not yet tested the methods on the real-world traffic data and they only focus on studying the rewards without interpreting the policies. In this paper, we propose a more effective deep reinforcement learning model for traffic light control. We test our method on a large-scale real traffic dataset obtained from surveillance cameras. We also show some interesting case studies of policies learned from the real data.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we firstly introduce conventional methods for traffic light control, then introduce methods using reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Conventional Traffic Light Control</head><p>Early traffic light control methods can be roughly classified into two groups. The first is pre-timed signal control <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23]</ref>, where a fixed time is determined for all green phases according to historical traffic demand, without considering possible fluctuations in traffic demand. The second is vehicle-actuated control methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> where the real-time traffic information is used. Vehicle-actuated methods are suitable for the situations with relatively high traffic randomness. However, this method largely depends on the handcraft rules for current traffic condition, without taking into account future situation. Therefore, it cannot reach the global optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement Learning for Traffic Light Control</head><p>Recently, due to the incapability of dealing with dynamic multidirection traffic in previous methods, more works try to use reinforcement learning algorithms to solve the traffic light control problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>. Typically, these algorithms take the traffic on the road as state, and the operation on light as action. These methods usually show better performance compared with fixed-time and traffic-responsive control methods. Methods in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">24]</ref> designed the state as discrete values like the location of vehicles or number of waited cars. However, the discrete state-action pair value matrix requires huge storage space, which keeps these methods from being used in large state space problems.</p><p>To solve the in-managablely large state space of previous methods, recent studies <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> propose to apply Deep Q-learning methods using continuous state representations. These studies learn a Q-function (e.g. a deep neural network) to map state and action to reward. These works vary in the state representation including hand craft features (e.g., queue length <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, average delay <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>) and image features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>) They are also different in reward design, including average delay <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22]</ref>,the average travel time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22]</ref>, and queue length <ref type="bibr" target="#b14">[15]</ref>.</p><p>However, all these methods assume relatively static traffic environments, and hence far from the real case. Further, they only focus on rewards and overlook the adaptability of the algorithms to the real traffic. Therefore, they cannot interpret why the learned light signal changes corresponding to the traffic. In this paper, we try to test the algorithms in a more realistic traffic setting, and add more interpretation other than reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>In our problem, we have the environment E as an intersection of two roads (and the traffic on this intersection). There is an intelligent traffic light agent G. To make the notation simpler, we use "N", "S", "W", "E" to represent north, south, west, and east respectively, and use "Red" and "Green" to represent red light and green light correspondingly. A setting of the traffic light is defined as a phase (e.g., green light on the west-east direction which can be simplified as Green-WE). When a light changes from green to red, there is a 3 second yellow light, while the other directions still keep red. So one green light and the subsequent yellow light can be represented together by "Green". To simplify the problem, we assume there are only two phases of the traffic light, i.e., 1) Green-WE, and 2) Red-WE. Due to the limitation of real-world setting, the traffic light can only change in a specific order (i.e., 1 -&gt; 2 -&gt; 1 -&gt; 2 -&gt; ...). Given the state s (describing the positions and speed of the traffic near this intersection), the goal of the agent G is to give the optimal action a (i.e., whether to change the light to the next phase), so that the reward r (i.e., the smoothness of traffic) can be maximized. Delay of lane i.</p><formula xml:id="formula_0">M ∈ R N ×N Image representation of vehicles' position P c Current phase P n Next phase C ∈ {0, 1}</formula><p>Light switches (1) or not (0) N Number of vehicles passed the intersection after the action. T Travel time in system of all vehicles that passed the intersection during ∆t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>Traffic light control has attracted a lot of attention in recent years due to its essential role in adjusting traffic. Current methods generally have two categories, conventional methods, and deep reinforcement learning based methods. Conventional methods usually rely on previous knowledge to set fixed time for each light phase or set changing rules. These rules are prone to dynamically changing traffic. Reinforcement learning methods usually take the traffic condition (e.g., queue length of waiting cars and updated waiting time) as state, and try to make actions that can improve the traffic condition based on the current state. However, the current methods do not consider the complex situations in real case, and hence may lead to stuck in one single kind of action. This will lead to inferior traffic adjusting performance under complex traffic situation.</p><p>In this section, we propose a deep reinforcement traffic light agent to solve this problem. We will first introduce the model framework in Section 4.1. Then, we show the design of agent in Section 4.2. We further describe the network structure in Section 4.3. In addition, we describe the memory palace in Section 4.4. Note that, although our model is designed for a four way intersection with two phases, it is not difficult to extend it to other types of intersections or to multiple phases scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offline</head><p>Online  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework</head><p>Our model is composed of offline part and online part (as shown in <ref type="figure" target="#fig_2">Figure 4</ref>). We extract five kinds of features describing the traffic conditions as state (detailed in Section 4.2), and use reward to describe how much the action has improved the traffic (detailed in Section 4.2). In offline stage, we set a fixed timetable for the lights, and let traffic go through the system to collect data samples. After training with the samples logged in this stage, the model will be put into the online part. In online stage, at every time interval ∆t, the traffic light agent will observe the state s from the environment and take action a (i.e., whether to change light signal to the next phase) according to ϵ-greedy strategy combining exploration (i.e., random action with probability ϵ) and exploitation (i.e., taking the action with maximum estimated reward). After that, the agent G will observe the environment and get the reward r from it. Then, the tuple (s, a, r) will be stored into memory. After several timestamps (e.g., t 2 in <ref type="figure" target="#fig_2">Figure 4</ref>), agent G will update the network according to the logs in the memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agent Design</head><p>First, we introduce the state, action and reward representation.</p><p>• State. Our state is defined for one intersection. </p><formula xml:id="formula_1">D i = 1 − lane speed speed limit<label>(1)</label></formula><p>(3) Sum of updated waiting time W over all approaching lanes. This equals to the sum of W over all vehicles on approaching lanes. The updated waiting time W for vehicle j at time t is defined in Equation 2. Note that the updated waiting time of a vehicle is reset to 0 every time it moves. For example, if a vehicle's speed is 0.01m/s from 0s to 15s, 5m/s from 15s to 30s, and 0.01m/s from 30s to 60s, W j is 15 seconds, 0 seconds and 30 seconds when t =15s, 30s and 60s relatively.</p><formula xml:id="formula_2">W j (t) = W j (t − 1) + 1 vehicle speed &lt; 0.1 0 vehicle speed ≥ 0.1<label>(2)</label></formula><p>(4) Indicator of light switches C, where C = 0 for keeping the current phase, and C = 1 for changing the current phase. (5) Total number of vehicles N that passed the intersection during time interval ∆t after the last action a. (6) Total travel time of vehicles T that passed the intersection during time interval ∆t after the last action a, defined as the total time (in minutes) that vehicles spent on approaching lanes. Hence, given the current state s of the traffic condition, the mission of the agent G is to find the action a (change or keep current phase) that may lead to the maximum reward r in the long run, following the Bellman Equation (Equation 4) <ref type="bibr" target="#b20">[21]</ref>. In this situation, the action value function q for time t is the summation of the reward of the next timestamp t + 1 and the maximum potential future reward. Through this conjecture of future, the agent can select action that is more suitable for long-run reward.</p><formula xml:id="formula_3">Reward = w 1 * i∈I L i + w 2 * i∈I D i + w 3 * i∈I W i + w 4 * C + w 5 * N + w 6 * T.<label>(3)</label></formula><p>q(s t , a, t) = r a,t +1 + γ max q(s a,t +1 , a ′ , t + 1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Structure</head><p>In order to estimate the reward based on the state, and action, the agent needs to learn a Deep Q-Network Q(s, a).</p><p>In the real-world scenario, traffic is very complex and contain many different cases need to be considered separately. We will illustrate this in Example 4.1.</p><p>Example 4.1. We still assume a simple intersection with twophase light transition here: 1) Green-WE, and 2) Red-WE. The decision process of whether to change the traffic light consists of two steps. The first step is the mapping from traffic condition (e.g., how many cars are waiting, how long has each car been waiting) to a partial reward. An example of this mapping could be r = −0.5 × L − 0.7 × W. This is shared by different phases, no matter which lane the green light is on. Then, to determine the action, the agent should watch on the traffic in different lanes during different phases. For instance, as is shown in <ref type="figure" target="#fig_1">Figure 3 (a)</ref>, when the red light is on the NS direction, more waiting traffic (i.e., lower reward in the first step) on the NS direction will make the light tend to change (because by changing the light on this lane from red to green, more cars on this lane can pass through this intersection), while more waiting traffic (i.e., lower reward in the first step) on the WE direction will make the light tend to keep. When the red light is on the In previous studies, due to the simplified design of the model for approximating Q-function under complex traffic condition, agents are having difficulties in distinguishing the decision process for different phases. Therefore, we hereby propose a network structure that can explicitly consider the different cases explicitly. We call this special sub-structure "Phase Gate".</p><p>Our whole network structure can be shown as in <ref type="figure">Figure 5</ref>. The image features are extracted from the observations of the traffic condition and fed into two convolutional layers. The output of these layers are concatenated with the four explicitly mined features, queue length L, updated waiting time W, phase P and number of total vehicles V. The concatenated features are then fed into fully-connected layers to learn the mapping from traffic conditions to potential rewards. Then, for each phase, we design a separate learning process of mapping from rewards to the value of making decisions Q(s, a). These separate processes are selected through a gate controlled by the phase. As shown in <ref type="figure">Figure 5</ref>, when phase P = 0, the left branch will be activated, while when phase P = 1, the right branch will be activated. This will distinguish the decision process for different phases, prevent the decision from favoring certain action, and enhance the fitting ability of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Memory Palace and Model Updating</head><p>Periodically, the agent will take samples from the memory and use them to update the network. This memory is maintained by adding the new data samples in and removing the old samples occasionally. This technique is noted as experience replay <ref type="bibr" target="#b18">[19]</ref> and has been widely used in reinforcement learning models.</p><p>However, in the real traffic setting, traffic on different lanes can be really imbalanced. As previous methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22]</ref> store all the state-action-reward training samples in one memory, this memory will be dominated by the phases and actions that appear most frequently in imbalanced settings. Then, the agent will be learned to estimate the reward for these frequent phase-action combinations well, but ignore other less frequent phase-action combinations. This will cause the learned agent to make bad decisions on the infrequent phase-action combinations. Therefore, when traffic on different lanes are dramatically different, these imbalanced samples will lead to inferior performance on less frequent situation.</p><p>Inspired by Memory Palace theory <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> in cognitive psychology, we can solve this imbalance by using different memory palaces for different phase-action combinations. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, training samples for different phase-action combinations are stored into different memory palaces. Then same number of samples will be selected from different palaces. These balanced samples will prevent different phase-action combinations from interfering each other's training process, and hence, improve the fitting capability of the network to predict the reward accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>In this section, we conduct experiments using both synthetic and real-world traffic data. We show a comprehensive quantitative evaluation by comparing with other methods and also show some interesting case studies 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setting</head><p>The experiments are conducted on a simulation platform SUMO (Simulation of Urban MObility) 2 . SUMO provides flexible APIs for road network design, traffic volume simulation and traffic light control. Specifically, SUMO can control the traffic moving according to the given policy of traffic light (obtained by the traffic light agent).</p><p>The environment for the experiments on synthetic data is a fourway intersection as <ref type="figure">Figure 2</ref>. The intersection is connected with four road segments of 150-meters long, where each road have three incoming and three outgoing lanes. The traffic light in this part of experiment contains two phases: (1) Green-WE (green light on WE with red light on SN ), (2) Red-WE (red light on WE with green light on SN ). Note that when a green light is on one direction, there is a red light on the other direction. Also, a green light is followed by a 3-second yellow light before it turns to red light. Although this is a simplification of the real world scenario, the research of more types of intersections (e.g., three-way intersection), and more complex light phasing (e.g., with left-turn phasing) can be further conducted in similar way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parameter Setting</head><p>The parameter setting and reward coefficients for our methods are shown in <ref type="table" target="#tab_2">Table 2 and Table 3</ref> respectively. We found out that the action time interval ∆t has minimal influence on performance of our model as long as ∆t is between 5 seconds and 25 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation Metric</head><p>We evaluate the performance of different methods using the following measures:  In summary, a higher reward indicates a better performance of the method, and a smaller queue length, delay and duration indicates the traffic is less jammed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Compared Methods</head><p>To evaluate the effectiveness of our model, we compare our model with the following baseline methods, and tune the parameter for all methods. We then report their best performance.</p><p>• Fixed-time Control (FT ). Fixed-time control method use a pre-determined cycle and phase time plan <ref type="bibr" target="#b17">[18]</ref> and is widely used when the traffic flow is steady.</p><p>• Self-Organizing Traffic Light Control (SOTL) <ref type="bibr" target="#b4">[5]</ref>. This method controls the traffic light according to the current traffic state, including the eclipsed time and the number of vehicles waiting at the red light. Specifically, the traffic light will change when the number of waiting cars is above a hand-tuned threshold.</p><p>• Deep Reinforcement Learning for Traffic Light Control (DRL). Proposed in <ref type="bibr" target="#b21">[22]</ref>, this method applies DQN framework to select optimal light configurations for traffic intersections. Specifically, it solely relies on the original traffic information as an image. In addition to the baseline methods, we also consider several variations of our model.</p><p>• IntelliLight (Base). Using the same network structure and reward function defined as in Section 4.2 and 4.3. This method is without Memory Palace and Phase Gate.</p><p>• IntelliLight (Base+MP). By adding Memory Palace in psychology to IntelliLight-Base, we store the samples from different phase and time in separate memories.</p><p>• IntelliLight <ref type="figure">(Base+MP+PG)</ref>. This is the model adding two techniques (Memory Palace and Phase Gate).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Datasets</head><p>5.5.1 Synthetic data. In the first part of our experiment, synthetic data is used with four traffic flow settings: simple changing traffic (configuration 1), equally steady traffic (configuration 2), unequally steady traffic (configuration 3) and complex traffic (configuration 4) which is a combination of previous three configurations. As is shown in <ref type="table" target="#tab_4">Table 4</ref>, the arriving of vehicles are generated by Poisson distribution with certain arrival rates. 5.5.2 Real-world data. The real-world dataset is collected by 1,704 surveillance cameras in Jinan, China over the time period from 08/01/2016 to 08/31/2016. The locations of these cameras are shown in <ref type="figure" target="#fig_7">Figure 7</ref>. Gathered every second by the cameras facing towards vehicles near intersections, each record in the dataset consists of time, camera ID and the information about vehicles. By analyzing these records with camera locations, the trajectories of vehicles are recorded when they pass through road intersections. The dataset covers 935 locations, where 43 of them are four-way intersections. We use the number of vehicles passing through 24 intersections as traffic volume for experiments since only these intersections have consecutive data. Then we feed this real-world traffic setting into SUMO as online experiments. It can be seen from <ref type="table" target="#tab_5">Table 5</ref> that traffic flow on different roads are dynamically changing in the real world.    <ref type="table" target="#tab_6">Table 6</ref>, 7, 8 and 9 we can see that our method performs better than all other baseline methods in configurations 1, 2, 3 and 4. Although some baselines perform well on certain setting, they perform badly in other configurations (e.g., SOTL achieves good rewards under configuration 1, almost the same as our method in 3 digit floats. This is because our method has learned to keep the light until 36000 s and switch the light after that, and SOTL is also designed to behave similarly. Hence, these two methods perform very similar). On the contrary, our method IntelliLight shows better performance under different configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Performance on Synthetic Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.6.2</head><p>Comparison with variants of our proposed method. <ref type="table" target="#tab_6">Table  6</ref>, 7, 8 and 9 show the performance of variants of our proposed method. First, we can see that adding Memory Palace helps achieve higher reward under configuration 3 and 4, although it does not boost the reward under configuration 1 and 2. This is because for the simple case (configuration 1 and 2), the phase is relatively steady for a long time (because the traffic only comes from one direction or keeps not changing in a long time). Therefore, the memory palace does not help in building a better model for predicting the reward. Further adding Phase Gate also reduces the queue length in most cases and achieves highest reward, demonstrating the effectiveness of these two techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.3">Interpretation of learned signal.</head><p>To understand what our method have learned w.r.t. dynamic traffic conditions, we show the percentage of duration for phase Green-WE (i.e., green light on WE direction with red light on SN direction), along with the ratio of traffic flow on WE over total traffic flow from all directions. With the changing of traffic, an ideal traffic light control method would be able to adjust its phase duration to traffic flows and get high reward. For example, as traffic changes from direction WE to SN , the traffic light agent is expected to adjust its phase duration      <ref type="table" target="#tab_0">Table 10</ref>. Our method IntelliLight achieves the best reward, queue length, delay and duration over all the compared methods, with a relative improvement of 32%, 38%, 19% and 22% correspondingly over the best baseline method. In addition, our method has a relatively steady performance over multiple intersections (small standard deviation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.7.2</head><p>Observations with respect to real traffic. In this section, we make observations on the policies we learned from the real data. We analyze the learned traffic light policy for the intersection of Jingliu Road (WE direction) and Erhuanxi Auxiliary Road (SN direction) under different scenarios: peak hours vs. non-peak hours, weekdays vs. weekends, and major arterial vs. minor arterial.</p><p>1. Peak hour vs. Non-peak hour. <ref type="figure">Figure 9</ref> (a) shows the average traffic flow from both directions (WE and SN ) on a Monday. On this day, there is more traffic on WE direction than SN for most of the time, during which an ideal traffic light control method is expected to give longer time for WE direction. It can be seen from <ref type="figure">Figure 9</ref> (c) that, the ratio of the time duration for phase Green-WE (i.e., green light on WE, while red light on SN ) is usually larger than 0.5, which means for most of the time, our method gives longer time for WE. And during peak hours (around 7:00, 9:30 and 18:00), the policies learned from our method also give longer time for green light on WE than non-peak hours. In early morning, the vehicle arrival rates on SN are larger than the rates on WE, and our method automatically gives longer time to SN . This shows our method can intelligently adjust to different traffic conditions.</p><p>2. Weekday vs. Weekend. Unlike weekdays, weekend shows different patterns about traffic condition and traffic light control policies. Our policy gives less green light on WE (more green light on SN ) during weekend daytime than it gives on weekday. This is because there is more traffic on SN than on WE during weekend daytime in <ref type="figure">Figure 9</ref> (b), while during weekday traffic on SN is less than on WE. Besides, by comparing <ref type="figure">Figure 9</ref> (a) with <ref type="figure">Figure 9</ref> (b), we can see that the traffic of WE and SN during late night time on Monday is similar, making the ratio of duration Green-We close to 0.5.  3. Major arterial vs. Minor arterial. Major arterials are roads that have higher traffic volume within a period, and are expected to have a longer green light time. Without prior knowledge about major arterial, learned traffic light control policy using our method prefer giving the major arterial green light (including keeping the green light already on major arterial, and tend to switching red light to green light for major arterial). Specifically, we look into three periods of time (3:00, 12:00 and 23:30) of August 1st. From <ref type="figure">Figure 9</ref> (a), we can tell that the road on WE direction is the main road, since traffic on WE is usually heavier than traffic on SN . As is shown in <ref type="figure" target="#fig_0">Figure 10</ref>, the dotted lines indicates the number of arriving cars for every second on two different directions. Along with the arrival rate, we also plot the change of phases (dashed area). It can be seen from <ref type="figure" target="#fig_0">Figure 10</ref> (a) that: 1) the overall time period of phase Red-WE is longer than Green-WE, which is compatible with traffic volume at this time. 2) although the traffic volume of SN is larger than WE, the traffic light change from Green-WE to Red-WE is usually not triggered by waiting cars on SN direction. On the contrary, in <ref type="figure" target="#fig_0">Figure 10</ref> (b) and <ref type="figure" target="#fig_0">Figure 10</ref> (c), the change from Green-WE to Red-WE is usually triggered by waiting cars on SN direction. This is mainly because the road on WE is the main road during these time periods, and the traffic light tends to favor phase Green-WE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we address the traffic light control problem using a well-designed reinforcement learning approach. We conduct extensive experiments using both synthetic and real world experiments and demonstrate the superior performance of our proposed method over state-of-the-art methods. In addition, we show in-depth case studies and observations to understand how the agent adjust to the changing traffic, as a complement to quantitative measure on rewards. These in-depth case studies can help generate traffic rules for real world application.</p><p>We also acknowledge the limitations of our current approach and would like to point out several important future directions to make the method more applicable to real world. First, we can extend the two-phase traffic light to multi-phase traffic light, which will involve more complicated but more realistic state transition. Second, our paper addresses a simplified one intersection case, whereas the real world road network is much more complicated than this. Although some studies have tried to solve the multi-intersection problem by using multiple reinforcement learning agents, they do not explicitly consider the interactions between different intersections (i.e., how can the phase of one intersection affect the state of nearby intersections) and they are still limited to small number of intersections. Lastly, our approach is still tested on a simulation framework and thus the feedback is simulated. Ultimately, a field study should be conducted to learn the real-world feedback and to validate the proposed reinforcement learning approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Deep reinforcement learning framework for traffic light control.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Case A and case B have the same environment except the traffic light phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Model framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For each lane i at this intersection, the state component includes queue length L i , number of vehicles V i , updated waiting time of vehicles W i . In addition, the state includes an image repre- sentation of vehicles' position M, current phase P c and next phase P n . • Action. Action is defined as a = 1: change the light to next phase P n , and a = 0: keep the current phase P c . • Reward. As is shown in Equation 3, reward is defined as a weighted sum of the following factors: (1) Sum of queue length L over all approaching lanes, where L is calculated as the total number of waiting vehicles on the given lane. A vehicle with a speed of less than 0.1 m/s is considered as waiting. (2) Sum of delay D over all approaching lanes, where the delay D i for lane i is defined in Equation 1, where the lane speed is the average speed of vehicles on lane i, and the speed limit is the maximum speed allowed on lane i:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5: Q-network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Memory palace structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5. 6 .</head><label>6</label><figDesc>1 Comparison with state-of-the-art methods. We first com- pare our method with three other baselines under different synthetic</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Traffic surveillance cameras in Jinan, China</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Percentage of the time duration of learned policy for phase Green-WE (green light on W-E and E-W direction, while red light on N-S and S-N direction) in every 2000 seconds for different methods under configuration 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Detailed average arrival rate on two directions (dotted lines) and changes of two phases (dashed areas) in three periods of time for Jingliu Road (WE) and Erhuanxi Auxiliary Road (SN ) in Jinan on August 1st, 2016. X-axis of each figure indicates the time of a day; left Y-axis of each figure indicates the number of cars approaching the intersection every second; right Y-axis for each figure indicates the phase over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Notations 

Notation 
Meaning 

E 
Environment 
G 
Agent 
a 
Action 
s 
State 
r 
Reward 
∆t 
Time interval between actions 
q 
Action value function 
Q 
Deep Q-Network 
L i 
Queue length on the lane i 
V i 
Number of vehicles on the lane i 
W i 
Updated waiting time of all vehicles on the lane i 
D i 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Settings for our method</head><label>2</label><figDesc></figDesc><table>Model parameter 
Value 

Action time interval ∆t 
5 seconds 
γ for future reward 
0.8 
ϵ for exploration 
0.05 
Sample size 
300 
Memory length 
1000 
Model update interval 
300 seconds 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Reward coefficients 

w1 
w2 
w3 
w4 
w5 
w6 

-0.25 
-0.25 
-0.25 
-5 
1 
1 

• Reward: average reward over time. Defined in Equation 3, 
the reward is a combination of several terms (positive and 
negative terms), therefore, the range of reward is from −∞ 
to ∞. Under specific configuration, there will be an upper 
bound for the reward when all cars move freely without any 
stop or delay. 
• Queue length: average queue length over time, where the 
queue length at time t is the sum of L (defined in Section 4.2) 
over all approaching lanes. A smaller queue length means 
there are fewer waiting vehicles on all lanes. 
• Delay: average delay over time, where the delay at time t 
is the sum of D (defined in Equation 1) of all approaching 
lanes. A lower delay means a higher speed of all lanes. 
• Duration: average travel time vehicles spent on approach-
ing lanes (in seconds). It is one of the most important mea-
sures that people care when they drive on the road. A smaller 
duration means vehicles spend less time passing through the 
intersection. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Configurations for synthetic traffic data</head><label>4</label><figDesc></figDesc><table>Config Directions 
Arrival rate 
(cars/s) 

Start time 
(s) 

End time 
(s) 

1 
WE 
0.4 
0 
36000 
SN 
0.4 
36001 
72000 

2 
WE 
0.033 
0 
72000 
SN 
0.033 
0 
72000 

3 
WE 
0.2 
0 
72000 
SN 
0.033 
0 
72000 

4 

Configuration 1 
0 
72000 
Configuration 2 
72001 
144000 
Configuration 3 
144001 
216000 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Details of real-world traffic dataset</head><label>5</label><figDesc></figDesc><table>Time Range 
Records 
Arrival Rate (cars/s) 
Mean Std 
Max Min 

08/01/2016 -
08/31/2016 
405,370,631 0.089 0.117 0.844 0.0 

traffic settings. From </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 : Performance on configuration 1. Reward: the higher the better. Other measures: the lower the better. Same with the following tables.</head><label>6</label><figDesc></figDesc><table>Model name Reward Queue length Delay Duration 

FT 
-2.304 
8.532 
2.479 
42.230 
SOTL 
0.398 
0.006 
1.598 
24.129 
DRL 
-36.247 
91.412 
4.483 
277.430 

IntelliLight (ours) 
Base 
-3.077 
10.654 
2.635 
92.080 
Base+MP 
-3.267 
6.087 
1.865 
38.230 
Base+MP+PG 
0.399 
0.005 
1.598 
24.130 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Performance on configuration 2 

Model name Reward Queue length Delay Duration 

FT 
-0.978 
1.105 
2.614 
34.278 
SOTL 
-21.952 
19.874 
4.384 
177.747 
DRL 
-2.208 
3.405 
3.431 
52.075 

IntelliLight (ours) 
Base 
-0.523 
0.208 
1.689 
27.505 
Base+MP 
-0.556 
0.259 
1.730 
27.888 
Base+MP+PG -0.514 
0.201 
1.697 
27.451 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Performance on configuration 3 

Model name Reward Queue length Delay Duration 

FT 
-1.724 
4.159 
3.551 
36.893 
SOTL 
-20.680 
20.227 
5.277 
69.838 
DRL 
-8.108 
16.968 
4.704 
66.485 

IntelliLight (ours) 
Base 
-0.836 
0.905 
2.699 
28.197 
Base+MP 
-0.698 
0.606 
2.729 
26.948 
Base+MP+PG -0.648 
0.524 
2.584 
26.647 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>Performance on configuration 4 

Model name Reward Queue length Delay Duration 

FT 
-1.670 
4.601 
2.883 
39.707 
SOTL 
-14.079 
13.372 
3.753 
54.014 
DRL 
-49.011 
91.887 
4.917 
469.417 

IntelliLight (ours) 
Base 
-5.030 
5.880 
3.432 
39.021 
Base+MP 
-3.329 
5.358 
2.238 
44.703 
Base+MP+PG -0.474 
0.548 
2.202 
25.977 

from giving WE green light to giving SN green light. As we can 
see from Figure 8, IntelliLight can adjust its phase duration as the 
traffic changes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 10 : Performances of different methods on real-world data. The number after ± means standard deviation. Reward: the higher the better. Other measures: the lower the better.</head><label>10</label><figDesc></figDesc><table>Methods 
Reward 
Queue Length 
Delay 
Duration 

FT 
-5.727 ± 5.977 
19.542 ± 22.405 3.377 ± 1.057 84.513 ± 60.888 
SOTL 
-35.338 ± 65.108 16.603 ± 17.718 4.070 ± 0.420 64.833 ± 23.136 
DRL 
-30.577 ± 26.242 54.148 ± 43.420 4.209 ± 1.023 166.861 ± 93.985 
IntelliLight 
-3.892 ± 7.609 10.238 ± 20.949 2.730 ±1.086 50.487 ± 46.439 

7:00 
9:30 
18:00 

Early morning 

Noon time 

Late night 

10:00 
17:30 

Weekend morning 

Weekend daytime 

Late night 

(a) Average arrival rate of August 1st (Monday) 
(b) Average arrival rate of August 7th (Sunday) 

7:00 
9:30 
18:00 

Early morning 
Noon time 
Late night 

Weekend daytime 

Late night 

10:00 
17:30 

Weekend morning 

(c) Phase time ratio from learned policy on August 1st (Monday) 
(d) Phase time ratio from learned policy on August 7th (Sunday) 

Figure 9: Average arrival rate on two directions (WE and SN ) and time duration ratio of two phases (Green-WE and Red-WE) 
from learned policy for Jingliu Road (WE) and Erhuanxi Auxiliary Road (SN ) in Jinan on August 1st and August 7th, 2016. 

Hour 

Switch to Green-WE 

Keep Green-WE 

Hour 

(a) 
Early morning when traffic on 
WE-EW is less than SN -NS 
(b) 
Noon when traffic on 
WE-EW is more than SN -NS 
(c) 
Late night when traffic on 
WE-EW is more than SN -NS 

</table></figure>

			<note place="foot" n="1"> Codes are available at the author&apos;s website. 2 http://sumo.dlr.de/index.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported in part by NSF awards #1544455, #1652525, #1618448, and #1639150. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Holonic multiagent system for traffic signals control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monireh</forename><surname>Abdoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Mozayani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana Lc</forename><surname>Bazzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1575" to="1587" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reinforcement learning for true adaptive traffic signal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baher</forename><surname>Abdulhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Pringle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigoris J</forename><surname>Karakoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Transportation Engineering</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="page" from="278" to="285" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reinforcement learningbased multi-agent system for network traffic signal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Arel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Urbanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Intelligent Transport Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Traffic light control by multiagent reinforcement learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Kester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interactive Collaborative Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="475" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-organizing traffic lights: A realistic simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung-Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Cools</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart D&amp;apos;</forename><surname>Gershenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hooghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in applied self-organizing systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Comparison of delay estimates at under-saturated and over-saturated pre-timed signalized intersections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Dion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hesham</forename><surname>Rakha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn-Soo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part B: Methodological</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="99" to="122" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The cost of traffic jams</title>
		<ptr target="https://www.economist.com/blogs/economist-explains/2014/11/economist-explains-1" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning for integrated network of adaptive traffic signal controllers (MARLIN-ATSC): methodology and large-scale application on downtown Toronto</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samah</forename><surname>El-Tantawy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baher</forename><surname>Abdulhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossam</forename><surname>Abdelgawad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1140" to="1150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minoru</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norio</forename><surname>Shiratori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02755</idno>
	</analytic>
	<monogr>
		<title level="m">Deep Reinforcement Learning Algorithm with Experience Replay and Target Network</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Using a deep reinforcement learning agent for traffic signal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Genders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saiedeh</forename><surname>Razavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01142</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Godwin-Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Emerging technologies.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Traffic Congestion Costs Americans $124 Billion A Year</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Guerrini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Report Says. Forbes</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiagent reinforcement learning for urban traffic control using coordination graphs. Machine learning and knowledge discovery in databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Kuyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="656" to="671" />
		</imprint>
	</monogr>
	<note>Bram Bakker, and Nikos Vlassis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building a memory palace in minutes: Equivalent memory performance using virtual versus conventional environments with the Method of Loci</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Legge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enoch</forename><forename type="middle">T</forename><surname>Christopher R Madan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><forename type="middle">B</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta psychologica</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="380" to="390" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Traffic signal timing via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="247" to="254" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cooperative Deep Reinforcement Learning for Tra ic Signal Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An experimental review of reinforcement learning algorithms for adaptive traffic signal control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enda</forename><surname>Howley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Autonomic Road Transport Support Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="47" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Settings for fixed-cycle traffic signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Operational Research Society</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="373" to="386" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adaptive look-ahead optimization of traffic signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Porche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lafortune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Transportation System</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="209" to="254" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Coordinated Deep Reinforcement Learners for Traffic Light Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elise</forename><surname>Van Der Pol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oliehoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Traffic signal settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Webster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Road Research Technical Paper</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning for traffic light control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning: Proceedings of the Seventeenth International Conference</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1151" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
