<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Locality-Aware Software Throttling for Sparse Matrix Operation on GPUs Locality-Aware Software Throttling for Sparse Matrix Operation on GPUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 11-13. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">B</forename><surname>Hayes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Salmon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">B</forename><surname>Hayes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Salmon</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><forename type="middle">Z</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chi Zhang</orgName>
								<orgName type="institution" key="instit1">Rutgers University</orgName>
								<orgName type="institution" key="instit2">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit3">Rutgers University</orgName>
								<orgName type="institution" key="instit4">Rutgers University</orgName>
								<orgName type="institution" key="instit5">Rutgers University</orgName>
								<orgName type="institution" key="instit6">University of Pittsburgh</orgName>
								<orgName type="institution" key="instit7">Rutgers University</orgName>
								<orgName type="institution" key="instit8">Rutgers University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Locality-Aware Software Throttling for Sparse Matrix Operation on GPUs Locality-Aware Software Throttling for Sparse Matrix Operation on GPUs</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18)</title>
						<meeting>the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18) <address><addrLine>Boston, MA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 11-13. 2018</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 2018 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc18/presentation/chen-yanhao This paper is included in the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper tackles the cache thrashing problem caused by the non-deterministic scheduling feature of bulk synchronous parallel (BSP) execution in GPUs. In the BSP model, threads can be executed and interleaved in any order before reaching a barrier synchronization point, which requires the entire working set to be in cache for maximum data reuse over time. However, it is not always possible to fit all the data in cache at once. Thus, we propose a locality-aware software throttling framework that throttles the number of active execution tasks, prevents cache thrashing, and enhances data reuse over time. Our locality-aware software throttling framework focuses on an important class of applications that operate on sparse matrices (graphs). These applications come from the domains of linear algebra, graph processing, machine learning and scientific simulation. Evaluated on over 200 real sparse matrices and graphs that suffer from cache thrashing in the Florida sparse matrix collection, our technique achieves an average of 2.01X speedup, a maximum of 6.45X speedup, and a maximum performance loss ≤5%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Operations on sparse matrix and graph are important for solving linear algebra and optimization problems that arise in data science, machine learning, and physicsbased simulation. In this paper we focus on a fundamental sparse matrix operation that relates an input vector x with an output vector y. Let x be an n × 1 vector, y be an m × 1 vector, and A be a m × n matrix, the relation between y and x is defined as y = Ax, where y i = reduce op{A ik x k }, 1 ≤ k ≤ n.</p><p>When the operator reduce op is sum and the binary operator is multiplication, the operation is sparse ma- * First authors Chen and Hayes have made equal contributions to this work and are listed alphabetically.</p><p>trix vector multiplication <ref type="bibr">(SpMV)</ref>. When the operator reduce op is min and the binary operator is +, the operation is an iterative step in the single source shortest path (SSSP) problem <ref type="bibr" target="#b12">[13]</ref>. An example is shown in <ref type="figure">Figure 1</ref>. However, poor data reuse is often a problem when running sparse applications on GPUs. Throttling is a useful technique to improve data reuse. Unlike other locality enhancement techniques that focus on spatial data reuse on many-core, for instance, the memory coalescing techniques <ref type="bibr" target="#b33">[33]</ref>, throttling improves data reuse over time by limiting the number of actively executed tasks.</p><p>Throttling prioritizes the execution of the tasks that reuse the data in the cache over those that do not reuse the data in the cache. <ref type="figure">Figure 2</ref> shows an example of how throttling improves cache data reuse. Assuming the cache capacity is 4, in the original case, the cache cannot hold all the data elements in the execution list which will inevitably cause cache (capacity) misses. Throttling helps by dividing the execution into two phases and scheduling one phase after another. Data elements in each phase can now fit into cache and be fully reused so that no cache (capacity) misses will occur.</p><p>Throttling for GPU has been studied extensively in the hardware context. Rogers and others <ref type="bibr" target="#b26">[27]</ref> discovered that limiting the number of active wavefronts (warps) enhances cache data reuse over time and alleviates cache thrashing. The DYNCTA framework <ref type="bibr" target="#b18">[19]</ref> limits the number of CTAs for memory intensive applications and results in better cache performance. The work by Chen and others <ref type="bibr" target="#b7">[8]</ref>  Figure 2: Throttling Example namic warp-throttling technique to improve both cache performance and energy efficiency. However, all these prior throttling techniques on GPUs have been developed as hardware modifications.</p><p>In this paper, we present a software throttling framework that targets irregular applications operating on sparse data. Our software throttling framework will first divide the entire workload into multiple partitions such that the working set of each partition fits into the cache and the data communication between different partitions is minimum (we will refer to each partition as cache-fit partition or cache-fit work group throughout this paper). Then we schedule the cache-fit partitions and let each of them be processed independently to ensure throttling.</p><p>There are three main challenges for realizing software throttling. First, the traditional work partition models focus on minimizing data reuse among different partitions with load-balancing constraints <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">29]</ref>. However, cache-fit work partitioning is not necessarily loadbalanced, it should be data-balanced across different partitions. Second, inappropriate scheduling of cache-fit partitions might result in low execution pipeline utilization. For each of the cache-fit partitions that have low data reuse, there may not be enough tasks running concurrently, which will make the execution pipeline units not fully utilized and degrade the computation throughput. Last, reducing the overhead of software throttling is important and yet challenging, especially for finding minimum communication cache-fit partitions, which is the most time-consuming step in software throttling.</p><p>To tackle these challenges, we propose the three following techniques. To obtain cache-fit partitions, we develop an efficient data-balanced work partition model. Our partition model can balance data while minimizing the communication cost among different partitions. We also introduce a split-join scheduling model to take advantage of the trade-off between throttling and throughput. The split-join scheduling model adaptively merges partitions to avoid low execution pipeline utilization and/or use a concurrent queue based implementation for relaxed barrier synchronization. We reduce the partition overhead by a coarse-grained partition model which was built upon a multi-level partition paradigm. Instead of partitioning the original work matrix (graph), our model partitions a coarsened matrix (graph) which can significantly reduce the partition overhead while maintaining similarly good partition quality.</p><p>Our throttling technique is a pure software based implementation. It is readily deployable and highly efficient. Evaluated over 228 sparse matrices and graphs from Florida matrix collection <ref type="bibr" target="#b10">[11]</ref> -the set of matrices which suffer from cache thrashing (their working set cannot entirely fit into the L2 cache on the Maxwell GPU and Pascal GPU we tested), our software throttling method can achieve an average 2.01X speedup (maximal 6.45X speedup).</p><p>As far as we know, this is the first work that systematically investigates software throttling techniques for GPUs and is extensively evaluated on real sparse matrices and graphs. The contribution and the outline of our paper is summarized as follows:</p><p>• We introduce an analytical model named databalanced work partition for locality-aware software throttling. Efficient heuristics are developed to achieve (near-)minimum communication cache-fit work partitions that can be further scheduled to alleviate GPU cache thrashing (Section 2).</p><p>• We exploit the trade-off between cache locality and execution pipeline utilization and provide a set of practical cache-fit work group scheduling policies based on adaptive merging and concurrent dequeuing. We discuss the advantages/disadvantages, the applicability, and the effectiveness of each scheduling policy in different settings. (Section 3).</p><p>• Our method requires no hardware modification. It is low overhead and readily deployable. We introduce efficient overhead control mechanisms for graph(matrix)-based work partition. (Section 4).</p><p>• We conduct a comprehensive data analysis for over 200 large real sparse matrices(graphs). Our framework in particular works well for the set of sparse matrices that have large working sets and suffer from high GPU cache contention (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data-Balanced Work Partition</head><p>Our software throttling framework first divides the entire workload into cache-fit partitions. A cache-fit partition's working set fits into the cache such that it will not cause any cache capacity miss. This section presents the concept and methodology of data-balanced work partition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Representation</head><p>In this paper, we focus on a fundamental operation in sparse linear algebra and optimization applications. It is defined as follows. Assume we have an m × n matrix A, an n × 1 vector x, and an m × 1 vector y such that:</p><formula xml:id="formula_0">y i = reduce op{A ik x k }, 1 ≤ k ≤ n<label>(1)</label></formula><p>The operator is a binary operator, and the operator reduce op is a reduction operator. When is product × and reduce op is sum, the operation is a sparse matrix vector multiplication (SpMV). When is plus + and reduce op is min, the operation is a min/product step in the single source shortest path (SSSP) problem.</p><p>We represent a computation unit as a 2-tuple (x j , y i ) which represents (1) one binary operation between x j and A i j , and (2) one step in the reduction operation reduce op for obtaining y i . We only focus on vector x and y, since the matrix entries will be used only once in Equation (1).</p><p>We represent the entire workload as a 2-tuple list. Using a graph representation, each data element in the 2-tuple is modeled as a vertex and each tuple is modeled as an edge that connects the corresponding two vertices. Performing a work partition is essentially performing an edge partition on the graph, as illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data-Balanced v.s. Load-Balanced</head><p>We formally define the data-balanced work partition model. The input is a list of 2-tuple modeled as a work graph and the output is a set of minimum-interaction work partitions such that the number of unique vertices, which represent data elements, in every work partition is less than or equal to the cache capacity.</p><p>In contrast to prior load-balanced work partition, we perform data-balanced work partition. We denote this problem as a Vertex-balanced Edge-Partition (V-EP) model and we give the definition below:</p><formula xml:id="formula_1">Definition 2.1. Vertex-balanced Edge-Partition (V-EP) Problem</formula><p>Given a graph G = (V, E) with the set of vertices V and the set of edges E, and vertex capacity constraint T . Let x ={e 1 , e 2 , ...e k } denote a partition of the edges of G into k disjoint subsets, and let V (e i ) denote the set of unique vertices in e i . ∀n ∈ V , let P(n) denote the number of subsets that n's incident edges fall into. We optimize the total vertex replication cost:</p><formula xml:id="formula_2">minimize x R(x) = ∑ n∈V (P(n) − 1) subject to ∀i ∈ [1..k], |V (e i )| ≤ T (2)</formula><p>In prior work, the Edge-balanced Edge-Partition (E-EP) problem has been well studied particularly in the distributed graph processing setting <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> and also for balancing workloads in GPU <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. However, the V-EP problem is not. Both the V-EP and E-EP problems minimize vertex replication cost, while the E-EP model aims to balance the load among processors in space, and the V-EP model aims to alleviate cache thrashing and maximize data reuse over time. We use an example in <ref type="figure" target="#fig_1">Figure 3</ref> to illustrate the difference between the V-EP work partition and the E-EP work partition. Assuming the cache capacity is 4, <ref type="figure" target="#fig_1">Figure  3</ref> (a) shows a 2-way V-EP work partition: one partition has 4 edges and the other has 2, the unique vertices of both (4 vertices) fit into cache. <ref type="figure" target="#fig_1">Figure 3 (b)</ref> shows another 2-way E-EP partition: Each partition has 3 edges, however partition 2 has 6 unique vertices and do not fit into cache. Thus the E-EP model might exacerbate rather than alleviate the cache thrashing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Partition Framework</head><p>We propose a data-balanced work partition framework that ensure the working set of each partition is of the same size and in the meantime the data reuse across different partitions is reduced as much as possible.</p><p>Our partition framework is a recursive bisection framework. Bisection is a 2-way balanced edge partition that ensures minimum vertex replica between two equalsize edge partitions. The optimal bisection is a well studied problem <ref type="bibr" target="#b24">[25]</ref>. We take the advantage of the bisection method and perform hierarchical partitioning.</p><p>During the recursive partition process, the framework bisects a sub-graph that has more unique vertices than specified by the capacity constraint. It keeps bisecting until no such sub-graph exists.</p><p>We use a tree data structure to keep track of the obtained sub-graphs. Starting from the root node that represents the entire work graph, the framework bisects the corresponding graph and generates two children nodes: each of the child nodes corresponds to a sub-graph that contains half of the edges from the parent node. If either or both children nodes violate the capacity constraint, either or both will be added to the list of sub-graphs that need to be further bisected. The process repeats until all leaf nodes become cache-fit work partitions.</p><p>The detailed algorithm is listed below in Algorithm 1. The data-balanced work partition (DBWP) procedure takes the work graph G and the cache capacity constraint T as input, and generates a set of cache-fit partitions P as output. The bisect function in Algorithm 1 we adopted is based on the best existing balanced edge partition algorithm named SPAC <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b23">24]</ref> by Li and others. We will discuss the implementation details and the overhead control mechanisms of the bisect function in Section 4.</p><formula xml:id="formula_3">Algorithm 1 Data-Balanced Work-Partition (DBWP) Input: work graph G, cache capacity T Output: cache-fit partition set P 1: procedure DBWP(G, T , P) 2: if |G.data elements| &gt; T then 3: (lchild, rchild) = bisect(G) 4: DBWP(lchild, T , P) 5:</formula><p>DBWP(rchild, T , P)</p><p>6:</p><formula xml:id="formula_4">else 7:</formula><p>add G to P 8:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>end if 9: end procedure</head><p>We use an example to illustrate the DBWP procedure in <ref type="figure">Figure 4</ref>. In this example, the graph has 8 edges and 8 vertices, and the cache capacity constraint is 4. Performing bisect for the sub-graph represented by the tree root node, we obtain two sub-graphs each of which has 4 edges. The vertex replica cost is optimum: 2, since two nodes y 2 and x 2 appear in both partitions. </p><formula xml:id="formula_5">{(x1, y1), (x1, y2), (x2, y1), (x2, y2), (x2, y3), (x3, y3), (x4, y2), (x4, y4)} AllTuples = } { } { } { y1 x1 y2 y3 y4 x2 x3 x4</formula><p>Partition A Partition B Partition C</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Hierarchical Bisection Example</head><p>The first sub-graph A in <ref type="figure">Figure 4</ref> (a) has 4 unique vertices and does not violate the capacity constraint, we do not perform further bisection on sub-graph A. The other sub-graph, however, has 6 unique vertices and does not fit into the cache. Therefore we perform the second bisection and obtain partitions B and C where the vertex replica cost is optimum (0 in this case). At this point, there is no sub-graph that does not fit into the cache, therefore we terminate the bisection process. The tree representation is shown in <ref type="figure">Figure 4</ref> (b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Cache-Fit Partitions Scheduling</head><p>DBWP model outputs a set of cache-fit partitions. All these partitions need to be processed independently to minimize cache-thrashing interference. However, naive scheduling of these partitions might result in low execution pipeline utilization. In this section, we introduce four different Cache-Fit Partitions Scheduling methods: Cache-Fit Scheduling (CF), Cache-Fit Queue Scheduling (CF-Q), Split-Join Scheduling (SJ) and Split-Join Queue Scheduling (SJ-Q).</p><p>CF works well when all cache-fit partitions have high data reuse. SJ is good for cases when the sparsity structure is already known, for instance, pruned deep leaning neural networks. Both CF-Q and SJ-Q can loosely enforce throttling and provide a better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cache-Fit Scheduling</head><p>A straightforward way to isolate the computation of different cache-fit partitions is to assign each partition a single kernel function and execute these kernels one by one. A kernel is a function that is executed on GPU. All threads within a GPU kernel will need to finish before the entire kernel complete -there is a strict barrier between different GPU kernels. Moreover, between two consecutive kernels, the data in the cache will be invalidated.</p><p>Here, we propose CF which separates the original kernel functions into multiple kernels, while the number of which is determined by the number of cache-fit partitions given by DBWP model. The code of the kernel function for each cache-fit partition is the same. The only difference is the input to each kernel. CF ensures that the data in the cache is fully reused before it was evicted from the cache within each cache-fit partition.  We show the idea of CF in <ref type="figure" target="#fig_4">Figure 5</ref>. The input 2-tuple task list T L is split into k 2-tuple lists T L , which corresponds to each of the cache-fit partitions. Each new tuple list will be processed by a single kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cache-Fit Queue Scheduling</head><p>CF makes sure that each cache-fit partition will be processed by one kernel. Although CF can provide good throttling performance for a lot of matrices, this scheduling method may cause low execution pipeline utilization for the type of matrices whose data reuse is low. For example, for a given cache size T and average data reuse ratio r for a cache-fit partition, the total work is T * r/2 using our work graph model. The variable T is fixed for a given architecture. If r is low, the number of concurrent tasks in one cache-fit partition p is low and may not keep the execution pipeline busy.</p><p>To avoid this problem, we propose CF-Q, which processes the whole tuple list in a single kernel instead of one invocation per cache-fit partition. However, using a single kernel means that elements in one cache-fit partition have no guarantee to be executed without any interference. To enable throttling, we set up a FIFO queue before launching the kernel. Each queue entry corresponds to a chunk of tuples so that adjacent chunks are from the same cache-fit partition. A warp automatically fetches a chunk from the queue and process the tuples from that chunk. We show an example of how CF-Q works in  Unlike CF which has explicit barriers to strictly enforce the independent execution of different cache-fit partitions, CF-Q uses no barrier. It is possible that the last chunk in one partition and the first chunk in the next partition are fetched within a very short time period. In <ref type="figure" target="#fig_6">Figure 6</ref>, chunk 1 and chunk 2 from partition 2 will be running concurrently with chunk N from partition 1. However, CF-Q can still provide relaxed barriers between different partitions since chunks from the same cache-fit partition in the queue will always be retrieved in consecutive time periods so that no following partitions can be executed before previous one starts. The pseudo code of CF-Q is provided in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Split-Join Scheduling</head><p>Split-Join (SJ) is another method that exploits the tradeoff between locality and execution pipeline utilization. SJ dynamically merges the cache-fit partitions that has low data reuse or combines low data reuse partitions with a high data reuse partition that is less likely to be interfered. SJ first constructs the tree structure that represents the hierarchical cache-fit partitions which we discussed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Cache-Fit Queue Scheduling</head><p>Input: cache-fit partition set P 1: procedure CF-Q(P) <ref type="bibr">2:</ref> for each partition p in P do end while 12: end procedure in Section 2.3, we will refer to this as SJ-tree. SJ merges sibling nodes in the SJ-tree conditionally in a bottomup manner. We only consider recombining sibling nodes as the sibling nodes have better data sharing than nonsibling nodes and the merging is logarithmic time.</p><p>SJ is performed with a fast online profiling process. We define a profiling pass as a profiling of all the nodes at one level of the SJ-tree which comprises one traversal of the entire work graph. So, the entire profiling process will take d profiling passes, where d is the depth of the SJ-tree. It takes at least log(k) and up to k profiling passes for any given SJ-tree, where k is the number of leaf nodes in the tree. The lower bound log(k) is reached when the binary tree is balanced and has log(k) levels. Moreover, in the worst case scenario, when the tree is not balanced and at every level there is at most one leaf node, k profiling passes are needed. We run every work partition that corresponds to a leaf node in the SJ-tree in stand-alone mode and record the running time.</p><p>We use the first d iterations of the linear algebra and optimization applications to collect information for profiling. Since those applications we tested take between 50 and 22,000 iterations to converge, the overhead of profiling can be amortized. For example, G3 circuit need 5 iterations for profiling, and the total profiling time for Conjugate Gradient (CG) solver takes 0.017 s. The running time for CG with 22824 iterations is 94.331 s which gives us 0.018% profiling overhead. In practice, among all the matrices we used in the experiment, we found that the SJ-tree had at most 8 levels and thus required at most 8 passes for profiling.</p><p>The tree node merging problem can be defined as a tree-based weighted set cover problem. Merging two sibling nodes is as if choosing their parent node. The problem becomes how to find a subset of tree nodes P that will cover all possible cache-fit partitions (leaf nodes) while minimizing the overall running time:</p><formula xml:id="formula_6">minimize ∑ x∈P c(x) subject to x∈P S(x) = L (3)</formula><p>where L is the set of all leaf nodes, P is the subset of the tree nodes (both leaf and non-leaf nodes) we want to find, c(x) is a cost function that denotes the standalone running time of node x and S(x) is a set function that returns all leaf nodes of the subtree under node x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3 Tree Recombination</head><p>Input: SJ-tree root Output: optimal running time of SJ-tree root 1: procedure TREERECOMB(root)</p><note type="other">2: return BESTCONFIG(root) 3: end procedure 4: procedure BESTCONFIG(r) 5: le f t t = BESTCONFIG(r.leftChild) 6: right t = BESTCONFIG(r.rightChild) 7: this node t = r.stime 8: r.btime = min(le f t t + right t, this node t) 9: return r.btime 10: end procedure</note><p>We develop a linear time algorithm that is capable of finding the optimum solution for the tree-based set cover problem. The algorithm processes the tree in posttopological order. Every node is associated with an attribute btime and an attribute stime. A sub tree's optimum time btime (annotated as an attribute of its root node) is the minimum of the two items: its root node's standalone running time stime and the summation of its two children subtree's btime. For a leaf node, its btime is the same as its standalone running time stime. The pseudo code is provided in Algorithm 3 together with an example in <ref type="figure" target="#fig_9">Figure 7</ref>. This process identifies the best set cover of the SJ-tree and determines how to recombine cache-fit partitions into every GPU kernel. SJ can achieve high execution pipeline utilization without sacrificing cache benefits (as data reuse is low for these low pipeline utilization cases).  partitions, SJ cannot guarantee the execution order of those cache-fit partitions inside the merged partitions. We propose SJ-Q which uses the idea of CF-Q that places cache-fit partitions in one merged work group (kernel) into a queue and each kernel will be using one independent queue. SJ-Q can provide both strict barriers between different merged partitions and also relaxed barriers between cache-fit partitions from the same merged partition. In the mean time, it inherits the advantage of SJ that avoids low execution pipeline utilization. Summary CF enforces strict barriers between different cache-fit partitions to ensure throttling. However, low execution pipeline utilization may happen which can degrade the computation performance; CF-Q uses a queue based method that can fully utilize the execution pipeline and loosely enforce barriers between consecutive cachefit partitions; SJ merges those low data reuse partitions into one based on a tree set cover algorithm and online profiling; SJ-Q enforces strict barriers between different merged partitions and relaxed barriers between different cache-fit partitions within one merged partition. We summarize the features of four scheduling methods in <ref type="table">Table 1</ref>. All methods ensure good throttling performance while different methods impose different levels of barrier synchronization and code change overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sched</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We perform the adaptive overhead control mechanisms and data reorganization to make our software throttling method more efficient. We reduce the overhead of bisection in the DBWP Model, which is the most timeconsuming part. In particular, we focus on two bisection algorithms: 1) Coarsened Bisection built upon the SPAC model by Li and others <ref type="bibr" target="#b24">[25]</ref>, and 2) K-D tiling built upon the k-d tree geometric space partitioning method <ref type="bibr" target="#b4">[5]</ref>.</p><p>We also use CPU-GPU pipelining to make the scheduling overhead transparent <ref type="bibr" target="#b33">[33]</ref>: the CPU determines the best schedule while GPU is doing the actual computation. We improve the kernel performance by transforming data layout after we get cache-fit partitions such that the data access within the same kernel is coalesced as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adaptive Overhead Control</head><p>Coarsened Bisection Coarsened Bisection is based on SPAC <ref type="bibr" target="#b24">[25]</ref> an effective sequential edge partition model. SPAC relies on a multi-level partition paradigm, in which, a graph is coarsened, partitioned, and refined/uncoarsened level by level. In Coarsened Bisection, we reduce the overhead of SPAC by eliminating the last few levels of refinement steps. We discovered that the last few coarsened levels (five to seven levels) of refinement stages in the multilevel partitioning scheme, if omitted, do not lead to much performance difference for our software throttling methods. While at the same time, a large amount of partition over can be saved.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>off between Eliminated Refinement Levels and Scheduling Overhead/Benefits</head><p>We show the trade-off between the number of levels where the refinement step is eliminated, and the SPAC partition overhead, and the SpMV speedup when applying the SPAC for scheduling, for the sparse matrix citpatents <ref type="bibr" target="#b10">[11]</ref> in <ref type="figure" target="#fig_11">Figure 8</ref>. It can be seen from the figure that by eliminating the last five to seven levels of refinement, the overhead is reduced by up to 7.5x, while the SpMV speedup only changed from 1.5x to 1.4x.</p><p>The detailed algorithm is showed in Algorithm 4. Notice that the input graph is already an coarsened graph, since we can perform first level coarsening while reading data from file. We also parallelize the merging phase in the coarsening phase to further reduce overhead. The merging phase is mainly for reconstructing the coarsened graph in each level and is amenable to parallelization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 Coarsened Bisection</head><p>Input: Coarsened Graph G Output: Partition P 1: procedure COARSENBISECT(G)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2:</head><p>// we call a set of edges -an entity <ref type="bibr">3:</ref> build entity based adjacent list L of G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for level ∈ {1, . . . , maxLevel} do sort L by entity degree <ref type="bibr">6:</ref> for each entity e do build coarsened graph G from L</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>P ←graphPartition(G ) 13: end procedure K-D Tiling Another bisection method we adopted is a tiling based method: K-D Tiling. Since any graph can be converted into a sparse matrix representation, we treat the partition as a partition in a geometric twodimensional space. This method is similar to the k-d tree structure <ref type="bibr" target="#b4">[5]</ref> used for partitioning a k-dimensional space. Every non-leaf node in a k-d tree represents a division point along a single spatial dimension, recursively splitting the space's points into two groups.</p><p>This partitioning method has even lower overhead than Coarsened Bisection. Each split can be performed in O(n) average time via the quickselect algorithm <ref type="bibr" target="#b15">[16]</ref>, and the number of rounds of splitting is logarithmic. However, unlike Coarsened Bisection, the tiling approach does not consider connectivity of the graph, and so it generates inferior results. This trade-off makes Coarsened Bisection preferable in applications where its overhead can be hidden via amortization, for instance, in optimization problems, and the K-D tiling method is better for overhead-sensitive applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Reorganization</head><p>After we perform Data-Balanced Work-Partition on the work graph, we reorganize the data in memory according to cache-fit partitions for efficient memory coalescing. We prioritize the partition that has the smallest amount of unique data -indicating a high data reuse if the amount of work in each partition is the same. We iterate over each partition's tuple list, and place all their nonboundary vertices (vertices that only appear in one kernel) consecutively in memory using data packing <ref type="bibr" target="#b11">[12]</ref>. After non-boundary vertices for each partition have been processed, we process boundary vertices. The data reordering algorithm is briefly described in Algorithm 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We perform experiments on two platforms: an NVIDIA GTX 745 GPU with Intel Core i7-4790 CPU and an NVIDIA TITAN X GPU with Intel Xeon CPU E5-2620. The GPU configurations are detailed in <ref type="table" target="#tab_2">Table 2</ref>. We evaluate our techniques using important real-world workloads including sparse linear algebra, neural networks, and graph analytics. end for 20: end procedure Sparse Linear Algebra Workloads We use sparse matrix vector multiplication (SpMV) and the conjugate gradient solver (CG). We present performance and sensitivity analysis, as well as the effectiveness of overhead control. Neural Networks We use a pruned form of AlexNet <ref type="bibr" target="#b14">[15]</ref>. The pruned neural network is essentially sparse matrix operation. Graph Processing Workloads We use two graph processing benchmarks: the Bellman-Ford (BMF) and PageRank (PR) programs <ref type="bibr" target="#b20">[21]</ref>. Bellman-Ford takes a weighted graph as input and iteratively calculates every node's distance -an important, basic operation used in path and network analysis applications. PageRank takes a weighted graph as input and calculates the importance of every node based on its incoming links. Computational Fluid Dynamics Workloads We use the CFD benchmark from the Rodinia benchmark suite <ref type="bibr" target="#b6">[7]</ref>. The CFD solver is an unstructured grid finite volume solver for the three-dimensional Euler equations for compressible flow. The CFD benchmark is already highly optimized in terms of data layout <ref type="bibr" target="#b8">[9]</ref>. We use three mesh input sets from Rodinia <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sparse Linear Algebra</head><p>SpMV We treat the sparse matrix as a bipartite graph, as described in Section 2.1, and then apply our techniques . We use the SpMV kernel function from the cusp library <ref type="bibr" target="#b9">[10]</ref> and the matrix format is COO.</p><p>Of the 2757 matrices in the University of Florida collection <ref type="bibr" target="#b10">[11]</ref>, we extract those where the working set cannot fit entirely into the L2 (last-level) cache, which leaves us with 228 matrices on GTX 745, and 192 on Titan X. Though we optimize for the L2 cache, our techniques can be generalized to other caches.</p><p>The performance summary for SpMV across these matrices is shown in <ref type="figure" target="#fig_14">Figure 9</ref>. We also include Org+R, which applies the data reorganization scheme described in Section 4.2 to the original program to optimize memory coalescing. Memory coalescing is a technique for enhancing spatial locality <ref type="bibr" target="#b33">[33]</ref>, which is orthogonal to our technique proposed in this paper. As our work also performs memory coalescing after obtaining and scheduling cache-fit partitions, we show the performance of memory coalescing only (Org+R) versus our technique + memory coalescing for fair comparison and for demonstrating the significant performance improvement from our technique.</p><p>Among the other methods shown, first is CF, the Cache-Fit method described in Section 3.1. This splits the kernel to run each of the cache-fit partitions in standalone mode. Second is SJ, the Split-Join method described in Section 3.3 and uses tree-based set cover algorithm to merge cache-fit partitions. Third is CF-Q from Section 3.2, which applies the concurrent queue for loosely enforcing cache-fit partition ordering within a single kernel invocation. Finally we show SJ-Q from Section 3.4, which applies the concurrent queue to merged partitions in SJ. We use our Coarsened Bisection partitioner for all four of these methods. The baseline is the original program performance. All four methods provide significant speedup compared to the original case and the Org+R case. But each method has trade-offs. The SJ and SJ-Q methods both require runtime profiling, whereas CF and CF-Q do not need runtime profiling. CF and SJ are easier to incorporate to a program as the kernel code does not change (only the input to each kernel invocation changes), whereas CF-Q and SJ-Q require code modification in order to implement the queue.</p><p>We find that our techniques provide significantly more improvement in the high contention environments of larger matrices with lower hit rates. We demonstrate the effectiveness of these methods with respect to matrix size, working set size, cache hit rate, and original running time. Matrix Size In <ref type="figure" target="#fig_16">Figure 10 (a)</ref>, each group of bars shows average speedup for sparse matrices with the specified amounts of non-zeros. Every method except Org+R is  Working Set Our techniques become more effective as the working set grows, alleviating the increased cache contention. In <ref type="figure" target="#fig_16">Figure 10</ref> (b), each group of bars shows average speedup for matrices with a working set of specified size. The unit used for the x-axis is the number of times the working set can completely fill the cache.</p><p>We see speedup improve as the working set grows, just as it tends to do when the matrix size grows. But the effects are more pronounced, with higher speedup. This shows working set size is more useful than matrix size for determining whether we should use locality-aware software throttling optimization.</p><p>Cache Hit Rate Our techniques are designed to improve matrices that suffer from low hit rates due to cache thrashing. As such, a lower original hit rate allows us to achieve higher speedup. In <ref type="figure" target="#fig_16">Figure 10</ref> (c), each group of bars shows average speedup for matrices with a specified range of cache hit rates for the original case.</p><p>For matrices with lowest hit rates, the speedup for the queue-based approaches is particularly extreme. This shows that the queue-based approach is especially effective in environments that have high cache contention. The implicit communication between thread warps competing for queue reservations allows warps to achieve higher temporal locality with each other.</p><p>Run Time In <ref type="figure" target="#fig_16">Figure 10 (d)</ref>, each group of bars shows the average speedup for matrices with the specified original runtime, measured in milliseconds. Since the Titan X device is much faster than the GTX 745, we use smaller thresholds for it. In general we can expect that the runtime correlates highly with the number of non-zeros, and so this figure shows a similar curve to the others.</p><p>CG We show the performance of conjugate gradient (CG) using SJ-Q. The major computation component is sparse matrix vector multiplication (SpMV). It calls SpMV iteratively until convergence. Therefore the overhead is amortized across different iterations. We show the overall performance in <ref type="figure" target="#fig_17">Figure 11</ref> for a representative set of inputs. We find the performance improvement of CG with overhead is similar to that of SpMV without overhead. The overhead of Coarsened Bisection and SJ-Q profiling is well amortized. The changes to cache hit rates correlate with the performance improvement. The matrix rgg n 2 23 s0 (RGG) has a much smaller cache hit rate on Titan X (0.44%) than on GTX 745 (36.75%), despite its larger cache size. There is more cache contention on Titan X since it uses more cores. We are able to improve the hit rate to 62.92% without changing the thread number or the implementation of the kernel code. Only the set of non-zero elements processed by each kernel is changed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Neural Networks</head><p>We explore the effectiveness of our techniques on the AlexNet neural network, achieving an overall speedup of up to 54% on the Titan X device, which is suited for deep learning. Each fully connected layer of the neural network AlexNet operates as a matrix-vector operation; the matrix is a weight matrix. The work by Han and others <ref type="bibr" target="#b14">[15]</ref> prunes the AlexNet network to remove elements of low weight and result in sparse matrices. Since AlexNet is designed for smaller, embedded devices, we run multiple instances in parallel, allowing the neural network to analyze 150 different images at once for our Titan X GPU. This provides a reasonable amount of computation and data for our more powerful hardware.</p><p>In <ref type="figure" target="#fig_1">Figure 13</ref>, we show the speedup achieved by our technique on each of the three pruned fc layers of AlexNet. We include the alternate baseline of the original case plus data reorganization, as well as the CacheFit and Split-Join strategies both with and without the queue-based implementation. When only applying data reorganization, we see no improvement or even some slowdown. But when we apply any of our partitioning techniques we see speedup up to 98%, and no degradation on the smaller layers. The reason for less-speedup in the smaller layers (fc 7 and fc 8) is that their vector size is smaller and can fit into last level cache entirely in our Titan X GPU. We believe the performance improvement will be more pronounced for Alexnet if we test with embedded devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Graph Applications</head><p>We show the performance of the Bellman-Ford and PageRank programs on a set of graphs from the University of Florida Sparse Matrix Collection <ref type="bibr" target="#b10">[11]</ref>, Stanford Large Network Dataset Collection <ref type="bibr" target="#b22">[23]</ref>, and DI-MACS implementation challenge <ref type="bibr" target="#b0">[1]</ref>. Information for each graph is listed in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>We demonstrate the efficiency of the K-D tiling (SJkdtiling) approach, since both BMF and PR take fewer iterations to converge compared with sparse linear system solvers. Thus we need a fast and approximate partitioner so that the overhead can be amortized. We use SJ rather than SJ-Q, since it still provides good speedup while avoiding the overhead of the queue.</p><p>We summarize the performance with overhead in Table 3. We see that our approach improves performance for both BMF and PR. RoadCal benefits least, due to small size, but sees improvement in some cases. We observe that the speedup for PR is greater than for BMF. There are more memory accesses in the PR algo-rithm than in the BMF algorithm, and so it benefits more from our locality-aware software throttling.</p><p>We show cache hit rates for each program in <ref type="figure">Figure  14</ref> and <ref type="figure" target="#fig_4">Figure 15</ref>. We show speedup with and without overhead for PR on Titan X in <ref type="table" target="#tab_4">Table 4</ref>. PR has fewer iterations than CG so cannot improve performance with Coarsened Bisection if overhead is considered. However, the KD-Tiling method is fast enough that for SJkdtiling's performance to remain high with overhead.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Computational Fluid Dynamics</head><p>The graph structure for CFD is a mesh in which every node has up to four neighbors. Since these meshes are small, We use SJ instead of SJ-Q for throttling. In <ref type="figure" target="#fig_6">Figure  16</ref> we show the performance on GTX 745. We achieve speedup of up to 10%. Input fvcorr 097 has the smallest number of nodes, thus the smallest improvement. CFD already has an optimized data layout <ref type="bibr" target="#b8">[9]</ref>. With our throttling method, we nonetheless see some speedup. This demonstrates the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Modern GPUs are equipped with massive amounts of parallelism and significant computing horsepower. How- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>Original SJ <ref type="figure" target="#fig_6">Figure 16</ref>: CFD Speedup ever, this also results in higher levels of cache contention. To achieve high performance, reusing data in cache is critical. Both software and hardware approaches have been proposed to address the cache contention problem. Warp Scheduling Policy Recent works focus on modifying GPU warp scheduling policy to reduce cache contention by throttling threads <ref type="bibr" target="#b17">[18]</ref> [27] <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b18">19]</ref> or to prioritize thread execution based on criticality <ref type="bibr" target="#b21">[22]</ref>. However, all those approaches require hardware modification and require fine-grained thread scheduling which is complicated in a massively parallel system. Our approach does not require hardware modification or fine-grained thread scheduling. Moreover, most warp scheduling policies aim to reduce the number of active warps for better performance. However, as we discovered in this paper, it is not always good to reduce the number of simultaneously running tasks for better cache performance. In some scenarios, i.e., when data reuse is low, having higher concurrency actually helps. Computation and Data Layout Transformation On GPUs, Baskaran et al. <ref type="bibr" target="#b2">[3]</ref> developed a compile-time transformation scheme coalescing loop nest accesses to achieve efficient global memory access. <ref type="bibr">Zhang et al. [32]</ref> focused on reducing irregular memory accesses and enhancing memory coalescing to improve GPU program performance. These and other works <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b3">4]</ref> all focus on improving memory coalescing for spatial locality. Our method is orthogonal to these approaches, as we optimize temporal locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper proposes a locality-aware software throttling framework that targets irregular sparse matrix applications on GPUs. We perform d ¯ ata-balanced work partition on the entire workload to get cache-fit partitions and use scheduling to exploit the trade-off between cache locality and execution pipeline utilization. Our framework is practical and effective. It requires no hardware modification and achieves an average 2.01X (maximal 6.45X) speedup on more than 200 real sparse matrices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: A Fundamental Sparse Matrix Operation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Data-Balanced v.s. Load-Balanced</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Original: Kernel&lt;&lt;&lt;blocknum, blockdim&gt;&gt;(TL, N); Phase 1: Kernel&lt;&lt;&lt;blocknum, blockdim&gt;&gt;(TL'[1], P 1 ); Phase 2: Kernel&lt;&lt;&lt;blocknum, blockdim&gt;&gt;(TL'[2], P 2 ); …… Phase k: Kernel&lt;&lt;&lt;blocknum, blockdim&gt;&gt;(TL'[k], P k ); TL is the original tuple list, TL[i] is the tuple list corresponding to the ith cache-fit partitions K is # of cache-fit partitions, N is # of tuples, P i is # of tuples in TL[i]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Kernel Splitting for Cache-Fit Scheduling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Queue Based Scheduling Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>while Q is not empty do 9: I ← next queue item (chunk) from Q 10: process I[laneID] 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>{B, C} is chosen as the best configuration.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Tree Node Recombination Example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Trade-off between Eliminated Refinement Levels and Scheduling Overhead/Benefits</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>7: merge e with its heaviest avaliable neighbor ne 8: end for 9: build coarsened L by results from above step 10: end for 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average Speedup for SpMV</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(b) The effect of working set size (in multiples of cache size) on speedup.(d) The effect of original runtime (milliseconds) on speedup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: SpMV Speedup on GTX 745 and Titan X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: CG Speedup on GTX 745 and Titan X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: CG Cache Hit Rate on GTX 745 and Titan X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Speedup for AlexNet layers on Titan X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 15 :</head><label>15</label><figDesc>Figure 14: Cache Hit Rates for Bellman-Ford</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Experimental Environment</head><label>2</label><figDesc></figDesc><table>GPU Model 
Titan X 
GTX 745 
Architecture 
Pascal 
Maxwell 
Core # 
5376 
576 
L2 Cache 
3MB 
2MB 
CUDA version CUDA 8.0 CUDA 8.0 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : BMF and PR Performance Summary</head><label>3</label><figDesc></figDesc><table>Graph 
GTX 745 
TITAN X 
BMF 
PR 
BMF 
PR 
Pokec [23] 
1.62 
2.98 
1.88 
3.17 
WebGoogle [23] 
2 
3.29 
1.8 
3.37 
Wikipedia-051105 [11] 
1.24 
1.99 
1.43 
1.97 
WikiTalk [11] 
1.74 
2.57 
2.09 
2.75 
IMDB [11] 
2.16 
3.22 
1.59 
2.62 
RoadCentral [11] 
1.19 
1.6 
1.69 
2.18 
RoadCal [1] 
1 
1 
1 
1.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : PageRank Speedup with and without Overhead</head><label>4</label><figDesc></figDesc><table>Graph 
SJ-kdtiling 
SJ-kdtiling 
w/ Overhead 
w/o Overhead 
Pokec 
3.17 
3.34 
WebGoogle 
3.37 
3.43 
Wikipedia 
1.97 
1.98 
WikiTalk 
2.75 
2.81 
IMDB 
2.62 
2.27 
RoadCentral 
2.18 
2.48 
RoadCal 
1.22 
1.21 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Dimacs implementation challenge -shortest paths</title>
		<imprint>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hypergraph partitioning based models and methods for exploiting cache locality in sparse matrix-vector multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akbudak</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kayaaslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aykanat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A compiler framework for optimization of affine loop nests for gpgpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Baskaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bondhugula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ramanujam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rountev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadayappan</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual International Conference on Supercomputing</title>
		<meeting>the 22nd Annual International Conference on Supercomputing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
	<note>ICS &apos;08, ACM</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Efficient sparse matrix-vector multiplication on cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garland</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>NVR-2008-004</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Nvidia Corporation</publisher>
		</imprint>
	</monogr>
<note type="report_type">Tech. rep., Nvidia Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multidimensional binary search trees used for associative searching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bentley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="509" to="517" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Balanced graph edge partition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bourse</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lelarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojnovic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rodinia: A benchmark suite for heterogeneous computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheaffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skadron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno>IISWC &apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the 2009 IEEE International Symposium on Workload Characterization (IISWC)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive cache management for energyefficient gpu computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-W</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename></persName>
		</author>
		<idno>MICRO-47</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Running unstructured grid cfd solvers on modern graphics hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corrigan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Camelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ohner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wallin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno>no. AIAA 2009-4001</idno>
	</analytic>
	<monogr>
		<title level="m">19th AIAA Computational Fluid Dynamics Conference</title>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CUSP: A C++ templated sparse matrix library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The university of florida sparse matrix collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Math. Softw</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving cache performance in dynamic applications through data and computation reorganization at run time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kennedy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation</title>
		<meeting>the ACM SIGPLAN 1999 Conference on Programming Language Design and Implementation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="229" to="241" />
		</imprint>
	</monogr>
	<note>PLDI &apos;99, ACM</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">New bounds on the complexity of the shortest path problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="83" to="89" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Powergraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonzalez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guestrin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And Dally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Algorithm 65: find</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoare</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="321" to="322" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploiting memory access patterns to improve memory performance in dataparallel architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaeli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="105" to="118" />
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Owl: Cooperative thread array aware scheduling techniques for improving gpgpu performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jog</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kayiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chidambaram Nachiappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="395" to="406" />
		</imprint>
	</monogr>
	<note>ASPLOS &apos;13, ACM</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neither more nor less: Optimizing thread-level parallelism for gpgpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayiran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 22Nd International Conference on Parallel Architectures and Compilation Techniques<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
	<note>PACT &apos;13</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Managing gpu concurrency in heterogeneous architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayiran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachiappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Jog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ausavarung-Nirun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>MICRO-47</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="114" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vertex-centric graph processing on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khorasani</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuyan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Cusha</surname></persName>
		</author>
		<idno>HPDC &apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on High-performance Parallel and Distributed Computing</title>
		<meeting>the 23rd International Symposium on High-performance Parallel and Distributed Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coordinated warp scheduling and cache prioritization for critical warp acceleration of gpgpu workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Cawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="515" to="527" />
		</imprint>
	</monogr>
	<note>ISCA &apos;15, ACM</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leskovec</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krevl</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Datasets</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Locality-aware cta clustering for modern gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cor-Poraal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="297" to="311" />
		</imprint>
	</monogr>
	<note>ASPLOS &apos;17, ACM</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A simple yet effective balanced edge partition model for parallel computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Geda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szegedy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Meas. Anal. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective balanced edge partition model for parallel computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Geda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szegedy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 2017 ACM SIGMETRICS / International Conference on Measurement and Modeling of Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6" to="6" />
		</imprint>
	</monogr>
	<note>SIGMETRICS &apos;17 Abstracts</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cacheconscious wavefront scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogers</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamodt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<biblScope unit="page">45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MICRO-45</title>
	</analytic>
	<monogr>
		<title level="m">Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="72" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Data layout transformation exploiting memory-level parallelism in structured grid many-core applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-J</forename><surname>Stratton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-M</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 19th International Conference on Parallel Architectures and Compilation Techniques<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="513" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fennel: Streaming graph partitioning for massive scale graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsourakakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gkantsidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Radunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vojnovic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno>WSDM &apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="333" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast sparse matrix-vector multiplication by exploiting variable block structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuduc</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing and Communications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A gpgpu compiler for memory optimization and parallelism management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 31st ACM SIGPLAN Conference on Programming Language Design and Implementation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="86" to="97" />
		</imprint>
	</monogr>
	<note>PLDI &apos;10, ACM</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Streamlining gpu applications on the fly: Thread divergence elimination through runtime thread-data remapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM International Conference on Supercomputing</title>
		<meeting>the 24th ACM International Conference on Supercomputing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="115" to="126" />
		</imprint>
	</monogr>
	<note>ICS &apos;10, ACM</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On-the-fly elimination of dynamic irregularities for gpu computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="369" to="380" />
		</imprint>
	</monogr>
	<note>ASPLOS XVI, ACM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
