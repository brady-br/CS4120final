<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 1-2, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Ma</surname></persName>
							<email>qingsong.mqs@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">MFF ÚFAL</orgName>
								<orgName type="institution" key="instit1">Tencent-CSIG</orgName>
								<orgName type="institution" key="instit2">AI Evaluation Lab</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">CICS</orgName>
								<orgName type="institution" key="instit5">Charles University</orgName>
								<orgName type="institution" key="instit6">Dublin City University</orgName>
								<address>
									<country>ADAPT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnny</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MFF ÚFAL</orgName>
								<orgName type="institution" key="instit1">Tencent-CSIG</orgName>
								<orgName type="institution" key="instit2">AI Evaluation Lab</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">CICS</orgName>
								<orgName type="institution" key="instit5">Charles University</orgName>
								<orgName type="institution" key="instit6">Dublin City University</orgName>
								<address>
									<country>ADAPT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wei</surname></persName>
							<email>jwei@umass.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MFF ÚFAL</orgName>
								<orgName type="institution" key="instit1">Tencent-CSIG</orgName>
								<orgName type="institution" key="instit2">AI Evaluation Lab</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">CICS</orgName>
								<orgName type="institution" key="instit5">Charles University</orgName>
								<orgName type="institution" key="instit6">Dublin City University</orgName>
								<address>
									<country>ADAPT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
							<email>bojar@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department">MFF ÚFAL</orgName>
								<orgName type="institution" key="instit1">Tencent-CSIG</orgName>
								<orgName type="institution" key="instit2">AI Evaluation Lab</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">CICS</orgName>
								<orgName type="institution" key="instit5">Charles University</orgName>
								<orgName type="institution" key="instit6">Dublin City University</orgName>
								<address>
									<country>ADAPT</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
							<email>graham.yvette@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">MFF ÚFAL</orgName>
								<orgName type="institution" key="instit1">Tencent-CSIG</orgName>
								<orgName type="institution" key="instit2">AI Evaluation Lab</orgName>
								<orgName type="institution" key="instit3">UMass Amherst</orgName>
								<orgName type="institution" key="instit4">CICS</orgName>
								<orgName type="institution" key="instit5">Charles University</orgName>
								<orgName type="institution" key="instit6">Dublin City University</orgName>
								<address>
									<country>ADAPT</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the Fourth Conference on Machine Translation (WMT)</title>
						<meeting>the Fourth Conference on Machine Translation (WMT) <address><addrLine>Florence, Italy</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="volume">2</biblScope>
							<biblScope unit="issue">1</biblScope>
							<biblScope unit="page" from="62" to="90"/>
							<date type="published">August 1-2, 2019</date>
						</imprint>
					</monogr>
					<note>62</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents the results of the WMT19 Metrics Shared Task. Participants were asked to score the outputs of the translations systems competing in the WMT19 News Translation Task with automatic metrics. 13 research groups submitted 24 metrics, 10 of which are reference-less &quot;metrics&quot; and constitute submissions to the joint task with WMT19 Quality Estimation Task, &quot;QE as a Met-ric&quot;. In addition, we computed 11 baseline metrics, with 8 commonly applied base-lines (BLEU, SentBLEU, NIST, WER, PER, TER, CDER, and chrF) and 3 reim-plementations (chrF+, sacreBLEU-BLEU, and sacreBLEU-chrF). Metrics were evaluated on the system level, how well a given metric correlates with the WMT19 official manual ranking, and segment level, how well the metric correlates with human judgements of segment quality. This year, we use direct assessment (DA) as our only form of manual evaluation.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To determine system performance in machine translation (MT), it is often more practical to use an automatic evaluation, rather than a manual one. Manual/human evaluation can be costly and time consuming, and so an automatic evaluation metric, given that it sufficiently correlates with manual evaluation, can be useful in developmental cycles. In studies involving hyperparameter tuning or architecture search, automatic metrics are necessary as the amount of human effort implicated in manual evaluation is generally prohibitively large. As objective, reproducible quantities, metrics can also facilitate cross-paper comparisons. The WMT Metrics Shared Task 1 annually serves as a venue to validate the use of existing metrics (including baselines such as BLEU), and to develop new ones; see <ref type="bibr" target="#b17">Koehn and Monz (2006)</ref> through <ref type="bibr" target="#b21">Ma et al. (2018)</ref>.</p><p>In the setup of our Metrics Shared Task, an automatic metric compares an MT system's output translations with manual reference translations to produce: either (a) system-level score, i.e. a single overall score for the given MT system, or (b) segment-level scores for each of the output translations, or both.</p><p>This year we teamed up with the organizers of the QE Task and hosted "QE as a Metric" as a joint task. In the setup of the Quality Estimation Task ( <ref type="bibr">Fonseca et al., 2019)</ref>, no humanproduced translations are provided to estimate the quality of output translations. Quality estimation (QE) methods are built to assess MT output based on the source or based on the translation itself. In this task, QE developers were invited to perform the same scoring as standard metrics participants, with the exception that they refrain from using a reference translation in production of their scores. We then evaluate the QE submissions in exactly the same way as regular metrics are evaluated, see below. From the point of view of correlation with manual judgements, there is no difference in metrics using or not using references.</p><p>The source, reference texts, and MT system outputs for the Metrics task come from the News Translation Task ( <ref type="bibr">Barrault et al., 2019</ref>, which we denote as Findings 2019). The texts were drawn from the news domain and involve translations of English (en) to/from Czech (cs), German (de), Finnish (fi), Gujarati (gu), Kazakh (kk), Lithuanian (lt), Russian (ru), and Chinese (zh), but excluding csen (15 language pairs). Three other language pairs not including English were also manually evaluated as part of the News Translation Task: German→Czech and German↔French.</p><p>In total, metrics could participate in 18 language pairs, with 10 target languages.</p><p>In the following, we first give an overview of the task (Section 2) and summarize the baseline (Section 3) and submitted (Section 4) metrics. The results for system-and segment-level evaluation are provided in Sections 5.1 and 5.2, respectively, followed by a joint discussion Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Setup</head><p>This year, we provided task participants with one test set for each examined language pair, i.e. a set of source texts (which are commonly ignored by MT metrics), corresponding MT outputs (these are the key inputs to be scored) and a reference translation (held out for the participants of "QE as a Metric" track).</p><p>In the system-level, metrics aim to correlate with a system's score which is an average over many human judgments of segment translation quality produced by the given system. In the segment-level, metrics aim to produce scores that correlate best with a human ranking judgment of two output translations for a given source segment (more on the manual quality assessment in Section 2.3). Participants were free to choose which language pairs and tracks (system/segment and reference-based/reference-free) they wanted to take part in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Source and Reference Texts</head><p>The source and reference texts we use are newstest2019 from this year's WMT News Translation Task (see <ref type="bibr">Findings 2019)</ref>. This set contains approximately 2,000 sentences for each translation direction (except Gujarati, Kazakh and Lithuanian which have approximately 1,000 sentences each, and German to/from French which has 1701 sentences).</p><p>The reference translations provided in newstest2019 were created in the same direction as the MT systems were translating.</p><p>The exceptions are German→Czech where both sides are translations from English and German↔French which followed last years' practice. Last year and the years before, the dataset consisted of two halves, one originating in the source language and one in the target language. This however lead to adverse artifacts in MT evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System Outputs</head><p>The results of the Metrics Task are affected by the actual set of MT systems participating in a given translation direction. On one hand, if all systems are very close in their translation quality, then even humans will struggle to rank them. This in turn will make the task for MT metrics very hard. On the other hand, if the task includes a wide range of systems of varying quality, correlating with humans should be generally easier, see Section 6.1 for a discussion on this. One can also expect that if the evaluated systems are of different types, they will exhibit different error patterns and various MT metrics can be differently sensitive to these patterns.</p><p>This year, all MT systems included in the Metrics Task come from the News Translation Task (see <ref type="bibr">Findings 2019)</ref>. There are however still noticeable differences among the various language pairs.</p><p>• Unsupervised MT Systems.</p><p>The German→Czech research systems were trained in an unsupervised fashion, i.e. without the access to parallel CzechGerman texts (except for a couple of thousand sentences used primarily for validation). We thus expect the research German-Czech systems to be "more creative" and depart further away from the references. The online systems in this language directions are however standard MT systems so the German-Czech evaluation could be to some extent bimodal.</p><p>• EU Election. The French↔German translation was focused on a sub-domain of news, namely texts related EU Election. Various MT system developers may have invested more or less time to the domain adaptation.</p><p>• Regular News Tasks Systems. These are all the other MT systems in the evaluation; differing in whether they are trained only on WMT provided data ("Constrained", or "Unconstrained") as in the previous years. All the freely available web services (online MT systems) are deemed unconstrained.</p><p>Overall, the results are based on 233 systems across 18 language pairs. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Manual Quality Assessment</head><p>Direct Assessment (DA, <ref type="bibr" target="#b7">Graham et al., 2013</ref><ref type="bibr" target="#b8">Graham et al., , 2014a</ref>) was employed as the source of the "golden truth" to evaluate metrics again this year. The details of this method of human evaluation are provided in Findings 2019.</p><p>The basis of DA is to collect a large number of quality assessments (a number on a scale of 1-100, i.e. effectively a continuous scale) for the outputs of all MT systems. These scores are then standardized per annotator.</p><p>In the past years, the underlying manual scores were reference-based (human judges had access to the same reference translation as the MT quality metric). This year, the official WMT19 scores are reference-based (or "monolingual") for some language pairs and reference-free (or "bilingual") for others. <ref type="bibr">3</ref> Due to these different types of golden truth collection, reference-based language pairs are in a closer match with the standard referencebased metrics, while the reference-free language pairs are better fit for the "QE as a metric" subtask.</p><p>Note that system-level manual scores are different than those of the segment-level. Since for segment-level evaluation, collecting enough DA judgements for each segment is infeasible, so we resort to converting DA judgements to 2 This year, we do not use the artificially constructed "hybrid systems" <ref type="bibr" target="#b10">(Graham and Liu, 2016)</ref> because the confidence on the ranking of system-level metrics is sufficient even without hybrids.</p><p>3 Specifically, the reference-based language pairs were those where the anticipated translation quality was lower or where the manual judgements were obtained with the help of anonymous crowdsourcing. Most of these cases were translations into English (fien, gu-en, kk-en, lt-en, ru-en and zh-en) and then the language pairs not involving English (de-cs, de-fr and fr-de). The reference-less (bilingual) evaluations were those where mainly MT researchers themselves were involved in the annotations: en-cs, en-de, en-fi, en-gu, en-kk, en-lt, en-ru, en-zh.</p><p>golden truth expressed as relative rankings, see Section 2.3.2.</p><p>The exact methods used to calculate correlations of participating metrics with the golden truth are described below, in the two sections for system-level evaluation (Section 5.1) and segment-level evaluation (Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">System-level Golden Truth: DA</head><p>For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed segments for each MT system to produce a scalar rating for the system's performance.</p><p>The underlying set of assessed segments is different for each system. Thanks to the fact that the system-level DA score is an average over many judgments, mean scores are consistent and have been found to be reproducible ( <ref type="bibr" target="#b7">Graham et al., 2013</ref>). For more details see Findings 2019.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Segment-level Golden Truth: daRR</head><p>Starting from <ref type="bibr" target="#b3">Bojar et al. (2017)</ref>, when WMT fully switched to DA, we had to come up with a solid golden standard for segment-level judgements. Standard DA scores are reliable only when averaged over sufficient number of judgments. <ref type="bibr">4</ref> Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as "daRR", to distinguish it clearly from the relative ranking ("RR") golden truth used in the past years.  <ref type="table">Table 1</ref>: Number of judgements for DA converted to daRR data; "DA&gt;1" is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; "Ave" is the average number of translations with at least one DA judgement available for the same source input sentence; "DA pairs" is the number of all possible pairs of translations of the same source input resulting from "DA&gt;1"; and "daRR" is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin.</p><p>From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conversion of scores in this way produced a large set of daRR judgements for all language pairs, rely on judgements collected from known-reliable volunteers and crowd-sourced workers who passed DA's quality control mechanism. Any inconsistency that could arise from reliance on DA judgements collected from low quality crowd-sourcing is thus prevented.</p><p>shown in <ref type="table">Table 1</ref> due to combinatorial advantage of extracting daRR judgements from all possible pairs of translations of the same source input. We see that only German-French and esp. French-German can suffer from insufficient number of these simulated pairwise comparisons.</p><p>The daRR judgements serve as the golden standard for segment-level evaluation in WMT19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline Metrics</head><p>In addition to validating popular metrics, including baselines metrics serves as comparison and prevents "loss of knowledge" as mentioned by <ref type="bibr" target="#b2">Bojar et al. (2016)</ref>.</p><p>Moses scorer 6 is one of the MT evaluation tools that aggregated several useful metrics over the time. <ref type="bibr">Since Macháček and Bojar (2013)</ref>, we have been using Moses scorer to provide most of the baseline metrics and kept encouraging authors of well-performing MT metrics to include them in Moses scorer. <ref type="bibr">7</ref> The baselines we report are:  smoothed version of BLEU for scoring at the segment-level. We used the standard tokenizer script as available in Moses toolkit for tokenization.</p><formula xml:id="formula_0">BLEU</formula><p>chrF and chrF+. The metrics chrF and chrF+ <ref type="bibr" target="#b27">(Popović, 2015</ref><ref type="bibr" target="#b28">(Popović, , 2017</ref>) are computed using their original Python implementation, see <ref type="table" target="#tab_2">Table 2</ref>. We ran chrF++.py with the parameters -nw 0 -b 3 to obtain the chrF score and with -nw 1 -b 3 to obtain the chrF+ score. Note that chrF intentionally removes all spaces before matching the n-grams, detokenizing the segments but also concatenating words. 10 sacreBLEU-BLEU and sacreBLEUchrF. The metrics sacreBLEU-BLEU and sacreBLEU-chrF (Post, 2018a) are re-implementation of BLEU and chrF respectively. We ran sacreBLEU-chrF with the same parameters as chrF, but their scores are slightly different. The signature strings produced by sacreBLEU for BLEU and chrF respectively are BLEU+case.lc+lang.de-en+numrefs.1+ smooth.exp+tok.intl+version.1.3.6 and chrF3+case.mixed+lang.de-en +numchars.6+numrefs.1+space.False+ tok.13a+version.1.3.6.</p><p>The baselines serve in system and segmentlevel evaluations as customary: BLEU, TER, WER, PER, CDER, sacreBLEU-BLEU and sacreBLEU-chrF for system-level only; sentBLEU for segment-level only and chrF for both.</p><p>Chinese word segmentation is unfortunately not supported by the tokenization scripts mentioned above. For scoring Chinese with baseline metrics, we thus pre-processed MT outputs and reference translations with the script tokenizeChinese.py 11 by Shujian Huang, which separates Chinese characters from each other and also from non-Chinese parts. <ref type="table" target="#tab_2">Table 2</ref> lists the participants of the WMT19 Shared Metrics Task, along with their metrics and links to the source code where available. We have collected 24 metrics from a total of 13 research groups, with 10 reference-less "metrics" submitted to the joint task "QE as a Metrich" with WMT19 Quality Estimation Task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Submitted Metrics</head><p>The rest of this section provides a brief summary of all the metrics that participated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">BEER</head><p>BEER ( <ref type="bibr" target="#b33">Stanojević and Sima'an, 2015</ref>) is a trained evaluation metric with a linear model that combines sub-word feature indicators (character n-grams) and global word order features (skip bigrams) to achieve a language agnostic and fast to compute evaluation metric. BEER has participated in previous years of the evaluation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BERTr</head><p>BERTr ( <ref type="bibr" target="#b24">Mathur et al., 2019</ref>) uses contextual word embeddings to compare the MT output with the reference translation.</p><p>The BERTr score of a translation is the average recall score over all tokens, using a relaxed version of token matching based on BERT embeddings: namely, computing the maximum cosine similarity between the embedding of a reference token against any token in the MT output. BERTr uses bert_base_uncased embeddings for the to-English language pairs, and bert_base_multilingual_cased embeddings for all other language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CharacTER</head><p>CharacTER ( <ref type="bibr">Wang et al., 2016b,a)</ref>, identical to the 2016 setup, is a character-level metric inspired by the commonly applied translation edit rate (TER). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence. CharacTER calculates the character-level edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis word is considered to match a reference word and could be shifted, if the edit dis-tance between them is below a threshold value. The Levenshtein distance between the reference and the shifted hypothesis sequence is computed on the character level. In addition, the lengths of hypothesis sequences instead of reference sequences are used for normalizing the edit distance, which effectively counters the issue that shorter translations normally achieve lower TER.</p><p>Similarly to other character-level metrics, CharacTER is generally applied to nontokenized outputs and references, which also holds for this year's submission with one exception. This year tokenization was carried out for en-ru hypotheses and references before calculating the scores, since this results in large improvements in terms of correlations. For other language pairs, no tokenizer was used for pre-processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">EED</head><p>EED ( <ref type="bibr" target="#b32">Stanchev et al., 2019</ref>) is a characterbased metric, which builds upon CDER. It is defined as the minimum number of operations of an extension to the conventional edit distance containing a "jump" operation. The edit distance operations (insertions, deletions and substitutions) are performed at the character level and jumps are performed when a blank space is reached. Furthermore, the coverage of multiple characters in the hypothesis is penalised by the introduction of a coverage penalty. The sum of the length of the reference and the coverage penalty is used as the normalisation term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">ESIM</head><p>Enhanced Sequential Inference Model (ESIM; <ref type="bibr" target="#b4">Chen et al., 2017;</ref><ref type="bibr" target="#b24">Mathur et al., 2019</ref>) is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation. It uses cross-sentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 toEnglish data for the to-English language pairs, and all WMT 2018 data for all other language pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">hLEPORb_baseline, hLEPORa_baseline</head><p>The submitted metric hLEPOR_baseline is a metric based on the factor combination of length penalty, precision, recall, and position difference penalty. The weighted harmonic mean is applied to group the factors together with tunable weight parameters. The systemlevel score is calculated with the same formula but with each factor weighted using weight estimated at system-level and not at segmentlevel.</p><p>In this submitted baseline version, hLEPOR_baseline was not tuned for each language pair separately but the default weights were applied across all submitted language pairs. Further improvements can be achieved by tuning the weights according to the development data, adding morphological information and applying n-gram factor scores into it (e.g. part-of-speech, n-gram precision and n-gram recall that were added into LEPOR in WMT13.). The basic model factors and further development with parameters setting were described in the paper ( <ref type="bibr" target="#b14">Han et al., 2012)</ref> and <ref type="bibr" target="#b15">(Han et al., 2013)</ref>.</p><p>For sentence-level score, only hLEPORa_baseline was submitted with scores calculated as the weighted harmonic mean of all the designed factors using default parameters.</p><p>For system-level score, both hLEPORa_baseline and hLEPORb_baseline were submitted, where hLEPORa_baseline is the the average score of all sentence-level scores, and hLEPORb_baseline is calculated via the same sentence-level hLEPOR equation but replacing each factor value with its system-level counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy)</head><p>Meteor++ 2.0 (Guo and Hu, 2019) is a metric based on Meteor <ref type="bibr">(Denkowski and Lavie, 2014</ref>) that takes syntactic-level paraphrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching, they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases. Meteor++ 2.0 extracts the knowledge from the Paraphrase Database (PPDB; <ref type="bibr" target="#b0">Bannard and Callison-Burch, 2005</ref>) and integrates it into Meteor-based metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">PReP</head><p>PReP ( <ref type="bibr" target="#b38">Yoshimura et al., 2019</ref>) is a method for filtering pseudo-references to achieve a good match with a gold reference. At the beginning, the source sentence is translated with some off-the-shelf MT systems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudoreferences are then filtered using BERT <ref type="bibr">(De- vlin et al., 2019</ref>) fine-tuned on the MPRC corpus ( <ref type="bibr">Dolan and Brockett, 2005</ref>), estimating the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT systems, a large portion of their outputs is indeed considered as a valid paraphrase.</p><p>The final metric score is calculated simply with SentBLEU with these multiple references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9">WMDO</head><p>WMDO ( <ref type="bibr">Chow et al., 2019b</ref>) is a metric based on distance between distributions in the semantic vector space. Matching in the semantic space has been investigated for translation evaluation, but the constraints of a translation's word order have not been fully explored. Building on the Word Mover's Distance metric and various word embeddings, WMDO introduces a fragmentation penalty to account for fluency of a translation. This word order extension is shown to perform better than standard WMD, with promising results against other types of metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10">YiSi-0, YiSi-1, YiSi-1_srl, YiSi-2, YiSi-2_srl</head><p>YiSi (Lo, 2019) is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources.</p><p>YiSi-1 is a MT evaluation metric that measures the semantic similarity between a machine translation and human references by aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from BERT and optionally incorporating shallow semantic structures (denoted as YiSi-1_srl).</p><p>YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any language. It uses longest common character substring to measure the lexical similarity.</p><p>YiSi-2 is the bilingual, reference-less version for MT quality estimation, which uses the contextual embeddings extracted from BERT to evaluate the crosslingual lexical semantic similarity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.11">QE Systems</head><p>In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the "QE as a Metric" track. The submitted QE systems are evaluated in the same settings as metrics to facilitate comparison. Their descriptions can be found in the Findings of the WMT 2019 Shared Task on Quality Estimation ( <ref type="bibr">Fonseca et al., 2019</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We discuss system-level results for news task systems in Section 5.1. The segment-level results are in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System-Level Evaluation</head><p>As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics. The Pearson correlation is as follows:</p><formula xml:id="formula_1">r = ∑ n i=1 (Hi − H)(Mi − M ) √ ∑ n i=1 (Hi − H) 2 √ ∑ n i=1 (Mi − M ) 2 (1)</formula><p>where H i are human assessment scores of all systems in a given translation direction, M i are the corresponding scores as predicted by a given metric. H and M are their means, respectively.</p><p>Since some metrics, such as BLEU, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER, aim for a strong negative correlation we compare metrics via the absolute value |r| of a de-en fi-en gu-en kk-en lt-en ru-en zh-en     YiSi.1 <ref type="figure">Figure 1</ref>: System-level metric significance test results for DA human assessment for into English and out-of English language pairs (newstest2019): Green cells denote a statistically significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to <ref type="bibr">Williams test.</ref> given metric's correlation with human assessment. <ref type="table" target="#tab_4">Tables 3, 4</ref> and 5 provide the system-level correlations of metrics evaluating translation of newstest2019. The underlying texts are part of the WMT19 News Translation test set (newstest2019) and the underlying MT systems are all MT systems participating in the WMT19 News Translation Task. As recommended by Graham and Baldwin (2014), we employ Williams significance test <ref type="bibr" target="#b36">(Williams, 1959)</ref> to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in <ref type="table" target="#tab_4">Tables 3, 4</ref> and 5.</p><formula xml:id="formula_2">− − 0.810 − − ibm1-pos4gram − 0.393 − − − − − − LASIM − 0.871 − − − − 0.823 − LP − 0.569 − − − − 0.661 − UNI 0.028 0.841 0.907 − − − 0.919 − UNI+ − − − − − − 0.918 − USFD − 0.224 − − − − 0.857 − USFD-TL − 0.091 − − − − 0.771 − YiSi-2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">System-Level Results</head><p>Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics significantly outperform the most widely employed metric BLEU, we include significance test results for every competing pair of metrics including our baseline metrics in <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 2</ref>.</p><p>This year, the increased number of systems participating in the news tasks has provided a larger sample of system scores for testing metrics. Since we already have sufficiently conclusive results on genuine MT systems, we do not need to generate hybrid system results as in <ref type="bibr" target="#b10">Graham and Liu (2016)</ref> and past metrics tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Segment-Level Evaluation</head><p>Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, again we were unable to follow the methodology outlined in <ref type="bibr" target="#b12">Graham et al. (2015)</ref> for evaluation of segment-level metrics because the sampling of sentences did not provide sufficient number of assessments of the same segment. We therefore convert pairs of DA scores for competing translations to daRR better/worse preferences as described in Section 2.3.2.</p><p>We measure the quality of metrics' segmentlevel scores against the daRR golden truth using a Kendall's Tau-like formulation, which is an adaptation of the conventional Kendall's Tau coefficient. Since we do not have a total order ranking of all translations, it is not possible to apply conventional Kendall's Tau ( <ref type="bibr" target="#b12">Graham et al., 2015)</ref>.</p><p>Our Kendall's Tau-like formulation, τ , is as follows:</p><formula xml:id="formula_3">τ = |Concordant| − |Discordant| |Concordant| + |Discordant| (2)</formula><p>where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons for which a given metric disagrees. The formula is not specific with respect to ties, i.e. cases where the annotation says that the two outputs are equally good. The way in which ties (both in human and metric judgement) were incorporated in computing Kendall τ has changed across the years of WMT Metrics Tasks. Here we adopt the version used in WMT17 daRR evaluation. For a detailed discussion on other options, see also <ref type="bibr" target="#b22">Macháček and Bojar (2014)</ref>.</p><p>Whether or not a given comparison of a pair of distinct translations of the same source input, s 1 and s 2 , is counted as a concordant (Conc) or disconcordant (Disc) pair is defined by the following matrix:</p><formula xml:id="formula_4">Metric s 1 &lt; s 2 s 1 = s 2 s 1 &gt; s 2 Human s 1 &lt; s 2 Conc Disc Disc s 1 = s 2 − − − s 1 &gt; s 2 Disc Disc Conc</formula><p>In the notation of <ref type="bibr" target="#b22">Macháček and Bojar (2014)</ref>, this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR):</p><formula xml:id="formula_5">Metric WMT12 &lt; = &gt; Human &lt; 1 -1 -1 = X X X &gt; -1 -1 1</formula><p>The key differences between the evaluation used in WMT14-WMT16 and evaluation used in WMT17-WMT19 were (1) the move from RR to daRR and (2) the treatment of ties. In the years 2014-2016, ties in metrics scores were not penalized. With the move to daRR, where the quality of the two candidate translations <ref type="bibr">de</ref>   <ref type="figure">Figure 2</ref>: System-level metric significance test results for DA human assessment in newstest2019 for German to Czech, German to French and French to German; green cells denote a statistically significant increase in correlation with human assessment for the metric in a given row over the metric in a given column according to Williams test.      is deemed substantially different and no ties in human judgements arise, it makes sense to penalize ties in metrics' predictions in order to promote discerning metrics.</p><p>Note that the penalization of ties makes our evaluation asymmetric, dependent on whether the metric predicted the tie for a pair where humans predicted &lt;, or &gt;. It is now important to interpret the meaning of the comparison identically for humans and metrics. For error metrics, we thus reverse the sign of the metric score prior to the comparison with human scores: higher scores have to indicate better translation quality. In WMT19, the original authors did this for CharacTER.</p><p>To summarize, the WMT19 Metrics Task for segment-level evaluation:</p><p>• ensures that error metrics are first converted to the same orientation as the human judgements, i.e. higher score indicating higher translation quality,</p><p>• excludes all human ties (this is already implied by the construction of daRR from DA judgements),  en-fi en-gu en-kk Figure 3: daRR segment-level metric significance test results for into English and out-of English language pairs (newstest2019): Green cells denote a significant win for the metric in a given row over the metric in a given column according bootstrap resampling.  <ref type="figure">Figure 4</ref>: daRR segment-level metric significance test results for German to Czech, German to French and French to German (newstest2019): Green cells denote a significant win for the metric in a given row over the metric in a given column according bootstrap resampling.</p><p>• counts metric's ties as a Discordant pairs.</p><p>We employ bootstrap resampling <ref type="bibr" target="#b16">(Koehn, 2004;</ref><ref type="bibr" target="#b11">Graham et al., 2014b</ref>) to estimate confidence intervals for our Kendall's Tau formulation, and metrics with non-overlapping 95% confidence intervals are identified as having statistically significant difference in performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Segment-Level Results</head><p>Results of the segment-level human evaluation for translations sampled from the News Translation <ref type="table" target="#tab_10">Task are shown in Tables 6, 7</ref> and 8, where metric correlations not significantly outperformed by any other metric are highlighted in bold. Head-to-head significance test results for differences in metric performance are included in <ref type="figure">Figures 3 and 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This year, human data was collected from reference-based evaluations (or "monolingual") and reference-free evaluations (or "bilingual").</p><p>The reference-based (monolingual) evaluations were obtained with the help of anonymous crowdsourcing, while the reference-less (bilingual) evaluations were mainly from MT researchers who committed their time contribution to the manual evaluation for each submitted system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Stability across MT Systems</head><p>The observed performance of metrics depends on the underlying texts and systems that participate in the News Translation Task (see Section 2). For the strongest MT systems, distinguishing which system outputs are better is hard, even for human assessors. On the other hand, if the systems are spread across a wide performance range, it will be easier for metrics to correlate with human judgements.</p><p>To provide a more reliable view, we created plots of Pearson correlation when the underlying set of MT systems is reduced to top n ones. One sample such plot is in <ref type="figure" target="#fig_4">Figure 5</ref>, all language pairs and most of the metrics are in Appendix A.</p><p>As the plot documents, the official correlations reported in <ref type="table" target="#tab_0">Tables 3 to 5</ref> can lead to wrong conclusions. sacreBLEU-BLEU correlates at .969 when all systems are considered, but as we start considering only the top n systems, the correlation falls relatively quickly. With 10 systems, we are below .5 and when only the top 6 or 4 systems are considered, the correlation falls even to the negave values. Note that correlations point estimates (the value in the y-axis) become noiser with the decreasing number of the underlying MT systems. <ref type="figure">Figure 6</ref> explains the situation and illus- Top 8</p><p>Top 10</p><p>Top 12</p><p>Top 15</p><p>All systems <ref type="figure">Figure 6</ref> trates the sensitivity of the observed correlations to the exact set of systems. On the full set of systems, the single outlier (the worstperforming system called en_de_task) helps to achieve a great positive correlation. The majority of MT systems however form a cloud with Pearson correlation around .5 and the top 4 systems actually exhibit a negative correlation of the human score and sacreBLEU-BLEU.</p><p>In Appendix A, baseline metrics are plotted in grey in all the plots, so that their trends can be observed jointly. In general, most baselines have similar correlations, as most baselines use similar features (n-gram or word-level features, with the exception of chrF). In a number of language pairs (de-en, de-fr, en-de, en-kk, lten, ru-en, zh-en), baseline correlations tend towards 0 (no correlation) or even negative Pearson correlation. For a widely applied metric such as sacreBLEU-BLEU, our analysis reveals weak correlation in comparing top stateof-the-art systems in these language pairs, especially in en-de, de-en, ru-en, and zh-en.</p><p>We will restrict our analysis to those language pairs where the baseline metrics have an obvious downward trend (de-en, de-fr, en-de, en-kk, lt-en, ru-en, zh-en). Examining the topn correlation in the submitted metrics (not including QE systems), most metrics show the same degredation in correlation as the baselines. We note BERTr as the one exception consistently degrading less and retaining positive correlation compared to other submitted metrics and baselines, in the language pairs where it participated.</p><p>For QE systems, we noticed that in some instances, QE systems have upward correlation trends when other metrics and baselines have downward trends. For instance, LP, UNI, and UNI+ in the de-en language pair, YiSi-2 in en-kk, and UNI and UNI+ in ru-en. These results suggest that QE systems such as UNI and UNI+ perform worse on judging systems of wide ranging quality, but better for top performing systems, or perhaps for systems closer in quality.</p><p>If our method of human assessment is sound, we should believe that BLEU, a widely applied metric, is no longer a reliable metric for judging our best systems. Future investigations are needed to understand when BLEU applies well, and why BLEU is not effective for output from our state of the art models.</p><p>Metrics and QE systems such as BERTr, ESIM, YiSi that perform well at judging our best systems often use more semantic features compared to our n-gram/char-gram based baselines. Future metrics may want to explore a) whether semantic features such as contextual word embeddings are achieving semantic understanding and b) whether semantic understanding is the true source of a metric's performance gains.</p><p>It should be noted that some language pairs do not show the strong degrading pattern with top-n systems this year, for instance en-cs, engu, en-ru, or kk-en. English-Chinese is particularly interesting because we see a clear trend towards better correlations as we reduce the set of underlying systems to the top scoring ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Overall Metric Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">System-Level Evaluation</head><p>In system-level evaluation, the series of YiSi metrics achieve the highest correlations in several language pairs and it is not significantly outperformed by any other metrics (denoted as a "win" in the following) for almost all language pairs.</p><p>The new metric ESIM performs best on 5 language languages (18 language pairs) and obtains 11 "wins" out of 16 language pairs in which ESIM participated.</p><p>The metric EED performs better for language pairs out-of English and excluding En-glish compared to into-English language pairs, achieving 7 out of 11 "wins" there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Segment-Level Evaluation</head><p>For segment-level evaluation, most language pairs are quite discerning, with only one or two metrics taking the "winner" position (of not being significantly surpassed by others). Only French-German differs, with all metrics performing similarly except the significantly worse sentBLEU.</p><p>YiSi-1_srl stands out as the "winner" for all language pairs in which it participated. The excluded language pairs were probably due to the lack of semantic information required by YiSi-1_srl. YiSi-1 participated all language pairs and its correlations are comparable with those of YiSi-1_srl. ESIM obtain 6 "winners" out of all 18 languages pairs.</p><p>Both YiSi and ESIM are based on neural networks (YiSi via word and phrase embeddings, as well as other types of available resources, ESIM via sentence embeddings). This is a confirmation of a trend observed last year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">QE Systems as Metrics</head><p>Generally, correlations for the standard reference-based metrics are obviously better than those in "QE as a Metric" track, both when using monolingual and bilingual golden truth.</p><p>In system-level evaluation, correlations for "QE as a Metric" range from 0.028 to 0.947 across all language pairs and all metrics but they are very unstable. Even for a single metric, take UNI for example, the correlations range from 0.028 to 0.930 across language pairs.</p><p>In segment-level evaluation, correlations for QE metrics range from -0.153 to 0.351 across all language pairs and show the same instability across language pairs for a given metric.</p><p>In either case, we do not see any pattern that could explain the behaviour, e.g. whether the manual evaluation was monolingual or bilingual, or the characteristics of the given language pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dependence on Implementation</head><p>As it already happened in the past, we had multiple implementations for some metrics, BLEU and chrF in particular.</p><p>The detailed configuration of BLEU and sacreBLEU-BLEU differ and hence their scores and correlation results are different.</p><p>chrF and sacreBLEU-chrF use the same parameters and should thus deliver the same scores but we still observe some differences, leading to different correlations. For instance for German-French Pearson correlation, chrF obtains 0.931 (no win) but sacreBLEUchrF reaches 0.952, tying for a win with other metrics.</p><p>We thus fully support the call for clarity by Post (2018b) and invite authors of metrics to include their implementations either in Moses scorer or sacreBLEU to achieve a long-term assessment of their metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper summarizes the results of WMT19 shared task in machine translation evaluation, the Metrics Shared Task. Participating metrics were evaluated in terms of their correlation with human judgement at the level of the whole test set (system-level evaluation), as well as at the level of individual sentences (segment-level evaluation).</p><p>We reported scores for standard metrics requiring the reference as well as quality estimation systems which took part in the track "QE as a metric", joint with the Quality Estimation task.</p><p>For system-level, best metrics reach over 0.95 Pearson correlation or better across several language pairs. As expected, QE systems are visibly in all language pairs but they can also reach high system-level correlations, up to .947 (Chinese-English) or .936 (EnglishGerman) by YiSi-1_srl or over .9 for multiple language pairs by UNI.</p><p>An important caveat is that the correlations are heavily affected by the underlying set of MT systems. We explored this by reducing the set of systems to top-n ones for various ns and found out that for many language pairs, system-level correlations are much worse when based on only the better performing systems. With both good and bad MT systems partic-ipating in the news task, the metrics results can be overly optimistic compared to what we get when evaluating state-of-the-art systems.</p><p>In terms of segment-level Kendall's τ results, the standard metrics correlations varied between 0.03 and 0.59, and QE systems obtained even negative correlations.</p><p>The results confirm the observation from the last year, namely metrics based on word or sentence-level embeddings (YiSi and ESIM), achieve the highest performance. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Pearson correlations of sacreBLEU-BLEU for English-German system-level evaluation for all systems (left) down to only top 4 systems (right). The y-axis spans from -1 to +1, baseline metrics for the language pair in grey.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>5</head><label>5</label><figDesc></figDesc><table>DA&gt;1 Ave DA pairs daRR 

de-en 
2,000 16.0 
239,220 85,365 
fi-en 
1,996 
9.5 
83,168 38,307 
gu-en 
1,016 11.0 
55,880 31,139 
kk-en 
1,000 11.0 
55,000 27,094 
lt-en 
1,000 11.0 
55,000 21,862 
ru-en 
1,999 11.9 
131,766 46,172 
zh-en 
2,000 10.1 
95,174 31,070 

en-cs 
1,997 
9.1 
75,560 27,178 
en-de 
1,997 19.1 
347,109 99,840 
en-fi 
1,997 
8.1 
59,129 31,820 
en-gu 
998 
6.9 
21,854 11,355 
en-kk 
998 
9.0 
37,032 18,172 
en-lt 
998 
9.0 
36,435 17,401 
en-ru 
1,997 
8.7 
69,503 24,334 
en-zh 
1,997 
9.8 
87,501 18,658 

de-cs 
1,997 
8.5 
65,039 35,793 
de-fr 
1,605 
4.1 
12,055 
4,862 
fr-de 
1,224 
3.0 
4,258 
1,369 

newstest2019 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Participants of WMT19 Metrics Shared Task. "•" denotes that the metric took part in (some of the language pairs) of the segment-and/or system-level 
evaluation. "⊘" indicates that the system-level scores are implied, simply taking arithmetic (macro-)average of segment-level scores. "−" indicates that the metric 

didn't participate the track (Seg/Sys-level). A metric is learned if it is trained on a QE or metric evaluation dataset (i.e. pretraining or parsers don't count, but 
training on WMT 2017 metrics task data does). For the baseline metrics available in the Moses toolkit, paths are relative to 

http://github.com/moses-smt/ 

mosesdecoder/. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Absolute Pearson correlation of to-English system-level metrics with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.</figDesc><table>en-cs en-de 
en-fi 
en-gu en-kk en-lt en-ru en-zh 

n 
11 
22 
12 
11 
11 
12 
12 
12 

Correlation 
|r| 
|r| 
|r| 
|r| 
|r| 
|r| 
|r| 
|r| 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Absolute Pearson correlation of out-of-English system-level metrics with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.</figDesc><table>de-en 
fi-en 
gu-en 

YiSi.1_srl 
YiSi.1 
ESIM 
BERTr 
chrF 
chrF. 
sacreBLEU.chrF 
BEER 
EED 
YiSi.0 
CharacTER 
Meteor.._2.0.syntax.copy. 
CDER 
Meteor.._2.0.syntax. 
PER 
TER 
WMDO 
WER 
UNI. 
BLEU 
UNI 
sacreBLEU.BLEU 
NIST 
YiSi.2_srl 
YiSi.2 
PReP 
LP.1 
ibm1.morpheme 
ibm1.pos4gram 
LASIM 

LASIM 
ibm1.pos4gram 
ibm1.morpheme 
LP.1 
PReP 
YiSi.2 
YiSi.2_srl 
NIST 
sacreBLEU.BLEU 
UNI 
BLEU 
UNI. 
WER 
WMDO 
TER 
PER 
Meteor.._2.0.syntax. 
CDER 
Meteor.._2.0.syntax.copy. 
CharacTER 
YiSi.0 
EED 
BEER 
sacreBLEU.chrF 
chrF. 
chrF 
BERTr 
ESIM 
YiSi.1 
YiSi.1_srl 

Meteor.._2.0.syntax.copy. 
Meteor.._2.0.syntax. 
EED 
BEER 
YiSi.0 
chrF. 
chrF 
PER 
CharacTER 
sacreBLEU.chrF 
YiSi.1_srl 
YiSi.1 
CDER 
WMDO 
NIST 
sacreBLEU.BLEU 
TER 
BERTr 
WER 
BLEU 
ESIM 
UNI 
UNI. 
ibm1.morpheme 
YiSi.2 
PReP 

PReP 
YiSi.2 
ibm1.morpheme 
UNI. 
UNI 
ESIM 
BLEU 
WER 
BERTr 
TER 
sacreBLEU.BLEU 
NIST 
WMDO 
CDER 
YiSi.1 
YiSi.1_srl 
sacreBLEU.chrF 
CharacTER 
PER 
chrF 
chrF. 
YiSi.0 
BEER 
EED 
Meteor.._2.0.syntax. 
Meteor.._2.0.syntax.copy. 

YiSi.0 
WMDO 
EED 
chrF 
sacreBLEU.chrF 
BEER 
chrF. 
BERTr 
NIST 
YiSi.1 
CharacTER 
YiSi.1_srl 
PER 
Meteor.._2.0.syntax. 
Meteor.._2.0.syntax.copy. 
TER 
ESIM 
CDER 
WER 
BLEU 
sacreBLEU.BLEU 
PReP 
YiSi.2 

YiSi.2 

PReP 

sacreBLEU.BLEU 

BLEU 

WER 

CDER 

ESIM 

TER 

Meteor.._2.0.syntax.copy. 

Meteor.._2.0.syntax. 

PER 

YiSi.1_srl 

CharacTER 

YiSi.1 

NIST 

BERTr 

chrF. 

BEER 

sacreBLEU.chrF 

chrF 

EED 

WMDO 

YiSi.0 

kk-en 
lt-en 
ru-en 

WMDO 
YiSi.1 
YiSi.1_srl 
YiSi.0 
BERTr 
ESIM 
BEER 
EED 
chrF 
chrF. 
hLEPORa_baseline 
hLEPORb_baseline 
Meteor.._2.0.syntax. 
Meteor.._2.0.syntax.copy. 
sacreBLEU.chrF 
CDER 
CharacTER 
BLEU 
sacreBLEU.BLEU 
NIST 
TER 
WER 
PReP 
PER 
YiSi.2 

YiSi.2 
PER 
PReP 
WER 
TER 
NIST 
sacreBLEU.BLEU 
BLEU 
CharacTER 
CDER 
sacreBLEU.chrF 
Meteor.._2.0.syntax.copy. 
Meteor.._2.0.syntax. 
hLEPORb_baseline 
hLEPORa_baseline 
chrF. 
chrF 
EED 
BEER 
ESIM 
BERTr 
YiSi.0 
YiSi.1_srl 
YiSi.1 
WMDO 

ESIM 
YiSi.1_srl 
YiSi.1 
CDER 
BLEU 
WER 
TER 
sacreBLEU.BLEU 
CharacTER 
BERTr 
PER 
BEER 
NIST 
chrF. 
chrF 
sacreBLEU.chrF 
EED 
Meteor.._2.0.syntax. 
Meteor.._2.0.syntax.copy. 
YiSi.0 
hLEPORb_baseline 
WMDO 
PReP 
ibm1.morpheme 
YiSi.2 

YiSi.2 
ibm1.morpheme 
PReP 
WMDO 
hLEPORb_baseline 
YiSi.0 
Meteor.._2.0.syntax.copy. 
Meteor.._2.0.syntax. 
EED 
sacreBLEU.chrF 
chrF 
chrF. 
NIST 
BEER 
PER 
BERTr 
CharacTER 
sacreBLEU.BLEU 
TER 
WER 
BLEU 
CDER 
YiSi.1 
YiSi.1_srl 
ESIM 

YiSi.1 
YiSi.1_srl 
BERTr 
ESIM 
YiSi.0 
Meteor.._2.0.syntax.copy. 
Meteor.._2.0.syntax. 
EED 
chrF 
chrF. 
WMDO 
NIST 
CharacTER 
PER 
sacreBLEU.chrF 
TER 
BEER 
WER 
CDER 
BLEU 
sacreBLEU.BLEU 
UNI. 
UNI 
PReP 
LP.1 
YiSi.2 
LASIM 

LASIM 
YiSi.2 
LP.1 
PReP 
UNI 
UNI. 
sacreBLEU.BLEU 
BLEU 
CDER 
WER 
BEER 
TER 
sacreBLEU.chrF 
PER 
CharacTER 
NIST 
WMDO 
chrF. 
chrF 
EED 
Meteor.._2.0.syntax. 
Meteor.._2.0.syntax.copy. 
YiSi.0 
ESIM 
BERTr 
YiSi.1_srl 
YiSi.1 

zh-en 
en-cs 
en-de 

ESIM 
YiSi.1 
YiSi.1_srl 
BERTr 
chrF. 
chrF 
sacreBLEU.chrF 
PER 
Meteor.._2.0.syntax.copy. 
EED 
Meteor.._2.0.syntax. 
hLEPORb_baseline 
hLEPORa_baseline 
YiSi.2_srl 
WMDO 
CharacTER 
BEER 
YiSi.2 
YiSi.0 
NIST 
CDER 
sacreBLEU.BLEU 
BLEU 
TER 
WER 
PReP 

PReP 
WER 
TER 
BLEU 
sacreBLEU.BLEU 
CDER 
NIST 
YiSi.0 
YiSi.2 
BEER 
CharacTER 
WMDO 
YiSi.2_srl 
hLEPORa_baseline 
hLEPORb_baseline 
Meteor.._2.0.syntax. 
EED 
Meteor.._2.0.syntax.copy. 
PER 
sacreBLEU.chrF 
chrF 
chrF. 
BERTr 
YiSi.1_srl 
YiSi.1 
ESIM 

sacreBLEU.BLEU 
CharacTER 
EED 
YiSi.0 
chrF. 
BEER 
chrF 
CDER 
sacreBLEU.chrF 
WER 
TER 
PER 
YiSi.1 
BLEU 
NIST 
ibm1.morpheme 
YiSi.2 
UNI 

UNI 

YiSi.2 

ibm1.morpheme 

NIST 

BLEU 

YiSi.1 

PER 

TER 

WER 

sacreBLEU.chrF 

CDER 

chrF 

BEER 

chrF. 

YiSi.0 

EED 

CharacTER 

sacreBLEU.BLEU 

ESIM 
YiSi.1 
YiSi.1_srl 
CharacTER 
EED 
YiSi.0 
BEER 
chrF. 
chrF 
sacreBLEU.chrF 
CDER 
PER 
sacreBLEU.BLEU 
TER 
WER 
YiSi.2_srl 
YiSi.2 
BLEU 
LASIM 
ibm1.morpheme 
UNI 
LP.1 
ibm1.pos4gram 
NIST 
USFD 
USFD.TL 

USFD.TL 
USFD 
NIST 
ibm1.pos4gram 
LP.1 
UNI 
ibm1.morpheme 
LASIM 
BLEU 
YiSi.2 
YiSi.2_srl 
WER 
TER 
sacreBLEU.BLEU 
PER 
CDER 
sacreBLEU.chrF 
chrF 
chrF. 
BEER 
YiSi.0 
EED 
CharacTER 
YiSi.1_srl 
YiSi.1 
ESIM 

en-fi 
en-gu 
en-kk 

BEER 
EED 
YiSi.0 
chrF 
chrF. 
PER 
TER 
sacreBLEU.chrF 
WER 
CDER 
YiSi.1 
NIST 
BLEU 
CharacTER 
sacreBLEU.BLEU 
ESIM 
UNI 
YiSi.2 
ibm1.morpheme 

ibm1.morpheme 

YiSi.2 

UNI 

ESIM 

sacreBLEU.BLEU 

CharacTER 

BLEU 

NIST 

YiSi.1 

CDER 

WER 

sacreBLEU.chrF 

TER 

PER 

chrF. 

chrF 

YiSi.0 

EED 

BEER 

CharacTER 
YiSi.1 
EED 
TER 
YiSi.0 
WER 
chrF. 
sacreBLEU.chrF 
chrF 
hLEPORa_baseline 
hLEPORb_baseline 
CDER 
PER 
BEER 
NIST 
BLEU 
sacreBLEU.BLEU 
YiSi.2 

YiSi.2 

sacreBLEU.BLEU 

BLEU 

NIST 

BEER 

PER 

CDER 

hLEPORb_baseline 

hLEPORa_baseline 

chrF 

sacreBLEU.chrF 

chrF. 

WER 

YiSi.0 

TER 

EED 

YiSi.1 

CharacTER 

YiSi.1 
ESIM 
EED 
chrF. 
YiSi.0 
chrF 
BEER 
hLEPORa_baseline 
hLEPORb_baseline 
sacreBLEU.chrF 
TER 
WER 
CharacTER 
NIST 
CDER 
PER 
sacreBLEU.BLEU 
BLEU 
YiSi.2 

YiSi.2 

BLEU 

sacreBLEU.BLEU 

PER 

CDER 

NIST 

CharacTER 

WER 

TER 

sacreBLEU.chrF 

hLEPORb_baseline 

hLEPORa_baseline 

BEER 

chrF 

YiSi.0 

chrF. 

EED 

ESIM 

YiSi.1 

en-lt 
en-ru 
en-zh 

TER 
NIST 
WER 
ESIM 
BLEU 
sacreBLEU.BLEU 
CDER 
PER 
BEER 
chrF. 
chrF 
hLEPORb_baseline 
EED 
YiSi.0 
sacreBLEU.chrF 
YiSi.1 
CharacTER 
ibm1.morpheme 
YiSi.2 

YiSi.2 

ibm1.morpheme 

CharacTER 

YiSi.1 

sacreBLEU.chrF 

YiSi.0 

EED 

hLEPORb_baseline 

chrF 

chrF. 

BEER 

PER 

CDER 

sacreBLEU.BLEU 

BLEU 

ESIM 

WER 

NIST 

TER 

TER 
WER 
CDER 
YiSi.1 
ESIM 
NIST 
BLEU 
CharacTER 
sacreBLEU.chrF 
PER 
sacreBLEU.BLEU 
BEER 
EED 
YiSi.0 
chrF. 
chrF 
UNI 
UNI. 
USFD 
LASIM 
USFD.TL 
YiSi.2 
LP.1 

LP.1 

YiSi.2 

USFD.TL 

LASIM 

USFD 

UNI. 

UNI 

chrF 

chrF. 

YiSi.0 

EED 

BEER 

sacreBLEU.BLEU 

PER 

sacreBLEU.chrF 

CharacTER 

BLEU 

NIST 

ESIM 

YiSi.1 

CDER 

WER 

TER 

YiSi.1 
YiSi.1_srl 
ESIM 
CDER 
BLEU 
PER 
NIST 
chrF 
chrF. 
WER 
CharacTER 
YiSi.0 
EED 
TER 
BEER 
sacreBLEU.BLEU 
sacreBLEU.chrF 
YiSi.2_srl 
YiSi.2 

YiSi.2 

YiSi.2_srl 

sacreBLEU.chrF 

sacreBLEU.BLEU 

BEER 

TER 

EED 

YiSi.0 

CharacTER 

WER 

chrF. 

chrF 

NIST 

PER 

BLEU 

CDER 

ESIM 

YiSi.1_srl 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Absolute Pearson correlation of system-level metrics for language pairs not involving English with DA human assessment in newstest2019; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.</figDesc><table>de-cs 
de-fr 
fr-de 

EED 
ESIM 
YiSi.0 
BEER 
sacreBLEU.chrF 
chrF 
YiSi.1 
chrF. 
CharacTER 
hLEPORb_baseline 
NIST 
BLEU 
hLEPORa_baseline 
TER 
PER 
WER 
sacreBLEU.BLEU 
CDER 
YiSi.2 
ibm1.morpheme 

ibm1.morpheme 

YiSi.2 

CDER 

sacreBLEU.BLEU 

WER 

PER 

TER 

hLEPORa_baseline 

BLEU 

NIST 

hLEPORb_baseline 

CharacTER 

chrF. 

YiSi.1 

chrF 

sacreBLEU.chrF 

BEER 

YiSi.0 

ESIM 

EED 

YiSi.1 
WER 
TER 
YiSi.0 
sacreBLEU.chrF 
ESIM 
CDER 
BEER 
EED 
chrF. 
chrF 
CharacTER 
NIST 
BLEU 
sacreBLEU.BLEU 
PER 
hLEPORb_baseline 
hLEPORa_baseline 
YiSi.2 
ibm1.morpheme 
ibm1.pos4gram 

ibm1.pos4gram 

ibm1.morpheme 

YiSi.2 

hLEPORa_baseline 

hLEPORb_baseline 

PER 

sacreBLEU.BLEU 

BLEU 

NIST 

CharacTER 

chrF 

chrF. 

EED 

BEER 

CDER 

ESIM 

sacreBLEU.chrF 

YiSi.0 

TER 

WER 

YiSi.1 

ESIM 
YiSi.1_srl 
YiSi.1 
PER 
TER 
WER 
sacreBLEU.chrF 
sacreBLEU.BLEU 
chrF 
BLEU 
NIST 
CDER 
EED 
CharacTER 
BEER 
chrF. 
YiSi.0 
ibm1.morpheme 
YiSi.2 
ibm1.pos4gram 

ibm1.pos4gram 

YiSi.2 

ibm1.morpheme 

YiSi.0 

chrF. 

BEER 

CharacTER 

EED 

CDER 

NIST 

BLEU 

chrF 

sacreBLEU.BLEU 

sacreBLEU.chrF 

WER 

TER 

PER 

YiSi.1 

YiSi.1_srl 

ESIM 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Segment-level metric results for to-English language pairs in newstest2019: absolute Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.</figDesc><table>en-cs 
en-de 
en-fi en-gu en-kk 
en-lt 
en-ru en-zh 

Human Evaluation 
daRR 
daRR 
daRR daRR daRR 
daRR 
daRR daRR 
n 
27,178 
99,840 
31,820 11,355 18,172 
17,401 
24,334 18,658 

BEER 
0.443 
0.316 
0.514 
0.537 
0.516 
0.441 
0.542 
0.232 
CharacTER 
0.349 
0.264 
0.404 
0.500 
0.351 
0.311 
0.432 
0.094 
chrF 
0.455 
0.326 
0.514 
0.534 
0.479 
0.446 
0.539 
0.301 
chrF+ 
0.458 
0.327 
0.514 
0.538 
0.491 
0.448 
0.543 
0.296 
EED 
0.431 
0.315 
0.508 0.568 
0.518 
0.425 
0.546 
0.257 
ESIM 
− 
0.329 
0.511 
− 
0.510 
0.428 
0.572 0.339 
hLEPORa_baseline 
− 
− 
− 
0.463 
0.390 
− 
− 
− 
sentBLEU 
0.367 
0.248 
0.396 
0.465 
0.392 
0.334 
0.469 
0.270 
YiSi-0 
0.406 
0.304 
0.483 
0.539 
0.494 
0.402 
0.535 
0.266 
YiSi-</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="true"><head>Table 7 :</head><label>7</label><figDesc>Segment-level metric results for out-of-English language pairs in newstest2019: absolute Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of metrics not significantly outperformed by any other for that language pair are highlighted in bold.</figDesc><table>de-cs 
de-fr 
fr-de 

Human Evaluation 
daRR 
daRR 
daRR 
n 
35,793 
4,862 
1,369 

BEER 
0.337 
0.293 
0.265 
CharacTER 
0.232 
0.251 
0.224 
chrF 
0.326 
0.284 
0.275 
chrF+ 
0.326 
0.284 
0.278 
EED 
0.345 
0.301 
0.267 
ESIM 
0.331 
0.290 
0.289 
hLEPORa_baseline 
0.207 
0.239 
− 
sentBLEU 
0.203 
0.235 
0.179 
YiSi-0 
0.331 
0.296 
0.277 
YiSi-1 
0.376 
0.349 
0.310 
YiSi-1_srl 
− 
− 
0.299 

QE as a Metric: 
ibm1-morpheme 
0.048 
−0.013 −0.053 
ibm1-pos4gram 
− 
−0.074 −0.097 
YiSi-2 
0.199 
0.186 
0.066 
newstest2019 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>Table 8 :</head><label>8</label><figDesc>Segment-level metric results for language pairs not involving English in newstest2019: ab- solute Kendall's Tau formulation of segment-level metric scores with DA scores; correlations of met- rics not significantly outperformed by any other for that language pair are highlighted in bold.</figDesc><table></table></figure>

			<note place="foot" n="1"> http://www.statmt.org/wmt19/metrics-task. html</note>

			<note place="foot" n="4"> For segment-level evaluation, one would need to collect many manual evaluations of the exact same segment as produced by each MT system. Such a sampling would be however wasteful for the evaluation needed by WMT, so only some MT systems happen to be evaluated for a given input sentence. In principle, we would like to return to DA&apos;s standard segment-level evaluation in future, where a minimum of 15 human judgements of translation quality are collected per translation and combined to get highly accurate scores for translations, but this would increase annotation costs. 5 Since the analogue rating scale employed by DA is marked at the 0-25-50-75-100 points, we use 25 points as the minimum required difference between two system scores to produce daRR judgements. Note that we</note>

			<note place="foot" n="6"> https://github.com/moses-smt/mosesdecoder/ blob/master/mert/evaluator.cpp 7 If you prefer standard BLEU, we recommend sacre-BLEU (Post, 2018a), found at https://github.com/ mjpost/sacreBLEU. 8 http://www.itl.nist.gov/iad/mig/tools/ 9 International tokenization is found to perform slightly better (Macháček and Bojar, 2013).</note>

			<note place="foot" n="10"> We originally planned to use the chrF implementation which was recently made available in Moses Scorer but it mishandles Unicode characters for now. 11 http://hdl.handle.net/11346/WMT17-TVXH</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Results in this shared task would not be possible without tight collaboration with organizers of the WMT News Translation Task. We would like to thank Marcin Junczys-Dowmunt for the suggestion to examine metrics performance across varying subsets of MT systems, as we did in Appendix A.</p><p>This study was supported in parts by the grants 19-26934X (NEUREM3) of the Czech Science Foundation, ADAPT Centre for Digital Content Technology (www.adaptcentre. ie) at Dublin City University funded under the SFI Research Centres Programme (Grant 13/RC/2106) co-funded under the European Regional Development Fund, and Charles University Research Programme "Progres" Q18+Q48.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Paraphrasing with bilingual parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Bannard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05</title>
		<meeting>the 43rd Annual Meeting on Association for Computational Linguistics, ACL &apos;05<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="597" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costajussà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<title level="m">Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting><address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translation Evaluation -From Fragmented Tools and Data Sets to an Integrated Ecosystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC 2016 Workshop</title>
		<meeting>the LREC 2016 Workshop<address><addrLine>Portorose, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
	<note>Ten Years of WMT Evaluation Campaigns: Lessons Learnt</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Results of the WMT17 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Task on Quality Estimation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Testing for Significance of Increased Correlation with Human Judgment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="172" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Continuous Measurement Scales in Human Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop &amp; Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop &amp; Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Is Machine Translation Getting Better over Time?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="443" to="451" />
		</imprint>
	</monogr>
	<note>Sweden</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Can machine translation systems be evaluated by the crowd alone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Alistair Moffat, and Justin Zobel</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Achieving Accurate Conclusions in Evaluation of Automatic Machine Translation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Randomized significance tests in machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the ACL 2014 Ninth Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accurate Evaluation of Segment-level Machine Translation Metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meteor++ 2.0: Adopt Syntactic Level Paraphrase Knowledge into Machine Translation Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinuo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lepor: A robust evaluation metric for machine translation with augmented factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron L.-F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012)</title>
		<meeting>the 24th International Conference on Computational Linguistics (COLING 2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="441" to="450" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Language-independent model for machine translation evaluation with reinforced factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron L.-F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangye</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Translation Summit XIV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="215" to="222" />
		</imprint>
	</monogr>
	<note>International Association for Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Empirical Methods in Natural Language Processing</title>
		<meeting>of Empirical Methods in Natural Language essing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Manual and Automatic Evaluation of Machine Translation Between European Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Statistical Machine Translation, StatMT &apos;06</title>
		<meeting>the Workshop on Statistical Machine Translation, StatMT &apos;06<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="102" to="121" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A novel string-to-string distance measure with applications to machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Leusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Mt Summit IX</title>
		<meeting>Mt Summit IX</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">CDER: Efficient MT Evaluation Using Block Movements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Leusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="241" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Results of the WMT18 metrics shared task: Both characters and embeddings achieve good performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shared Task Papers, Brussels, Belgium. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the Third Conference on Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Results of the WMT14 metrics shared task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matouš</forename><surname>Macháček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Workshop on Statistical Machine Translation</title>
		<meeting>the Ninth Workshop on Statistical Machine Translation<address><addrLine>Baltimore, MD, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="293" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Results of the WMT13 Metrics Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matouš</forename><surname>Macháček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Workshop on Statistical Machine Translation</title>
		<meeting>the Eighth Workshop on Statistical Machine Translation<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Putting evaluation in context: Contextual embeddings improve machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>short papers. To appear</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BLEU: A Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
		<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Morpheme-and POS-based IBM1 and language model scores for translation quality estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT@NAACL-HLT 2012</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation, WMT@NAACL-HLT 2012<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06-07" />
			<biblScope unit="page" from="133" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">chrF: character n-gram Fscore for automatic MT evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">chrF++: words helping character n-grams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
		<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Shared Tasks Papers</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
		<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting bleu scores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation</title>
		<meeting>the Third Conference on Machine Translation<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A study of translation edit rate with targeted human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Makhoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Association for Machine Translation in the Americas</title>
		<meeting>Association for Machine Translation in the Americas</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="223" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">EED: Extended Edit Distance Measure for Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stanchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BEER 1.1: ILLC UvA submission to metrics and tuning task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miloš</forename><surname>Stanojević</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
		<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisboa</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Portugal. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Character: Translation edit rate on character level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2016 First Conference on Machine Translation</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="505" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CharacTer: Translation Edit Rate on Character Level</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Thorsten</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">James</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>Wiley</publisher>
			<biblScope unit="volume">14</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quality Estimation and Translation Metrics via Pre-trained Word and Sentence Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizaveta</forename><surname>Yankovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Tättar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Fishel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Filtering Pseudo-References by Paraphrasing for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Shimanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukio</forename><surname>Matsumura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayahide</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoru</forename><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
