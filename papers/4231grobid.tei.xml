<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Distinguishable Visual Key Fingerprints Human Distinguishable Visual Key Fingerprints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhgan</forename><surname>Azimpourkivi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bloomberg;</roleName><forename type="first">Umut</forename><surname>Topkara</surname></persName>
							<email>topkara@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>FIU</roleName><forename type="first">Bogdan</forename><surname>Carbunar</surname></persName>
							<email>carbunar@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhgan</forename><surname>Azimpourkivi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bloomberg</forename><surname>Lp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umut</forename><surname>Topkara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bloomberg</forename><surname>Lp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><forename type="middle">Carbunar</forename><surname>Fiu</surname></persName>
						</author>
						<title level="a" type="main">Human Distinguishable Visual Key Fingerprints Human Distinguishable Visual Key Fingerprints</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX. https://www.usenix.org/conference/usenixsecurity20/presentation/azimpourkivi</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Visual fingerprints are used in human verification of identities to improve security against impersonation attacks. The verification requires the user to confirm that the visual fingerprint image derived from the trusted source is the same as the one derived from the unknown source. We introduce CEAL, a novel mechanism to build generators for visual fingerprint representations of arbitrary public strings. CEAL stands out from existing approaches in three significant aspects: (1) eliminates the need for hand curated image generation rules by learning a generator model that imitates the style and domain of fingerprint images from a large collection of sample images , hence enabling easy customizability, (2) operates within limits of the visual discriminative ability of human perception , such that the learned fingerprint image generator avoids mapping distinct keys to images which are not distinguish-able by humans, and (3) the resulting model deterministically generates realistic fingerprint images from an input vector, where the vector components are designated to control visual properties which are either readily perceptible to a human eye, or imperceptible, yet necessary for accurately modeling the target image domain. Unlike existing visual fingerprint generators, CEAL factors in the limits of human perception, and pushes the key pay-load capacity of the images toward the limits of its generative model: We have built a generative network for nature landscape images which can reliably encode 123 bits of entropy in the fingerprint. We label 3,996 image pairs by 931 participants. In experiments with 402 million attack image pairs, we found that pre-image attacks performed by adversaries equipped with the human perception discriminators that we build, achieve a success rate against CEAL that is at most 2 ×10 −4 %. The CEAL generator model is small (67MB) and efficient (2.3s to generate an image fingerprint on a laptop).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Text-based KFGs transform an input key (e.g., a hashed public key) into a shorter, human readable format. The most commonly used textual KFG encodes the key into a hexadecimal or Base32 representation. On a study with 1047 participants, Dechand et al. <ref type="bibr" target="#b12">[14]</ref> show that the hexadecimal representation is more vulnerable to partial preimage attacks. Visual KFG (VKFG) solutions synthetically generate images to act as visual key fingerprints. For instance, Random art <ref type="bibr" target="#b39">[41]</ref> and its implementation Vash <ref type="bibr" target="#b0">[1]</ref> use the input key to generate a structured image (see § 10.4 for a detailed description). Other solutions, e.g., <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> similarly generate visual key fingerprints using a combination of colors, patterns and shapes. Avatar representation techniques such as Unicorn <ref type="bibr" target="#b50">[52]</ref>, can also be used as visual key representations <ref type="bibr" target="#b48">[50]</ref>. WP_MonsterID <ref type="bibr" target="#b3">[5]</ref> generates the visual representation as a collage of randomly selected parts from an existing dataset of images. However, Hsiao et al. <ref type="bibr" target="#b23">[24]</ref> argue that an increase of the capacity of such visual solutions, requires an increase in the number of colors, patterns and shapes used, which consequently makes the images hard to distinguish. Thus, even though such solutions use an additional source of entropy (e.g., PRNG), they have not been designed to generate humandistinguishable images.</p><p>The user studies of Tan et al. <ref type="bibr" target="#b48">[50]</ref> that compare multiple text and visual KFG solutions suggest that VKFGs can speed up the verification of key fingerprints. In their experiments, Vash <ref type="bibr" target="#b0">[1]</ref> outperforms the unicorn solution <ref type="bibr" target="#b50">[52]</ref> and several text-based KFGs (e.g. hexadecimal and numeric representations) in terms of both attack detection rate and comparison time. However, the attack success rate against Vash is fairly high, at 12%. In § 10.4, we study Vash, and confirm that despite its reliance on a PRNG, Vash is unable to satisfy the properties that we introduce in § 3. Particularly, we show that not all the images generated by Vash are human-distinguishable, especially when the number of overlaid shapes and colors on the canvas increases. This is expected, as the visual sensitivity of humans to changes, diminishes with increased spatial frequency <ref type="bibr" target="#b51">[53]</ref>.</p><p>In contrast, CEAL is the first VKFG designed to ensure that the human visual system can differentiate between image fingerprints generated from different keys. This endows CEAL with resilience to adversaries that exceeds the strength assumed in state-of-the-art attacks (of Tan et al. <ref type="bibr" target="#b48">[50]</ref> and Dechand et al. <ref type="bibr" target="#b12">[14]</ref>). In § 10.3, we show that even adversaries who control all but one of the bits of the input string hash, achieve only a 1.7% success rate. Further, in Section 10.5, we confirm that when compared to Vash, CEAL is not only more attack resilient, but also enables faster human verifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Informally, we consider the scenario depicted in <ref type="figure" target="#fig_0">Figure 2</ref>: the user is presented with two images, one acquired from a trusted source, and one generated from data received from an untrusted source (e.g., public key, shared key, Bitcoin address, IP address, domain name). The images are not necessarily available or presented on the same device. Instead, they could be displayed on different screens (e.g., of a smartphone and a laptop) or on a device and a physical medium, e.g., a printed card. To authenticate the untrusted data, the user needs to compare the two images for equality. We note that the user does not need to memorize images, but only visually compare the two images for equality.</p><p>More generally, we seek to construct a set of images, where each image can be easily and quickly distinguished from any other image in the set, by a human. Furthermore, we desire to construct a hash-like mapping function, from an input space of strings of the same size to this set of images. In the following, for simplicity, we also refer to input strings as keys. We represent a given key with an image, which will not be confused for another key's image representation. To prevent brute-force attacks, we require the set of images to be large, and infeasible to store and enumerate. Therefore, we define our set through a generator, which takes an input key and outputs the corresponding element in the set. In the rest of this section, we provide a formal definition of the visual fingerprint problem, and introduce mechanisms which we have used to build our solution.</p><p>We define the set of RGB images I , and a function HPD ratio : I × I → <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> that captures the proportion of experiments where humans would perceive the pair of images to be distinguishable. Let P i, j u ∈ {0, 1}, denote the result of the u th human perception experiment on an image pair I i , I j ∈ I , P i, j u = 1 if and only if the human perceives the images to be different, P i, j u = 0 otherwise. Then, if n is the number of human experiments conducted per each image pair,</p><formula xml:id="formula_0">HPD ratio (I i , I j ) = ∑ n u=1 P i, j u n .</formula><p>We seek to build a strong visual key fingerprint generation function V s : {0, 1} γ → I S , where, I S ⊂ I and γ is the input string length. V s , and thereby I S , has the following desired property: For all binary input strings K i , K j ∈ {0, 1} γ , and their corresponding mapped images I i , I j ∈ I S , where</p><formula xml:id="formula_1">V s (K i ) = I i ,V s (K j ) = I j , K i = K j ⇐⇒ HPD ratio (I i , I j ) = 1</formula><p>(1) In practice, it is very challenging to build a generator that satisfies this strong requirement for all possible human visual systems. Instead, we propose to first build a weak visual key fingerprint generation function V w : {0, 1} γ → I W , where I W ⊂ I . Let d H denote the Hamming distance. The V w is not able to guarantee that key pairs will be distinguishable if their d H is within d, i.e., E(HPD ratio <ref type="bibr">(I i</ref> </p><formula xml:id="formula_2">, I j ) | d H (K i , K j ) &lt; d) &lt; 1 − ε.</formula><p>However, for key pairs whose d H value is at least d, V w is able to guarantee human distinguishability, i.e., ∀K i , K j ∈ {0, 1} γ , and I i = V w (K i ), and</p><formula xml:id="formula_3">I j = V w (K j ), we have d H (K i , K j ) ≥ d ⇐⇒ HPD ratio (I i , I j ) = 1.</formula><p>Weak-to-strong problem decomposition. We thus decompose the problem of building a strong VKFG function into two sub-problems. First, build a weak VKFG function, and identify the minimum value d that satisfies the above requirements. Second, use the identified d to convert the weak VKFG into a strong VKFG function.</p><p>In addition to the human-distinguishability of generated fingerprints, developed solutions should also (1) have a sufficiently large capacity to be resistant against preimage attacks, i.e., the number of unique and human-distinguishable generated images should be large, and (2) ensure that humans are able to quickly compare any pair of generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adversary Model</head><p>We assume an adversary who attempts preimage attacks, i.e., to find input strings whose visual fingerprints will be perceived by a human verifier to be similar to the fingerprint of a specific victim. We assume that the adversary has blackbox access to the weak V w and strong V s functions.</p><p>We consider a (γ, d)-adversary <ref type="bibr" target="#b12">[14]</ref>, able to identify candidate strings that hash within Hamming distance d &lt; γ to the victim's key hash K (γ-bit long). In Section 10.3, we evaluate our solution against an adversary that controls up to 122 out of 123 bits of the input key hash, which is stronger than the 80 out of 112 bits adversary of Dechand et al. <ref type="bibr" target="#b12">[14]</ref>. The strength of our adversary is thus more similar to that of the adversary considered by Tan et al. <ref type="bibr" target="#b48">[50]</ref>, who can perform 2 60 brute force attempts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications</head><p>Immediate applications of visual key fingerprints include traditional remote authentication solutions such as SSH and OpenPGP/GnuPG <ref type="bibr" target="#b7">[9]</ref>, that encode the hash of a public key into a human readable format, for manual comparison <ref type="bibr" target="#b21">[22]</ref>. The more recent End-to-End Encrypted (E2EE) applications (e.g., secure messaging apps <ref type="bibr" target="#b49">[51]</ref> such as WhatsApp <ref type="bibr" target="#b2">[4]</ref>, Viber <ref type="bibr">[3]</ref>, Facebook messenger <ref type="bibr" target="#b11">[13]</ref>), further offer a particularly appealing use case for visual key fingerprints. To authenticate a communication peer, the user needs to visually compare the peer's public key fingerprint against a reference fingerprint that she has previously acquired through a secure channel (e.g., in person, from a trusted site, etc).</p><p>Visual key fingerprints can also be used for device pairing (e.g., Bluetooth Secure Simple Pairing using ECDH <ref type="bibr" target="#b37">[39]</ref>), by having the user visually compare visual key fingerprint images of device keys, displayed on both paired devices.</p><p>The dependence of the HPD model performance on human annotations can be used to setup a mechanism which not only provides a non-cognitive human user detection, but also further improves the HPD. That is, the developed CEAL generator can be used to construct a matching based CAPTCHA where users are asked to mark pairs of "unmatching" images. Pairs labeled by the large number of CAPTCHA answers could then be used to build even more powerful HPD classifiers and CEAL generators, thereby setting up a selfimproving mechanism. In § 11, we further discuss how adversarial interest in breaking CEAL-generated CAPTCHAs would further improve research on human visual perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Background</head><p>We now describe the architecture and training process of Generative Adversarial Networks (GANs) and error correcting codes, which we use to build CEAL. GAN. Deep Generative Models (DGMs) are DNNs that are usually trained, using unsupervised learning, to learn and summarize key features of samples in the training data. The trained model can be used to draw samples from the modeled data distribution, i.e. generate previously unseen, but realistic and plausible instances similar to the samples in the training dataset. There are two major classes of generative models: Variational AutoEncoder (VAE) <ref type="bibr" target="#b29">[30]</ref> and Generative Adversarial Networks (GAN) <ref type="bibr" target="#b16">[18]</ref>.</p><p>CEAL uses a GAN model and trains a generator that takes as input a latent vector, i.e., a set of components randomly selected from a uniform distribution over (−1, 1), to produce realistic and human-distinguishable images. The conventional GAN consists of two competing neural networks: (1) a generator network (G) that transforms the input latent vector into an image, and (2) a discriminator network (D) that differentiates synthetic images, generated by G, from real images in a training dataset. G and D are trained alternately. The competition drives G to generate images that look like images from a training, real image dataset. For CEAL, we use a DC-GAN <ref type="bibr" target="#b40">[42]</ref>-like architecture to generate images that represent a key fingerprint corresponding to an input key string.</p><p>Our approach is also related to, and inspired by work on learning disentangled representations, i.e. interpretable factors of data variation <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>, that seeks to learn a representation that captures the underlying generative structure of data. For instance, InfoGAN <ref type="bibr" target="#b9">[11]</ref> learns to disentangle visual characteristics (e.g. style, color, pose of objects, etc.) of GANgenerated images. Further, SDGAN <ref type="bibr" target="#b14">[16]</ref> uses a supervised approach to train a GAN with latent vector components representing both identities (e.g. individual humans) and observations (e.g. specific photographs) of human faces. In addition, Grathwohl and Wilson <ref type="bibr" target="#b19">[20]</ref> disentangle spatial and temporal features in videos, in an unsupervised fashion. We propose instead a disentanglement of major from minor components: decompose the latent vector into major and minor components, and train major components to encode information about human distinguishability, and the minor components to encode image realism properties. This captures the observation that only a subset of latent vector components are able to trigger human-distinguishable changes in generated images. Error Correcting Codes. We use binary BCH <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b22">23]</ref> codes to map a key fingerprint into the input of CL-GAN (see § 7.3 and § 9.3). A t-error correcting BCH code can correct up to t bits, and the generated code words are guaranteed to be within Hamming distance at least d ≥ 2t + 1 of each other. We represent a t-error correcting code with a message length of n and code word length of k bits as BCH(n, k,t).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Approach</head><p>We introduce the CEAL (CrEdential Assurance Labeling) approach to build a visual key fingerprint generator that will generate realistic images and address the requirements of § 3. CEAL consists of two steps, each solving one of the sub-problems of § 3, see <ref type="figure">Figure 3</ref> .</p><p>In the first step, we train a generator network to be the weak V w function ( § 7.2). That is, the network takes as input a latent vector, and produces a realistic image, human-distinguishable from other images generated from latent vectors that are in <ref type="figure">Figure 3</ref>: The CEAL approach: Train a generator to convert a latent vector to a realistic image, human-distinguishable from other generator produced images. Then, train an input mapper, that converts arbitrary input strings to latent vectors suitable for the previously trained generator.</p><p>Hamming distance at least d (see § 6). We experimentally identify d in § 9.3.</p><p>In the second step, we build an input mapper that converts arbitrary input strings into latent vectors that are within Hamming distance of at least d from other mapped inputs ( § 9.3). We show that it is possible to build the input mapper, thus convert the trained weak V w into a strong V s function, using an error correcting code encoder, e.g., <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b22">23]</ref>,</p><formula xml:id="formula_4">ECC : {0, 1} γ → {0, 1} γ , with minimum distance of d. Specifically, ∀K i , K j ∈ {0, 1} γ , K i = K j =⇒ d H (ECC(K i ), ECC(K j )) ≥ d.</formula><p>Then, the trained system first applies ECC to the input, then applies the V w generator to the encoded string. This ensures that the input to V w will always result in a human-distinguishable output, by the definition of the V w . Therefore, V w • ECC : {0, 1} γ → I W , where I W ⊂ I W , and ∀I 1 , I 2 ∈ I W , HPD ratio (I 1 , I 2 ) = 1.</p><p>How to Train Your Generator. Having access to a HPD ratio function, would allow a generator training algorithm to tap into golden annotations of which images are suitable to generate. In practice, we are not able to run a large number of human perception experiments for any given pair of images. However, given a sufficient number of annotations, a regression predictor model HPD predict :</p><formula xml:id="formula_5">I × I → [0, 1] may be used to approximate the HPD ratio function, E(|HPD predict (I 1 , I 2 ) − HPD ratio (I 1 , I 2 )|) &lt; ε.</formula><p>We show that, even with a small number of annotated data, a very limited classification model HPD equal : I × I → {0, 1} which can detect distinguishable image pairs with high precision at the cost of low recall, P(HPD ratio &gt; 0 | HPD equal (I 1 , I 2 ) = 1) &lt; ε, is sufficient for training a generator which satisfies the strong V s requirement (see Equation 1 and § 3).</p><p>In the following, let K be the input key string (see <ref type="figure">Figure 3)</ref>, and let γ = |K|. The input module converts K into a latent vector L, λ = |L|, γ &lt; λ. The generator network converts L into a fingerprint image. <ref type="table" target="#tab_2">Table 1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">The CEAL System</head><p>We now describe the CEAL training process outlined in Figure 3. Unlike existing techniques that generate images using handcrafted rules, CEAL uses GANs (see § 5) to generate realistic, human-distinguishable images from input strings. While a DCGAN <ref type="bibr" target="#b40">[42]</ref> can model the distribution of the training data and generate previously unseen, but realistic samples from the estimated distribution, in § 8 we show that human workers recruited from Amazon Mechanical Turk (MTurk) often cannot perceive differences between images that are generated by a DCGAN, from similar inputs.</p><p>To address this problem, we introduce CL-GAN, a DCGAN-based deep generative model that we train to generate images that are not only realistic, but also humandistinguishable. We train CL-GAN's generator network G-CEAL, using two classifiers (see <ref type="figure" target="#fig_1">Figure 4</ref>): (1) the CL-GAN discriminator (D-CEAL) that is trained to differentiate synthetically generated images by G-CEAL from a dataset of real images, and (2) the HPD classifier, trained to estimate the likelihood that a human will label a pair of images as either identical or different.</p><p>In the following, we first describe the HPD employed by CL-GAN, then detail the training process of CL-GAN. Finally, we describe CEAL's input mapper module, see <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Human Perception Discriminator (HPD)</head><p>The Human Perception Discriminator (HPD) module takes two images as input, and computes the probability that the images are perceived as being different images by humans. We build HPD using a deep neural network (DNN). The high level architecture of the HPD classifier network (see <ref type="figure" target="#fig_2">Figure 5</ref>), is similar to a Siamese network <ref type="bibr" target="#b10">[12]</ref>. Specifically, the HPD consists of two identical, twin networks (with shared weights). Each network accepts as input one of the two input images and passes it through the layers of a trained network.</p><p>To train a DNN with millions of parameters, we need a large training dataset of labeled samples. However, collecting labeled data is a time consuming and expensive process. To address this problem, we leverage previous studies that have shown that the features that are learned by a DNN are transferable and can be used to perform similar tasks <ref type="bibr" target="#b44">[46,</ref><ref type="bibr" target="#b52">54]</ref>. This is because the representation learned by deep neural network is distributed across different layers, where shallow layers capture low level features (e.g. Gabor-like filters in the image domain), and deeper layers capture more abstract and complicated features (e.g. the face of a cat). It is common practice to use one or a combination of the representations learned by different layers of an existing network, as features for a new, related task. Experimentally investigating which representation performs best for the problem of image matching in HPD is hence a typical feature selection task in machine learning.</p><p>Specifically, we employ a transfer learning approach using the Inception.v1 model <ref type="bibr" target="#b47">[49]</ref>, trained for image classification tasks on the ImageNet dataset <ref type="bibr" target="#b13">[15]</ref> (1.2 million images from 1, 000 categories). We extract 50,176 features for each image, from the Inception.v1 network, i.e., the activations of the "Mixed_5c" layer of Inception.v1. In § 9.1, we experimentally justify the choice of this layer.</p><p>Following the Inception.v1 network, HPD consists of 3 additional fully connected layers, see <ref type="figure" target="#fig_2">Figure 5</ref>. In Section 9.1, we describe our hyper-parameter search process to find the number of layers and the number of nodes in each layer. We use these layers in order to train the HPD. This is because we cannot update the weights of the Inception.v1 layers. Instead, we optimize the weights of the 3 fully-connected layers, using weighted contrastive loss <ref type="bibr" target="#b10">[12]</ref> with L2 regularization. The loss will enable the network to differentiate between the two images; regularization will prevent overfitting. Equation 2 shows how the weights are updated based on the weighted contrastive loss for two input samples I 1 and I 2 : <ref type="bibr" target="#b1">2</ref> (2) θ denotes the model parameters (weights and biases), and Y is the actual class label of the image pair, i.e. 1 for different and 0 for identical images. D w is the Euclidean distance between the outputs of the twin networks (O 1 and O 2 in <ref type="figure" target="#fig_2">Figure 5</ref>) for the input image pairs. r ∈ [0, 1] is the weight (importance) assigned to the positive (different) class and </p><formula xml:id="formula_6">L(θ,Y, I 1 , I 2 ) = 1 2 (1 − r)(1 −Y )(D w 2 ) + 1 2 rY (max(0, µ − D w ))</formula><formula xml:id="formula_7">µ ∈ R, µ &gt; 1 is a margin.</formula><p>After training the 3 additional layers in the twin Siamese network using contrastive loss, the network has learned to differentiate between the input image pairs, i.e. generate distant representations (O 1 and O 2 ) for dissimilar images and similar representations for similar images. We freeze the network weights and feed their derived output, i.e., the component-wise squared differences between the activations of the last layers in the twins networks, to an additional fully connected layer consisting of 1 neuron, i.e., the HPD output, with sigmoid activation function, see <ref type="figure" target="#fig_2">Figure 5</ref>. We optimize this layer's weights using a weighted cross-entropy loss and L2 regularization. The purpose of this last layer is to classify the image pairs into either the "identical" or "different" class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Training CL-GAN</head><p>As described in § 7, we train CL-GAN's generator, G-CEAL, to generate images that are both realistic, and visually distinguishable by humans. We seek to thwart even adversaries who can generate input values K that are at small Hamming distance from a victim K value (see § 3.1). For this, we design G-CEAL to generate fingerprint images that are visually different even when the input keys are similar. We define then the following image pair generation (IPG) process, that takes as input a seed latent vector v of length λ, and an index i ∈ {1, 2, 3, ..., λ}, and outputs two vectors v 1 and v 2 , also of length λ, that differ from v only in their i-th component:   Major and Minor Components. Preliminary experiments with DCGAN revealed that not all input bits have an equal impact on the human-distinguishability of generated images: when changed, some bits produce images that are not human distinguishable. To address this problem, we leverage recent successes in learning disentangled representations <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b9">11,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>, to conjecture that we can decompose the latent vector into (1) a subset of major components, that when changed individually, produce human-perceptible changes in the generated images, and (2) the remaining, minor components, that encode relatively imperceptible characteristics of the images, see <ref type="figure" target="#fig_5">Figure 6</ref>.</p><formula xml:id="formula_8">[ j] = v 2 [ j] = v[ j], ∀ j ∈ {1, 2, 3, ..., λ}, j = i.</formula><p>We build a CL-GAN that verifies this conjecture, by training M (system parameter) latent vector components to become major components, and the rest, i.e., m = λ − M, to become minor components, see <ref type="figure" target="#fig_5">Figure 6</ref>. Consistent with the above IPG, we select extreme values for the M major components, i.e., from the set {−1, 1}, to maximize the effect of each component on the visual characteristics of the generated images. However, we select the values for each of the m minor components, uniformly random from (−1, 1). Train for Human Distinguishability. We leverage the input latent vector to control the visual characteristics of the images generated by G-CEAL. While major components contribute to the image distinguishability thus the capacity of the CEAL VKFG, we leverage minor components to help G-CEAL generate realistic images and maintain other visual aspects of the image. To achieve this, in each adversarial training epoch, we train G-CEAL using 3 steps, to generate (1) visually distinguishable images when the values of individual major components in the latent vectors are changed (flipped between 1 and -1) and (2) visually indistinguishable images when the values for minor components are flipped (see § 7.2).</p><p>Although in each of the 3 steps we use a different set of latent vector pairs as input to G-CEAL, the objective functions for all the steps has the general form shown in Equation 3.</p><formula xml:id="formula_9">L(θ G CEAL ) = α × HPD loss + G loss<label>(3)</label></formula><p>In each step, we implicitly use this equation as the loss function to train G-CEAL. In this equation, HPD loss is the HPD loss that we define exclusively for the step. This loss is an indicator of how different the generated images are, as perceived by a human. α ∈ IR is a weight that determines the contribution of HPD loss to the overall loss value for the step. G loss is the generator loss in the conventional GAN (i.e.,</p><formula xml:id="formula_10">G loss = −log(D CEAL (G CEAL (z))),</formula><p>where z is a sample latent vector). This loss is an indicator of how realistic and visually similar the generated images are, compared to the images in the real image dataset used for training D-CEAL.</p><p>We now describe the 3 training steps M and m = λ − M are the number of major and minor components in the latent vector input to G-CEAL.</p><p>• Step 1: Train major components. This step encourages specific (i.e., the first M) latent vector components to have a visual effect on the generated images. For this, generate M random seed latent vectors. Then, for each index i ∈ {1, 2, 3, ..., M}, use the IPG(i) of Definition 1, along with the corresponding generated seed latent vector, to generate two random latent vectors v 1 and v 2 . Use G-CEAL to generate images I 1 and I 2 from v 1 and v 2 respectively. Use the HPD classifier to compute HPD predict (I 1 , I 2 ). To force the i th component of the latent vector to be a major component, i.e., maximize the effect of the i th component on the visual characteristics of the generated images, we want the HPD classifier to classify all these image pairs (I 1 , I 2 ) as different (class 1). To achieve this, define the HPD loss for the pair of latent vectors to be HPD loss (v 1 , v 2 ) = cross_entropy(1, HPD predict (I 1 , I 2 )).</p><p>• Step 2: Train minor components. This training step encourages minor components to have minimal impact on image distinguishability. For this, generate m random seed latent vectors. Then, for each minor position i ∈ {M + 1, M + 2, ..., λ}, form sample latent vector pairs v 1 and v 2 as in Definition 1. Use G-CEAL on v 1 and v 2 to generate images M 1 and M 2 . To force the i th component of the latent vector to be a minor component, we want the HPD classifier to classify (M 1 , M 2 ) as identical (class 0). For this, define the HPD loss for this pair to be: HPD loss (v 1 , v 2 ) = cross_entropy(0, HPD predict (M 1 , M 2 )).</p><p>• Step 3: Train for variety. During preliminary experiments we observed that using only steps 1 and 2, can lead to training a G-CEAL that generates similar images from randomly picked latent vectors (see <ref type="figure" target="#fig_7">Figure 7)</ref>. Step 3 addresses this problem, by encouraging any 2 major components to impose different effects on the visual characteristics of generated images. For this, generate several batches of random seed latent vectors. Then, for each seed latent vector, pick two random major components i, j ∈ R {1, 2, 3, ..., M} and i = j. Copy seed latent vector v into two other latent vectors v 1 and v 2 , then set v 1 [i] = 1 and v 2 [ j] = 1. Let N 1 and N 2 be the images that are generated by G-CEAL from v 1 and v 2 respectively. Define the loss of the generator as: HPD loss = cross_entropy <ref type="bibr">(1, HPD predict</ref>  <ref type="figure" target="#fig_0">(N 1 , N 2 )</ref>). Train for Realism. As described in § 7, we train G-CEAL to also generate realistic images. We do this because the human visual system was shown to be better at distinguishing changes in images when their content is more natural <ref type="bibr" target="#b38">[40]</ref>. To achieve this, we train G-CEAL also using the output (i.e., real vs. fake) issued by the D-CEAL discriminator for the G-CEAL-generated images of the previous epoch. Then, in each epoch, D-CEAL is also trained, similar to a conventional DCGAN, to discriminate the synthetic images generated by G-CEAL in the above 3 training steps from real images of a training dataset. Subsequently, we train G-CEAL using the classification signal provided by D-CEAL, i.e., G loss in Equation 3. This process encourages G-CEAL to generate (previously unseen) images, that look like the images in the training dataset of real images, and deceive D-CEAL to classify them as real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Input Mapper</head><p>The CEAL approach trains the above G-CEAL generator to be a weak V w function ( § 3). In § 9.3, we detail our experimental process to find the G-CEAL's d value (defined in § 6). We now describe the input mapper module, that solves the second sub-problem of Section 3: convert input keys into codes that are at Hamming distance at least d from each other.</p><p>Specifically, as also illustrated in <ref type="figure" target="#fig_5">Figure 6</ref>, the input mapper takes as input a key (e.g., public key, shared key, Bitcoin address, IP address, domain name) and outputs a latent vector L of length λ, i.e., a code word at Hamming distance at least d from the code word of any other input key. For this, the input mapper first computes a cryptographic hash of the input to produce K, its binary key fingerprint, of length γ. It then uses K to generate both the major and minor components of an output latent vector L as follows. Generate the major components. To generate the major components of the latent vector L, we use an error correcting encoder ECC (see § 5 and § 6). First, we compute ECC(K), then perform a one-to-one mapping between its bits and the major components of L: </p><formula xml:id="formula_11">L[i] = -1 if ECC(K)[i] = 0 and L[i] = 1 if ECC(K)[i] = 1, i = {1, ..., M}. If |ECC(K)| &lt; M,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Data</head><p>To train HPD and CEAL, we use several datasets of real and synthetically generated images, described in the following. Real Outdoor Image Dataset. We used a subset of 150,113 outdoor landscape images (mountains, ocean, forest) of 64 by 64 pixels, from the MIT Places205 dataset <ref type="bibr">[38,</ref><ref type="bibr" target="#b53">55]</ref> to train discriminator networks (vanilla DCGAN and CL-GAN models) to differentiate between real and synthetic images. Ground Truth Human Perception Dataset. We used the following process to collect human-assigned labels to image pairs, which we use to train the HPD network. First, we trained a DCGAN network with a random uniform input latent vector of length λ = 100, using the above real outdoor image dataset. We stopped training the network when we started to observe realistic images similar to the ones in the training dataset (i.e., after 10 epochs). We refer to this trained network as the vanilla DCGAN. We then used the vanilla DCGAN to generate two datasets of synthetic image pairs (see below) and collected their labels using Amazon Mechanical Turk (MTurk) workers. GANs tend to generate similar images for close input vectors. Thus, since our objective is to collect labeled data to be used for identifying the boundaries of human visual distinguishability, we generate image pairs from key fingerprints that are only different in 1 component.</p><p>We followed an IRB-approved protocol to recruit 500 adult workers located in the US, to label 558 unique image pairs. We asked each worker to label each image pair as being of either "identical" or "different" images and paid them 1 cent per image comparison. We performed two image labeling studies. In the first study, we asked participants to label 35 image pairs, and in the second study 50 image pairs (see below). In each study, we randomized the order of the image pairs shown. We showed 1 image pair per screen. Both images had the same size (64x64) and resolution; we showed one image on the top left of the screen, the other on the bottom right. Across the two studies, 318 image pairs were labeled as different and 240 pairs were labeled as identical. In the following, we detail the two labeling studies that generated this dataset.</p><p>• Labeling Study 1. We used the vanilla DCGAN network to generate 100 synthetic different image pairs using 100 random seed latent vectors (v) and the IPG of Definition 1 for i ∈ {1, 2, 3, ..., 100}. Further, we generated a set of 40 identical image pairs consisting of duplicates of landscape images that we collected using Google's image search. We used proportional sampling to divide the total of 140 image pairs (100 different, 40 identical) into 4 groups of size 35 (25 assumed different, 10 assumed identical). We then recruited 400 MTurk workers and asked each to label one of the 4 groups, such that each image pair was labeled by 100 workers.</p><p>To weed out inattentive workers, we included an attention test (a.k.a. golden task <ref type="bibr" target="#b31">[32]</ref>) at the beginning of the study. We did not collect the labels from workers who failed to answer the attention test correctly. In addition, we removed responses of "speeders" <ref type="bibr" target="#b20">[21]</ref>, i.e., workers who completed the study in less than one minute. We also removed the answers from workers who made more than 10 errors with respect to the assumed labels for the image pairs they processed. In total, we have removed the responses of 34 of the 400 workers, leaving us with labels from at least 94 workers for each image pair.</p><p>We then assigned to an image pair, an "identical" or "different" label, only if more that 90% of the worker responses concurred. 75 image pairs were labeled as different, and 65 were labeled as identical. <ref type="figure" target="#fig_7">Figure 7</ref> (top) shows samples of identical image pairs that were labeled as different by about 10% of workers. <ref type="figure" target="#fig_7">Figure 7</ref> (middle and bottom) shows samples of different image pairs that were labeled as identical by more than 15% of workers.</p><p>We studied the quality of responses collected from workers. 35 workers used a mobile device (smartphone or tablet) to work on our image comparison tasks. A Kruskal-Wallis test, did not show a significant difference between the number of errors made (w.r.t. the hypothetical labels) by participants in our studies using either of devices, i.e., desktop, laptop, mobile phone, or tablet, to complete the study (P-value = 0.93). We did not observe significant difference between the overall time it took for the participants using different devices to complete the studies (P-value = 0.06).</p><p>• Labeling Study 2. At the completion of the above study, we identified the index of components in the input latent vector whose corresponding generated images were labeled with the highest error rates by workers. We then performed a second labeling study to determine if the high error rate we observed was due to the fact that an observed "faulty" component always produces indistinguishable image pairs when its value is flipped, or this is due to other factors, e.g. the contribution of other components on the generated image.</p><p>First, for each of 3 image pairs with the highest error rate in labeling study 1, we generated 99 variant image pairs as follows: Let j be the index of the component that we flipped to generate this particular image pair in study 1 (which resulted in a high error rate). Also, let v be the seed latent vector (see Definition 1) corresponding to this image pair. For all i ∈ {1, 2, 3, ..., 100} index values, where i = j, we used the IPG of Definition 1 to obtain two copies of v that only differ in the i-th component, then used the vanilla DCGAN to obtain an assumed "different" image pair. In total, we generated 297 (99 × 3) image pairs that are hypothetically different.  <ref type="table">Table 2</ref>: Size of 6 generated image pair datasets, of either "identical", "different" or "mixed" image pairs, used to train the HPD classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX</head><p>Second, for an additional set of randomly selected 7 components, plus the 3 "faulty" components above we generated 10 image pairs using a new seed latent vector randomly. We created two copies of the new seed latent vector and set the values of the j th components to 1 and -1 in the first and second copy respectively. Thus, in total, we generated 100 image pairs. Further, we used another set of 49 hypothetically identical pairs, which we used to enrich our training dataset.</p><p>We have then collected labels for these 446 image pairs (99 × 3 + 100 + 49) using 100 workers who labeled 50 image pairs each. As before, we eliminated the answers provided by speeders, those who failed the attention check, or made more than 10 errors with respect to the hypothetical labels of image pairs. In total, we removed responses from 13 workers. Then, for each image pair, we assigned it the assumed "different" or "identical" label, only if more than 80% of the workers agreed with it. Otherwise, we assigned the opposite of the hypothetical label as the true label of the image pair. 243 images were labeled as different; 203 image pairs were labeled as identical.</p><p>The Spearman correlation test did not reveal any significant monotonic correlation between the error rate for components in study 1, and image pairs corresponding to these components, in both experiments. This suggests that the components generating high error rates in study 1 alone, are not at fault. Therefore, we conjecture that the visual characteristics of a generated image are determined by a combination of effects of each component in the latent vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">HPD Classifier Dataset</head><p>In order to train the HPD, we have generated 6 different datasets of synthetic image pairs, containing a total of 26,802 image pairs. <ref type="table">Table 2</ref> lists these datasets and their corresponding number of image pairs. One of these datasets is the above ground truth human perception dataset. In the following, we describe each of the other 5 datasets. Unrealistic DCGAN Image Pairs. In order to train the HPD to correctly classify visually similar, but random noise images, as "identical", we generated an unrealistic image dataset of 11,072 image pairs using a poorly trained vanilla DCGAN:</p><p>(1) 10,048 image pairs using a vanilla DCGAN trained for only 1400 iterations, i.e., less than an epoch, and (2) 1,024 image pairs using the same vanilla DCGAN trained for 3600 iterations (slightly more than an epoch).</p><p>We generated each of these image pairs as follows: randomly generate a latent vector, then select a random component and set its value to 1 once and -1 the other time. Convert the latent vectors to images using the poorly trained vanilla DCGAN, then label each pair as "identical". That is, we wish to train the HPD classifier to classify these image pairs as being identical, as this is how a human verifier will see them (gray images with random noise). Minor Change in Latent Vector. We also generated synthetic "identical" image pairs, as follows. First, choose a random seed latent vector and use it to generate one image of the pair. Second, choose a random component of the seed latent vector uniformly, multiply its value by a small factor c ∈ <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>, then generate the other image in the pair. We generated 1024 image pairs with c = 0.5, 3008 pairs with c = 0.6 and 3008 pairs with c = 0.7, for a total of 7,040 image pairs. We randomly sampled 100 image pairs and 2 authors manually verified that they look identical. Blob Image Pair Dataset. First, we generated 20 different blobs of random shapes and colors. Then, we generated 1,000 realistic images using the fully trained vanilla DCGAN model, using random input latent vectors. We formed image pairs that consist of (1) one synthetic image and (2) the same image, overlayed with one randomly chosen blob. We only accept the composite image (2) if its dominant color is dissimilar in the blob overlap position, to the color of the blob. To measure the similarity between colors we used the Delta E CIE 2000 <ref type="bibr" target="#b45">[47]</ref> score, representing colors that are perceived to be different by humans <ref type="bibr" target="#b43">[45]</ref>. We used the composite image if the score exceeded 50. In total, we generated 2,108 "blob" image pairs. 10%-different Image Pair Dataset. We generated 1,024 different image pairs, each as follows: generate a random seed latent vector, copy it to v 1 and v 2 , select 10 random latent components (out of 100) uniformly and set the values of these components to 1 in v 1 and -1 in v 2 . We then used the trained vanilla DCGAN to generate the corresponding image pair. Thus, these 1,024 image pairs are generated from latent vectors that are different in 10% of the components. We set this percentage experimentally: we generated 500 image pairs using input vector pairs that differ in x ∈ [2, 20] percent of their components, then manually compared them for visual equality. We found 10% to be the smallest percentage of difference that resulted in always distinguishable image pairs. Enhanced Synthetic Image Pair Dataset. We generated 5,000 different image pairs as follows. For each of 1,000 random, vanilla DCGAN-generated images, we generated 5 images, by applying 5 enhancements, to change (1) image brightness, (2) contrast, <ref type="formula" target="#formula_9">(3)</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Implementation</head><p>We have built CEAL in Python using Tensorflow 1.3.0 <ref type="bibr" target="#b4">[6]</ref>. In this section, we describe the process we used to identify the parameters for which CEAL performs best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">HPD Training and Parameter Choice</head><p>Inception.v1 Layer Choice. We experimented with using activations of different layers of the Inception.v1, for HPD's image feature extraction effort (see 7.1). We performed 3 experiments, where we used activations from either the (1) "Mixed_5c", (2) "MaxPool_5a_2x2" or (3) "MaxPool_4a_-3x3" layers of the Inception.v1. In each run, we used an identical architecture and initial weights of the fully connected layers weights in HPD. We trained the 3 networks for 1000 epochs. We repeated this process 200 times. We compared the performance of these classifiers, using a paired t-test. We found a significant difference between the performance (over holdout datasets) of HPD classifiers trained using the "Mixed_5c" layer features, compared to the other two layers (P-Value = 0.000 when compared to "MaxPool_4a_3x3" layer, and P-Value = 0.000, when compared to "MaxPool_5a_2x2" features). In the following, we implicitly use the features extracted based on the activations of the "Mixed_5c" layer. The length of the activations vector for this layer is 50,176. Training the HPD. We used the 6 datasets of § 8.1 to train and evaluate HPD. Specifically, we randomly split each synthetic dataset (except the ground truth human perception set), into training ( 80% of samples) and holdout ( 20%) sets: we used the training sets to train the HPD classifier, then tested its performance over the holdout sets. For the ground truth human perception dataset, we made sure that the number of image pairs that are labeled as identical and different, were distributed to training and test sets proportionally to their size.</p><p>We hyper-tuned the architecture and parameters of the HPD classifier to find a classifier which accurately identifies samples from the "different" class (has high precision). Such a classifier is necessary when training the CEAL to ensure G-CEAL stays away from generating images that are not human-distinguishable. Among the classifiers that we have trained with high precision, we chose the one with the highest F1. <ref type="figure" target="#fig_2">Figure 5</ref> shows the best performing architecture for the HPD network. We refer to this network as HPD_model_1. <ref type="table" target="#tab_5">Table 3</ref> shows the performance of the Siamese network and the HPD networks that we trained and used in this paper.</p><p>In addition, we also trained an HPD model that has the same weights as HPD_model_1 in the Siamese layers, but different weights in the fully connected layer on top of the twin networks in the HPD architecture. This network, referred to as HPD_attacker, has a higher recall (lower FPR) when identifying the samples from the "identical" (negative) class on the holdout datasets. Therefore, this classifier would be preferred by an attacker, to identify potentially successful attack samples for a target CEAL image.</p><p>We note that the high FNR of our HPD models is mostly a problem for the adversary. This is because the FNR measures the ratio of the image pairs that HPD mistakenly detects to be identical. A high FNR means that the HPD is conservative: it will incorrectly identify attack image pairs, that are in fact perceived to be different by a real user. Thus, an HPD with a high FNR imposes either a lower success rate for an adversary, or more overhead on the adversary, who will have to manually verify attack images returned by the HPD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">CL-GAN Training and Parameter Choice</head><p>We trained CL-GAN using the above HPD models. Further, we also experimented using CL-GAN variants with different architectures (e.g., different number of neurons in the first layer of G-CEAL, <ref type="bibr">8,192 and 16,384</ref>) and values for hyper parameters, including λ and the number of major and minor components.</p><p>We also performed a grid search in the parameters of the CL-GAN including (1) the input size (λ ∈ 64, 128, 256, 512), (2) the number of major and minor components ( λ 2 , and (3) the α ∈ <ref type="bibr" target="#b24">[25,</ref><ref type="bibr">75]</ref> with step size 5, in the loss functions of the CL-GAN generator (see Equation 3). For best performing parameters, we also tested with different weight initialization for the networks weights.</p><p>We trained the CL-GAN using the process described in § 7.2, for 5 epochs, with batch size 64, and the Adam optimizer <ref type="bibr" target="#b28">[29]</ref> to minimize Equation 3 for each step. We completed an epoch when all the images in the outdoor image dataset were shown to the discriminator. To make the training process more stable, we trained the generator 3 times for every time we train the discriminator, but using the same inputs.</p><p>We observed that, when α is increased, the HPD loss decreases faster (see Equation 3). However, the quality of the images is reduced for large values of α. We also observed that it is harder to train networks with larger values of λs: the quality of images generated by CEAL and their distinguishability decreases as we increase λ. We also observed that when the size of the nodes in the first layer of G-CEAL is increased, it generates smoother or lower quality (blurred) images.</p><p>We also experimented with the number of times that the generator network is trained using the three steps described in § 7.2, in each training epoch of G-CEAL. We observed that when the minor components are trained using Step 2 twice, there is a better balance between G loss and HPD loss of the trained network. Therefore in the following, we implicitly train G-CEAL twice using Step 2. We have manually evaluated the quality of the images generated by the networks we trained. We built two CL-GAN model. The parameters for the best performing network using HPD_model_1 (i.e. alpha-CL-GAN) are α = 35, λ =512, and M = m = 256. We also built and evaluated an earlier CEAL model (i.e. alpha-CL-GAN) using α = 40, that has λ = 256 and M = m = 128. In the following, we describe the process to identify the input mapper parameters for both models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Choice of Input Mapper Parameters</head><p>To determine the value of d, i.e., the minimum number of major components that need to be modified to achieve consistent human distinguishability (see § 6), we used the following procedure. For each possible value of d ∈ {1, ..M = 256}, we generated 1 million random target keys. For each target key, we generated an attack key by randomly flipping (i.e., 1 vs. -1) the values of d major components, then generated the CEAL images corresponding to the (target, attack) pairs. We used HPD_model_1 and HPD_attacker to find pairs likely perceived as identical by humans.</p><p>We manually checked the distinguishability of the image pairs flagged by the HPD models and observed that when d &gt; 30, the image pairs are consistently distinguishable. To validate our observation, we showed the images identified as identical by HPD models for d=39 and d=43 to human subjects. We selected these values to be conservative and also to ensure the availability of BCH codes with corresponding minimum Hamming distances. When d = 39, HPD_model_1 identified 17 image pairs as identical, and the HPD_attacker identified 194. When d = 43, HPD_model_1 identified 7 identical image pairs, while the HPD_attacker identified 124 image pairs. All the pairs identified by HPD_model_1 were also identified by the HPD_attacker.</p><p>We used the procedure of § 10.2 to label these pairs using 34 human workers. None of the image pairs were confirmed as being identical by human workers. Therefore, we conclude that despite training limitations and the presence of local optima, when enough number of major components (i.e., ≥ 39) are flipped, the generated images are human-distinguishable. Thus, in the remaining experiments we set d to 39.</p><p>To ensure that major components of input vector to G-CEAL are at least in d Hamming distance of each other, we use a BCH (n=255, k=123, t=19) encoder to transform a key of length γ = 123 into the values for major components. This ECC has a minimum Hamming distance of 39 bits that transforms a message of length 123 into a code word of length 255. Thus, the major components in the latent vector of any CEAL images are at least 39 Hamming distance apart.</p><p>Based on the above setting, CEAL accepts binary key fingerprints of length γ = 123 bits. Therefore, the maximum capacity of CEAL is 2 123 , i.e., it can generate up to 2 123 unique, distinguishable images to represent binary key fingerprints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">alpha-CEAL</head><p>In addition to the above CEAL model that uses a CL-GAN with λ =512, we have built and evaluated a preliminary model, named alpha-CEAL, that uses its own CL-GAN network, named alpha-CL-GAN, with parameters λ =256, and M = m = 128 (vs. 256 in CL-GAN). We followed a similar process to the one described above, using α = 40 to determine the best parameters for alpha_CL-GAN. alpha-CL-GAN Input Mapper Parameters. To identify the minimum number of major components that need to be modified to achieve consistent human distinguishability for alpha-CL-GAN, we generated 1,000 image pairs by flipping (i.e., 1 vs. -1) 5, 10, 15, 20 and 30 randomly chosen major components in each of 200 latent vectors respectively. We then used a procedure of § 10.2, to label these images using 69 MTurk worker.</p><p>In these experiments, the smallest number of different major components for which participants labeled as different, all the generated alpha-CL-GAN samples, was d = 15. Therefore, for the Input Mapper module, we used a BCH(n=127, k=78, t=7), i.e., an ECC with minimum Hamming distance of 15 bits that transforms a message of length 78 into a code word of length 127. Thus, alpha-CEAL accepts binary key fingerprints of length γ = 78 bits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Empirical Evaluation</head><p>In this section, we evaluate the CEAL system with parameters identified in § 9.2 and § 9.3. First, we present the memory and computation overhead of the CEAL model ( § 10.1). We then describe the procedure we employed to run the user studies that we used for evaluation <ref type="bibr">( § 10.2)</ref>. We evaluate the resilience of CEAL against the adversary described in § 3.1 ( § 10.3). We investigate Vash <ref type="bibr" target="#b0">[1]</ref>, identified as a state-of-theart VKFG in terms of usability and attack detection <ref type="bibr" target="#b48">[50]</ref>, and report vulnerabilities that we identified ( § 10.4). We then compare CEAL against Vash, in terms of their capacity and human verification speed ( § 10.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">CEAL Overhead</head><p>The size of the CEAL generator model is 66.7MB. To determine the overhead of CEAL in generating image fingerprints, we performed experiments using a MacBook laptop with a 2.2 GHz Intel Core i7 CPU and 16GB memory, and a desktop equipped with a GeForce GTX TITAN X GPU. Over 1 million images, the average time to generate a CEAL image on the laptop was 2.3s (SD = 0.1s), while on the desktop it was 0.3s (SD=0.005s). This reveals that even without a GPU, CEAL can efficiently generate image fingerprints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">User Study Procedure</head><p>We followed an IRB-approved protocol to recruit MTurk workers to evaluate the performance of CEAL and Vash <ref type="bibr" target="#b0">[1]</ref>. Specifically, we have recruited a total of 374 adult , US-based participants, 132 female and 242 male, with an age range of 18 to 64 (M=35.01, SD=9.23). 90.23% of our participants had college education or higher. 50%, 49% and 1% participants used a desktop, laptop and mobile device in our studies, respectively.</p><p>We asked the participants to compare a total of 3,496 image pairs: 1,918 CL-GAN-generated image pairs (219 workers), 1,308 alpha-CL-GAN-generated image pairs (100 workers) and 270 Vash-generated image pairs (55 workers).</p><p>We informed each participant on the purpose of the image comparison tasks, explaining their relationship to a security check that requires their full attention. We showed 1 image pair per screen, both with the same size (64x64) and resolution. One image was shown on the top left of the screen, the other on the bottom right.</p><p>We conducted several studies, each needing labels for a different number of image pairs (e.g., the CEAL resilience to attacks, or the study comparing CEAL and Vash, see below). For flexibility reasons, we asked participants in these studies to compare between 35 to 50 pairs of images. However, in each study, all participants labeled the same number of image pairs. We paid participants 10 cent per image pair comparison.</p><p>To avoid collecting low quality labels from inattentive workers, we have included an attention test at the beginning of the surveys and did not collect the labels from workers who failed to answer this question correctly. Further, we have included 5 attention check questions in each study: 3 obviously different pairs of images, and 2 pairs of identical (duplicated) images. In order to keep the type of images shown to the user consistent throughout these studies, we selected attention check questions using the same visual fingerprint generator that were used to generate the other images in the  We randomly selected the image pairs and the order in which they were shown, for each participant, however, we did not mix Vash an CEAL images in any experiment. We removed the answers from 12 participants who had incorrectly answered 3 or more (out of 5) attention check questions in the study. Overall, for each image pair, we collected annotations from at least 3 workers. In the Vash studies of § 10.4, we collected at least 10 labels for each image pair. We used majority voting <ref type="bibr" target="#b31">[32]</ref> to aggregate the labels assigned by the workers to each image pair, and produce the final human-assigned label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Resilience of CEAL to Preimage Attacks</head><p>We evaluate CEAL under preimage attacks perpetrated by the adversary defined in § 3.1. Resilience to (γ, 1)-Attack. We first consider a powerful, (γ, 1)-adversary, who can find input keys that are within 1-Hamming distance of the victim's, and uses them to generate attack CEAL images. Specifically, for γ=123, we generated 1 million target inputs randomly. Then, for each such input, we considered all γ "attack" strings that are within 1-Hamming distance of target, and used CEAL to generate images corresponding to the target and attack strings. Thus, in total we generated 123 million CEAL image pairs.</p><p>To avoid the high cost and delay of using humans to compare these image pairs, we used the HPD_model_1 and HPD_-attacker to determine if they would be perceived as being identical by a human verifier. Out of 1 million target CEAL images, 121 of them were broken (only) once according to the HPD_attacker (see <ref type="table" target="#tab_7">Table 4</ref> top). Only 1 of these images was also identified by HPD_model_1. We then presented the 121 candidates to human verifiers (see § 10.2). Only 2 (1.65%) of these images were labeled as being identical by the recruited workers. The success rate of this (γ, 1) attack can thus be either interpreted as 2 ×10 −4 % (2 target inputs broken out of the 1,000,000 attempted), or 1.62 ×10 −6 % (2 successes out of 123 million trials).  The goal is to see if such an adversary can find successful attack images generated from attack keys more different than the target. Specifically, for each value of d ∈ {1, 2, 3, ..., γ}, we have built an attack dataset as follows: We generated 1 million random "target" inputs, and, for each target input, we randomly selected an "attack" string that is within Hamming distance d from the target. We then generated the CEAL images corresponding to each target and attack strings pair. Thus, in total we generated 123 million CEAL image pairs, organized into 123 datasets, each containing 1 million (target, attack) image pairs.</p><p>We have used both the HPD_attacker and the HPD_model_-1 to compare the 123 million (target, attack) image pairs that we generated. The HPD_attacker predicted 1,473 of the image pairs to be indistinguishable. HPD_model_1 only identified a subset of these samples (48 samples) as similar. When we presented the 1,473 candidate image pairs to human workers ( § 10.2), 23 (1.6%) of them were confirmed to be identical, one of which were identified by both HPD models (see <ref type="table" target="#tab_7">Table 4</ref> bottom). This suggests a success rate of 1.86 ×10 −5 % (23 successes out of the 123 million trials) for this (γ, d) attack, performed by an adversary equipped with our HPD classifiers. <ref type="figure" target="#fig_9">Figure 8</ref> shows the portion of broken CEAL-generated images in each of the 123 datasets of (γ, d)-Attack (see § 10.3) according to (left) HPD_model_1 and (right) HPD_attacker. As expected, the number of broken CEAL images decreases as the Hamming distance between the target and attack binary key fingerprints increases.</p><p>We note that the KFG evaluation performed by Tan et al. <ref type="bibr" target="#b48">[50]</ref>, assumed an adversary able to perform 2 60 brute force attempts, which is similar to the effort required to control 122 of 123 bits of the input hash key, required by a (γ, 1)-attack. Under such an adversary, Tan et al. <ref type="bibr" target="#b48">[50]</ref> report a false accept rate of 12% for Vash <ref type="bibr" target="#b0">[1]</ref> and 10% for the OpenSSH Visual Host Key <ref type="bibr" target="#b33">[34]</ref>). This is significantly larger than CEAL's 2 ×10 −4 % for the (123, 1) attack and 1.86 ×10 −5 % for the (123, d) attack or even the 1.7% human error rate we observed <ref type="figure">Figure 9</ref>: Distribution of "different" and "identical" labels as annotated by human workers for Vash image pairs. The number of image pairs that are identified as identical decreases as the number of buckets (b) and number of nodes (n) in the tree are decreased. in our user studies. alpha-CL-GAN under (γ, 1) and (γ, d) Attacks. We now report the performance of a (γ, 1) and (γ, d)-adversary when breaking alpha-CEAL with γ of 78. Similar to (γ, d) attack performed for CL-GAN, we generate γ = 78 million pairs of (target, attack) samples. We observe that only a 295 images were broken according to HPD_model_1. In addition, we launched a (γ, 1) attack on alpha-CL-GAN. Particularly, we generate 1M target keys. For each target key, we consider all 78 attack keys that are in 1-Hamming distance of the target key. We used the HPD_model_1 to decide if the generated image pairs would be perceived as being the same by a human verifier. This model identified 13 image pairs as identical.</p><p>We used the procedure described in §10.2 to label the 295 pairs of images that were identified by HPD_model_1 for (γ, d) attack and 13 pairs of images identified for (γ, 1) attack using 31 MTurk workers. Our workers identified 3 of 295 and none of the 13 images to be identical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Human-Distinguishability of Vash</head><p>Vash <ref type="bibr" target="#b0">[1]</ref> is an open source implementation for random art <ref type="bibr" target="#b39">[41]</ref>, that converts input binary strings into structured, visual fingerprints. Vash seeds a PRNG with the input string, then uses the PRNG to construct an expression tree structure. The number of nodes of the tree, N, is chosen randomly using the PRNG. Vash then converts this tree to an image: Each node in the tree corresponds to an operation function, which modifies pixel values of the image. Each operation is chosen randomly (using the PRNG), from an existing pool of 17 operations, e.g., ellipse, flower, "squircle", etc. The operation parameters (e.g., the two foci of an ellipse or the size of a flower) are chosen randomly using the PRNG.</p><p>To study the ability of Vash <ref type="bibr" target="#b0">[1]</ref> to generate humandistinguishable images, we generated 120 different Vash im-age pairs, as follows. We first quantized the random values used to select each Vash tree operation, into 32 buckets, and quantized the operation parameter values into b buckets (b ∈ {4, 8, 16}) of the same size. We then generated random trees until we had 30 trees of each size N ∈ {15, 20, 40, 60}, and corralled these trees into groups of 10. For each tree, we selected a random node (i.e., operation) and changed the value of one of its parameters by each of q ∈ {0.25, 0.125, 0.0625}, i.e., for each value of b. When selecting the operations, we made sure that each operation type appears in almost the same number of trees in each group. We generated thus 10 image pairs for each of the 12 combinations of q and n.</p><p>We used the procedure of Section 10.2 to label these pairs using 40 human workers. Each image pair was labeled by 10 workers. <ref type="figure">Figure 9</ref> shows the portion of image pairs in each category that were labeled as either identical or different. We observe that human workers were able to consistently label image pairs correctly as different, only when the number of nodes N in the tree was 15, and the number of quantization buckets was 4 (i.e., a parameter needed to be changed by at least 0.25). Thus, Vash images are human-distinguishable only when the generating tree is small. However, when we generated 10,000 random Vash images (see the experiment in § 10.5), 99.98% of them were constructed from trees of more than 15 nodes. This suggests that most of Vash-generated images are vulnerable to attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5">CEAL vs. Vash</head><p>We compared CEAL and Vash <ref type="bibr" target="#b0">[1]</ref> in terms of their capacity and the human speed of verification. Tan et al.</p><p>[50] compare multiple text and visual KFG solutions, including Vash, though against a weaker adversary. Our results for Vash are consistent with those reported by Tan et al. <ref type="bibr" target="#b48">[50]</ref>. For our comparison, we have generated 10,000 images randomly (from random keys) using CEAL, and 10,000 images using Vash. Then, separately for these datasets, we used the HPD_model_1 to predict if all pairwise images are humandistinguishable, i.e., using a total of 49,995,000 comparisons per dataset.</p><p>Since HPD_model_1 was not trained on Vash images, we sought to estimate its performance on Vash images. For this, we evaluated HPD_model_1 on the 120 Vash image pairs and their human-assigned labels, from § 10.4. HPD_model_1 achieved a FAR of 0.21, FNR of 0.14 and F1 of 0.76. Thus, HPD_model_1 achieves decent performance on Vash images, even though it was trained on dramatically different images (of nature not geometric shapes).</p><p>To estimate the number of distinguishable images for CEAL and Vash, we use the formulâ k(N r , r) = N r 2 2r , where N r is the number of samples until observing r repetition, i.e., human indistinguishable images (see <ref type="bibr" target="#b36">[37]</ref>). This method provides only a lower bound estimate for the capacity of a visual key fingerprint generator function, as any estimation method  fails when k &gt;&gt; s 2 , where k is the real population size and s is the sample size used for the estimate. Among the almost 50 million Vash image pairs compared, HPD_model_1 labeled 150 (3 × 10 −4 %) pairs as being human-indistinguishable. In contrast, for the same number of CEAL image pairs, HPD_model_1 did not label any pair as human-indistinguishable. To build confidence that we did not miss relevant images, we also used the more conservative HPD_attacker model, to identify potentially indistinguishable CEAL image pairs. We found 6 such pairs.</p><p>We then used the human workers and process described in § 10.2 to confirm these 150 Vash and 6 CEAL image pairs, i.e., with each image pair being labeled by 3 humans. 24 of the 150 (16%) Vash image pairs were confirmed as being identical by the workers. Therefore, we estimate the number of perceptually different images generated by Vash asˆkasˆ asˆk(N r , r) = 10K 2 2×24 = 2 20.99 . This result is consistent with the findings of Hsiao et al. <ref type="bibr" target="#b23">[24]</ref>. We note that the 24 collisions occurred among 10,000 images chosen at random, and not images engineered for an attack. Section 10.4 shows that an adversary can engineer a collision for 99.98% of these images.</p><p>In contrast, none of the 6 CEAL image pair predicted to be perceived as identical by HPD_attacker, was found identical by the workers (see <ref type="table" target="#tab_8">Table 5</ref>). Thus, we found no indistinguishable pairs among the 10,000 CEAL images. Human Comparison Time. We studied the response times of human participants when asked to compare the above 150 Vash image pairs and 48 CEAL image pairs of § 10.3, i.e., identified as potential attacks by HPD_model_1. We measured the comparison time to be the interval between the moment when the image pair is shown to the worker, and the moment when the worker selected the response (different vs. identical). The workers were not allowed to change their selection. The average comparison time over Vash attack images was 3.03s (M=1.4s, SD=5.42s), and for CEAL it was 2.73s (M=1.83s, SD=2.33s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Discussion and Limitations</head><p>Increasing Entropy. To increase the entropy of the CEAL key fingerprint generator, one could design and train multiple generators (see § 9.2), then use the input key to decide which generator to use (e.g., the value of the key's first two bits to pick one out of 4 generators). However, this approach imposes an exponential increase on computation and storage: to achieve k bits of entropy, we need to train and access 2 k generators. Instead, in the proposed CEAL approach, we use careful training to achieve its entropy. Improving HPD. Due to false positives, our evaluation is bound to have missed attack images. This is hard to avoid, given the need to evaluate millions of image pairs, a task that is infeasible with humans. We note that when modeling attacker capabilities, we used an HPD with higher recall than the one used to train the generator. This suggests that an adversary who has a better HPD, thus a higher chance of identifying potentially successful attacks, does not have substantial advantage against a CEAL image generator trained with a simpler HPD (though we cannot generalize this result).</p><p>However, both the attacker and the adopters of CEAL have the incentive to build a powerful HPD classifier. The attackers seek to find key fingerprint images most likely to be confused by users. The organizations adopting CEAL to protect their users would like to train a CEAL generator that uses the most of the key bandwidth available through human perception and the image generation algorithm. Thus, we expect adoption of CEAL (e.g., the CAPTCHA application of § 4) would increase interest in research of models for the limits of human visual perception. Generalization of Results. The results of our studies do not generalize to the entire population, as we performed them on only a subset of Mechanical Turk workers. Such workers are generally better educated and more tech saavy than the broader population <ref type="bibr" target="#b25">[26]</ref>, thus are not representative of the entire population. For instance, we conjecture that workers who work on visualization tasks are less likely than the general population, to suffer from vision problems. Mechanical Turk workers also have different goals (minimize their time investment, maximize financial gains) which may differ from those of regular key fingerprint based authentication users, i.e., not only minimize time investment, but also correctly detect attacks. However, Redmiles et al. <ref type="bibr" target="#b41">[43]</ref> have shown that in terms of security and privacy experiences, Mechanical Turk worker responses are more representative of the U.S. population than responses from a census-representative panel.</p><p>Our experiments are not equivalent to a lab study, since we do not know the circumstances or experience of the annotators. In addition, since a majority of the image pairs that we show to participants are attack images (i.e., believed to be visually similar), our studies also differ from the simulated attack format of Tan et al. <ref type="bibr" target="#b48">[50]</ref>, where the attack images form a small minority. The evaluation of Tan et al. is indeed more suitable for evaluating how humans react to attack images, in realistic settings. However, the goal of our experiments was different: we needed humans to validate image pairs predicted by HPD to be successful attacks. For instance, in one experiment we had to "human-validate" 1,473 image pairs. While our experiments are seemingly biased toward labeling image pairs as attack images, in this particular experiment, only 23 (1.6%) of the 1,473 image pairs were confirmed to be successful attacks. Further studies are needed to evaluate CEAL under realistic attack conditions such as the one proposed by Tan et al. <ref type="bibr" target="#b48">[50]</ref>.</p><p>In addition, we have trained CEAL to only generate nature landscape images. Our results do not generalize to other types of images.</p><p>More experiments are needed to verify that results of comparisons are consistent in scenarios where key fingerprints are displayed on devices with different screen properties (e.g., size and resolution), or even when printed on paper, to be compared against an image shown on a screen. Our experiments showed no difference between the responses from users comparing the key fingerprint on different devices. However, an extensive study is required to properly evaluate this aspect.</p><p>Finally, we have explored only (γ, d) attacks, for various values of d, for an adversary equipped with the HPD networks that we have developed. Future endeavors may investigate other types of attacks, including e.g., ones that attempt to find collisions for input latent vectors that are not similar. Resistance to Adversarial Machine Learning. An attacker who has gained access to the CEAL network weights can leverage adversarial machine learning (e.g. gradient based) techniques to infer the input string from a target output CEAL image. While this problem is outside the scope of this work (e.g., CEAL images are often computed from input strings whose values are public) we note that in cases where this input is sensitive, one can apply CEAL to a hash of the input. This would force the adversary to further invert the hash to recover sensitive information.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visual key fingerprint generator (VKFG) model and scenario. Given an arbitrary input string, the VKFG generates an image fingerprint representation. A human verifier compares this image against a securely acquired (e.g., from a trusted site, or person-to-person) reference image fingerprint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CL-GAN architecture and training. We use the combination of Discriminator loss and HPD loss to train the generator to generate distinguishable and realistic images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Human Perception Discriminator (HPD) architecture. HPD passes input images I 1 and I 2 through the Inception.v1 network, applies 3 fully connected layers to generate image feature vectors O 1 and O 2 , computes the squared Euclidean distance between O 1 and O 2 and passes it through a fully connected layer. HPD classifies I 1 and I 2 as different or identical, based on this distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 1 (</head><label>1</label><figDesc>Image Pair Generation: IPG(v, i)). Generate vectors v 1 and v 2 , such that v 1 [i] = 1 and v 2 [i] = -1, and v 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>and -1 are extreme values of each component, which we use to maximize the component effect in generated images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Major and minor components. The G-CEAL generator trains M components to be major components and the rest λ − M to be minor components. Major components impact the human-distinguishability of generated images; minor components are used to generate realistic images. Input Mapper converts the input key into a latent vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>we set L[i] = -1 for the remaining M − |ECC(K)| major components. Generate the minor components. We use a PRNG to ran- domly generate the m = λ − M minor components of L: L(i) ∈ IR and L(i) ∈ U(−1, 1), i ∈ {M + 1, ..., λ}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: (top) Example real images that when shown as duplicates, were identified as different by 10% of participants in labeling study 1. (middle &amp; bottom) Example different image pairs, generated with an early CEAL, that were identified as being identical by more than 15% of participants. This motivates the training step 3 of CEAL ( § 7.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Resilience to (γ, d)-Attack. Second, we consider the more general, (γ, d)-adversary ( § 3.1), where 1 ≤ d ≤ γ, γ=123.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: (γ = 123, d)-adversary: The break ratio of 1 million target CEAL images for each value of d, the Hamming distance between the attack and the target binary fingerprints, according to (left) HPD_model_1 and (right) HPD_attacker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>summarizes the notations that we use in this paper.</figDesc><table>USENIX Association 
29th USENIX Security Symposium 2241 

Symbol 

Description 

γ 

Length of input string (e.g. hash of key) 

λ 

Length of input latent vector (M+m) 

M 

Number of major components 

m 

Number of minor components 

d 

Hamming distance between two input strings 

CL-GAN 
CEAL training network 
G-CEAL 
Generator network in CL-GAN 
D-CEAL 
Discriminator network in CL-GAN 
HPD 
Human Perception Discriminator 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : CEAL notations.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>color, (4) add noise to the image, and (5) apply a blur filter to the image. We experimented with multiple parameters for each enhancement function and</figDesc><table>Network 

Hyper-parameters 
labeled synthetic 
dataset 

Unrealistic DCGAN 
image pairs (itr 1400) 

Unrealistic DCGAN 
image pairs (itr 3600) 

All other 
synthetic datasets 

m 
w 
r 
F1 
FPR FNR Precision 
F1 FPR 
FNR 
F1 FPR 
FNR 
F1 
FPR 
FNR 

Siamese_model 
1.64 
0.49 
0.02 
0.72 
0.20 
0.35 
0.82 
-
0.06 
-
-
0.32 
-
0.77 
0.01 
0.35 

HPD_model_1 
-
1.57 
0.24 
0.82 
0.24 
0.21 
0.84 
-
0.15 
-
-
0.47 
-
0.83 
0.02 
0.29 

HPD_attacker 
-
2.97 
0.13 
0.72 
0.20 
0.37 
0.84 
-
0.05 
-
-
0.31 
-
0.77 
0.0008 
0.36 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of the best HPD classifier that we trained and used to train CEAL (HPD_model_1) and the HPD model 
used by the attacker (HPD_attacker) and their underlying Siamese-like network, over different HPD classifier datasets. 

selected them so that the generated image pairs (the original 
image and its enhanced version) are visually distinguishable. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Attack image datasets generated to break CEAL. 
We show the dataset size, the portion of the (target, attack) 
samples that were identified by HPD_attacker and the number 
of attack images validated by human workers. 

study: For CL-GAN, we generated obviously different at-
tention check image pairs from a random seed latent vector 
where we flipped (1 vs. -1) a random set of 200 major compo-
nents (out of 256). For Vash, we generated obviously different 
attention check image pairs randomly. We manually verified 
that all these image pairs look indeed different. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Attack datasets generated using 10K random images 
for each key fingerprint representation and the result of user 
study to label identified attacks by HPD_model_1. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12">Conclusions</head><p>In this paper, we proposed and have built the first human perception discriminator, a classifier able to predict whether humans will perceive input images as identical or different. We have used HPD to introduce CEAL, a new approach to train visual key fingerprint generation solutions, that provide inputdistribution properties. We have shown that a CEAL-trained VKFG is substantially superior to state-of-the-art solutions, in terms of entropy, human accuracy and speed of evaluation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Vash: Visually pleasing and distinct abstract art, generated uniquely for any input data</title>
		<ptr target="https://github.com/thevash" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://www.whatsapp.com/security/WhatsApp-Security-Whitepaper.pdf" />
		<title level="m">Whatsapp encryption overview</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end encryption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whatsapp</surname></persName>
		</author>
		<ptr target="https://faq.whatsapp.com/en/general/28030015?lang=en" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Last accessed</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wp_Monsterid</surname></persName>
		</author>
		<ptr target="http://scott.sherrillmix.com/blog/blogger/wp_monsterid/,Lastaccessed" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for largescale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Encode random bitstrings as pseudo-random poems</title>
		<ptr target="https://github.com/akwizgran/basic-english" />
		<imprint/>
	</monogr>
	<note>Last accessed 2019</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On a class of error correcting binary group codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwijendra K Ray-Chaudhuri</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="79" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Openpgp message format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Callas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Donnerhacke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Finney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Thayer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">2440</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Tian Qi Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David K</forename><surname>Roger B Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2610" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems, NIPS&apos;16</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2180" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">TechCrunch: Facebook Messenger adds end-to-end encryption in a bid to become your primary messaging app</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Conger</surname></persName>
		</author>
		<ptr target="https://tinyurl.com/uetk9b5" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An empirical study of textual key-fingerprint representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergej</forename><surname>Dechand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Schürmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karoline</forename><surname>Busse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasemin</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Fahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Security Symposium</title>
		<meeting>the USENIX Security Symposium</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="193" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantically decomposing the latent spaces of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Public-key support for group collaboration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Ellison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Dohrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information and System Security (TISSEC)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="547" to="565" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Loud and clear: Humanverifiable authentication based on audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Michael T Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Sirivianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gene</forename><surname>Solis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Tsudik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uzun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EEE International Conference on Distributed Computing Systems</title>
		<meeting>the EEE International Conference on Distributed Computing Systems</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Disentangling space and time in video with hierarchical variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04440</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring the effects of removing &quot;too fast&quot; responses and respondents from web surveys. Public Opinion Quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Greszki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Schoen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="471" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Do users verify ssh keys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gutmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Login</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="35" to="36" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Hocquenghem</surname></persName>
		</author>
		<title level="m">Codes correcteurs d&apos;erreurs. Chiffres</title>
		<imprint>
			<date type="published" when="1959" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="147" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A study of user-friendly hash comparison schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hsun</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahren</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandra</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King-Hang</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Min</forename><surname>Perrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Yin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Computer Security Applications Conference</title>
		<meeting>the Annual Computer Security Applications Conference</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The bubble babble binary data encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huima</surname></persName>
		</author>
		<ptr target="https://tinyurl.com/phra64b" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Privacy attitudes of mechanical turk workers and the us public</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruogu</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Dabbish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Kiesler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Symposium On Usable Privacy and Security</title>
		<meeting>the 10th Symposium On Usable Privacy and Security</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="37" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR 2018</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disentangling by Factorising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2654" to="2663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference for Learning Representations</title>
		<meeting>the 3rd International Conference for Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR 2015</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Variational Inference of Disentangled Latent Concepts from Unlabeled Observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Balakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crowdsourced data management: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE 33rd International Conference on Data Engineering</title>
		<meeting>the IEEE 33rd International Conference on Data Engineering</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spate: small-group pki-less authenticated trust establishment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahren</forename><surname>Yue-Hsun Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hsin</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Hsiang</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">M</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">King-Hang</forename><surname>Mccune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Krohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Yin</forename><surname>Perrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1666" to="1681" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The drunken bishop: An analysis of the openssh fingerprint visualization algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Loss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Limmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gernler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conference on Machine Learning</title>
		<meeting>the Intl. Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Developing and testing a visual hash scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Olembo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stockhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Information Security Multi-Conference</title>
		<meeting>the European Information Security Multi-Conference</meeting>
		<imprint>
			<publisher>EISMC</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
	<note>Andreas Hülsing, and Melanie Volkamer</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Population estimation with performance guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Viswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Information Theory</title>
		<meeting>the IEEE International Symposium on Information Theory</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2026" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Guide to bluetooth security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Padgette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIST Special Publication</title>
		<imprint>
			<biblScope unit="volume">800</biblScope>
			<biblScope unit="page">121</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The human visual system is optimised for processing the spatial information in natural visual images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Parraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Troscianko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Tolhurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="38" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hash visualization: A new technique to improve real-world security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Perrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Workshop on Cryptographic Techniques and E-Commerce</title>
		<meeting>the Intl. Workshop on Cryptographic Techniques and E-Commerce</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Intl. Conference on Learning Representations</title>
		<meeting>the Intl. Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">How well do my results generalize? comparing security and privacy survey results from mturk, web, and telephone samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Elissa M Redmiles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">L</forename><surname>Kross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mazurek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy</title>
		<meeting>the IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title/>
		<ptr target="http://zschuessler.github.io/DeltaE/learn" />
	</analytic>
	<monogr>
		<title level="j">Zachary Schuessler. Delta E</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transfer learning for visual categorization: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The ciede2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations. Color Research &amp; Application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencheng</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Report: 2.3 Million Bitcoin Addresses Targeted by Malware That &apos;Hijacks&apos; Windows Clipboard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Suberg</surname></persName>
		</author>
		<ptr target="https://www.viber.com/security-overview" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Can unicorns help users compare crypto key fingerprints?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lujo</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorrie</forename><forename type="middle">Faith</forename><surname>Cranor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blase</forename><surname>Ur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3787" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sok: Secure messaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dechand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Perl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Security and Privacy</title>
		<meeting>the IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="232" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Benjamin Dumke von der Ehe. go-unicornify overview</title>
		<imprint>
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A perceptual metric for production testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Yangli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH</title>
		<meeting>the ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
