<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynacache: Dynamic Cloud Caching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Cidon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Eisenman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT CSAIL</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Katti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dynacache: Dynamic Cloud Caching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Web-scale applications are heavily reliant on memory cache systems such as Memcached to improve through-put and reduce user latency. Small performance improvements in these systems can result in large end-to-end gains, for example a marginal increase in hit rate of 1% can reduce the application layer latency by over 25%. Yet, surprisingly many of these systems use generic first-come-first-serve designs with simple fixed size allocations that are oblivious to the application&apos;s requirements. In this paper, we use detailed empirical measurements from a widely used caching service, Memcachier [3] to show that these simple default policies can lead to significant performance penalties, in some cases increasing the number of cache misses by as much as 3×. Motivated by these empirical analyses, we propose Dynacache, a cache controller that significantly improves the hit rate of web applications, by profiling applications and dynamically tailoring memory resources and eviction policies. We show that for certain applications in our real-world traces from Memcachier, Dy-nacache reduces the number of misses by more than 65% with a minimal overhead on the average request performance. We also show that Memcachier would need to more than double the number of Memcached servers in order to achieve the same reduction of misses that is achieved by Dynacache. In addition, Dynacache allows Memcached operators to better plan their resource allocation and manage server costs, by estimating the cost of cache hits as a function of memory.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Memory caches like Memcached <ref type="bibr" target="#b10">[11]</ref> and Redis <ref type="bibr" target="#b3">[4]</ref> have become a vital component of cloud infrastructure. Major web service providers such as Facebook, Twitter, Pinterest, Box and Airbnb have large deployments of Memcached, while smaller providers utilize cachingas-a-service solutions like Amazon ElastiCache <ref type="bibr" target="#b0">[1]</ref> and Memcachier <ref type="bibr" target="#b2">[3]</ref>. These applications rely heavily on caching to improve the latency of web requests, reduce load on backend databases and lower operating costs.</p><p>Even modest improvements to the cache hit rate have a significant impact on user perceived performance in modern web services, because reading data from a diskbased database (like MySQL) is orders of magnitude slower than in-memory cache. For instance, the hit rate of one of Facebook's main Memcached pools has been reported to be 98.2% <ref type="bibr" target="#b5">[6]</ref>. Assuming the latency for a typical cache hit and MySQL read is 200µs and 10ms, respectively, increasing the hit rate by just 1% would reduce the average read latency by over 25% (from 376.4µs at a 98.2% hit rate to 278.4µs at a 99.2% hit rate). The end-to-end benefits can be substantially larger for user queries, which often must wait on hundreds of reads <ref type="bibr" target="#b16">[17]</ref>.</p><p>Memory caching systems are very simple and are roughly modeled on CPU caches with tables of nearly equal-sized items and Least Recently Used (LRU) evictions. Importantly, current memory caches are oblivious to application request patterns and requirements. Memory allocation across slab classes <ref type="bibr" target="#b0">1</ref> and across different applications sharing a cache server is based on fixed policies like first-come first-serve or static reservations. The eviction policy, LRU, is also fixed.</p><p>In this paper, we argue that this one-size-fits-all cache behavior is poorly suited to cloud environments. Our position is that cloud memory caches must be dynamically tuned based on application access patterns, hit rate requirements, and operator policy. Like any other critical resource (e.g., compute, storage, network bandwidth), memory caches require intelligent application-aware resource management and policy.</p><p>We observe that cloud memory caches are very different from CPU caches in terms of workloads and capabilities. On the one hand, cloud application request patterns are significantly more variable than CPU workloads, which access fixed-sized cache lines, and often sequentially due to inherent program structures like loops. On the other hand, cloud caches are much more capa-ble than CPU caches. Unlike CPU caches, free from the restrictions of hardware implementation at GHz frequencies, a memory caching system can use sophisticated application profiling and optimization to adapt to the unique characteristics of each application.</p><p>While a dynamic application-aware cache is conceptually appealing, is it worth the additional complexity in practice? We answer this in the affirmative by analyzing of a week-long trace of over 490 web applications at Memcachier <ref type="bibr" target="#b2">[3]</ref>, a popular Memcached-as-a-service platform. We find significant room for improvement over the application-oblivious baseline cache system. Specifically, Memcached's default first-come first-serve slab memory allocation performs very poorly for some applications. In some cases, the default slab class allocations result in over 3× more misses than could have been achieved with an optimal allocation of the same amount of memory. For the same hit rate, default Memcached sometimes requires over 2.5× more memory.</p><p>Our trace analysis demonstrates the need for a cache that can dynamically adjust its memory allocation to satisfy the varying demands of different applications. To this end, we develop Dynacache, a lightweight, minimally invasive, application-aware cache controller for cloud environments. Dynacache detects the applications that are most likely to benefit from optimization using a simple, easy-to-compute entropy metric. It then profiles the request pattern of the selected applications and optimizes their slab allocation to maximize hit rate.</p><p>We have built a prototype implementation of Dynacache in C for Memcached. Our preliminary evaluation with realistic workloads based on measurements at Facebook <ref type="bibr" target="#b5">[6]</ref> shows that Dynacache's application profiling is feasible and has a low overhead on the performance of other applications.</p><p>Our current design focuses on slab memory allocation to demonstrate how rigid cache policies hurt performance. However, we believe that other parameters such as eviction policy and memory allocation across applications also hold great promise for improvement. Further, we expect that our application profiling may inform developers and operators to make more intelligent decisions about their cache needs and resource sharing policy, which today are mostly determined arbitrarily. We intend to explore these directions in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MEMCACHIER TRACE ANALYSIS</head><p>In this section, we provide empirical evidence that the one-size-fits-all resource allocation policies of Memcached are not ideal for real web applications that exhibit variable behavior. We collected a trace of 490 applications on Memcachier, a multi-tenant Memcached service. Each application reserves a certain amount of memory in advance, which is uniformly allocated across mul-  <ref type="table">Table 1</ref>: Analysis of the affect of optimizing the slab class allocation for the top 10 applications in the Memcachier trace. The sixth column in the table depicts the amount of memory required to achieve the improved hit rate using the default slab class allocation of Memcachier, and Miss Entropy is a metric for classifying which application will benefit most from optimizing its slab class allocation.</p><p>tiple Memcached servers comprising the Memcachier cache. We obtained packet captures on one of the Memcachier servers for a week. In all, the raw captures amount to 220 GB of compressed data. We analyzed the packet captures to reconstruct all Memcached requests and responses, including the type of request (e.g., GET, SET), key and value sizes, whether the request hit or missed, and the slab class for each request.</p><p>In particular, we focus on one dimension of cache resource allocation, namely, the allocation of memory resources across slab classes for each application. In Memcached, memory pages are divided into 1MB pages assigned to one of several slab classes. Each slab class stores items whose sizes are within a specific range (e.g., between 1KB and 2KB). For example, a 1MB page that belongs to the 1-2KB slab class, would be split into 512 chunks of 2KB in size. Each slab class has its own LRU eviction queue per application.</p><p>In Memcachier, memory is allocated to slab classes greedily based on the sizes of the initial items at the beginning of the workload. Since each slab class has its own eviction queue, the length of the queues can vary greatly among slab classes, and some slab classes may be under or over provisioned. <ref type="table">Table 1</ref> presents the hit rate achieved by Memcachier for the top 10 applications (sorted by number of requests) in our trace, and the hit rate that could be achieved with an optimal allocation of memory to slab classes (we will show how to derive the optimal slab class allocation in Section 3).</p><p>The results show that some applications can benefit greatly from better slab class allocation, and some do not.  For example, the number of misses in applications 3 and 5 is reduced by 51% and 65% respectively. About half of the applications in the trace do not benefit at all from optimized slab allocation, because applications are typically well over provisioned. In some applications (like application 7), while the optimal slab class allocation does improve the hit rate, the improvement is small. The table also shows the amount of memory that would be required to achieve the improved hit rate, using the default slab allocation. For applications 3 and 5, Memcachier must more than double the amount of allocated memory to achieve the same hit rate with the default policy.</p><p>The problem with Memcachier's first-come-first-serve memory allocation to slab classes is that if some applications change their request distribution, some slab classes may not have enough memory and their eviction queues will be too short. Some Memcached implementations have tried to solve this problem by periodically evicting an entire slab of a slab class, and reassigning it to the corresponding slab class of new incoming requests <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. However, even these improved slab class allocation may suffer from sub-optimal slab class allocation. For example, they may tend to favor large slab classes over small ones, because if a request for a large item size is received at the server, it will be treated equally as a request with a small item size, even though the large item size occupies much more memory in the Memcached server. This is demonstrated in <ref type="table" target="#tab_2">Table 2</ref>, which details the default hit rate and the slab class allocation of two Memcachier applications. In both applications, Memcachier allocates relatively much more memory to the larger slab classes, compared to the number of requests from each slab class. It is clear that in both applications, the Memcachier slab class allocation favors large slab classes relative to the number of times they are requested.</p><p>Intuitively, we expect that in application 3 this problem will be more severe, since the requests are more evenly distributed among the slab classes. In application 4, since the vast majority of the requests belong to slab class 1, the default slab allocation will perform reasonably well. This is the reason some applications' performance can be improved more than others with optimal slab class allocation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DESIGN</head><p>In this section we describe the design of Dynacache. In order to tailor the resource allocation of Memcached for different applications, Dynacache first classifies and profiles the applications' access patterns, and then computes and sets their optimal slab class allocation. The architecture of Dynacache is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Dynacache runs as a module that integrates with the Memcached server. We describe its three components below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Slab Class Allocation Optimization</head><p>We first describe how to compute the optimal memory allocation to each slab class for a single application. The problem can be expressed as an optimization:</p><formula xml:id="formula_0">maximize m s i=1 f i h i (m i , e) subject to s i=1 m i ≤ M (1)</formula><p>Where s is the number of slab classes, f i is the frequency of GETs for each slab class, h i (m i , e) is the hit rate of each slab class as a function of the its available memory (m i ) and cache eviction policy (e), and M is the amount of memory reserved by the application on the Memcached server.</p><p>In order to solve this optimization problem, we need to compute h i (m i , e), or the hit rate curve for each slab class. Stack distances <ref type="bibr" target="#b15">[16]</ref> provide a convenient means of computing the hit rate curve beyond the allocated memory size, for a given eviction policy. The stack distance of a requested item is its rank in the cache, counted from the top of the eviction queue. For example, if the requested item is at the top of the eviction queue, its stack distance is equal to 1. If an item has never been requested before, its stack distance would be infinite.</p><p>The stack distances can be computed offline for each incoming request in the Memcachier trace (e.g., once every 6-12 hours). This allows us to compute the hit rate curve as a function of the memory allocated to each slab class. For example, <ref type="figure" target="#fig_1">Figure 2</ref> depicts the hit rate curve of slab class 9 of application 3.</p><p>To solve the optimization in Equation <ref type="formula">(1)</ref> efficiently, the hit rate curves need to be approximated as concave functions. Fortunately, hit rate curves in the Memcachier traces are concave or nearly concave. We use convex piecewise-linear fitting <ref type="bibr" target="#b14">[15]</ref> to approximate the hit rate curves. The piecewise-linear hit rate curves allow us to solve the optimization (1) using a simple LP solver.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Practical Profiling</head><p>Computing the exact stack distance for each incoming request requires maintaining a "shadow" eviction queue and tracking the position of every incoming request in the queue to calculate its stack distance. This can be computationally prohibitive, especially when the application accesses a large number of unique keys (the computation is O(n), where n is the size of the shadow queue).</p><p>Instead, Dynacache uses a bucketing scheme similar to Mimir <ref type="bibr" target="#b18">[19]</ref> (similar schemes were described in CRAMM <ref type="bibr" target="#b20">[21]</ref> and Path <ref type="bibr" target="#b6">[7]</ref>). In this bucketing scheme, instead of keeping track of a shadow eviction queue, there is a linked list of buckets, each containing a fixed number of items. Each incoming request enters the top bucket, and when the top bucket is filled, we remove the bucket at the end of the queue. We maintain a hash function that maps each item to the bucket in which it is stored. We can estimate the stack distance of an incoming request, by summing the size of all buckets that appear in the queue before it, and adding it to the size of its own bucket divided by 2. This stack distance computation algorithm is much faster than the naïve method, since its complexity is O(B), where B is the number of buckets. For Memcachier, we utilized 100 buckets with 100 keys each. The difference in the hit rate improvement for the optimized slab class allocation based on the bucket algorithm versus exact stack distances is less than 10% for all the applications we analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Which Applications to Optimize?</head><p>The naïve approach would be to simply calculate the hit rate curves of all the applications and run the optimization function. However, estimating the hit rate curves for hundreds of applications on each server is costly. As our results in <ref type="table">Table 1</ref> have shown, not all applications benefit from optimized slab class allocation.</p><p>Dynacache should ideally only optimize the slab class allocation of applications that are likely to benefit. To this end, we derive a metric that is simple to compute and with a high degree of certainty predicts the hit rate improvement due to optimized slab class allocation.</p><p>Intuitively, the default slab class allocation will perform poorly when there is a relatively uniform distribution of unique requests among slab classes. When all the requests are concentrated on a small number of adjacent slab classes (or in a single slab class), there won't be a big difference between a first-come-first-serve slab allocation policy and the optimal slab allocation policy.</p><p>Entropy is a metric that provides a measure of the uniformity of a distribution function. When a probability function behaves completely uniformly, its entropy will be the highest, and when it behaves deterministically, its entropy will be 0. The last column in <ref type="table">Table 1</ref> shows the Miss Entropy across slab classes for the different applications in the trace. The Miss Entropy is calculated by treating the misses per slab class as a probability density function and calculating its entropy <ref type="bibr" target="#b8">[9]</ref>.</p><p>We have found that Miss Entropy is a good indicator for applications that would benefit from optimal slab class allocation. The reason is that high Miss Entropy implies that accesses to unique keys are evenly distributed across slab classes, and the default slab class al- location tends to disproportionately prioritize large slab classes in such workloads. The only exception to this rule is application 9. This is because Memcachier allocated a very small amount of memory (only 1.6MB) to application 9. The optimization function does not work with a very small number of data points, and therefore cannot improve the slab allocation, given the memory constraints. If application 9 had been allocated more memory, it would have benefited significantly from an improved slab class allocation, similar to applications 3 and 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION</head><p>We implemented a prototype of the Dynacache profiler in C, and integrated it with Memcached 1.4.22. Our implementation consists of about 170 code lines. In order to measure the overhead added by Dynacache, we leveraged Mutilate <ref type="bibr" target="#b13">[14]</ref>, a load generator that emulates Memcached workloads from the 2012 Facebook study <ref type="bibr" target="#b5">[6]</ref>. We used the same key, value and read/write distributions as described in the Facebook paper. We used the Facebook workload because it is much more CPU intensive than the applications measured in the Memcachier trace. We ran our simulation on an Intel Xeon E5-2670 system with 32 GB of RAM and an SSD drive, using 5 minutes experiments. We measured the achieved throughput and latency overhead while running the Dynacache profiler, under different request loads.</p><p>We examined the throughput and latency achieved using different request loads, ranging from 4000 requests per second (Memcachier's average load) to 100,000 requests per second. Our evaluation shows an average of 5.8% latency slowdown for read queries (GETs) and 9.6% latency slowdown for write queries (SETs), with negligible deviations between the experiments. <ref type="figure" target="#fig_2">Figure 3</ref> presents the throughput overhead. In order to measure throughput, we generated a series of requests at the client, and measured the number of requests returned during a 5 minute period. The figure shows that throughput was not affected at lower loads, but had an overhead of 6% compared to the default Memcached implementation, once Memcached became CPU bounded. The overhead remained at 6% after 40,000 requests per second, because the Memcached server becomes saturated. Note that in the case of Memcachier, Memcached is memory bound and not CPU bound, and therefore the Dynacache would not impose any overhead on throughput. Initial profiling shows that further optimizations can be made in our implementation by reducing the number of hash computations in cases of updates and bucket deletions, and by running the profiler asynchronously (i.e., not in the critical path of incoming requests).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>This work is related to previous work on improving and profiling the performance of Memcached. Mimir <ref type="bibr" target="#b18">[19]</ref> and Blaze <ref type="bibr" target="#b7">[8]</ref> also profile cache hit rate curves to enforce QoS guarantees in multi-tenant web caches. Similarly, Wires et. al. profile hit rate curves using Counter Stacks <ref type="bibr" target="#b19">[20]</ref> in order to better provision Flash based storage resources. In addition, Hwang et. al. have proposed a dynamic hashing system <ref type="bibr" target="#b12">[13]</ref> that can evenly distribute requests across servers, taking into account varying item sizes. A recent study on the Facebook photo cache demonstrates that modifying LRU can significantly improve web cache performance <ref type="bibr" target="#b11">[12]</ref>. Twitter <ref type="bibr" target="#b17">[18]</ref> and Facebook <ref type="bibr" target="#b16">[17]</ref> have tried to improve Memcached slab class allocation to better adjust for varying item sizes, by periodically shifting pages from slabs with a high hitrate to those with a low hitrate. Both of these schemes are far from optimal, since they do not take into account the hit rate curves across all the slab classes, and we plan to conduct a quantitative comparison with them in future work. There is a body of prior work on algorithms for calculating stack distances in the context of CPU caches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22]</ref>. These techniques may be applicable for Dynacache to enhance the performance of the profiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>By analyzing a multi-tenant Memcached cluster, we demonstrated that a web-based cache can be improved significantly by tuning its behavior to dynamically adjust to the requirements of different applications. We showed that the performance of certain applications can be significantly improved by simply better allocating the memory slab allocation within the Memcached servers, without interfering with the data path of the cache. Our next step is to generalize dynamic tuning to the other parameters in cache systems such as relative allocations across applications and eviction policies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High level architecture of Dynacache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hit rate curve over an entire week for Application 3, Slab Class 9 (item sizes of 16-32 KB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Throughput overhead of Dynacache profiler. The overhead remains 6% after 40,000 requests per second, because the Memcached server becomes saturated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Hit rate and slab class allocations of two applications in the 
Memcachier trace in a single server. Each application's workload is 
spread uniformly across 12 servers. In Memcachier, each slab class 
size is: 64bytes · 2 SlabId . So for example, slab class 0 is for items 
with value between 0-64 bytes, while slab class 1 is for values between 
64-128 bytes. The slab class optimization improved the overall hit rate 
of application 4 from 97.5% to 97.7%, and of application 3 from 97.5% 
to 98.7%. 

</table></figure>

			<note place="foot" n="1"> To avoid memory fragmentation, Memcached divides its memory into several slabs. Each slab stores items with size in a specific range (e.g., &lt; 128B, 128-256B, etc.) [2]</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Amazon Elasticache. aws.amazon.com/elasticache</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memcached</surname></persName>
		</author>
		<ptr target="code.google.com/p/memcached/wiki/NewUserInternals" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Memcachier. www.memcachier.com</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Redis</forename><surname>Redis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Io</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Calculating stack distances efficiently</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Almási</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ca¸scavalca¸scaval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Padua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="37" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-METRICS Performance Evaluation Review</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Path: page access tracking to improve memory management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th international symposium on Memory management</title>
		<meeting>the 6th international symposium on Memory management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic performance profiling of cloud caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bjornsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chockler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saemundsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vigfusson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th annual Symposium on Cloud Computing</title>
		<meeting>the 4th annual Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page">59</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting whole-program locality through reuse distance analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="245" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Distributed caching with Memcached. Linux journal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fitzpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An analysis of Facebook photo caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="167" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive performance-aware distributed memory caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leverich</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convex piecewise-linear fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization and Engineering</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluation techniques for storage hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gecsei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Slutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Traiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems journal</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="117" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scaling Memcache at Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fugal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcelroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13)</title>
		<meeting><address><addrLine>Lombard, IL</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="385" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rajashekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<ptr target="Twemcache.blog.twitter.com/2012/caching-with-twemcache" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic performance profiling of cloud caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saemundsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bjornsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chockler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vigfusson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Characterizing storage workloads with counter stacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ingram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Drudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Data</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX conference on Operating Systems Design and Implementation</title>
		<meeting>the 11th USENIX conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="335" to="349" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">CRAMM: Virtual memory support for garbage-collected applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E B</forename><surname>Moss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th symposium on Operating systems design and implementation</title>
		<meeting>the 7th symposium on Operating systems design and implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="103" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Miss rate prediction across all program inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Dropsho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architectures and Compilation Techniques, 2003. PACT 2003. Proceedings. 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="79" to="90" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
