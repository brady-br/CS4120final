<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">To Relay or Not to Relay for Inter-Cloud Transfers?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Lai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Madhyastha</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Michigan</orgName>
								<orgName type="institution" key="instit2">University of Michigan</orgName>
								<orgName type="institution" key="instit3">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">To Relay or Not to Relay for Inter-Cloud Transfers?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Efficient big data analytics over the wide-area network (WAN) is becoming increasingly more popular. Current geo-distributed analytics (GDA) systems employ WAN-aware optimizations to tackle WAN heterogeneities. Although extensive measurements on public clouds suggest the potential for improving inter-datacenter data transfers via detours, we show that such optimizations are unlikely to work in practice. This is because the widely accepted mantra used in a large body of literature-WAN bandwidth has high variability-can be misleading. Instead, our measurements across 40 datacenters belonging to Amazon EC2, Microsoft Azure, and Google Cloud Platform show that the available WAN bandwidth is often spatially homogeneous and temporally stable between two virtual machines (VMs) in different datacen-ters, even though it can be heterogeneous at the TCP flow level. Moreover, there is little scope for either bandwidth or latency optimization in a cost-effective manner via relaying. We believe that these findings will motivate the community to rethink the design rationales of GDA systems and geo-distributed services.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many popular cloud applications nowadays host their services all around the world to meet the performance and regulatory requirements of their customers. Their customer-facing functionalities are often enabled by multiple internal services that depend on data and computation spread across the globe, which introduces new research issues at the intersections of networking, systems, and database. Consequently, a growing body of recent work has focused on enabling big data analytics across geo-distributed datacenters <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref> -also known as geo-distributed analytics (GDA) -where computation is applied to in-place data at different sites. These GDA systems often involve large data transfers across the wide-area network (WAN) when they aggregate intermediate results generated at multiple sites.</p><p>Recent studies suggest that, in sharp contrast to intradatacenter networks, inter-datacenter WANs experience high spatial and temporal bandwidth variations across different datacenters <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, and they impose new challenges in designing GDA systems. For example, they suggest that the available WAN bandwidth between geographically-close regions can be up to 12× more than that between distant regions <ref type="bibr" target="#b2">[3]</ref>. To this end, state-of-theart GDA systems propose heuristics to co-design intradatacenter computation and inter-datacenter communication -subject to the scarce and heterogeneous WAN bandwidth -to improve GDA performance.</p><p>One possible implication of these WAN measurements between public cloud datacenters is that tenants could potentially improve their WAN performance by relaying inter-datacenter transfers via detours. Specifically, data transfers between two distant datacenters could be sped up by relaying the transfers via a third datacenter. For instance, WAN bandwidth measurements between cloud datacenters show that about 40% data transfers between Amazon EC2 datacenters can achieve more than 1.5× bandwidth increase when they go through a one-hop relay instead of using the direct path. Such potential for relaying can seem more promising when it comes to multiple cloud providers, because the union of datacenters across multiple providers results in a geographically denser set of datacenters.</p><p>In this paper, we show that such optimizations are unlikely to work because the variations in WAN bandwidth shown in prior work are often artificial; as such, seemingly sensible rules of thumb -e.g., available bandwidth varies significantly across datacenter pairs -may be inappropriate. We argue that the measurement methodologies in prior work are misleading in at least two ways: (i) the revealed bandwidth heterogeneity is that of a single TCP connection -not the real bandwidth between two virtual machines (VMs) in different cloud datacentersand this bandwidth is a function of the round-trip-time (RTT) between datacenters and the TCP window size; (ii) the available bandwidth between two VMs in different sites should be measured as the aggregate bandwidth of multiple TCP connections between them.</p><p>We substantiate our findings via extensive measurements on three popular cloud providers: Amazon EC2, Microsoft Azure, and Google Cloud Platform. Our experiments reconfirm some of the earlier findings (e.g., per-flow rate limiting), but provide some distinctive insights on the characteristics of inter-cloud WANs: (i) the available WAN bandwidth is homogeneous at the VM level and capped per-VM; (ii) the available WAN bandSystem Problem Statement Clarinet <ref type="bibr" target="#b0">[1]</ref> Improve query response time via joint planning and scheduling, subject to heterogeneous bandwidth Iridium <ref type="bibr" target="#b1">[2]</ref> Balance the transfer times among the heterogeneous WAN by optimizing data and task placement Gaia <ref type="bibr" target="#b2">[3]</ref> Perform efficient machine learning across multiple datacenters, but WAN variations degrade performance Amoeba <ref type="bibr" target="#b6">[7]</ref> Difficult to ensure timely data delivery, as WAN bandwidth varies in distance and time <ref type="table">Table 1</ref>: Selected recent GDA designs.</p><p>width is stable over periods of time; and (iii) bandwidth contention occurs at VMs instead of inter-datacenter links. Moreover, we show that there is not much scope for latency optimizations via relaying either. We believe that these findings will prove useful for practitioners, designers, and users of GDA systems.</p><p>Overall, we make the following contributions in this paper: (i) Extensive measurements on a multi-cloud deployment and novel insights regarding the characteristics of WAN across multiple cloud providers; (ii) An in-depth analysis of the opportunities to improve WAN performance by leveraging the power of multiple clouds, and we present several interesting angles for the design of GDA systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Understanding the performance of WAN is crucial to several research communities. To a first approximation, our work can contribute to at least three categories.</p><p>Cloud Measurements Studies on bandwidth have shown prevalence and persistence of network variations over the WAN (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>). These studies have either focused on the heterogeneity of bandwidth (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8]</ref>) or temporal WAN variations (e.g., <ref type="bibr" target="#b6">[7]</ref>). Few studies provide in-depth analysis of inter-datacenter network across cloud providers (e.g., Azure and Google Cloud) and interpret those characteristics. Our work identifies the similarities and differences over three popular cloud providers.</p><p>WAN-Aware Optimization for GDA Recent work establishes the emerging problem of designing GDA systems <ref type="table">(Table 1)</ref>. Given WAN bandwidth heterogeneity, these works intelligently propose heuristics to design WAN-aware data analytics frameworks, showing very promising system-level performance improvements (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) and WAN bandwidth usage reductions <ref type="bibr" target="#b8">[9]</ref>. Throughout the paper, we refer to the insights provided by these studies and analyze the impacts of our new findings relevant to these GDA systems.</p><p>Overlay Routing on Clouds Geographically distributed datacenters connected by cloud providers' backbone networks provide the opportunity for cloud users</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Provider</head><p>VM Type # of DCs Measured Amazon t2 micro 11 Microsoft f1 micro 17 Google n1-standard-1 12 <ref type="table">Table 2</ref>: VM type and measured datacenters.</p><p>to construct performant overlay alternatives. Although extensive work have shown the effectiveness of overlay routing to protect against network outages or packet losses (e.g., <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref>), they do not focus on improving network bandwidth or latency on clouds (e.g., VIA <ref type="bibr" target="#b12">[13]</ref>, ARROW <ref type="bibr" target="#b13">[14]</ref>). Instead, we conduct a detailed analysis of the tenant-level opportunities to improve the WAN bandwidth and latency by relaying through multiple regions or clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inter-Cloud WAN Measurements</head><p>We start by presenting our measurement methodology. Then we quantify the bandwidth, latency, and opportunities for improving WAN performance via relaying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measurement Methodology</head><p>Overview Our measurements span three popular cloud providers (Amazon, Microsoft, and Google) and aim to measure WAN bandwidth and latency of paths interconnecting VMs, including both intra-datacenter connections and inter-datacenter connections. This real cloud deployment can capture the performance that a tenant running a geo-distributed service in the cloud can expect. We conducted extensive measurements over 40 datacenter regions across three cloud providers' datacenters and covered many trans-oceanic connections. Although our bandwidth measurements are limited by budget constraints, the results are generalized enough to support our findings. Details about the type of VMs and datacenters measured are noted in <ref type="table">Table 2</ref>.</p><p>Measurement Tools We use the widely adopted measurement tool iperf3 to measure the bandwidth between VMs. As already done in previous works <ref type="bibr" target="#b11">[12]</ref>, we use both ICMP-based ping and TCP-based hping3 to measure the RTT between two VMs, because ICMP (a) Bandwidth between 11 Amazon EC2 regions.</p><p>(b) Bandwidth from Virginia to California in a day.</p><p>(c) Throughput improvement via best relay. ping is not allowed on Azure. Note that results using ping and hping3 are similar on EC2 and Google Cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bandwidth</head><p>In this section, we first substantiate our findings by reproducing measurements from existing work. However, we show that observations in recent advances are misleading due to their measurement methodology, and we discuss possible reasons why WAN bandwidth was found to be heterogeneous and variable in prior measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Prior Measurements are Misleading</head><p>We first measure the WAN bandwidth between 11 Amazon EC2 regions with the measurement methodology adopted in prior work. Specially, we use iperf3 to measure the bandwidth of each pair of different regions every 15 minutes over a total period of 36 hours. <ref type="figure" target="#fig_0">Figure 1a</ref> reports the average WAN bandwidth between each pair of different regions, and <ref type="figure" target="#fig_0">Figure 1b</ref> shows the measured bandwidth from Virginia to California in 24 hours. Observations from these measurements are in line with recent work (e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WAN Bandwidth Varies Greatly Across Different</head><p>Pairs of Regions As shown in <ref type="figure" target="#fig_0">Figure 1a</ref>, bi-directional bandwidth between the same pair of VMs is not strictly symmetric. For example, the average bandwidth from Oregon to Virginia is 146 Mbps, while the bandwidth of the opposite direction is 56 Mbps. Moreover, bandwidth between geographically-close regions (e.g., Frankfurt → Ireland) is up to 21× of the bandwidth between distant regions (e.g., Mumbai → Sao Paulo).</p><p>This bandwidth heterogeneity across the WAN has been accepted as a mantra in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WAN Bandwidth Varies Temporally</head><p>Measurement results in <ref type="figure" target="#fig_0">Figure 1b</ref> show that even bandwidth variations within every 60 seconds over 24 hours is highly dynamic, suggesting the difficulty to ensure expected data delivery in GDA systems. This intractable variation in interdatacenter bandwidth has imposed a great challenge toward designing GDA systems <ref type="bibr" target="#b2">[3]</ref>.</p><p>Relaying Can Improve Throughput Observations from <ref type="figure" target="#fig_0">Figure 1c</ref> show that about 40% of data transfers on EC2 can obtain 1.5× or more bandwidth than the direct path when they go through a one-hop relay. This benefit becomes more promising (up to 10×) when tenants relay their traffic through the datacenters of other cloud providers. However, we argue that similar measurement results in the literature are misleading in two ways: (i) The variation and heterogeneity of WAN bandwidth are due to RTT variations and window sizes of the single TCP connection; (ii) The available bandwidth between two different sites should be considered as the aggregate bandwidth of multiple TCP connections. To generalize a step further, the well-known variability and heterogeneity of WAN bandwidth in prior work may be artificial! We substantiate these hypotheses next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">WAN Bandwidth is Often Homogeneous</head><p>In contrast to intra-datacenter networks, inter-datacenter WAN spreads across multiple regions in different conti-  nents. The RTT between two remote regions can reach up to 400 ms (see next section), which makes the WAN a long fat network with large bandwidth-delay products. In our measurements, we observe that the bandwidth-delay product of single TCP connections are nearly identical across different pairs of datacenters, confirming our hypothesis that the default TCP configuration in Linux is the culprit behind the misleading measurements.</p><p>In the rest of our measurements, we modify the TCP buffer size and window size in /proc/sys/net/core and /proc/sys/net/ipv4, thus achieving a maximum TCP window size of 7 MByte in iperf3. Furthermore, we study the available WAN bandwidth by setting up multiple TCP connections per VM. We conducted extensive experiments over multiple weeks on a subset of regions, and the results are quantitatively similar, without extra variations due to time of day impact. In the following, we provide a discussion of the obtained results contrasting them to existing literature.</p><p>Aggregate Outbound and Inbound Rates are Limited by the VM Rate Cap Most existing work for GDA treat the WAN as a full mesh and assume that bandwidth contentions occur at the uplinks or downlinks of VM pairs. However, our results from <ref type="figure" target="#fig_2">Figure 2b</ref> show that TCP connections of different inter-datacenter pairs will adaptively share the maximum available bandwidth when TCP connections are created or stopped, though their aggregate thoughput is always around 2 Gbps. This means that the aggregate outbound rate of VMs is limited by the VM cap instead of the rate limiting associated with links. We have similar observations on the aggregate inbound rate of VMs. We hypothesize that all outbound traffics may travel through the same physical NIC.</p><p>Available Inter-Datacenter Bandwidth is Spatially Homogeneous Measurement results in <ref type="figure" target="#fig_2">Figure 2a</ref> reconfirm the rate-limiting on a per-flow basis in inter-  datacenter connections <ref type="bibr" target="#b6">[7]</ref>. However, our results highlight three new aspects. First, although we notice a significant increase of per-TCP throughput on Azure and Google Cloud, such an increase does not show up on our measured EC2 VMs. We hypothesize that more conservative rate-limiting policies are used on EC2. Second, while the measured per-flow cap on EC2 is divergent among different region pairs, the throughput of single TCP connections across different regions can uniformly reach up to 1 Gbps on Azure and GCP. It suggests that changing TCP configurations does not work on our measured EC2 VMs. Third, if we focus on multiple TCP connections, the aggregate throughput is capped at the same limit for various inter-datacenter connections. As shown in <ref type="figure" target="#fig_2">Figure 2a</ref> and <ref type="table" target="#tab_1">Table 3</ref>, although different VM types have divergent rate limiting, the maximum available WAN bandwidth from US Center to other remote regions is homogeneously capped per-VM.</p><p>Available Inter-Datacenter Bandwidth is Temporally Stable We further investigate the WAN bandwidth variations on these three cloud platforms. <ref type="figure" target="#fig_2">Figure 2b</ref> shows that the available bandwidth on Google Cloud is stable over 500 seconds. We note that this observation holds for Azure and EC2 high performance VMs in our sample measurements over multiple days, with no extra variations. This reconfirms our hypothesis that available inter-datacenter bandwidth is capped per-VM. Discussion The performance assessment presented above carries advantageous information to the design of WAN-aware cloud systems. In particular, the simplifying assumption in previous works that the runtime WAN environment of wide-area data analytics is temporally stable can be satisfied on some types of VMs, which makes it possible to ensure expected timely data delivery for data transfers. However, the VM rate cap on outbound traffics motivates us to focus on the bandwidth contentions inside VMs, instead of link-level optimizations used in existing GDA work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Latency</head><p>We now present results of our measurement study of inter-datacenter latency. Our measurements of WAN latency span 40 datacenter regions across three cloud providers. It is worth noting that our measurements in August 2017 and January 2018 are quantitatively similar, as RTT mainly depends on the physical distance in the geo-distributed scenario. <ref type="figure" target="#fig_3">Figure 3</ref> shows the measurement results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WAN Latency Varies Greatly Between Different</head><p>Regions Latencies between datacenters vary widely based on inter-datacenter distances. For example, latency between remote regions -e.g., Sao Paulo and Mumbai is up to 390 ms -imposes challenges for interactive cloud services across regions. Meanwhile, we observe that latency across the WAN is predictable with no significant variations over time (similar to <ref type="bibr" target="#b11">[12]</ref>). The superior performance of inter-datacenter WAN over the public Internet is mainly due to the more manageable backbone networks on clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">To Relay or Not to Relay?</head><p>It is time to revisit the question: Can we leverage the power of relay between clouds? Prior work <ref type="bibr" target="#b11">[12]</ref> has substantiated that detour routing is highly effective for cloud paths to improve network reliability. Unfortunately, there is little scope for higher available bandwidth and better latency performance on the WAN by relaying. Observations of the VM rate cap show that relay may not be helpful for higher network throughput. However, there are two interesting takeaways. First, it is encouraging for tenants to use multiple TCP connections for higher inter-datacenter throughput, because single TCP connections generally cannot saturate the per-VM rate cap. It is worth noting that multiple TCP connections will not result in extra cost for the same amount of inter-WAN data transfers, since cloud providers charge their users network traffic in terms of the volume of transfers. Second, when we consider EC2 applications that rely on single TCP connections, applications that can afford to pay extra can relay data transfers through Azure or Google Cloud to significantly improve network throughput. This is because the per-flow cap on our measured EC2 VMs is heterogeneous across different regions but greatly depends on latency, while the throughput of single TCP connections on Azure and Google Cloud are uniform around 1 Gbps.</p><p>On the other hand, there is little room to improve latency by relaying through different regions. Although the union of datacenters across multiple cloud providers results in a geographically denser set of datacenters than any single provider, datacenters on different clouds are geographically-close in the same continent. Hosting the service in a datacenter region that is close to users should be a better way to decrease network latency.</p><p>Finally, our measurements on clouds are far from comprehensive. Over 40 VM instance types with a variety of CPU, memory, disk, and network options are available on EC2 and Azure. Google provides 18 types and also allows customizing customer requirement for VMs memory and the number of CPU cores <ref type="bibr" target="#b14">[15]</ref>. This flexible customization makes it pretty difficult to cover the entire VM-WAN performance landscape. In particular, the recently introduced "burstable" instances (e.g., t2.nano on EC2) that are significantly cheaper than the regular instances may represent a departure from the WAN performance on other regular VMs as their fine-grained token bucket like mechanisms for resource sharing <ref type="bibr" target="#b15">[16]</ref>. Recall that the observable bandwidth from a tenant's perspective only occupies a small part of WAN capacity, so measuring variations in the underlying WAN can be more challenging; as such, there may be an opportunity to improve WAN bandwidth by detours when we consider data transfers on large-scale VMs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: WAN bandwidth on EC2 varies spatially and temporally when bandwidth is measured with a single TCP flow. In (b), the bottom and top ends of each boxplot represent the 25 th and 75 th percentiles, and the line in the middle represents the median. Samples are measured every 30 minutes, where each box reports the bandwidth variation over 60 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>a) Throughput from other regions to US Center. (b) Outbound throughput from US Center to other regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Available WAN bandwidth is homogeneous on Google Cloud. The default per-TCP throughput in (a) means the measured throughput without modifying the maximum window size in Linux. In (b), each line notes the aggregate throughput of 50 TCP connections. TCP connections from the VM in US Center to other sinks are created at different time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Latency across 40 datacenters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Throughput to US Center on EC2. (Mbps) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Special thanks go to Zhe Wu and Muhammed Uluyol for collecting parts of our bandwidth and latency measurement datasets. This work was supported in part by National Science Foundation grants CNS-1563095, CNS-1463126, and CNS-1563849.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clarinet: Wan-aware optimization for analytics queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raajay</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low latency geodistributed data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gaia: Geo-distributed machine learning approaching lan speeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Konomis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Global analytics in the face of bandwidth and regulatory constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vulimiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jungblut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Applicationspecific configuration selection in the cloud: impact of provider policy and potential of systematic testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajjat</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Ruiqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Yiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scheduling jobs across geo-distributed datacenters with max-min fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Guaranteeing deadlines for inter-data center transfers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flutter: Scheduling tasks closer to data across geodistributed datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing shuffle in wide-area data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDCS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Resilient overlay networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving the reliability of internet paths with one-hop source routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><forename type="middle">V</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wetherall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Measuring and improving the reliability of widearea cloud paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Haq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoon</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">R</forename><surname>Dogar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Via: Improving internet telephony call quality using predictive relay selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Junchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rajdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Esbjorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marcin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dalibor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Renat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Doug Woos, Thomas Anderson, and Arvind Krishnamurthy. One tunnel is (often) enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Zhang</surname></persName>
		</author>
		<editor>SIG-COMM</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptively unearthing the best cloud configurations for big data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Omid Alipourfard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Hongqiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlan</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using burstable instances in the public cloud: When and how</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neda</forename><surname>Nasiriani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kesidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvan</forename><surname>Urgaonkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
