<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">System Combination of RBMT plus SPE and Preordering plus SMT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terumasa</forename><surname>Ehara</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Ehara NLP Research Laboratory Seijo</orgName>
								<address>
									<settlement>Setagaya, Tokyo</settlement>
									<country>JAPAN eharate</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">System Combination of RBMT plus SPE and Preordering plus SMT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>System architecture, experimental settings and evaluation results of EHR group in the en-ja, zh-ja, JPCzh-ja and JPCko-ja tasks are described. Our system concept is combination of a rule based method and a statistical method. System combination of rule-based machine translation (RBMT), RBMT plus statistical post-editing (SPE) and preordering plus statistical machine translation (SMT) is conducted. From the multiple outputs of three systems, candidate selection part selects the best output by language model score. For JPCzh-ja task devtest data translation, SPE improves BLEU score by 17.81, preordering improves BLEU score by 1.89 and system combination improves BLEU score by 0.26.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Two main processes of machine translation are lexical transfer and structural transfer. Machine translation techniques and related techniques are classified in terms of these two processes as shown in <ref type="table" target="#tab_1">Table 1</ref>.  RBMT and SMT conduct lexical transfer and structural transfer simultaneously. On the other hand, monotone SPE and monotone SMT, which are technically the same process, conduct lexical transfer only. Preordering conducts structural transfer only.</p><p>We have made researches combining a rule based method and a statistical method that is RBMT plus SPE <ref type="bibr" target="#b1">(Ehara, 2014)</ref>. This year, we add preordering plus SMT part to our system for WAT2015. This new system is also the combination of rule based method (RBMT and preordering) and statistical method (SPE and SMT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">System architecture</head><p>Our basic system architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Input sentence is fed to RBMT system, RBMT plus SPE system and Preordering plus SMT system <ref type="bibr">1</ref> . From outputs of three systems, candidate selection part selects best output as the system output. Here, our SPE and SMT are semi-monotone, because distortion limit of decoder is set to 6 instead of 0. We will explain details of the each part of the system in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">RBMT part</head><p>We use a commercial based RBMT system for the RBMT part. We, also, use user terminology dictionaries for zh-ja, JPCzh-ja and JPCko-ja tasks. For zh-ja and JPCzh-ja tasks, we use a part of Chinese to Japanese technical term dictionary of JPO (Japan Patent Office) (Japio, 2015) 2 . Original JPO's dictionary includes 2,210,294 words (nouns and verbs). We filter out all verbs and the nouns which have identical Japanese translations with the commercial based RBMT outputs. As the result, we select 1,463,265 terms for the user dictionary for JPCzh-ja and zh-ja tasks. For JPCkoja task, we make a user dictionary from the training corpus of the task. We get 434,334 terms for the user dictionary for the JPCko-ja task. For enja task, we don't use any user dictionary.</p><p>We also use sentence pattern dictionary for JPCzh-ja task. We use only 13 sentence patterns for the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SPE part</head><p>SPE part intends to improve the translation quality of the output of the RBMT part. All target languages of the tasks are Japanese. So SPE part translates Japanese to Japanese. We use phrase based Moses ( <ref type="bibr" target="#b5">Koehn et al., 2003</ref>) with default options as the SPE engine. Word segmenter for Japanese is Juman ver.7.01 ( <ref type="bibr" target="#b6">Kurohashi et al., 1994)</ref>.</p><p>Translation models (TM) of each task is built from RBMT output and reference translation of the training corpus of each task. Training corpus size (number of sentence pairs) will be listed in <ref type="table" target="#tab_6">Table 3</ref>.</p><p>Language model (LM) is built from the following monolingual corpora. LM for en-ja task and zh-ja task is built from target side of the training corpora both of the en-ja task and zh-ja task (3.6M sentences). LM is built by lmplz tool in Moses (KenLM) with order 6. LM for JPCko-ja task and JPCzh-ja task is built from target side of the training corpora both of the JPCko-ja task and JPCzh-ja task and also Japanese side of NTCIR-10's training corpora of PatentMT task EJ subtask ( <ref type="bibr" target="#b2">Goto et al. 2013</ref>) <ref type="bibr">3</ref> . Total number of sentences for this LM training is 5M. This LM is also built by lmplz with order 6.</p><p>Distortion limit (DL) of the decoder is set to 6. Usual setting of DL between Japanese and English or between Japanese and Chinese is 10 or over. So we call our SPE part semi-monotone SPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Preordering part</head><p>Preordering is one of the effective technique to improve structural transfer accuracy <ref type="bibr" target="#b3">(Isozaki, 2010)</ref>. Our preordering method uses context free parsing rules with reordering rules. <ref type="figure" target="#fig_2">Figure 2</ref> shows examples of parsing rules with reordering rules and example of parsed phrases <ref type="bibr">4</ref> . First example is English grammar rule with reordering rule. The right hand side of the parsing rule "ADVP VBN PP" is reordered to "PP ADVP VBN" by the reordering rule "2 0 1 5 ." Second example is Chinese grammar rule with reordering rule. Reordering rules are built by a heuristic algorithm. Parsing accuracy directly affects preordering accuracy. We use Stanford Chinese word segmenter ( <ref type="bibr" target="#b10">Tseng et al., 2005;</ref><ref type="bibr" target="#b0">Chang et al., 2008)</ref> and Berkeley parser ( <ref type="bibr" target="#b9">Petrov et al., 2006</ref>) as the parsing engine of our preordering part. Two improvements for the parsing are conducted. First one is grammar improvement for Chinese grammar. For en-ja task, we use the original English grammar of the Berkeley parser, eng_sm6.gr. For JPCko-ja task, we don't conduct preordering because of the similarity of word order of Korean and Japanese. For zh-ja task and JPCzh-ja task, we improve the original Chinese grammar, chn_sm5.gr. It will be explained in section 2.3.1. Second improvement of parsing is reranking of k-best parse trees that will be explained in section 2.3.2. <ref type="bibr">4</ref> All sample sentences and phrases in this paper are from ASPEC Corpora or JPO Patent Corpora provided by the workshop organizer. <ref type="bibr">5</ref> Reordering rule "2 0 1" means that position of right hand side tags permutates from "0 1 2" to "2 0 1". Then, "ADVP VBN PP" is reordered to "PP ADVP VBN".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VP ⇒ ADVP VBN PP 2 0 1 (widely utilized in many fields)</head><p>VP ⇒ VV NP IP 1 2 0 (使 各个 电动机 13 旋转 驱动)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Grammar improvement</head><p>The idea for grammar improvement is to use word alignment of JPCzh-ja bilingual training corpus. Firstly, word alignment is conducted from JPCzhja training corpus (1M sentence pairs) by GIZA++ ( <ref type="bibr" target="#b8">Och and Ney, 2003)</ref>. For each sentence pairs, we decide sentence head word both for Japanese and Chinese using word alignment. Since Japanese is a typical head final language, head word of Japanese sentence is positioned at the end of the sentence. So it is easy to find the sentence head word for Japanese sentences. We find Chinese sentence head word as the aligned word to the Japanese sentence head word. For example, in the ja-zh sentence pair shown in <ref type="figure" target="#fig_3">Figure 3</ref>, Japanese sentence head word "発生する" is aligned to Chinese word "产生". So "产生" is decided as the head word of this Chinese sentence. We consider it as the gold standard head word. Next step is to make a tree bank. Chinese sentences of training corpus are parsed by the original grammar i.e. chn_sm5.gr, and we get k-best parse trees for each sentence (k is set to 100). Then we select the best parse, in which the sentence head word is same as the gold standard head word. For example, the Chinese sentence in <ref type="figure" target="#fig_3">Figure 3</ref> is kbest parsed shown in <ref type="figure">Figure 4</ref>. The top ranked tree has "接收" as the sentence head word and the second ranked tree has "产生" as the sentence head word, which is same as the gold standard. So we <ref type="bibr">6</ref> Devtest data is not included in the training data for the grammar improvement. Gold standard of Chinese put the second ranked tree to our tree bank in this case. From 1M JPCzh-ja training corpus, the number of second or lower ranked tree is selected is about 151K. Re-training Berkeley Chinese grammar using this 151K tree bank, we get new grammar named chn_jpo.gr. Comparing original chn_sm5.gr and chn_jpo.gr, the agreement rate of sentence head word of top ranked parse tree and gold standard for devtest data is shown in <ref type="table" target="#tab_3">Table  2</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Reranking of k-best parse trees</head><p>Another improvement of preordering part is reranking of k-best parse trees. For the training corpus, reordered source sentence is compared to gold standard reordered source sentence. Here, gold standard reordered source sentence is determined using alignment to a target sentence. For example, Chinese sentence shown in <ref type="figure" target="#fig_3">Figure 3</ref> "闪 烁器 54 接收 X 射线 产生 光 。" is gold standard reordered to "闪烁器 54 X 射线 接收 光 产生 。" using alignment to the target sentence. This comparison is measured by word error rate and select the parse tree which has the minimum word error rate in the kbest parse trees as the best parse tree. The parse tree shown in <ref type="figure">Figure 4</ref> (a) is reordered to "闪烁器 54 X 射线 光 产生 接收 。" and the parse tree shown in <ref type="figure">Figure 4</ref> (b) is reordered to " 闪 烁 器 54 X 射 线 接 收 光 产 生 。". Then, <ref type="figure">Figure 4</ref> (b) is selected as the best parse tree in this case.</p><p>For the dev, devtest and test sets, we use LM based reranking to select the best parse tree. Firstly, we make reordered source sentence corpus from the training corpus by the above method and build LM using this corpus. Next, we select the best parse tree which has the maximum LM score in the k-best reordered sentences in dev, devtest and test sets. Here, LM score of a sentence is a score calculated by the tool "query" in Moses divided by the number of words in the sentence.</p><p>sentence head for devtest data is determined by the method described above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">SMT part</head><p>SMT part uses phrase based Moses same as SPE part. For JPCko-ja task, SMT part translates source sentences to target sentences as the usual phrase based SMT. As the segmenter for Korean, we use MeCab-ko <ref type="bibr">7</ref> . For JPCzh-ja task, zh-ja task and en-ja task, SMT part translates reordered source sentences to the target. Reordering is made by the method described in 2.3. Distortion limit is set to 6 both JPCko-ja task and other tasks. So, we call our SMT semi-monotone SMT. LM for SMT is same as LM for SPE. TM is trained from the training corpus provided by the workshop organizer. Training corpus sizes (number of sentence pairs) are listed in <ref type="table" target="#tab_6">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Candidate selection part</head><p>The last part of our translation system is a candidate selection part. This part select the candidate which has the maximum LM score from the outputs of RBMT part, RBMT+SPE part and Preordering+SMT part. Here, LM score is calculated from the LM for SMT part by the method described in section 2.3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Other ad-hoc processing</head><p>For JPCko-ja task, we conduct an ad-hoc preprocessing for Korean source sentences of the train, dev, devtest and test corpora and their RBMT outputs. It is deletion of brackets surrounding the number, because the use of brackets between Korean and Japanese is different shown in <ref type="figure" target="#fig_4">Figure 5</ref>. In Korean sentence, number "2" is surrounded by the brackets. However, in Japanese sentence, number "２" is not surrounded by the brackets. So we delete brackets surrounding the number in Korean side to improve alignment accuracy of brackets. Another ad-hoc processing is to convert all half width characters in RBMT and SMT outputs to full width characters, because Japanese document tend to use full width characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Issues for context-aware machine translation</head><p>We have no consideration for context-awareness in our system. <ref type="table">Table 4</ref> shows the official evaluation results of the translation of the test data ( <ref type="bibr" target="#b7">Nakazawa et al., 2015)</ref>. In all cases, BLEU and RIBES are calculated using Japanese segmenter Juman.  <ref type="table">Table 4</ref>: Evaluation results of the translation In JPCko-ja task and JPCzh-ja task, system combination using candidate selection by LM score is more accurate than RBMT+SPE system both in automatic and human evaluation. In zh-ja task, Preordering+SMT system has higher BLEU and RIBES than system combination. However, we don't have human score for preordering+SMT system for the zh-ja task.  <ref type="table" target="#tab_8">Table 5</ref>: The number of each system outputs selected by the candidate selection part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Translation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Candidate selection results</head><p>To confirm effectiveness of candidate selection process, we compare LM scores and human evaluation scores for JPCzh-ja task. <ref type="table" target="#tab_10">Table 6</ref> shows human evaluation score of SPE 8 outputs and SMT outputs when the case of LM score for SMT output exceeds LM score for SPE output.  From the <ref type="table" target="#tab_10">Table 6</ref>, we can see this candidate selection process makes human score better in 65 cases (SMT &gt; SPE) and worse in 32 cases (SPE &gt; SMT). The number of tie cases is 125.</p><p>To investigate worsen cases, we show several translation examples. <ref type="table" target="#tab_11">Table 7</ref> shows SMT output and SPE output, baseline (BASE) output, reference (REF) and source (SRC) of two translation examples. In these cases, LM score of SMT output is greater than LM score of SPE output. But human score of SMT output (-1) is less than human score of SPE output (1).</p><p>In the first example, the word "オートコリメ ータ" is less general than the word "コリメータ". Actually, LM score of the former word is -5.61676 and LM score of the latter word is -4.12944. Then LM score of the former sentence is less than LM score of the latter sentence.</p><p>In the second example, "アノード４１０と陰 極４１５" is less general than "アノード４１５ とカソード４１０" in our LM. Actually, LM score of SMT output sentence is -66.1355 and the LM score of the sentence which is converted the term "アノード４１５とカソード４１０" of SMT output to "アノード４１０とカソード４ 8 In this section, the term "SPE" is used for RBMT+SPE and the term "SMT" is used for preordering+SMT for the simplicity.</p><p>１５" is -71.1855. These two examples indicate the limitation of LM score based candidate selection method.   User dictionary, SPE and preordering greatly improve RIBES score. Improving of grammar, reranking of parse trees and system combination slightly improve RIBES score. For BLEU score, results are almost similar as RIBES case. However, preordering with original grammar makes BLEU score worse compared with simple SMT. RBMT+SPE with user dictionary improves BLEU score by 17.81 compared with simple RBMT. Preordering+SMT with the improved grammar and reranking of parse trees improves BLEU score by 1.89 compared with simple SMT with DL 10. System combination improves BLEU score by 0.26 compared with preordering+SMT.</p><formula xml:id="formula_0">SMT 図５は、コリメータ装置による調整動作を説明する ための概略図である。 SPE 図５は、オートコリ メータ装置による調整動作を説 明するための概略図である。 BASE 図５は、自動コリメータ装置による調整動作を説明 するための概略図である。 REF 図５は、オートコリ メータ装置による調整動作を説 明するための概略図である。 SRC 图 5是用于 说 明由自动准直仪装置 进 行的 调 整操 作的概略 图 。 SMT 図４は、名称からア ノード４１５とカソード４１０との 間に挟まれた発光層４０５のＯＬＥＤ４００とを含む第 １の例示的な実施の形態について説明する。 SPE 図４は、概念上からア ノード４１０と陰極４１５との 間の発光層４０５のＯＬＥＤ４００の第１の例示的実 施形態を含むと説明する。 BASE 図４は、陽極４１０とカソード４１５との間の発光層 ４０５のＯＬＥＤ４００の第１で概念的に説明は、例示 的な実施形態が挟まれている。 REF 図４は、ア ノード４１０とカソード４１５とに挟まれた 発光層４０５を含むＯＬＥＤ４００の第１実施例の概念 図である。 SRC 图 4从概念上 说 明包括 夹 在阳极4 1 0 与阴极4 1 5 之 间 的 发 射 层 405的OLED 400的第一示例性 实 施 方案。</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Other results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>System architecture, experimental settings and evaluation results of the EHR group in the WAT2015 tasks are described. Our system design concept is combining of rule-based method and statistical method and it gives the good effect to the translation accuracy. One of the future issues is to improve parsing accuracy both in RBMT part and preordering part.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Basic system architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>RBMT Semi-monotone SPE Preordering Semi-monotone SMT Candidate selection Input Output RBMT 29 Proceedings of the 2nd Workshop on Asian Translation (WAT2015), pages 29 34, Kyoto, Japan, 16th October 2015. 2015 Copyright is held by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of parsing rules and reordering rules with examples</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of zh-ja word alignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Different bracket usage in Korean and Japanese.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Classification of MT and 
related techniques. 

1 For JPCko-ja task, we don't use preordering part. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Agreement rate of sentence head word 
for the devtest data 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 : Training corpus size</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 shows</head><label>5</label><figDesc>the candidate selection results. Most of outputs of RBMT part are not selected. Outputs of RBMT+SPE part and outputs of preor- dering+SMT part are selected about half and half.</figDesc><table>또한, 산화피막(2)이 존재하는 경우에는, ・・・ 
また、酸化皮膜２が存在する場合には、・・・ </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc>The number of human evaluation scores for SPE outputs and SMT outputs when the case of LM score of SMT output exceeds LM score of SPE output.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table>Example of candidate selection part 
making worse output. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="true"><head>Table 8 shows other evaluation results for JPCzh- ja task on devtest data translation.</head><label>8</label><figDesc></figDesc><table>System 
Additional feature 
BLEU 
RIBES 
RBMT 
16.55 
0.7192 
RBMT 
User dictionary 
23.54 
0.7510 
RBMT+SPE 
User dictionary 
41.35 
0.8203 
SMT 
(DL=10) 
40.86 
0.8071 
Preordering+SMT 
original grammar 
40.15 
0.8164 
Preordering+SMT 
improved grammar 
41.84 
0.8218 

Preordering+SMT 

improved grammar + 
reranking of parse 
trees 

42.75 
0.8237 

System combination 
43.01 
0.8265 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Evaluation results for JPCzh-ja devtest 
data translation 

</table></figure>

			<note place="foot" n="2"> https://alaginrc.nict.go.jp/resources/jpo-info/jpolist.html 3 Dev, devtest and test data of JPCko-ja task and JPCzh-ja task are extracted from Japanese patent documents published in 2013. On the other hand, NTCIR-10&apos;s training corpora is extracted from Japanese patent documents published in 1990 to 2005. They are not overlapped.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing Chinese Word Segmentation for Machine Translation Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation</title>
		<meeting>the Third Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A machine translation system combining rule-based machine translation and statistical post-editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terumasa</forename><surname>Ehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Asian Translation (WAT2014)</title>
		<meeting>the 1st Workshop on Asian Translation (WAT2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="50" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Overview of the Patent Machine Translation Task at the NTCIR-10 Workshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ka</forename><forename type="middle">Po</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">K</forename><surname>Tsou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th NTCIR Conference</title>
		<meeting>the 10th NTCIR Conference</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="260" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Head finalization: A simple reordering rule for SOV languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Isozaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR</title>
		<meeting>the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="244" to="251" />
		</imprint>
	</monogr>
	<note>Hajime Tsukada and Kevin Duh</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Investigation report of dictionary improvement and quality evaluation of machine translation for Chinese patent documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Japio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Japan Patent Information Organization). in Japanese</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Statistical Phrase-Based Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2003</title>
		<meeting>HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihisa</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Nagao</surname></persName>
		</author>
		<title level="m">Improvements of Japanese morphological analyzer JUMAN. Proceedings of The International Workshop on Sharable Natural Language Resources</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="22" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideya</forename><surname>Mino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
		<title level="m">Overview of the 2nd Workshop on Asian Translation, Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Systematic Comparison of Various Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="51" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Accurate, Compact, and Interpretable Tree Annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Thibaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COL-ING-ACL 2006</title>
		<meeting>COL-ING-ACL 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="433" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Conditional Random Field Word Segmenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihsin</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth SIGHAN Workshop on Chinese Language Processing</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
