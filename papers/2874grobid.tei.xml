<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using Elimination and Delegation to Implement a Scalable NUMA-Friendly Stack</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Calciu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">E</forename><surname>Gottschlich</surname></persName>
							<email>justin.e.gottschlich@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurice</forename><surname>Herlihy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Brown University</orgName>
								<orgName type="institution" key="instit2">Intel Labs</orgName>
								<orgName type="institution" key="instit3">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using Elimination and Delegation to Implement a Scalable NUMA-Friendly Stack</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Emerging cache-coherent non-uniform memory access (cc-NUMA) architectures provide cache coherence across hundreds of cores. These architectures change how applications perform: while local memory accesses can be fast, remote memory accesses suffer from high access times and increased interconnect contention. Because of these costs, performance of legacy code on NUMA systems is often worse than their uniform memory counterparts despite the potential for increased parallelism. We explore these effects on prior implementations of concurrent stacks and propose the first NUMA-friendly stack design that improves data locality and minimizes interconnect contention. We achieve this by using a dedicated server thread that performs all operations requested by the client threads. Data is kept in the cache local to the server thread thereby restricting cache-to-cache traffic to messages exchanged between the clients and the server. In addition, we match reciprocal operations (pushes and pops) by using the rendezvous elimination algorithm before sending requests to the server. This has the dual effect of avoiding unnecessary interconnect traffic and reducing the number of operations that change the data structure. The result of combining elimination and delegation is a scalable and highly parallel stack that outperforms all previous stack implementations on NUMA systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The current trend in computer architecture is to increase system performance by adding more cores so that more work can be done simultaneously. In order to enable systems to scale to hundreds of cores, the main hardware vendors are switching to non-uniform memory access (NUMA) architectures. Recent examples include Intel's Nehalem family and the SPARC Niagara line.</p><p>NUMA systems contain multiple sockets connected by an interconnect. Each socket (also called a node) consists of multiple processing cores with a shared last level cache (LLC) and a local memory (as in <ref type="figure">Figure 1</ref>). A thread can quickly access the local memory on its own socket and it can access the memory on another socket using the interconnect, so the programming model is similar to uniform memory architectures. The NUMA design allows systems to scale to hundreds of cores and provides inexpensive data sharing for cores on the same socket. However, remote cache invalidations and remote memory access can drastically degrade performance because of the interconnect's high latency and limited bandwidth. Therefore, in many cases, legacy code exhibits worse throughput when ported to NUMA machines than on non-NUMA ones.</p><p>Prior research addresses this by using a NUMA aware contention manager that migrates threads closer to the data they access <ref type="bibr" target="#b0">[1]</ref>. However, migrating threads is a complex solution that, while feasible for operating systems, is not generally realistic for end-user applications. Alternatively, one could devise solutions in which the data are moved to the accessing threads. For example, cohort locks <ref type="bibr" target="#b1">[2]</ref> and NUMA readerwriter locks <ref type="bibr" target="#b2">[3]</ref> keep the data local to one cache as long as possible. This is implemented by transferring ownership of the locks from the threads finishing their critical sections to other threads on the same socket. Similarly, Metreveli et al.</p><p>[4] minimize cache data transfers by partitioning a concurrent hash table and distributing operations for each partition to a specifically assigned thread. All threads wanting to access the hash table submit requests to these server threads through message passing implemented in shared memory. Essentially, the hash table resides in the caches of the accessing threads and the cache-to-cache traffic is limited to requests sent to and from the servers.</p><p>Making Data Structures NUMA-Friendly. To maximize performance, Metreveli et al. <ref type="bibr" target="#b3">[4]</ref> leverage the concurrency properties of hash tables in their partition implementation. Namely, hash tables are highly concurrent, easily partitionable data structures. However, many data structures do not have the inherent concurrency benefits of hash tables. In this paper, we focus on a NUMA-friendly implementation of a stack. Nevertheless, the method presented can be applied to other data structures as well.</p><p>Stacks have a broad range of uses: from calculators to evaluate expressions to compilers to parse the syntax of expres- <ref type="figure">Figure 1</ref>: Example of a NUMA system with two nodes and 128 hardware threads. sions and program blocks. In addition, stacks can easily be used to implement unfair thread pools and any containers without ordering guarantees. An example is the Java unfair synchronous queue <ref type="bibr" target="#b4">[5]</ref>.</p><p>Unfortunately, stacks cannot be easily partitioned without forfeiting their last-in-first-out (LIFO) property. Because of this, multiple threads often contend on the single entry point providing access into the stack. It is primarily for this reason that stacks seem to be inherently sequential. However, prior work has shown that stacks can benefit from parallelism under balanced workloads (i.e., a similar number of push and pop operations) using a method called elimination <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This is implemented by canceling concurrent inverse operations from different threads even before they reach the stack. Elimination is not specific to stacks. Moir et al. <ref type="bibr" target="#b7">[8]</ref> have shown how to use elimination with queues. Although this method significantly improves scalability of stacks, it does not address our primary concern: i.e., remote cache invalidations on NUMA systems.</p><p>We combine a variation of this method and a slightly modified version of the delegation method introduced by CPHash <ref type="bibr" target="#b3">[4]</ref> using only one server thread, to design what is, to the best of our knowledge, the first NUMA-friendly stack. We describe in more detail how we use elimination in Section 2.2 and delegation in Section 2.1.</p><p>Our goal is to reduce cache traffic and maintain data locality while using the properties of the underlying data structure to enable parallelism. The result is a scalable and highly parallel stack that outperforms all previous stack implementations on NUMA systems.</p><p>The technical contributions of this paper are as follows. 1) We explore previous concurrent stack implementations in the context of NUMA systems; 2) We design, implement, and evaluate a scalable concurrent stack optimized for NUMA machines by delegating responsibility for all requests to one server thread, keeping the data local to this thread's cache, avoiding synchronization, contention and cache to cache traffic; 3) We enable parallelism by using elimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">NUMA-Friendly Stack</head><p>In this section we describe our use of delegation to implement a NUMA-friendly stack. At the highest level, our design provides efficiencies in increased cache locality and reduced interconnect contention. After discussing the design, we show how we employ the rendezvous elimination algorithm <ref type="bibr" target="#b6">[7]</ref> to make this stack scalable. Moreover, we differentiate between global elimination, which is implemented using one rendezvous structure shared by all threads, and local elimination, which contains an elimination structure for each NUMA node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Delegation</head><p>The idea of one thread helping other threads complete their work is a well-known concept <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>. A recent example of this helping mechanism is called flat combining <ref type="bibr" target="#b8">[9]</ref>, in which a thread that acquires a lock for a data structure executes operations for itself and also for other threads that are waiting on the same lock. The global lock and the data remain in this thread's cache while it executes operations on behalf of other threads, thereby decreasing the number of cache misses and contention on the lock. Moreover, flat combining aligns well for data structures that are sequential, because only one thread would be able to operate on it at a time, regardless.</p><p>Due to the increasing number of hardware threads in a system, the helper thread could be a dedicated thread (called a server thread) used only to service requests from other threads (client threads). This is especially useful on heterogeneous architectures, where some cores could be faster than others. An example of this approach is CPHash <ref type="bibr" target="#b3">[4]</ref>, a partitioned hash table. Each partition has an associated server thread that receives add and remove requests from clients and sends back the responses obtained from performing the operations requested. Each client-server pair share a location where they exchange messages.</p><p>We use this delegation approach to implement a NUMA friendly stack. In particular, we use one dedicated server thread that accepts push and pop requests from many client threads. The communication is implemented in shared memory, using one location (which we call slot) for each client. The server loops through all the slots, collecting and processing requests and writing the responses back to the slots. The clients post requests to their private slots and spin-wait on that slot until a response is provided by the server. We note that only the pop operations need to spin-wait until a response is provided. The push operations could return as soon as the server notices their requests. This optimization improves throughput, but we decided not to use it in our experiments, for a more fair comparison with the other methods.</p><p>A weakness of this design is that using a reserved slot for each client can result in wasted space if the clients' workloads are not evenly distributed. Furthermore, the server must loop through all slots, even those not in use, when looking for requests. These two drawbacks can result in increased space and time complexity. To overcome these limitations, we statically assign several threads to the same slot by thread id. 1 To synchronize the access of multiple threads to the same slot, we introduce an additional spinlock for each slot. <ref type="figure" target="#fig_2">Figure 3b</ref> reflects these changes to the communication protocol. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overall interaction between the server and the clients. Clients posts their requests in shared local slots and wait for the server to process them. The server loops through all the slots, processes requests as it finds them and immediately posts back the response in the same slot. The sequential stack is only accessed by the server thread; therefore, the top part of the stack remains in the server's L1 cache or LLC all throughout execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Elimination</head><p>Stacks are generally seen as sequential data structures. This is because all threads contend for access to the stack at its top location. However, prior work has shown that stacks can be parallelized using a technique called elimination <ref type="bibr" target="#b5">[6]</ref>. This technique uses an additional data structure to allow threads performing push operations to meet threads performing pop operations and exchange their arguments. This is equivalent to the push being executed on the stack and immediately being followed by a pop. The elimination data structure, generally implemented as an array, allows multiple such pairs to exchange arguments in parallel and decrease contention on the underlying lock-free stack. If one thread fails to find its inverse operation being performed by another thread, then the elimination attempt times out and the thread accesses the stack directly.</p><p>The rendezvous method <ref type="bibr" target="#b6">[7]</ref> improves the elimination algorithm by using an adaptive circular ring for the additional elimination data structure. In this paper, whenever we refer to an elimination layer, we use a rendezvous structure to implement it.</p><p>Elimination generally works best when the number of inverse operations are roughly equivalent. For inequivalent, unbalanced workloads, many operations cannot be eliminated, thereby requiring a thread to access the data structure directly. This creates contention and cache-to-cache traffic because these operations could originate from different NUMA nodes. In order to solve these problems, we augment the delegation stack presented in the previous section with a rendezvous elimination layer. Threads first try to eliminate and, if they time out, they delegate their operation to the server thread. Delegation ensures that the data remains in the server's cache, while elimination enables parallelism, thus making the NUMA-friendly stack more scalable. Moreover, threads can continue to try to eliminate while they wait for the spinlock of their slot to be released. The complete algorithm is described in <ref type="figure" target="#fig_2">Figure 3c</ref>. (c) (Red) Elimination: Threads first try to eliminate; if they fail they then try to acquire the slot spinlock and submit a request, but if the lock is already taken, they go back to the elimination structure; they continue this loop until either they eliminate, or they acquire the spinlock.</p><p>Local vs. Global Elimination. For the rendezvous stack, threads first try elimination and, in the case of failure, they then directly access the stack. Our NUMA-friendly stack is an improvement over this design, because it increases locality and reduces contention on the stack by replacing direct access to the stack with delegation. However, the initial stage of elimination can still cause a number of invalidations between different NUMA nodes' caches because each of the threads accesses the same shared structure when performing elimination. To overcome this bottleneck, our NUMAfriendly stack splits the single elimination data structure into several local structures, equal to the number of NUMA nodes. To minimize interconnect contention, we limit elimination to occur only between those threads located on the same socket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Advantages and Limitations</head><p>Our stack design is optimized for the NUMA architecture. Local elimination and delegation both contribute to removing the contention on the interconnect and on the stack. The number of remote cache misses is reduced to exchanging messages between the server and the clients. Furthermore, the stack is only accessed by the server thread, relieving it of any form of synchronization.</p><p>Finally, our stack uses explicit communication, so it is easier to reason about its performance penalties because underlying cache coherency performance issues are eliminated.</p><p>One potential drawback of this approach is that the access to the stack is serialized by using only the server thread. However, the direct access of multiple threads to a stack would also be serialized by a lock to keep the stack's integrity. Moreover, we enable parallelism by using elimination, which compensates for accessing the stack sequentially.</p><p>Another drawback is the potential for additional communication overhead between the clients and the server. For example, if the stack is only rarely accessed, then direct access to it would likely be more efficient. However, the overhead of elimination and delegation is eclipsed by their benefits when there are many threads contending for access to the stack.</p><p>Finally, our description assumes one server thread for each shared stack. In order to maintain high throughput, this thread must always be available to handle queries. Therefore, each server thread is assigned a hardware thread and runs at high priority. Unfortunately, we might not have enough hardware threads if an application uses multiple shared data structures, so some of the structures might have to share a server. In this situation, the server could become a performance bottleneck. However, most applications use only a few shared data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results</head><p>We conducted our experiments on an Oracle SPARC T5240 machine with two Niagara T2+ processors running at 1.165GHz. Each chip has 8 cores and each core has 8 hardware threads for a total of 128 hardware threads (64 per chip). We implemented our NUMA stack algorithm in C++ and we compared it to previous stack implementations using the same microbenchmark as <ref type="bibr" target="#b6">[7]</ref>: a rendezvous stack, a flat combining stack and a lock-free stack. The benchmark has flexible parameters, allowing us to measure throughput under different percentages of push and pop operations. The results we present were obtained using threads with fixed roles (e.g. push-only threads and pop-only threads). We allow the scheduler to assign our software threads to NUMA nodes and then pin them to their respective processors. <ref type="bibr" target="#b1">2</ref> The server thread is created with increased priority compared to the client threads, to guarantee its availability.</p><p>For our experiments, we started by comparing our local elimination and delegation NUMA stack (nstack el) with a lock-free stack (lfstack) <ref type="bibr" target="#b14">[15]</ref>, which has been the basis for other stack implementations such as rendezvous <ref type="bibr" target="#b6">[7]</ref> and flat-combining <ref type="bibr" target="#b8">[9]</ref>. Then, we compared our stack to the flat combining stack (fcstack) <ref type="bibr" target="#b8">[9]</ref>, which outperforms the rendezvous stack when there is no significant potential for elimination (i.e., in unbalanced workloads).</p><p>The scalable performance of the lock-free stack begins to degrade around 16 threads. The flat-combining stack, however, seems unaffected by the type of workload and achieves relatively stable scalability across different thread counts. However, the elimination based NUMA stack outperforms both of them by a large margin. These results can be observed in  Effect of elimination. To judge the effect of the local elimination structures used in our implementation, we compared our NUMA stack (nstack el) against two other versions; one without elimination (nstack) and one with global elimination (nstack el gl). As expected, the global elimination algorithm <ref type="figure">Figure 5</ref>: Results for 70% pushes and 30% pops <ref type="figure">Figure 6</ref>: Results for 90% pushes and 10% pops outperforms the algorithm without elimination, while both perform worse than local elimination. From Figures 4, 5 and 6, we conclude that local elimination is crucial for the scalability of our algorithm. Our experiments were performed on a 2-node NUMA system, but we expect that these results generalize to bigger systems with the same number of cores per node as our system, as long as the push and pop operations are distributed uniformly across all the nodes.</p><p>Effect of delegation. To better understand and characterize the impact of delegation, and because elimination has such a strong influence on performance, we compare our stack against two elimination-variations of the rendezvous stack: one uses local elimination and the other uses global elimination. The rendezvous stack (rendezvous) consists of global elimination and direct access. To provide a more fair comparison, we modified the rendezvous stack to perform elimination locally on each NUMA node (rendezvous loc). Threads that fail to eliminate on each node must access the data structure directly. This local version of the rendezvous stack improves the scalability of the rendezvous stack for NUMA systems. However, our NUMA stack performs even better, indicating there is an observable performance benefit using delegation under high contention, for both balanced and unbalanced workloads <ref type="figure" target="#fig_4">(Figures 4, 5 and 6</ref>). We believe the benefit of delegation would become more apparent on a NUMA system with more sockets. Although the latency of an individual operation could increase because the server needs to inspect slots on more nodes, cache and memory locality would play an even more significant role than they do on a 2-node system. We leave evaluation on such a system as future work.</p><p>Balanced workloads. We experimented with different percentages of push and pop operations. Elimination works best when the number of pushes is very similar to the number of pops. In the balanced workload case, we use 50% push threads and 50% pop threads. Experimental results are shown in <ref type="figure" target="#fig_4">Figure 4</ref>. For this setting, elimination plays a significant role, as most operations will manage to eliminate. There is some benefit from delegation, as we can see when we compare to the local rendezvous algorithm, but not that significant.</p><p>Unbalanced workloads. For unbalanced workloads, elimination plays a much smaller role in reducing the number of operations. We present results for 70% pushes, 30% pops in <ref type="figure">Figure 5</ref> and 90% pushes, 10% pops in <ref type="figure">Figure 6</ref>. In both cases, there is some elimination, but not as significant as in the balanced workload case. However, delegation plays a much more important role for these workloads, as more operations fail to eliminate and need to access the stack. Results show that preserving cache locality through delegation works much better than direct access to the stack.</p><p>Number of slots. Finally, we want to measure the impact of the synchronization introduced with sharing slots by different threads. We compared the implementation of the NUMA stack using shared slots (nstack el) with the implementation using one slot per client thread, which does not require any synchronization to access the slots (nstack el st -nstack elimination single thread per slot). The results indicate that there is no clear winner in this case, which can be explained by the fact that the server has to loop through all the slots to service requests. Each request might have to wait a linear time in the number of slots to be found by the server. If the server finds too many of the slots empty, then much of the work performed by the server is wasted. However, if the server finds requests in most of the slots, then the algorithm can benefit from more slots because of the lack of synchronization. Our results seem to support this claim: the single thread (ST) per slot version outperforms the multiple threads per slot version (MT) for very unbalanced workloads as in <ref type="figure">Figure 6</ref>, while MT outperforms ST for more balanced workloads, as in Figures 4 and 5. This is due to the elimination algorithm significantly reducing the number of requests sent to the server for balanced workloads, while for unbalanced workloads there is less elimination and more requests sent to the server.</p><p>In our experiments, we assumed that we know the maximum number of client threads in the system and always check all the slots, even when running with fewer threads. This could be improved using an adaptive way of determining the number of slots, but we leave that as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>Hardware's shift towards NUMA systems urges a compatible software redesign. Basic data structures are not optimized for these architectures. We propose the first NUMAfriendly design of a stack, using local elimination and delegation. Combining these two methods is favorable across a number of scenarios: elimination works best when the number of pushes and pops is roughly the same, while delegation significantly reduces contention in the cases in which there is not enough potential for elimination because the workload is not very balanced. Our NUMA-friendly stack outperforms prior stack implementations across different scenarios from completely balanced workloads to the more unbalanced ones.</p><p>However, this is just the first step in transitioning to NUMA systems. There are vast and exciting opportunities for exploring the design of other NUMA-friendly data structures. We presented one technique and showed that it works well for a stack. The same technique could be applied to other data structures, such as queues and lists, which also admit inverse operations. In contrast, other data structures might not be suitable for elimination or might suffer from the serialized access of the server thread. For these data structures, we need to find new tools that allow us to redesign them for the NUMA space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig- ure 3a provides a high-level overview of this communication protocol.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Clients posts their requests in shared local slots and wait for the server to process them. The server loops through all the slots, processes requests as it finds them and immediately posts back the response in the same slot. The sequential stack is only accessed by the server thread; therefore, the top part of the stack remains in the server's L1 cache or LLC all throughout execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Communication protocol between a client thread and the server thread using slots (a) (Black) Single thread per slot: each thread posts requests in its private slot, without any synchronization. (b) (Blue) Multiple threads per slot: threads share slots, so they need to acquire the slot's spinlock before writing the request.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results for 50% pushes and 50% pops</figDesc></figure>

			<note place="foot" n="1"> It is important to note that all threads using the same slot need to be on the same NUMA node in order to maintain the slot&apos;s locality.</note>

			<note place="foot" n="2"> We also experimented with unbounded and variable role threads, but the results were too similar to warrant inclusion in this paper.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yehuda Afek and Tel-Aviv University for access to the NUMA machine we used for our experiments, Dave Dice for useful discussions and our anonymous reviewers for valuable feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A case for numa-aware contention management on multicore systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zhuravlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 USENIX conference on USENIX annual technical conference, USENIXATC&apos;11</title>
		<meeting>the 2011 USENIX conference on USENIX annual technical conference, USENIXATC&apos;11<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lock cohorting: a general technique for designing numa locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Virendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, PPoPP &apos;12</title>
		<meeting>the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, PPoPP &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Numa-aware reader-writer locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Luchangco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Virendra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming, PPoPP &apos;13</title>
		<meeting>the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming, PPoPP &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cphash: a cache-partitioned hash table</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zviad</forename><surname>Metreveli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nickolai</forename><surname>Zeldovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frans Kaashoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="319" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable synchronous queues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">N</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="100" to="111" />
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A scalable lock-free stack algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Yerushalmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel Distrib. Comput</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and scalable rendezvousing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Afek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hakimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Morrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Distributed computing, DISC&apos;11</title>
		<meeting>the 25th international conference on Distributed computing, DISC&apos;11<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using elimination to implement scalable and lock-free fifo queues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Moir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPAA</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flat combining and the synchronization-parallelism tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itai</forename><surname>Incze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nir</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moran</forename><surname>Tzafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM symposium on Parallelism in algorithms and architectures, SPAA &apos;10</title>
		<meeting>the 22nd ACM symposium on Parallelism in algorithms and architectures, SPAA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Stm2: A parallel stm for high performance simultaneous multithreading systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokcen</forename><surname>Kestor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrin</forename><surname>Unsal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Cristal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lawrence Rauchwerger and Vivek Sarkar, editors, PACT</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Accelerating critical section execution with asymmetric multi-core architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aater Suleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on Architectural support for programming languages and operating systems</title>
		<meeting>the 14th international conference on Architectural support for programming languages and operating systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Corey: an operating system for many cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silas</forename><surname>Boyd-Wickizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksey</forename><surname>Pesterev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lex</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehua</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX conference on Operating systems design and implementation, OSDI&apos;08</title>
		<meeting>the 8th USENIX conference on Operating systems design and implementation, OSDI&apos;08<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="43" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting the combining synchronization technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiota</forename><surname>Fatourou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><forename type="middle">D</forename><surname>Kallimanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, PPoPP &apos;12</title>
		<meeting>the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, PPoPP &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Executing parallel programs with synchronization bottlenecks efficiently</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Taura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yonezawa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Systems programming: Coping with parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kent</forename><surname>Treiber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Almaden Research Center</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
