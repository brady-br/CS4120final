<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Efficient Container Startup at the Edge via Dependency Scheduling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvery</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">UIUC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhika</forename><surname>Mittal</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">UIUC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">UIUC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">UIUC</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Efficient Container Startup at the Edge via Dependency Scheduling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Containers are becoming the canonical way of deploying compute tasks at the edge. Unfortunately, container startup latency and overhead remain high, limiting responsiveness and resource efficiency of edge deployments. This latency comes mostly from fetching container dependencies including system libraries, tools, configuration files, and data files. To address this, we propose that schedulers in container or-chestrators take into account a task&apos;s dependencies. Hence, in dependency scheduling, the scheduler tries to place a task at a node that has the maximum number of the task&apos;s dependencies stored locally. We implement dependency scheduling within Kubernetes and evaluate it through extensive experiments and measurement-driven simulations. We show that, for typical scenarios, dependency scheduling improves task startup latency by 1.4-2.3x relative to current dependency-agnostic schedulers. Our implementation of dependency scheduling has been adopted into the mainline Kubernetes codebase.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Application containers and Kubernetes are emerging as the canonical way of deploying services at the edge <ref type="bibr">[8,</ref><ref type="bibr">10,</ref><ref type="bibr" target="#b6">12,</ref><ref type="bibr" target="#b9">16,</ref><ref type="bibr">17]</ref>. Unfortunately, container startup latency can significantly limit the efficiency of short tasks. At the same time, a growing number of edge computing workloads are characterized by short task times (e.g., processing periodic updates from IoT sensors) and/or the need for rapid response times (e.g., robot motion, self-driving cars, factory automation <ref type="bibr">[3,</ref><ref type="bibr" target="#b4">7,</ref><ref type="bibr" target="#b5">11]</ref>). For these and other applications, reducing container startup latency is increasingly important to ensure low end-to-end latency.</p><p>In current deployments, high startup latency comes primarily from the time it takes to fetch and install container dependencies on the host machine at which the task will run. These dependencies include system libraries, tools, configuration files, and data files that must be present on the host machine before the container is launched.</p><p>In this paper, we ask whether scheduling can be leveraged to reduce this startup latency. We have an extensive literature on (and practice of) schedulers that are designed to improve task performance but these have typically focused on improving the task's processing time -e.g., scheduling to avoid contention over shared resources <ref type="bibr" target="#b18">[27]</ref>, to improve data locality <ref type="bibr" target="#b28">[37]</ref>, and so forth. Given the above trends, we propose extending the traditional view of scheduling to also improve task launch time. To achieve this, we propose that task dependencies be treated as another dimension to resource consumption and that schedulers take into account a task's dependencies when placing tasks. Specifically, we propose that a scheduler should aim to place a task T at the node that maximizes the amount of T 's dependencies that are already present at the node, thereby reducing the task startup time. We refer to this approach as dependency scheduling.</p><p>We propose two designs for dependency scheduling that explore different trade-offs. Our first design treats the container image in its entirety as a dependency and hence the scheduler attempts to place a task T at the node that has the maximum overlap (in bytes) between the images it has cached locally and those requested in T . We call this the image-match policy. Image-match is very simple to implement and dramatically reduces startup time when a match is found. However, because it does not consider the internal composition of an image, it cannot improve startup latency in situations when two images only partially overlap in their internal dependencies.</p><p>Our second design avoids these missed opportunities by tracking and matching dependencies at the finer granularity of the layers that constitute the image. Hence, our layer-match policy places a task T at the node that has the maximum overlap with T 's layers (in bytes). Layer match is driven by the intuition that container technology, as it simplifies package reuse, has encouraged non-trivial overlap in the dependencies of different tasks; layer-match exploits this overlap.</p><p>We implement dependency scheduling with both policies in Kubernetes, modifying the Kubernetes scheduler, internal APIs, and node agent to support image and layer awareness. We evaluate our scheduling schemes using extensive measurement-driven simulation and experiments. We show that dependency scheduling substantially improves task startup latency: e.g., dependency awareness leads to a 1.4-2.3x reduction in startup latency relative to dependency-agnostic schedulers while introducing as little as 0.3ms in scheduling overhead. Further, we show that the benefits of dependency scheduling arise not only because it reduces the latency and overhead associated with pulling images but also because of its ability to pack more images into its local image storage.</p><p>We have shared our scheduler implementations with the Kubernetes developer community: our image-match scheduler has been incorporated into the mainline Kubernetes codebase as a default scheduling policy and has been used in production <ref type="bibr" target="#b0">[1]</ref>, while layer-match is currently under review <ref type="bibr">[15]</ref>, corroborating the relevance and practical nature of dependency scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>We give an overview of containers, container dependencies ( ยง2.1), and Kubernetes ( ยง2.2); we then motivate dependency scheduling for container orchestration at the edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Containers, Images, and Layers</head><p>Containers are based on lightweight OS-level virtualization technology that isolates and manages an application's resource usage, and optionally provide tools for managing the application's dependencies. Containers offer two major benefits: lightweight resource isolation and container images.</p><p>The latter allows developers to package and distribute applications using a standard format. An application's image includes all its dependencies, including the code, binaries, system tools, and configurations files. An image is read-only, copy-on-write, and can be shared by multiple containers: when a container wants to apply changes to the image, the target files or directories will be copied to the container's own independent layer, as described later in this section.</p><p>To use an image, users specify the image name in the container request. The container runtime normalizes the specified image name -e.g., replacing a default generic image name with the specific name of the latest version. It then resolves the image name to get its constituent layers (described below) and pulls them from the image repository if they are not already cached. Once the entire image has been retrieved, the container is installed and booted.</p><p>Containers are backed by a layered file-system. Each layer encapsulates a set of files and directories that are put together when the image is built and is associated with a collisionresistant hash digest taken over its content. Every layer can be uniquely identified using its digest. Tracking layer digests decouples the image contents from the image name. This allows users to rename images without invalidating the entire image cache and is a common practice today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Container Orchestration using Kubernetes</head><p>While the concept of dependency scheduling is a general one, we focus on its application to Kubernetes <ref type="bibr" target="#b8">[14]</ref> (henceforth k8s) as the latter is widely used and open-source. <ref type="figure" target="#fig_0">Figure 1</ref> Master  shows a high-level view of the relevant components in k8s. The k8s master node has two main components: (1) an API server, which interfaces with the user and is backed by a distributed key-value store that maintains cluster state, and (2) the scheduler. Incoming requests for executing a jobcalled pod requests -are submitted to the API server. A pod request consists of one or more tasks, each running in a separate container. The API server communicates the request parameters, including required image names, to the scheduler.</p><p>Each worker node runs (1) the "kubelet" agent that interacts with the master node, and (2) a container runtime engine (such as Docker) that manages the lifecycle of a container: creation, removal, pausing, and monitoring. The container runtime interacts with the kubelet via the container runtime interface. Each worker node has its own local image store where it can cache images. The required layers that are not already cached in the local image store are fetched from an external image store (such as Amazon's Elastic Container Registry) by the container runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The case for dependency scheduling at the edge</head><p>We now examine the case for dependency scheduling and argue that it is well suited to emerging edge workloads/trends such as connected vehicles <ref type="bibr">[3]</ref>, smart buildings <ref type="bibr" target="#b3">[6]</ref>, robotics <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b17">26]</ref>, and IoT <ref type="bibr" target="#b5">[11]</ref>. Our discussion in this section provides a highly simplified model that ignores many details of how dependency scheduling works; the remainder of the paper considers the relevant implementation aspects in detail.</p><p>We start by addressing when dependency scheduling is needed or useful. We can view a task's completion time, T , as T = S + R where S is the time required to start/launch the task and R is the application running time required once the task is launched. We further assume that S is dominated by the time it takes to download and install the task's container image (we validate this latter assumption empirically in ยง4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our observation is:</head><p>Dependency scheduling is useful when S is a non-trivial portion of T (i.e., S โ R). As prior works have noted <ref type="bibr" target="#b19">[28,</ref><ref type="bibr" target="#b23">32,</ref><ref type="bibr" target="#b24">33]</ref>, this regime is becoming relevant given there has been a trend towards shorter tasks. More generally, we expect that the trend towards short tasks will only increase with the deployment of IoT applications in which a large number of sensors periodically report to a backend edge-based service. Further, we conjecture the reason why dependency locality did not attract much attention in practice and in literature is because R was typically large (e.g., long-running services and batch jobs) and thereby startup time was negligible.</p><p>Assuming the above condition holds, we next examine when dependency scheduling is effective. As mentioned earlier, dependency scheduling aims to reduce S by caching previously used images at the nodes and scheduling tasks at nodes that cache either the task's entire image (our image-match policy) or a subset of the layers in the image (our layer-match policy). The main parameters that impact the effectiveness of this policy are: N, the number of nodes in the cluster; C, the size of the cache at each node; L, the total size of popular layers involved for the workload in question. Then, if: (a) the total size of the popular layers L is less than the layer cache size of a single node C. Then, at the steadystate, every node is able to store all popular layers in its cache. Hence the dependency scheduling policies (image-match and layer-match alike) should perform similarly as the agnostic policy, because even if the policy randomly picks a node, the node would have the layers. (b) the total size of the popular layers L exceeds the layer cache size of a single node C; but is less than or close to the total layer cache size of all nodes combined (N ยท C). Intuitively, dependency scheduling wins in this case because, although popular layers cannot be cached on every node, they can be cached on some nodes in the cluster. Dependency scheduling identifies these nodes to improve startup time S.</p><p>For example, consider the Connected Vehicle Platform (CVP) <ref type="bibr">[3]</ref> in which each user/vehicle has a different image with some unique layers, e.g., a unique machine learning model customized to each vehicle's travel and/or application stacks developed for each car model and make. To get a sense of N, C, and L: we consider reports published by Ericsson that cite 4 million connected cars as using their platform <ref type="bibr" target="#b2">[5]</ref>.</p><p>Assuming just 0.1% of these cars are active at any time and issuing one update every 1 seconds, we'd see a total load of 4,000 requests per second on a CVP cluster (L = 4000). Handling this load would require a cluster of N = 40 nodes if we assume each node can run 100 containers (the latter from a target provided by the Kubernetes community <ref type="bibr" target="#b7">[13]</ref>). Finally, if we assume each node has 32G of storage (a typical disk space reserved for rootfs) and each image in the CVP contains a customized ML model sized 250MB (YOLO v3 <ref type="bibr">[9,</ref><ref type="bibr" target="#b25">34]</ref>); assuming the image size equals to this size). This gives us C = 128 images and hence N รC = 5120. In this scenario, no n* = arg max n score[n]; bind(n*, j) 9: end for single node can store all 4,000+ layers, but the CVP cluster in its entirety could do so comfortably, and hence dependency scheduling can greatly reduce the startup time S associated with user requests. (c) the total size of the popular layers L is much larger than the total layer cache size of all nodes combined (N ยท C). In this scenario, dependency scheduling performs the same as agnostic policy due to low layer cache-hit ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design and Implementation</head><p>We discuss our overall approach ( ยง3.1) and then describe the detailed design of our image-match ( ยง3.2) and layer-match ( ยง3.3) scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Rationale</head><p>Our approach aims to avoid pulling dependencies altogether by modifying the scheduler to place tasks at nodes where some or all of a task's dependencies are already present. The benefit of this scheduler-based approach is that it requires no change to the infrastructure hardware (servers or networks), nor to containerized applications, restricting all changes to the container orchestrator (e.g., k8s).</p><p>This dependency scheduling policy is presented in Algorithm 1, where dep() extracts the dependency information (i.e., as image or layer) from a node or a job. We rank nodes based on how much their locally-stored dependencies overlap with those of the request. The degree of overlap depends on the granularity of the dependencies we consider.</p><p>We propose images and layers as two candidate definitions of dependencies since these are common concepts already present in applications and container frameworks (though not used by the framework's schedulers) and hence they simultaneously capture the trade-off between coarse vs. fine-grained dependencies and are practical for implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Image-match</head><p>Container image is the package that includes all application dependencies (see ยง2.1). The changes for supporting imagematch are as follows. Kubelets. The container runtime in the worker nodes already tracks the image state (names and sizes of the cached images). We simply extend the kubelet to retrieve this state from the container runtime and peoriodically communicate it to the API server as described next. API-server. Currently, the interface between the API-server and the kubelet lacks image-awareness. We extend the RPCs between the kubelet and the API-server to communicate the image state. Likewise, we also extend the global cluster state that backs the API-server to store this per-node image state.</p><p>Scheduler. We extend the scheduler to implement imagematch using the per-node image information stored in the global cluster state. This involves an additional key change: Image Name Normalization. To implement the image match policy, we must compare the name of each image in a new pod request with the names of the images cached at each node. Recall that the latter is obtained from the container runtime at each node, and can be different from the name specified by the user in a pod request even for the same image. We therefore normalize the image names following canonical image naming rules before image matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Layer-match</head><p>Layer is a (sub-)group of dependencies insides a container image (see ยง2.1). We now describe the additional changes required for supporting layer-match. Kubelets. Since the container-runtime only tracks cached images, we extend the kubelet to track cached layers as well. We add a layer tracking subroutine to the kubelet to collect and parse layer metadata from the node-local layer filesystem. The collected layer state (its digests and sizes) are periodically communicated to the API-server. API-server. Similar to adding image-awareness, we extend the RPCs between the kubelet and API-server to communicate per-node layer state, along with adding this information in the global cluster state.</p><p>Scheduler. Extending the scheduler to support layer-match involves the following key change. Image Resolution. We use this term to describe the process of mapping an image name to its corresponding layer digests. Dependency scheduling needs a container's layer information in order to assign it to a node and hence resolution must happen earlier so that the scheduler knows the mapping between the image of an incoming pod request and its layer digests.</p><p>We thereby implement the image resolver in the scheduler. Whenever a pod request with a new image comes in, the resolver obtains the image to layers mapping by querying the external image repository and caches them. This takes about 200ms. However, this is just a one-time penalty paid for new image requests, with the local image resolution from the cached mapping being the common-case occurrence. We implement this external image resolution outside of the scheduler's critical path to avoid any head-of-the-line blocking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate dependency scheduling using a combination of measurement-driven simulations and experiments on a k8s  <ref type="table">Table 1</ref>: Summary of the trace collected for simulations cluster. The reason for simulation is two-fold: first, we found the real k8s cluster has overhead due to head-of-line blocking issues at the container runtime when pulling images and this overhead can mask the benefits of dependency scheduling. Second, using simulation allows us to do a comprehensive sweep of the parameter space. We compare the following scheduling policies: (i) imagematch, (ii) layer-match, and (iii) an agnostic scheduling policy that places a task on one of the available nodes at random. We first describe our simulation methodology and experimental setup ( ยง4.1) and then present the results ( ยง4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Trace-driven Simulation</head><p>Our simulation process comprises of four steps: (i) Image Mirroring: We select the latest versions of the 5K most frequently used images from DockerHub <ref type="bibr">[4]</ref>, forking them to the Amazon Elastic Container Registry (ECR). This saturates our repository limit on Amazon ECR (which was increased from the default of 1K images on request).</p><p>(ii) Latency Profiling: We deploy a Docker engine on an m4.xlarge dedicated Amazon EC2 instance, and pull the images from ECR. We instrument the Docker engine to log the size of each layer in the image, and the time taken to pull the layer (its pull latency) 1 . (iii) Extrapolating Latency Profile: We extrapolate the results from the latency profile of the above 5K images to create a latency profile for approximately 56K of the most frequently used DockerHub images. We use K-nearest neighbours for this extrapolation. <ref type="table">Table 1</ref> gives a high-level summary of this trace and the profiling results. (iv) Cluster-level simulation: We wrote a simulator 2 that models a k8s cluster and implements the image-and layer-match policies, as well as the dependency agnostic policy. Experiment Setup: We use the following parameters throughout the simulations: 200 nodes in the cluster, with at most 16 running containers and 32GB image cache size per node. Pod requests arrive with Poisson inter-arrival times; load is selected such that the cluster utilization is โผ 80% for the agnostic policy. This setup falls into the regime (b) in ยง2.</p><p>The execution time for each task is uniformly sampled from 1-10 seconds. Our workload uses a realistic Zipf distribution (with an exponent value of 0.75) when picking the container image for each pod request, since it is the common access pattern observed in a wide range of scenarios <ref type="bibr" target="#b10">[19,</ref><ref type="bibr" target="#b11">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Key Results</head><p>We first present the results from our default setup. Startup Latency: <ref type="figure" target="#fig_2">Figure 2</ref> shows the CDF of the startup latency for our default simulation setting. These results confirm the design rationale discussed in ยง3: dependency scheduling (both image-match and layer-match) result in smaller startup latencies when compared to the agnostic policy, with layer-match generally performing better than image-match. On average, the image-match and layer-match policies result in 1.44x and 2.33x smaller startup latency than the agnostic policy respectively, with layer-match performing 1.6x better than image-match.</p><p>Resource Usage: In addition to reducing startup latency, dependency scheduling makes more efficient use of cluster resources, both compute and storage. Compute: The second column of <ref type="table" target="#tab_3">Table 2</ref> describes compute usage with the three policies for the same input load, measured as the sum of the total time each core is occupied divided by the product of the total number of cores in the cluster and the simulation duration. As shown, reduced provisioning time directly translates to smaller usage of compute resources in the cluster. Layermatch is most efficient, followed by image-match, with the agnostic policy being the least efficient. Storage: In <ref type="table" target="#tab_3">Table 2</ref>, we report the number of cached images per node and the amount of unused space in the per-node image store, computed as an average across all nodes and all scheduling rounds from the second half of the simulation (the latter to avoid startup effects). Dependency scheduling allows better packing of images in each node by co-locating images with larger numbers of shared dependencies. This results in more images stored per node as well as slightly higher unused (or free) space in the local image store. We also used the simulator to conduct sensitivity analysis over key setup parameters including the number of nodes in the cluster, the per-node image cache size, and the image popularity distributions. We validated the trends of improvements under change of setups. Due to space limit, we include the results in the technical report <ref type="bibr">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">System Evaluation</head><p>We evaluated dependency scheduling on a 60-node k8s cluster with the same setup as the simulation (except we are run-  ning containers on real nodes). We measured the container startup latency (1.83x and 2.34x speedup from image-and layer-match respectively at mean latency), the scheduling latency (8% and 13% higher than the agnostic policy, which has โ0.6ms mean latency), and the container boot latency (โ1s mean latency). Note the scheduling latency is an order of magnitude lower than the boot and the overall startup latency; thereby it is not a major latency contributor. Further, the difference in startup latency between the agnostic-and dependency-aware policies are much higher at higher-load (i.e., having larger number of pod submitted to the k8s). This is caused by the queuing and consequential head-of-line blocking at the container runtime: there weren't enough resources (e.g., network, CPU) to pull the image. We plan on investigating this fully in future works. More details on system evaluation can be found in our technical report <ref type="bibr">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Related Work</head><p>Storage Optimization: Slacker <ref type="bibr" target="#b15">[24]</ref> uses a proprietary NFS implementation to lazily pull the contents of the container image, and thus improve startup latency. Such strategies increase the complexity of the storage backend (e.g., to maintain many active client connections) and non-trivial infrastructure changes (e.g. modifications to the linux kernel and the use of a proprietary NFS server). Dependency scheduling is an orthogonal technique that is simpler to implement, and that can complement such storage optimization techniques to get even smaller startup latencies. Dependency Trimming: Trimming dependencies is an orthogonal approach to reduce startup time that has been studied in the context of unikernels <ref type="bibr" target="#b21">[30,</ref><ref type="bibr" target="#b22">31]</ref>. These use an offline process for trimming dependencies and report lower startup latency than untrimmed unikernels and containers. Dependency scheduling is, again, orthogonal and complementary to dependency trimming, and offers benefits without requiring that users change their submitted container images. Container Reuse: An edge provider can cache a popular pool of running containers such that they can immediately accommodate new function requests without incurring the time to provision and boot containers, similar to the way FaaS is implemented in the cloud <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">11,</ref><ref type="bibr" target="#b19">28,</ref><ref type="bibr" target="#b27">36]</ref>. In edge environment, however, such hot-caching can be prohibitive due to the excessive use of limited resources such as memory.</p><p>Cluster Scheduling: There is a large body of work on cluster scheduling that focuses on reducing contention over shared resources, achieving better data locality and so on <ref type="bibr">[21-23, 25, 29, 32, 33, 35]</ref>; these schemes do not optimize for startup times which is our focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion Topics</head><p>Feedback and discussion points: Data locality has been a fundamental problem that repeatedly manifests itself in new contexts. A goal of this paper is to bring the discussion about data locality, especially dependency locality, to the fore in the context of edge computing. To this end, we are looking for feedback on (i) whether and how the real-world production systems and workloads on edge would fit into the application regimes we outline in ยง2.3; (ii) besides dependency locality, what are the other forms of data locality related issues that are pronounced for edge workloads; (iii) in the edge context, how valuable are the improvements that scheduling based optimization enables in terms of both performance and resource efficiency?</p><p>We are exploring other scheduling and system techniques to further improve the container startup latency and overhead. During the workshop, we would like feedback on these ongoing efforts and/or would welcome a discussion of future extensions and alternate approaches. Open issues: There are open issues the paper does not address: (i) how dependency scheduling interacts with other scheduling policies such as load balancing and bin-packing. For example, dependency scheduling may lead to overutilization of nodes having large set of popular dependencies. (ii) how much overhead does dependency scheduling introduce to k8s such as the API Server and KV store? (iii) container runtime bottleneck (mentioned in the end of ยง4). (iv) How much smarter caching would help improve the startup latency, e.g., caching top-k popular layers per node ( ยง2 includes some initial discussions)? We hope to learn additional issues from the workshop. Controversial points and when does our work fail: We assumed short tasks are common for edge workloads (i.e., much like serverless/Function-as-a-Service types of workloads). In such contexts, startup latency emerges as a potential bottleneck to low-latency processing. This characterization of edge workloads is up for debate! In fact, it would be great to discuss whether/how one might corroborate this assumption.</p><p>More generally, we anticipate debate surrounding our assumption that containers and Kubernetes are emerging as the de-facto technologies at the edge, and look forward to discussion on what compute abstractions and frameworks are best suited to edge workloads.</p><p>Our work will fail (or rather, will be less relevant to this venue) if the trend in edge workloads does not match our workload assumptions, i.e., for long-running workloads, the overhead of pulling dependencies are substantially amortized by the task compute time and hence the benefits that dependency scheduling offers in terms of lower startup latency and improved resource efficiency are less valuable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of Kubernetes system architecture with the dependency scheduling extension (in dotted boxes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Dependency Scheduling 1: at the cluster scheduler: 2: for each job j queued do 3: /*on nodes meeting resource constraints*/ 4: for n in nodes do 5: score[n] = size(|dep(n) โฉ dep( j)|) 6: end for 7: /*tie-break with other scheduling criteria*/ 8:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Latency cumulative distribution function (CDF) under different policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Node</head><label></label><figDesc></figDesc><table>Scheduler 

API Server 
CLI 

Kubelet 

Worker Node 

Container 
Runtime 

Local 
Image 
Store 

External Image Store 

etcd 

Dependency Scheduling 

Image/Layer Info 

Image/Layer Tracking 

Kubelet 

Worker Node 

Container 
Runtime 

Local 
Image 
Store 

Image/Layer Tracking 

Image resolution/normalization 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Cluster compute usage and the per-node image cache utilization for the three policies.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> https://depsched.s3.amazonaws.com/layer_stats.csv 2 https://github.com/depsched/sim</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank our shepherd, Istemi Ekin Akkus, and the anonymous reviewers for their helpful suggestions and insights.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Enable ImageLocalityPriority as a default scheduling policy</title>
		<ptr target="https://github.com/kubernetes/kubernetes/pull/68081" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Understanding container reuse in aws lambda</title>
		<ptr target="https://aws.amazon.com/blogs/compute/container-reuse-in-lambda/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ericsson</surname></persName>
		</author>
		<ptr target="https://www.ericsson.com/en/internet-of-things/automotive/connected-vehicle-cloud" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The extensible building operating system</title>
		<ptr target="https://github.com/SoftwareDefinedBuildings/XBOS" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">irobot ready to unlock the next generation of smart homes using the aws cloud</title>
		<ptr target="https://aws.amazon.com/solutions/case-studies/irobot/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greengrass</forename><surname>Aws Iot</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/greengrass/,2020" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Kubeedge: A kubernetes native edge computing framework</title>
		<ptr target="https://kubeedge.io/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Building large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kubernetes</surname></persName>
		</author>
		<ptr target="https://kubernetes.io/docs/setup/best-practices/cluster-large/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Kubernetes: Production-grade container orchestration</title>
		<ptr target="https://kubernetes.io/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lightweight kubernetes: The certified kubernetes distribution built for iot &amp; edge computing</title>
		<ptr target="https://k3s.io/" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zipf&apos;s law and the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernardo A Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glottometrics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Web caching and zipf-like distributions: Evidence and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Breslau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brewer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omega</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubernetes</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Firmament: Fast, centralized cluster scheduling at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionel</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">Nicholas</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX OSDI</title>
		<meeting>USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiresource packing for cluster schedulers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Grandl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Slacker: Fast distribution with lazy docker containers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi H Arpaci-Dusseau</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX FAST</title>
		<meeting>USENIX FAST</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mesos: A platform for finegrained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hindman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cloud robotics: architecture, challenges and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee</forename><surname>Peng Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE network</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quincy: fair scheduling for distributed computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SOSP</title>
		<meeting>ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Occupy the cloud: Distributed computing for the 99</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SoCC</title>
		<meeting>ACM SoCC</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Ion Stoica, and Benjamin Recht</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Titus: introducing containers to the netflix cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Spyker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Bozarth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Just-in-time summoning of unikernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Madhavapeddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX NSDI</title>
		<meeting>USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">My vm is lighter (and safer) than your container</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><surname>Manco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SOSP</title>
		<meeting>ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The case for tiny tasks in compute clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX HotOS</title>
		<meeting>USENIX HotOS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sparrow: distributed, low latency scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SOSP</title>
		<meeting>ACM SOSP</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Largescale cluster management at google with borg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Pedrosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhukar</forename><surname>Korupolu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Oppenheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM EuroSys</title>
		<meeting>ACM EuroSys</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Peeking behind the curtains of serverless platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinqian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATC</title>
		<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruba</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><forename type="middle">Sen</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM EuroSys</title>
		<meeting>ACM EuroSys</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
