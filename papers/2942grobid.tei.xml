<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feeding the Pelican: using archival hard drives for cold storage racks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Donnelly</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Harper</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Ogus</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Rowstron</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<orgName type="institution" key="instit3">Microsoft Research</orgName>
								<address>
									<settlement>Microsoft</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feeding the Pelican: using archival hard drives for cold storage racks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Microsoft&apos;s Pelican storage rack uses a new class of hard disk drive (HDD), known by vendors as archival class HDD. These HDDs are explicitly designed to store cooler and archival data, differing from existing HDDs by trading performance for cost. Our early Pelican experiences have helped some vendors define the particular characteristics of this class of drive. During the last twelve or so months we have gained considerable data on how these drives perform in Pelicans and in this paper we present data gathered from a test and a production environment. A key design choice for Pelican was to have only a small fraction of the HDDs concurrently spun up making Pelican a harsh environment to operate a HDD. We present data showing how the drives have been used, their power profile, their AFR, and conclude by discussing some issues for the future of these archive HDDs. As flash capacities increase eventually all HDDs will be archive class, so understanding their characteristics is of wide interest.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many cloud and large-scale data center operators have been exploring cloud storage optimized for colder and archival data, for example Amazon's Glacier Service <ref type="bibr">[1]</ref>, Facebook's Cold Data Storage <ref type="bibr" target="#b9">[10]</ref>, Google near-line storage <ref type="bibr" target="#b10">[11]</ref> and Microsoft's Pelican <ref type="bibr" target="#b2">[3]</ref>. In contrast to online storage <ref type="bibr" target="#b8">[9]</ref>, this storage trades data access latency and throughput for lower cost; access latencies of multiple seconds to minutes (to even hours) are normal with a per-file throughput that is often restricted.</p><p>One way to make these services cost-effective is by using custom designed racks. These designs achieve cost savings by right-provisioning resources at the rackscale, such as power, cooling, network bandwidth, CPU, memory and disks. Sufficient resources are provided to only support these colder and archival workloads. Examples include Microsoft's Pelican <ref type="bibr" target="#b2">[3]</ref> and Facebook's Open Compute Storage <ref type="bibr" target="#b11">[12]</ref>. These systems provision insufficient rack-level power to allow all the HDDs to be concurrently spun up; in Pelican it is 8% of the HDDs, with the rest in standby. By implication, the rack cooling is then only provisioned to handle the heat generated from a subset of the HDDs spinning.</p><p>This creates a harsh environment for HDDs, where they are frequently spun up and down. Pelican was designed together with a new class of HDD: archival class HDDs. We briefed all the major HDD vendors, and worked closely with one vendor to help specify the performance we wanted from these new HDDs.</p><p>We have been running Pelicans for over a year, and in this paper we report on our experiences with this new archival class HDD. We have experimented in our test lab with drives from multiple vendors, and have also profiled drives in a production environment. Anecdotally, many storage experts are skeptical that an HDD can be spun up and down tens of thousands of times per year without significantly increasing failure rates. Has it impacted our disks so far?</p><p>We explore multiple metrics of interest. We start with the power profile of eight archive HDDs including both SMR and PMR variants, and note that drives which differ only in their firmware can behave very differently in their power draws. We then look at HDD workload characteristics that could impact failure rates, from the amount of data passing through the head, to the number of poweron-hours, the temperature profile for the drives and the number of spin ups. Finally, we look at the AFR and then discuss interesting options for the future design of these archival class HDDs.</p><p>Before exploring their characteristics, we give a short high-level overview of achival class HDDs and Pelican.</p><p>Archival class HDDs. An example of such a HDD is the Western Digital Ae model WD6001F4PZ 1 which is a 6TB drive operating at 5,400 RPM. The marketing literature states that "The WD Ae hard drive is best suited for cold storage, backup and data archiving where data is stored on disk but rarely if almost never read again." All the main vendors have equivalent HDDs, for example Seagate's ST8000AS0022 2 .</p><p>Pelican uses these archive drives because both Pelican and the HDD vendors are optimising for lowest cost per GB of capacity. These HDDs have three basic characteristics that impact lifetime, usually specified per year: power-on-hours (POH), head wear in terms of data transferred across the head, and number of controlled spin downs. POH is defined as the number of hours when the HDD is active, i.e. not in idle B (heads unloaded) or deeper power saving modes, including: idle C (low RPM), standby, or powered off. Head wear is important as HDDs use a thermal actuator to move the head to a lower fly height while writing and reading <ref type="bibr">[13]</ref>. At the lower height, the risk of damaging interactions between the head and the media is elevated. "TBs transferred" is used as a proxy for the length of time the heads have been at the lower fly height.</p><p>Pelican. A Pelican storage rack has 1,152 HDDs and two servers. Each HDD is connected to a SATA 4-port multiplier, which is connected to a 4-port SATA HBA. Pelican uses PCIe to connect the 72 HBAs to the servers, such that each HBA can be attached to either server. There is sufficient power and cooling provisioned to allow only a small fraction of the HDDs to be spinning and performing IO (active) while the other HDDs are spun down (standby).</p><p>A Pelican power domain contains 16 HDDs and has sufficient power to support two HDDs transitioning from standby to active, with the 14 other HDDs in standby. A Pelican cooling domain has 12 HDDs, each in a different power domain, and can provide sufficient heat dissipation to support one HDD transitioning from standby to active and 11 in standby. These domains represent constraints on which HDDs can be concurrently spinning, imposed by the physical rack design. The cooling domain means that at most 96 HDDs can be concurrently active in a Pelican, one per cooling domain. The power domain means that two per tray can be spinning, further restricting the choice about which HDDS can be concurrently active.</p><p>To handle this HDDs are placed into 48 sets of 24 HDDs, referred to as groups. Each blob stored in a Pelican is striped across 21 HDDs within a single group, using 18+3 erasure coding. In order to read or write to a group, the group needs to be spun up before the HDD can be accessed. Due to the power and cooling con-straints at most 4 groups can be concurrently spun up. In a Pelican spin up is the new seek latency, and the software stack needs to schedule IOs to maximize throughput while minimizing impact on per IO latency. For full details please refer to Pelican <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Studying Archival HDDs</head><p>The data shown in this section is gathered from two different deployment environments. The first is a Pelican operated in a test lab used to experiment with the hardware and evaluate HDD products. This test rack is populated with HDDs from multiple vendors and, in some cases, with multiple versions of the same HDD. Data presented from this environment will be labelled as test. The second data set is from a deployment in a production environment, which will be labelled as production.</p><p>We start by describing the different HDDs used in the test and production environments. We use eight different kinds of HDD from three different vendors, referred to as vendors A, B and C. We refer to the HDD kinds as A1, A2, B1, B2, B3v1, B3v2, B4 and C1. It should be noted that B3v1 and B3v2 use the same physical drive hardware and only differ in their firmware, B3v2 aims to achieve lower spin up latency and has a modified power draw profile. We worked closely with vendor B to develop their archival class HDD, hence they represent five of the HDDs used. <ref type="table">Table 1</ref> summarizes the key features of each of the HDDs. Five of the HDDs use PMR <ref type="bibr">[14]</ref>, while three use SMR <ref type="bibr" target="#b5">[6]</ref>. The labels HA and Auto denote Host-Aware or Autonomous SMR, respectively <ref type="bibr" target="#b5">[6]</ref>, where HA is an autonomous HDD-managed mode with a Host Aware Zone feature set <ref type="bibr" target="#b0">[2]</ref>. This allows a host OS to determine it uses SMR and optimize the file system to enable better performance. C1 (we believe) is an autonomous drivemanaged SMR HDD. We infer this from the power profile and delay between the platters spinning and being ready, and there is an additional power draw during spin down. However, the vendor has never confirmed it is an SMR HDD.</p><p>All B HDDs are ranked by release date, so B1 was the first and B4 the latest. B3v1 is an updated version of B2, with increased capacity and reduced spin up power draw, but this increases the spin up time. B4 is a larger capacity version of B3, and also has a reduced power draw when idle. B1 through B3 are experimental HDDs which are not generally available. We include them to help understand how these HDDs have evolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Spin up</head><p>In Pelican, in order to read data from a set of HDDs, they will normally need to be spun up.  represents a lower bound on the time to first byte for any read request which accesses data on HDDs that are spun down. <ref type="figure" target="#fig_0">Figure 1</ref> shows a CDF of spin up latency for all eight kinds of HDD. We use data gathered from several samples of each kind of HDD. The spin up times are grouped into 100ms buckets and their CDF plotted. For any HDD kind the minimum number of spin ups is 342, and the maximum is 114,727. There are two key observations: (i) the distribution is tightly centered around the mean, (ii) there is about a factor of 1.8 between the fastest and slowest. To the first approximation the spin up latency is a function of the power draw and the required RPM for the platters.</p><p>We therefore ran an experiment where we measured the power draw for each HDD through a spin up, idle, and then spin down cycle. We measure the power relative to each HDD's standby power. <ref type="figure" target="#fig_1">Figure 2</ref> shows the power draw for each of the HDDs versus time. At time zero, each HDD is sent a spin up command. The colored upwards-pointing arrows show the time when each HDD's spin up completion is received by the OS. Each HDD then remains idle until 18 seconds when the filesystems are unmounted and the HDD is sent a standby immediate command. The colored downwards-pointing arrows show when the standby immediate completes. <ref type="figure" target="#fig_1">Figure 2</ref> shows a number of interesting events. After A1 and A2 have completed their power draw for spin up, they run at active power levels for a further two seconds before completing the spin up to the host OS. Since they use SMR we speculate that they are loading internal state (e.g. remap tables) from the media. They also take longer to spin down, especially A2 which uses host-aware SMR.</p><p>The B results show the progress made from early versions of the HDD to the latest (e.g. B3v2 and B4). From <ref type="figure" target="#fig_0">Figure 1</ref> we see a reduction in spin up time, and from <ref type="figure" target="#fig_1">Figure 2</ref> we see the power draw reduced while in idle. Interestingly, B1 and B2 complete the spin down command but continue to draw elevated levels of power for a little over two seconds before going completely idle. We speculate that this may be due to the HDD electronics remaining in a higher-power mode until some timeout triggers, and this behavior does not occur after B2. It is also interesting that B3v1 and B3v2 only have different firmware revisions, yet they behave very differently in their power draws and thus spin up times. B3v1 limits its peak draw, but this results in a much longer spin up time: 9 seconds, compared to 6 seconds for B3v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Using Archival Class HDDs</head><p>We have B3v2 HDDs currently deployed in a production environment, and now look at the stress Pelican puts on them. We gathered detailed statistics from 7 May 2015 to 21 December 2015 (228 days) for a constant set of HDDs and scale to 365 days to aid comparison with HDD parameters which are quoted per year. There are four basic metrics that impact the failure rate of these HDDs: (i) the number of spin up spin down cycles, (ii) the volume of data transferred across the head, (iii) the number of power-on-hours, and (iv) temperature. We now look at these metrics for a Pelican. <ref type="figure" target="#fig_3">Figure 3</ref>(a) shows a normalized distribution of HDDs versus spin ups per year. The median HDD does 70,800 spin ups and the maximum is 100,000. It should be noted that Pelicans do a "controlled" spin down, i.e. the spindle is stopped before power removal. This is as advised by the vendors and the number of cycles is within what they believe the HDD can handle.</p><p>HDD failure can also be induced by head wear. <ref type="figure" target="#fig_3">Fig- ure 3(b)</ref> shows the distribution of TBs transferred per HDD per year. The strictest limit across all vendors is 60 TB/year for a HDD, and only 0.7% of our population were over. Obviously, this is highly dependent on the workload, and is an important consideration when designing a system. In Pelican we do not do any explicit wear-levelling, although a number of policies are implicitly load aware. We believe that wear-levelling may need to become a first class concern in HDD-based storage.</p><p>The next metric of interest is the power-on-hours. <ref type="figure" target="#fig_3">Fig- ure 3(c)</ref> shows the distribution of power-on-hours for the HDD population. The median is only 165 hours, which is well below the strictest limit from all vendors of 3120    The final metric is temperature. <ref type="figure" target="#fig_4">Figure 4</ref> shows a snapshot of the temperatures in a single rack in the production environment on 15 February 2016. A Pelican rack is vertically cooled; cool air enters at the bottom front of the rack, and is exhausted out at the top rear <ref type="bibr" target="#b2">[3]</ref>. For this single rack, we have shown as black squares all HDDs that have failed in the rack in the last 12 months. <ref type="figure" target="#fig_6">Figure 5</ref> shows mean daily temperature outside the datacenter hosting this rack, as well as the mean hourly inlet   and exhaust temperatures across this rack. This rack is in a direct evaporative cooled data center <ref type="bibr" target="#b6">[7]</ref>, so during the summer months the relative humidity rises, keeping the temperature controlled. <ref type="figure" target="#fig_6">Figure 5</ref> also shows the humidity near the rack, but we only have data from May to November 2015. While the exterior temperature fluctuates due to the passing seasons, the inlet temperature is fairly consistent across the year, but the humidity increases with external temperature. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Disk failures</head><p>Having looked at potential sources of disk failures, we now report on the actual failures observed. Over the whole deployment for 440 days, the average AFR (annualized failure rate) is 3.96%. <ref type="figure" target="#fig_7">Figure 6</ref> shows the normalized cumulative disk failures versus age in days since they were deployed. It also compares our failure rates with those reported by Backblaze <ref type="bibr" target="#b3">[4]</ref>. Unlike them, we do not see higher rates of failure for young HDDs.</p><p>Surprisingly, our failures are inversely correlated with temperature: the cooler disks are more likely to fail than the hotter disks. This is possibly because the cooler disks are closer to the air inlet vents, and thus see a higher relative humidity than the hotter disks. Correlations between humidity and elevated failure rates have recently been shown <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discussion</head><p>We believe that our experiences with Pelican and archival class HDDs highlight a number of interesting challenges. The most obvious one is the issue of head wear, and limiting the volume of data transferred across the head. This highlights the need for a deeper understanding of the trade-offs and benefits of scrubbing and the correct strategy for using these low-cost HDDs. We also believe that to use these archival class HDDs we need to think about treating wear-levelling as a first class citizen, as we do for Flash. If we can address these issues this will increase the temperature of data that can be stored on archival class HDDs.</p><p>Given our experiences with B3v1 and B3v2, we would like to see vendors expose more control over their HDD performance -an aspiration shared by others <ref type="bibr" target="#b4">[5]</ref>. We are increasingly seeing cross-layering being used in the data center, where more control of lower layers increases performance per dollar at a system level. The difference achieved between B3v1 and B3v2 demonstrates how changing the low-level parameters can significantly impact performance. This also extends to the HDDs reporting their current power usage. Pelican uses Power Up In Standby (PUIS) and floats pin 11 of the SATA power connector and yet we cannot always faithfully determine the current state of the drive. Therefore, Pelican also monitors power consumption at a tray-level <ref type="bibr">(16 HDDs)</ref> in order to understand the disk states. It would be easier if we could ask the HDDs, and they faithfully tell us!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Spin up latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Power draw while spinning up, idle, and spinning down.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: CDFs of basic HDD metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Temperature snapshot from a single rack. The squares are HDDs, colored by their temperature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Humidity near rack (%), and temperatures: mean rack exhaust (exh) • C, inlet • C, and datacenter exterior (ext) • C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Age of failed HDDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>The spin up latency</head><label></label><figDesc></figDesc><table>Name 

Technology 
Spin up (s) 
Capacity (TB) 
A1 
Auto SMR 
10.1 
8.0 
A2 
HA SMR 
10.2 
8.0 
B1 
PMR 
7.9 
4.6 
B2 
PMR 
7.8 
4.5 
B3v1 
PMR 
9 
4.9 
B3v2 
PMR 
6 
4.9 
B4 
PMR 
6.4 
6.1 
C1 
Auto SMR (?) 
8.6 
8.0 

Table 1: Summary of HDDs, spin up time in seconds. 

</table></figure>

			<note place="foot" n="1"> http://www.wdc.com/wdproducts/library/SpecSheet/ ENG/2879-800045.pdf accessed 15th February 2016.</note>

			<note place="foot" n="2"> http://www.seagate.com/www-content/ product-content/hdd-fam/seagate-archive-hdd/en-us/ docs/100795782a.pdf accessed 2nd March 2016.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the other original members of the Pelican team, and in particular Shobana Balakrishnan, Adam Glass, Sergey Legtchenko and Eric Petersen.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">American National Standards</forename><surname>Institute</surname></persName>
		</author>
		<imprint>
			<publisher>ANSI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Working Draft Revision 04j</title>
	</analytic>
	<monogr>
		<title level="m">T13/BSR INCITS 537-201x, Zoned Device ATA Command Set (ZAC)</title>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pelican: A Building Block for Exascale Cold Data Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>England</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legtchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowstron</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How long do disk drives last?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beach</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<ptr target="http://www.backblaze.com/blog/how-long-do-disk-drives-last/" />
		<imprint>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Disks for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brewer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cypher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And T&amp;apos;so</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shingled magnetic recording -models, standardization, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dunn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feldman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SNIA Storage Developer Conference Tutorial</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>SNIA</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Best practices for data centers: Lessons learned from benchmarking 22 data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greenberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tschudi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rumsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myatt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In 14th biennial ACEEE conference on Energy Efficiency in Buildings</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Environmental conditions and disk reliability in free-cooled datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manousakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianchini</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;16</title>
		<meeting><address><addrLine>USENIX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Storage pod 4.0: Direct wire drives -faster, simpler, and less expensive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">March</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<ptr target="http://blog.backblaze.com/2014/03/19/backblaze-storage-pod-4/" />
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Facebook loads up innovative cold storage datacenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename></persName>
		</author>
		<ptr target="http://www.enterprisetech.com/2013/10/25/facebook-loads-innovative-cold-storage-datacenter/" />
		<imprint>
			<date type="published" when="2013-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Google cloud storage nearline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Whitepaper</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/files/GoogleCloudStorageNearline.pdf" />
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Open</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storage</surname></persName>
		</author>
		<ptr target="http://www.opencompute.org/projects/storage/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
