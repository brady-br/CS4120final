<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards an Unwritten Contract of Intel Optane SSD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Arpaci-Dusseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><surname>Arpaci-Dusseau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards an Unwritten Contract of Intel Optane SSD</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>New non-volatile memory technologies offer unprecedented performance levels for persistent storage. However, to exploit their full potential, a deeper performance characterization of such devices is required. In this paper, we analyze a NVM-based block device-the Intel Optane SSD-and formalize an &quot;unwritten contract&quot; of the Optane SSD. We show that violating this contract can result in 11x worse read latency and limited throughput (only 20% of peak bandwidth) regardless of parallelism. We present that this contract is relevant to features of 3D XPoint memory and Intel Optane SSD&apos;s controller/interconnect design. Finally, we discuss the implications of the contract.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>New NVM technologies provide unprecedented performance levels for persistent storage. Such devices offer significantly lower latency than Flash-based SSD and can be a costeffective alternative to DRAM. One excellent example is Intel's 3D XPoint memory <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">25]</ref> from Intel and Micron, available on the open market under the brand name Optane <ref type="bibr" target="#b10">[11]</ref>. It is available in various form factors, including Optane memory <ref type="bibr" target="#b8">[9]</ref> (a caching layer between DRAM and block device), Optane SSD <ref type="bibr" target="#b9">[10]</ref> (a block device), and Optane DC Persistent Memory <ref type="bibr" target="#b7">[8]</ref>. Among these devices, the Optane SSD is currently the most cost-effective and widely available option.</p><p>Optane SSD offers numerous opportunities for applications. For example, Intel's Memory Direct Technology (IMDT) <ref type="bibr" target="#b6">[7]</ref> enables the use of Optane SSD as a DRAM alternative. Use cases of IMDT/Optane SSD include Memcached <ref type="bibr" target="#b3">[4]</ref>, Redis <ref type="bibr" target="#b4">[5]</ref>, and Spark <ref type="bibr" target="#b5">[6]</ref>. Evaluations of those scenarios demonstrate the potential role of Optane SSD as a cost-effective alternative of DRAM. Optane SSDs are also deployed to support key workloads in Facebook <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23]</ref>, both as a caching layer between DRAM and Flash SSD for RocksDB and for crucial workloads such as machine learning. According to <ref type="bibr">Eisenman et al. [23]</ref>, Facebook stores embeddings of trained neural networks on Optane SSDs.</p><p>Using new technology effectively requires understanding its performance and reliability characteristics. For traditional devices, such as hard-drives and Flash-based SSDs, these are well known <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b34">33]</ref>. However, for new devices like the Optane SSD, much remains unclear, and is thus the focus of our work.</p><p>In terms of immediate performance, we summarize six  <ref type="table">Accesses rule)</ref>. Fourth, to achieve optimal latency, the user needs to control the overall load of both reads and writes (Control Overall Load rule). Fifth, to exploit the bandwidth of Optane SSD, clients should never issue requests less than 4KB (Avoid Tiny Accesses rule). Sixth, to get the best latency, requests issued to Optane SSD should align to eight sectors (Issue 4KB Aligned Requests rule). Finally, when serving sustained workloads, there is no cost of garbage collection in Optane SSD (Forget Garbage Collection rule). Overall, these rules are relevant to features of 3D XPoint memory and Optane SSD's controller/interconnect design. The unwritten contract provides numerous implications for systems and applications. According to the Access with Low Request Scale rule and the Control Overall Load rule, the design of heterogeneous storage systems including Optane SSD and Flash SSD must be carefully considered. Many rules (e.g., Random Access is OK) present opportunities and new challenges to external data structure design for Optane SSD. Finally, the Random Access is OK rule may enable new applications on Optane SSD that don't work well on existing technologies.</p><p>The rest of this paper is structured as follows. We describe the unwritten contract and explain the insights for each rule in Section 2. We provide the implications of the unwritten contract in Section 3, conclude in Section 4 and point to potential research directions in Section 5.   <ref type="table">Table 1</ref>. We begin with six rules related to immediate performance like latency and throughput. We then present rules for sustainable performance.</p><formula xml:id="formula_0">-0.2 -0.1 -0.2 -0.2 -0.2 -0.2 -0.2 -0.2 -0.1 -0.3 -0.2 -0.2 -0.2 -0.2 0.0 -0.2 -0.3 -0.2 -0.2 -0.2 -0.2 0.1 -0.1 -0.2 -0.2 -0.2 -0.2 -0.2 0.3 0.0 -0.1 -0.1 -0.2 -0.1 -0.2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Access with Low Request Scale</head><p>The Optane SSD is based on 3D XPoint memory which is said to provide up to 1,000 times lower latency than NAND Flash <ref type="bibr" target="#b1">[2]</ref>. Does 3D XPoint memory always lead to better performance for Optane SSD compared to Flash SSD? We answer this question with our first rule: to obtain low latency, Optane SSD users should issue small requests and maintain a small number of outstanding IOs. This rule is needed not only to extract low latency but also enough to exploit the full bandwidth of the Optane SSD. We uncovered this rule when quantitatively comparing Intel Optane SSD 905P (960GB) with a "high-end" Flash SSD: Samsung 970 Pro (1TB) <ref type="bibr" target="#b11">[12]</ref>. From our experiments, we found that Optane SSD does not show improvement for workloads with many outstanding requests.</p><p>We compare Optane and Flash SSDs with random readonly and write-only workloads. Each workload has two variables: request size and queue depth (QD) (or number of inflight I/Os). <ref type="figure" target="#fig_0">Figure 1</ref> compares the two devices in terms of average access latency. The temperature T in each rectangle shows the scaled difference between the latencies of the two systems; T &gt; 0 indicates Optane has smaller latency, while</p><formula xml:id="formula_1">T &lt; 0 indicates Flash SSD has smaller latency. Specifically, T = L higher âˆ’L lower L lower</formula><p>. As shown, Optane SSD and Flash SSD outperform each other in different cases.</p><p>For read-only workloads, Optane SSD has lower latency than Flash SSD when request size and queue depth are small. Specifically, when the request size 16KB, Optane SSD is better than Flash SSD. Thanks to 3D XPoint memory's low access latency, the read latency from Optane SSD can be 8.4x faster than Flash SSD. However, when QD&gt; 8 and request size&gt; 16KB, Flash SSD achieves lower latency, by as much as 40%. This difference occurs because the latency of Optane SSD increases linearly with higher queue depths (e.g., the latency of a 4KB random read with QD= 64 is 8x slower than QD= 8 and is 11x slower than QD= 1).</p><p>Similarly, for write-only workloads, Optane SSD has lower latency for small requests and low QD, while Flash SSD is  again better in the opposite cases; however, the difference for writes is not as high as for reads (Optane is up to 1.7x faster than Flash). This result is due to Flash SSDs log-structured layout and buffering optimizations. Flash SSD outperforms Optane SSD when request size&gt; 4KB and QD&gt; 2, which includes most of our tested workloads. The device's internal parallelism dictates its behavior when serving workloads with high request scale. We are thus motivated to uncover the internals of the Optane SSD. Like Flash SSD, Optane SSD utilizes a RAID-like organization of memory dies <ref type="figure">(Figure 2</ref>). Through a fine-grained experiment <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b19">19]</ref> we examine a critical parameter for internal parallelism: the interleaving degree, or number of channels. We maintain a read stream with stride S from the devices, where S is the distance between two consecutive chunks (a chunk is 4KB or 8 sectors as shown in Section 2.6). <ref type="figure" target="#fig_1">Figure 3</ref> presents the throughput of workloads with various strides. An individual line represents workloads with the same QD.</p><p>Optane SSD has a significantly smaller interleaving degree (7) than Flash SSD (128). In <ref type="figure" target="#fig_1">Figure 3</ref>, the distance between the lowest dips in each line indirectly indicates the interleaving degree of the device. For Optane SSD, we observe the pattern is 7, though the difference between dips is not visible until QD= 8. Our finding agrees with the hardware description of Optane SSD <ref type="bibr" target="#b2">[3]</ref>: it has a controller connected to seven channels, each of which is connected to memory dies.</p><p>The tested Flash SSD reaches its lowest throughput every 128 chunks. For high queue depths (e.g., 16), it presents a richer pattern with dips at different levels, indicating copious levels of parallelism (channel, package and die).</p><p>Overall, Optane SSD has limited internal parallelism, compared to Flash SSD. This characteristic explains the  The limited internal parallelism of the Optane SSD impacts its throughput in two ways. First, the tested Flash SSD achieves larger maximum read (3500MB/s) and write (2700MB/s) bandwidth than the Optane SSD (2500MB/s); it outperforms Optane SSD serving workloads with large request scale. Flash SSD's richer internal parallelism enables it to serve more parallel requests. Second, Optane SSD can achieve peak throughput when serving small requests with low queue depth. This is due to Optane SSD's limited internal parallelism which requires only a small number of requests to utilize all of its resources. Hence, the rule to access the device at a low scale not only guides users to obtain low access latency but also is enough to achieve full bandwidth.</p><p>The influence of contention in Optane SSD is modest compared to that in Flash SSD. The dips in <ref type="figure" target="#fig_1">Figure 3</ref> are due to concurrent requests contending for shared resources (e.g., channels). In Flash SSD, contention significantly restricts overall throughput; for example, with QD = 16, S = 127 chunks, read throughput is 88 MB/s, which is only 6% of the maximum throughput with the same queue depth. Although parallel requests to Optane SSD can also introduce contention (limiting throughput to within 86% of maximum), this influence is much less than that in Flash SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Random Access is OK</head><p>With hard drives or SSDs, clients often expect better performance from sequential than random accesses. With Optane SSD, this is no longer true. Optane SSD is a random access block device, where clients can observe the same performance for random and sequential workloads.</p><p>We study Optane SSD's average latency when serving ran- dom and sequential workloads; throughput and tail latency yield similar results. Each workload maintains four worker threads, while each thread issues IOs randomly or in a sequential stream. As shown in <ref type="figure" target="#fig_2">Figures 4 and 5</ref>, on Optane SSD, for requests &gt; 1KB, random and sequential workloads achieve comparable performance. The difference between sequential and random latency is within 17% for reads and within 5% for writes. In contrast, Flash SSD prefers sequential over random reads, especially at a low request scale; sequential reads can be 7x faster. Flash SSD achieves much better sequential performance due to prefetching and simplified address translation. For writes, Flash SSD presents similar random and sequential latency due to log-structuring. However, as we will show in Section 2.7, log-structuring introduces significant overhead when the device fills; Optane does not have such concerns.</p><p>The Random Accesse is OK rule in Optane SSDs occurs due to the ability to perform in-place updates in 3D XPoint memory. In Optane SSD, there is no difference in address translation costs for random versus sequential workloads; in Section 2.7, we will show that the mapping policy in Optane SSD is based on logical addresses. In addition, as indicated by our read latency study, there is no prefetching for sequential reads within Optane SSD.</p><p>Workloads with 1KB requests are special on Optane SSD compared both to other request sizes and to Flash SSD. We investigate this in the next rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Avoid Crowded Accesses</head><p>The Optane SSD contains shared resources (e.g., channels). To avoid contention, the Avoid Crowded Accesses rule dictates that clients of Optane SSD should never issue parallel accesses to a single chunk (4KB). We uncover this rule by investigating the 1KB workload performance shown in <ref type="figure">Figure 4</ref> and 5. In Optane SSD, sequential 1KB accesses can increase latency by 63% for reads and by 3.6x for writes, compared to random 1KB accesses.</p><p>We study the difference between random and sequential accesses for small requests by performing parallel accesses to a single 4KB chunk. In this experiment, we randomly choose chunks from the device, then issue P parallel reads to different sectors within one chunk. <ref type="figure" target="#fig_3">Figure 6</ref>  Figure 7: Latency of Mixed Reads and Writes of latencies for different values of P. We observe a "stair" pattern for QD&gt; 1. For each line, the number of levels equals the queue depth and the steps occur at evenly-spaced intervals. This pattern indicates queuing and/or contention across the issued parallel requests. Parallel small requests to a single chunk introduce contention. This experiment illustrates why sequential 1KB workloads have worse performance than random 1KB workloads: although random 1KB accesses may introduce contention, sequential 1KB accesses must introduce contention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Control Overall Load</head><p>To achieve optimal latency from Optane SSD, the client must control the overall load of both reads and writes. This rule indicates distinct performance characteristics between Optane SSD and Flash SSD.</p><p>We discover this rule by looking into the performance of Optane SSD serving mixed reads and writes. In the experiment, we issue random 4KB requests, varying the percentage of writes from 0% to 100%, with QD= 64 (large enough to achieve full throughput for both Optane SSD and Flash SSD). <ref type="figure">Figure 7</ref> shows the access latency of Optane SSD and Flash SSD. Within Optane SSD, reads and writes are treated equally. Specifically, on Optane SSD, each type of request is served with the same latency and the latency is related to the overall load, not to the percentage of writes.</p><p>Flash SSD exhibits distinct characteristics for read-versus write-dominated workloads. On the left side of <ref type="figure">Figure 7</ref>, for Flash SSD, the read latency is similar to that in read-only workloads with the same queue depth (38% slower than Optane SSD). However, the write latency is similar to that of a pure-write workload with very low queue depth (and only 19% of that on Optane SSD). With an increasing number of writes, reads to Flash SSD achieve poor latency due to the influence of writes; when the workload is write-dominated, read latency can be as high as 1.1ms (10x Optane access latency).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Avoid Tiny Accesses</head><p>Does the byte-addressabilily of 3D XPoint memory enable efficient tiny accesses to Optane SSD? We answer this question with our rule to Avoid Tiny Accesses: to exploit bandwidth of the SSD, the client must not issue requests less than 4KB.  <ref type="figure">Figure 8</ref> shows the latency and throughput of random reads less than 4KB, with separate lines for two sectors and eight sectors requests. As shown, the latency of two sector requests is the same as eight sector (4KB) requests. However, the throughput of tiny requests is limited by the maximum IOPS supported by Optane SSD (575K); for 1KB requests, throughput is only 20% of the full bandwidth of the device. Given these two results, there is no benefit in issuing requests smaller than 4KB to Optane SSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Issue 4KB Aligned Requests</head><p>To achieve the best latency, requests issued to Optane SSD should always align to eight sectors. We present the difference between aligned and misaligned requests. In the experiment, we measure the latency of individual read requests (QD= 1); each read is issued to a position A+offset, where A is a random position aligned to 32KB and offset is a 512-byte sector within that 32KB. <ref type="figure" target="#fig_4">Figure 9</ref> shows the read latency of requests issued to different offsets, averaged over a half million requests to the same offset. Each line represents a workload with a different request size. In contrast to what one might expect given 3D XPoint's byte-addressability, Optane SSD favors aligned requests. Requests of one sector have the same latency no matter the offset, and larger requests aligned to eight sectors always get the lowest latency. For a request crossing the boundary of 4KB, its latency is linearly correlated to the part it issues to the second chunk after the boundary. The difference between the high and low latencies of 4KB requests is 21%. Note that the eight-sector chunk here is not related to a concept like a page or block in Flash SSD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Forget Garbage Collection</head><p>We now study the long-term performance of Optane SSD. First, we explore its performance when the device gets full. According to our experiments, there is no need to worry about garbage collection in Optane SSD. We examine sustained 4KB random and sequential writes on Optane SSD and Flash SSD over three hours. The device is completely unmapped using the trim command before each experiment; the two devices tested share a similar capacity (960GB vs. 1TB). We maintain QD= 32 for each workload. <ref type="figure" target="#fig_0">Figure 10</ref> presents sustained write performance. For Flash SSD, after the device becomes full, write throughput drops significantly because subsequent writes constantly trigger garbage collection. After about 6000 seconds, write throughput stabilizes: sequential throughput is around 350MB/s and random throughput is 170MB/s (only 7% of the maximum throughput). The throughput of sequential writes is better than random because of lower garbage collection cost. Different from Flash SSD, Optane SSD maintains maximum throughput for sustained writes. The flat throughput for Optane SSD indicates no cost for garbage collection.</p><p>Finally, we study the mapping policy (LBAâ†’PBA) in Optane SSD by comparing three workload variations. The first is the same workload as for the interleaving experiments in <ref type="figure" target="#fig_1">Fig- ure 3</ref>: blocks are first written in logical address order and then read back in that same LBA order (LBA-order write:LBAorder read). The second workload preconditions the working zone with random writes (random write:LBA-order read). The third workload preconditions with random writes, but then reads in the order in which the chunks were written (random write:written-order read). <ref type="figure" target="#fig_0">Figure 11</ref> shows the throughput of the three workloads.</p><p>For Flash SSD, when the read order does not match the write order, the throughput pattern disappears; therefore, its mapping policy is not based on LBA. Flash SSD uses a mapping policy based on written-order (log-structured) and therefore the throughput pattern only occurs when we read according to the written order; this is why Flash SSD requires garbage collection. Optane SSD behaves quite differently; no matter how we precondition the device, the pattern occurs when reading according to LBA. Hence, Optane SSD likely adopts LBA-based mapping. This design is enabled by 3D XPoint memory's capability to perform in-place updates. As a result, Optane SSD doesn't require garbage collection and can deliver sustainable performance over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implications From the Contract</head><p>The following implications can be drawn from our unwritten contract. We have two audiences in mind; first, those who design systems for Optane SSD; second, those who combine Flash and Optane in a hybrid setting.</p><p>The Random Access is OK rule suggests possible restructuring of external data structures on Optane SSD. Previous designs try hard to convert unstructured accesses into sequential ones, which is now less necessary. Applications which behave poorly on Flash thus become potential consumers of Optane. The No Crowded Accesses rule, No Tiny Access rule, and Alignment rule suggest pitfalls that fine-grained data structures must be aware.</p><p>Heterogeneous storage also needs careful design. The motivation for hybrid designs comes directly from the Low Request Scale rule: Flash SSD outperforms Optane SSD in some cases. Optane SSD provides low access latency for workloads with low request scale, while workloads with high request scale might prefer Flash SSD. For workloads with mixed reads and writes, the Control Overall Load rule suggests that read-dominated workloads should be deployed on Flash SSD to achieve low write latency. However, write-intensive workloads prefer the Optane SSD, thus avoiding excessive read latency (influenced by writes in classic Flash SSD). Finally, our results for sustainable performance suggest that when devices become full (and thus would cause garbage collection on an SSD), Optane may be a better choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We analyze a popular NVM-based block device: the Intel Optane SSD. We formalize the rules that Optane SSD users need to follow. We provide experiments to present the impact when violating each rule, and examine the internals of Optane SSD to provide insights for each rule. The unwritten contract provides implications and points to directions for potential research on NVM-based devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Avg Latency of random workloads, Optane vs. Flash the rules. We summarize the impact and cause of each rule in Table 1. We begin with six rules related to immediate performance like latency and throughput. We then present rules for sustainable performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: RAID-like Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Avg Write Latency Random vs. Sequential high latency of Optane SSD for workloads with a large request scale, and supports the need to access the Optane SSD with a low request scale. The limited internal parallelism of the Optane SSD impacts its throughput in two ways. First, the tested Flash SSD achieves larger maximum read (3500MB/s) and write (2700MB/s) bandwidth than the Optane SSD (2500MB/s); it outperforms Optane SSD serving workloads with large request scale. Flash SSD's richer internal parallelism enables it to serve more parallel requests. Second, Optane SSD can achieve peak throughput when serving small requests with low queue depth. This is due to Optane SSD's limited internal parallelism which requires only a small number of requests to utilize all of its resources. Hence, the rule to access the device at a low scale not only guides users to obtain low access latency but also is enough to achieve full bandwidth. The influence of contention in Optane SSD is modest compared to that in Flash SSD. The dips in Figure 3 are due to concurrent requests contending for shared resources (e.g., channels). In Flash SSD, contention significantly restricts overall throughput; for example, with QD = 16, S = 127 chunks, read throughput is 88 MB/s, which is only 6% of the maximum throughput with the same queue depth. Although parallel requests to Optane SSD can also introduce contention (limiting throughput to within 86% of maximum), this influence is much less than that in Flash SSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Parallel Sector Reads to a Chunk(Lat. Distribution)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Figure 8: Performance of Small Requests</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Throughput of Sustained Writes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>presents the distribution</figDesc><table>Optane Read Latency 
Optane Write Latency 
970 Read Latency 
970 Write Latency 

Two Lines for Optane Overlap 

Latency (Î¼s) 
10 2 

10 3 

Write Ratio (%) 

0 
20 
40 
60 
80 
100 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and Anirudh Badam (our shepherd) for their feedback. This material was supported by the Microsoft Gray Research Lab and funding from NSF grants CNS-1763810 and CNS-1838733. VII The whole stack: The storage hierarchy is changing. The stack includes not only DRAM and HDD/SSD, but also Optane SSD and Optane Persistent Memory. What does this richer hierarchy mean for the operating system <ref type="bibr" target="#b12">[13]</ref>, the file system <ref type="bibr" target="#b28">[27]</ref>, and applications?</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>The unwritten contract introduces some open questions requiring discussion and future research: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xpoint</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/3D_XPoint" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>3d Xpoint Technology</surname></persName>
		</author>
		<ptr target="https://www.micron.com/products/advanced-solutions/3d-xpoint-technology" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="https://www.anandtech.com/show/12136/the-intel-optane-ssd-900p-480gb-review" />
		<title level="m">Decompsition of Intel Optane SSD 900P</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Case</forename><surname>Imdt Use</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/support/articles/000026359/memory-and-storage/data-center-ssds.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Case</forename><surname>Imdt Use</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/solution-briefs/imdt-solution-brief-in-memory-data-store.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Case</forename><surname>Imdt Use</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/software/apache-spark-optimization-technology-brief.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<ptr target="https://www.intel.com/content/www/us/en/software/intel-memory-drive-technology.html" />
	</analytic>
	<monogr>
		<title level="j">Intel Memory Drive Technology</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html" />
	</analytic>
	<monogr>
		<title level="j">Intel Optane DC Persistent Memory</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel Optane Memory</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-memory.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ssd</forename><surname>Intel Optane</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/memory-storage/solid-state-drives/data-center-ssds/optane-dc-p4800x-series.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel Optane Technology</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/architecture-and-technology/intel-optane-technology.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<ptr target="https://www.samsung" />
	</analytic>
	<monogr>
		<title level="j">Samsung</title>
		<imprint>
			<biblScope unit="volume">970</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Twizzler: An Operating System for Next-Generation Memory Hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bittman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjiang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Govindjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaak</forename><surname>Cherdak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><forename type="middle">D E</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<idno>UCSC-SSRC-17-01</idno>
		<imprint>
			<date type="published" when="2017-12" />
			<pubPlace>Santa Cruz</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bittman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Alvaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<title level="m">Optimizing Systems for Byte</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Addressable NV M by Reducing Bit Flipping</title>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>FAST 19</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Solid-State Drive Caching with Differentiated Storage Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mesnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Yoshii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>Intel White Paper</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Performance Models of Flash-based Solid-State Drives for Real Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simona</forename><surname>Boboila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 27th Symposium on Mass Storage Systems and Technologies (MSST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding Flash IO Patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Bouganim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>BjÃ¶rn ThÃ³r JÃ³nsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth Conference on Innovative Data Systems Research (CIDR &apos;09)</title>
		<meeting>the fourth Conference on Innovative Data Systems Research (CIDR &apos;09)<address><addrLine>Pacific Grove, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Essential Roles of Exploiting Internal Parallelism of Flash Memory Based Solid State Drives in High-speed Data Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Symposium on High Performance Computer Architecture (HPCA-11)</title>
		<meeting>the 17th International Symposium on High Performance Computer Architecture (HPCA-11)<address><addrLine>San Antonio, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="page" from="266" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">E</forename><surname>Denehy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentina</forename><forename type="middle">I</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpacidusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deconstructing Storage Arrays. In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2004" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of NAND Flash Memory Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What Systems Researchers Need to Know about NAND Flash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 5th USENIX Workshop on Hot Topics in Storage and File Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chris Petersen, Asaf Cidon, and Sachin Katti. Reducing DRAM Footprint with NV M in Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siying</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference</title>
		<meeting>the Thirteenth EuroSys Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Naumov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Pupyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05922</idno>
		<title level="m">Using Nonvolatile Memory for Storing Deep Learning Models</title>
		<meeting><address><addrLine>Bandana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-Scale Graph Processing on Emerging Storage Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Elyasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>FAST 19</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Platform Storage Performance With 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Frank T Hady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Veal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">XPoint Technology. Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Unwritten Contract of Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Sudarsun Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi H Arpaci-Dusseau</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth European Conference on Computer Systems</title>
		<meeting>the Twelfth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strata: A Cross Media File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Fingler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmett</forename><surname>Witchel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
		<meeting>the 26th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">hStorage-DB: Heterogeneityaware Data Management to Exploit the Full Capability of Hybrid Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mesnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Persistent Memcached: Bringing Legacy Code to Byte-Addressable Persistent Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margo</forename><surname>Virendra J Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Byan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th USENIX Workshop on Hot Topics in Storage and File Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>HotStorage 17</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MEMSbased Storage Devices and Standard Disk Interfaces: A Square Peg in a Round Hole</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX Symposium on File and Storage Technologies (FAST &apos;04)</title>
		<meeting>the 3rd USENIX Symposium on File and Storage Technologies (FAST &apos;04)<address><addrLine>San Francisco, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cascade Mapping: Optimizing Memory Efficiency for Flash-based Key-value Caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kefei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cooperative Caching with Return on Investment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gala</forename><surname>Yadgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Factor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 29th Symposium on Mass Storage Systems and Technologies (MSST)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Avoiding the Streetlight Effect: I/O Workload Analysis with SSDs in Mind</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gala</forename><surname>Yadgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Gabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage &apos;16)</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Write-Optimized and High-Performance Hashing Index Scheme for Persistent Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
