<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Multi-streamed Solid-State Drive</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong-Uk</forename><surname>Kang</surname></persName>
							<email>ju.kang@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Memory Solutions Lab. Memory Division</orgName>
								<orgName type="institution">Samsung Electronics Co</orgName>
								<address>
									<settlement>Hwasung</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeeseok</forename><surname>Hyun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Memory Solutions Lab. Memory Division</orgName>
								<orgName type="institution">Samsung Electronics Co</orgName>
								<address>
									<settlement>Hwasung</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjoo</forename><surname>Maeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Memory Solutions Lab. Memory Division</orgName>
								<orgName type="institution">Samsung Electronics Co</orgName>
								<address>
									<settlement>Hwasung</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Memory Solutions Lab. Memory Division</orgName>
								<orgName type="institution">Samsung Electronics Co</orgName>
								<address>
									<settlement>Hwasung</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Multi-streamed Solid-State Drive</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper makes a case for the multi-streamed solid-state drive (SSD). It offers an intuitive storage interface for the host system to inform the SSD about the expected lifetime of data being written. We show through experimentation with a real multi-streamed SSD prototype that the worst-case update throughput of a Cassandra NoSQL DB system can be improved by nearly 56%. We discuss powerful use cases of the proposed SSD interface.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>NAND flash based solid-state drives (SSDs) are widely used for main storage, from mobile devices to servers to supercomputers, due to its low power consumption and high performance. Most SSD users do not (have to) realize that the underlying NAND flash medium disallows in-place update; the illusion of random data access is offered by the SSD-internal software, commonly referred to as flash translation layer or FTL. The block device abstraction paved the way for wide adoption of SSDs because one can conveniently replace a HDD with an SSD without compatibility issues.</p><p>Unfortunately, maintaining the illusion of random data access through the block device interface comes at costs. For example, as the SSD is continuously written, the underlying NAND flash medium can become fragmented. When the FTL tries to reclaim free space to absorb further write traffic, internal data movement operations are incurred between NAND flash locations (i.e., garbage collection or GC) <ref type="bibr" target="#b5">[6]</ref>, leaving the device busy and sometimes unable to properly process user requests. The resultant changing performance behavior of a given SSD is hard to predict or reason about, and remains an impediment to full-system optimization <ref type="bibr" target="#b0">[1]</ref>.</p><p>In order to address the problem from the root, we propose and explore multi-streaming, an interface mechanism that helps close the semantic gap between the host system and the SSD. With the multi-streamed SSD, the host system can explicitly open "streams" in the SSD and send write requests to different streams according to their expected lifetime. The multi-streamed SSD then ensures that the data in a stream are not only written together to a physically related NAND flash space (e.g., a NAND flash block or "erase unit"), but also separated from data in other streams. Ideally, we hope the GC process would find the NAND capacity unfragmented and proceed with no costly data movements.</p><p>In the remainder of this paper, we will delve first into the problem of SSD aging and data fragmentation in Section 2, along with previously proposed remedies in the literature. Section 3 will explain our approach in detail. Experimental evaluation with a prototype SSD will be presented in Section 4. Our evaluation looks at Cassandra <ref type="bibr" target="#b6">[7]</ref>, a popular open-source key-value store, and how an intuitive data mapping to streams can significantly improve the worst-case throughput of the system. We will conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Aging effects of SSD</head><p>SSD aging <ref type="bibr" target="#b15">[16]</ref> explains why the SSD performance may gradually degrade over time; GC is executed more frequently as the SSD is filled with more data and fragmented. Aging effects start to manifest when the "clean" NAND flash capacity is consumed, and in this case, the FTL must proactively recover a sufficient amount of new capacity by "erasing" NAND flash blocks before it can digest new write data. The required erase operations are often preceded by costly GC; to make matters worse, a NAND block, a unit of erase operation, is fairly large in modern NAND flash memory with 128 or more pages in it <ref type="bibr" target="#b14">[15]</ref>. When the SSD is filled up with more and more data, statistically, the FTL would need to copy more valid pages for GC before each NAND flash erase operation. This phenomenon is analogous to "segment cleaning" of a log-structured file system <ref type="bibr" target="#b12">[13]</ref> and is well studied. <ref type="figure" target="#fig_6">Figure 3</ref> (in Section 4) clearly depicts the effects of SSD aging: The performance of Cassandra throughput (in the case of "Normal" SSD) is seriously degraded as the data set in the SSD is continuously updated-by as much as âˆ¼56%. This problem will be exacerbated in systems with many threads (or virtual machines) that perform I/O concurrently. Moreover, for density improvement, a NAND block size will only grow in the future, adding to the cost of GC in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior work to mitigate SSD aging</head><p>Prior proposals reduce GC overheads by classifying access patterns and adapting to workloads inside a storage device <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>. Since the effectiveness of these proposals depends on the accuracy of workload classificationrandom or sequential, they are especially vulnerable to workloads that have frequently changing behavior.</p><p>In another approach, the device detects and separates hot data from cold data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>. These techniques determine "hotness" of data based on access history of locations and require sizable resources to bookkeep the history information. Chiang et al. <ref type="bibr" target="#b1">[2]</ref> use "time-stamps", to indicate how old given data are, and Hsieh et al. <ref type="bibr" target="#b3">[4]</ref> employ multi-hashing to reduce the size of history information. The accuracy and benefits of hot data identification decrease when the access pattern of specific locations is changed, e.g., as in a log-structured file system.</p><p>In a practical sense, robustly deriving accurate information about data hotness and future access patterns is hard. Accordingly, enterprise SSDs where consistent access latency is of paramount importance, tend to set aside ("overprovision" or sacrifice) a generous amount of flash capacity to increase the efficiency of GC <ref type="bibr" target="#b16">[17]</ref>.</p><p>Lastly, "TRIM" is a standardized SSD command (not applicable to HDDs in general), with which the host system can pass information about what LBAs have been "unmapped" by the upper-layer software (typically the file system). This information is useful for the SSD's GC efficiency, because without the information the SSD has to assume conservatively that all NAND flash pages mapped to previously written LBAs (not overwritten) are valid and their content must be copied for preservation when the NAND flash blocks having those pages are vacated. <ref type="figure" target="#fig_6">Figure 3</ref> shows that TRIM improves Cassandra's update throughput significantly; however, it does not alone match the full benefits of multi-streaming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Multi-streamed SSD</head><p>Before describing the proposed multi-stream approach, let's consider revealing examples that explain why traditional write pattern optimizations like append-only logging do not fully address the SSD aging problem.   Block 0 and Block 1, have been filled up and new data are written to fill Block 2. In the first example (left), a sequential write pattern was applied, and as the result, some data become invalid in Block 0 and 1. On the other hand, in the second example, a random write pattern was applied, invalidating all data in Block 0 but none in Block 1. Clearly, future GC will proceed more efficiently in this example, because an empty NAND flash block (Block 0) can be reclaimed quickly without copying data around. These examples demonstrate that an SSD's GC overheads depend not only on the current write pattern but on how data have been already placed in the SSD. Naturally, one might argue that re-scheduling of future write requests to the SSD might solve the aging problem (like in <ref type="figure" target="#fig_1">Figure 1(b)</ref>). However, it is next to impossible for an application (and the host system in general) to know where exactly previously written data have been placed within the SSD, because the algorithms in the FTL vary from SSD to SSD and the written data are moved around by the internal GC process. Moreover, modern OS has a layered I/O subsystem comprised of file system, buffer cache, I/O scheduler and volume manager. So, perfectly controlling the order and target of writes would be extremely challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our approach</head><p>At the heart of the SSD aging problem are the issues of how to predict the lifetime of data written to the SSD and how to ensure that data with similar lifetime are placed in the same erase unit. This work proposes multi-streaming, an interface that directly guides data placement within the SSD, separating the two issues. We argue that the host system should (and can) provide adequate information about data lifetime to the SSD. It is the responsibility of the SSD, then, to place data with similar lifetime (as dictated by the host system) into the same erase unit.</p><p>Our design introduces the concept of stream. A stream is an abstraction of SSD capacity allocation that stores a set of data with the same lifetime expectancy. An SSD that implements the proposed multi-stream interface allows the host system to open or close streams and write to one of them. Before writing data, the host system opens streams (through special SSD commands) as needed. Both the host system and the SSD share a unique stream ID for each open stream, and the host system augments each write with a proper stream ID. A multi-streamed SSD allocates physical capacity carefully, to place data in a stream together and not to mix data from different streams. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates how this can be achieved. We believe that the multi-stream interface is abstract enough for the host system to be able to tap, with convincing use cases and results (as discussed in Section 4). Furthermore, the level of information delivered through the interface is concrete enough for the SSD to optimize its behavior with. There are other proposals to specify write data attributes, like access frequency <ref type="bibr" target="#b10">[11]</ref>. However, it is not straightforward for the SSD to derive data lifetime from the expected frequency of data updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation</head><p>We implemented the proposed multi-stream interface on the currently marketed Samsung 840 Pro SSD <ref type="bibr" target="#b13">[14]</ref>. Because 840 Pro is based on the SATA III interface, we piggyback stream ID on a reserved field of both regular and queued write commands as specified in the AT attached (ATA) command set <ref type="bibr" target="#b4">[5]</ref>. Our multi-streamed SSD prototype currently supports four streams (Stream 1 to 4) on top of the default stream (Stream 0).</p><p>We modified the Linux kernel (3.13.3) to have a conduit between an application and the SSD, through the file system and the layers below. More specifically, an application passes a stream ID to the file system through the fadvise system call, which, in turn, stores the stream ID in the inode of the virtual file system. When dirty pages are flushed into the SSD, or the application directly requests a write operation with the direct I/O facility, we send along the write request the stream ID (that can be retrieved from the associated inode).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>To evaluate the multi-streamed SSD, we conduct experiments that run Cassandra <ref type="bibr" target="#b6">[7]</ref> (version 1.2.10), a widely deployed open-source key value store. All experiments were performed on a commodity machine with a quad- core Intel i7-3770 3.4GHz processor. We turned off power management for reliable measurements. Cassandra optimizes I/O traffic by organizing its data set in or append-only "sorted strings tables" (SSTables) in disk. New data are first written to a commit log (CommitLog) and are put in a table in the main memory (MemTable) as they are inserted. Contents in the MemTable are flushed to a SSTable once they accumulate to a certain size. Since SSTables are immutable, several of them are "compacted" periodically to form a new (large) SSTable to reduce the space and time overheads of maintaining many (fragmented) SSTables. As the compaction process repeats, valid data gradually move from a (small) SSTable to another in a different size tier. We take into account how data are created and destroyed in Cassandra when we map writes to streams. <ref type="table" target="#tab_1">Table 1</ref> lists four different mappings that we examine. Normal implies that all data are mapped to the default stream (Stream 0), equivalent to a conventional SSD with no multi-streaming support and is the baseline configuration. In Single, we separate all data from Cassandra into a stream (Stream 1). System data, not created by the workload itself, include the ext4 file system meta and journal data and still go to Stream 0. Multi-Log carves out the CommitLog traffic to a separate stream, making the total stream count three (including the default stream). Finally, Multi-Data further separates SSTables in different tiers to three independent streams. Intuitively, SSTables in the same tier would have similar lifetime while SSTables from different tiers would have disparate lifetime.</p><p>For workloads, we employ the Yahoo! Cloud Serving Benchmark (YCSB) <ref type="bibr" target="#b2">[3]</ref> (0.1.4). We run both YCSB and Cassandra on the same machine, not to be limited by the 1Gb Ethernet. In addition, we limit the RAM size to 2GB to accelerate SSD aging by increasing Cassandra's flush frequency. The compaction throughput parameter of Cassandra was modified from 16 MB/s to 32 MB/s, as recommended by the community for SSD users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Figure 3(a) plots the normalized update throughput of all mapping configurations studied. We introduce a Normal configuration with the TRIM facility turned off, to gain insight about the impact of TRIM. We make the follow-  <ref type="formula">7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37</ref>    <ref type="formula">1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37</ref>   ing key observations: (1) TRIM is shown to be critically important for the sustained performance; (2) GC overheads <ref type="figure" target="#fig_6">(Figure 3(b)</ref>) correlate very well with the throughput; and (3) Multi-Data outperforms all other configurations and sustains the throughput.</p><p>Without TRIM, Normal's performance approaches a dismal level-20% of the peak performance-, shown only briefly at the beginning of the experiment (when the SSD was relatively fresh). To put it in a different way, TRIM is very effective for Cassandra because it organizes data in a log-structured manner, writes a large file of data and deletes an entire file at a time. However, even with the benefit of TRIM, Normal still performs poorlyits performance drops from the peak by up to 53%! We do not consider Normal without TRIM any further.</p><p>The (poor) performance of Normal and others can be attributed well to the GC overheads: Valleys in plot (b) match with peaks in plot (a), and vice versa. GC overheads can be approximated by the number of valid pages that must be copied (to vacate a NAND flash block before erasing it). Because copying of valid pages involves programming (and hence consuming the bandwidth of) NAND flash memory, the ability of the SSD to serve user requests is hurt in direct proportion.</p><p>In general, our result shows that the use of multistreaming cuts down GC overheads and in turn increases throughput (by nearly 56% when Multi-Data is com- pared to Normal). Also shown is that how streams are allocated and mapped to application data makes a critical impact. Single bears little difference to Normal-mainly because the traffic separated with multi-streaming corresponds to only 1% of the total traffic. Multi-Log improves GC efficiency and throughput much more noticeably. The throughput now lies in between 65% and 85% of the peak. Lastly, Multi-Data, our best mapping, hits roughly 90% of the peak performance sustainably. As we intended, few GC activities are incurred, if any. We also experimented with a 15K-RPM enterprise HDD (not shown); it showed fairly consistent throughput that peaks at less than one third the performance of Multi-Data. <ref type="figure" target="#fig_7">Figure 4</ref> presents the latency profile obtained from the previous experiment. The plot shows that multistreaming improves latency as well: At the 99.9th percentile, Multi-Data lowers the latency by 54%, compared with Normal and at the 99.99th percentile, by 61%.</p><p>In another experiment, we design and evaluate an in-SSD mechanism that detects multiple sequential access patterns on the fly and assigns an adequate stream ID. The goal was to gain insights into how much improvement we can obtain through automatic stream detection. Note that write patterns in Cassandra are mostly sequential. In our implementation, we detect maximum four concurrent sequential patterns and assign a stream ID to each of them. If there are more than four sequential patterns, we handed the stream ID of the least recently used sequential pattern to a newly detected one. With this mechanism, the SSD classified 71% of all data to be sequential. However, the resultant performance gain was rather marginal. This counter-intuitive result is due to how LBAs are allocated by the ext4 file system: A large file may not always get sequential LBAs due to fragmentation. Moreover, successively created files may not get consecutive LBAs as they expand. Accordingly, in Cassandra, the chances that SSTables in the same size tier are detected as a sequential stream decrease. In the end, SSTables from different tiers and even CommitLog start to mix up across streams. Our result underscores that concrete semantic information passed by the host system through the multi-stream interface is much more relevant and robust than automatically learned access patterns.</p><p>Finally, we examine how multi-streaming can extend the lifetime of SSDs. We have iterated a few times already that proper use of multi-streaming can improve GC efficiency. Higher GC efficiency implies that the multistreamed SSD lowers the number of required NAND flash erase operations. Indeed, we found that Multi-Data would extend the SSD lifetime by 23%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Our study with Cassandra showed that an intuitive data to stream mapping can lead to large benefits in throughput, consistent latency and NAND flash lifetime on the multi-streamed SSD. We further believe that many applications and use cases will enjoy similarly large gains from the multi-streamed SSD, if reasonably good mapping is done. Other database management systems that use log-structured merge trees (like Cassandra) include HBase, LevelDB, SQLite4 and RocksDB. These applications explicitly manage data streams and orient their I/O to be sequential. In another example, consider commit (transaction) log, undo (roll-back) log and temporary table data in OLTP applications <ref type="bibr" target="#b8">[9]</ref>. They map nicely into separate streams on the multi-streamed SSD. Lastly, some multi-head log-structured file systems and flash storage OS could relatively effortlessly steer their data writes into streams for higher, consistent performance and better media lifetime.</p><p>There are several avenues for further research. It will be interesting to develop a systematic data to stream mapping strategy that can handle multiple applications (virtual machines) running concurrently. It is also worthwhile to look at if and how multi-streaming could provide performance and fault isolation <ref type="bibr" target="#b9">[10]</ref>. How to effectively support multi-streaming without application-level changes remains a challenge and research question. How to organize and utilize streams on multiple SSDs would be a practical, rewarding topic to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig- ure 1 gives the examples, where two NAND flash blocks,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Relationship between data placement and updates. For simplicity, we assume that there are four pages per block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The multi-streamed SSD writes data into a related NAND flash block according to stream ID regardless of LBA. In this example, three streams are introduced to store different types of host system data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>39 Valid pages copied (ops/sec) Time (Minutes) Normal(TRIM off) Normal(TRIM on) Single Multi-Log Multi-Data (b) The number of valid pages copied during GC execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cassandra update throughput and GC overheads, normalized. The update throughput is shown to depend heavily on the GC overheads. We estimate GC overheads with the number of valid NAND flash pages that must be copied during GC. Trends of throughput and GC activities are similar after the 40-min. period captured in the plots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Cassandra's cumulated latency distribution. The long tail of latency distribution is also shown to be subject to GC overheads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Stream ID Assignment</head><label>1</label><figDesc></figDesc><table>system Commit-
Log 

flushed 
data 

compaction 
data 
Normal 
0 
0 
0 
0 
Single 
0 
1 
1 
1 
Multi-Log 
0 
1 
2 
2 
Multi-Data 
0 
1 
2 
3âˆ¼4 
Ratio of written 
data (%) 

1.0 
48.6 
31.3 
4.4, 
14.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>39</head><label>39</label><figDesc></figDesc><table>Throughput 

(ops/sec) 

Time (Minutes) 

Normal(TRIM off) 
Normal(TRIM on) 
Single 
Multi-Log 
Multi-Data 

(a) Cassandra's normalized update throughput. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We made a case for the multi-streamed SSD in this paper. We found the proposed multi-streaming concept powerful and the interface expressive; by mapping application and system data with different lifetimes to SSD streams, we demonstrated that the SSD throughput and latency QoS are significantly improved. The data mapping used in our Cassandra case study is intuitive, and similar benefits are expected from other applications with proper data to stream mapping. Our prototype SSD proves that multi-streaming can be supported on a state-of-the-art SSD and can co-exist with the traditional block interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>We thank the anonymous reviewers for their constructive comments. Jaegeuk Kim and other MSL members gave helpful comments on earlier drafts of this paper. Donggun Kim contributed to the development of the prototype multi-stream firmware on the Samsung 840 Pro SSD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding intrinsic characteristics and system implications of flash memory based solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="181" to="192" />
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Managing flash memory in personal communication devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1997 IEEE International Symposium on</title>
		<meeting>1997 IEEE International Symposium on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="177" to="182" />
		</imprint>
	</monogr>
	<note>Consumer Electronics, 1997. ISCE&apos;97</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with ycsb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cooper</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM symposium on Cloud computing</title>
		<meeting>the 1st ACM symposium on Cloud computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient identification of hot data for flash memory storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsieh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="22" to="40" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">International Committee For Information Technol-Ogy</forename><surname>Standards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
<note type="report_type">Technical Committee T13 AT Attachment</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A flashmemory based file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawaguchi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nishioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motoda</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Usenix Winter</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cassandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Last: localityaware sector translation for nand flash memory-based storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="36" to="42" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A case for flash memory ssd in enterprise database applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1075" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fault isolation and quick recovery in isolation file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Conference on Hot Topics in Storage and File Systems</title>
		<meeting>the 5th USENIX Conference on Hot Topics in Storage and File Systems<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>HotStorage&apos;13, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">NVM Express 1.1a Specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvm</forename><surname>Express Consortium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A pattern adaptive nand flash memory storage structure. Computers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weems</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="134" to="138" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The design and implementation of a log-structured file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosenblum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousterhout</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="26" to="52" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samsung</forename><surname>Samsung</surname></persName>
			<affiliation>
				<orgName type="collaboration">SSD 840 PRO</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Micron Announces 16nm 128Gb MLC NAND, SSDs in 2014</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimpi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The ssd anthology: Understanding ssds and new drivers from ocz</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimpi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Understanding ssd over-provisioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smith</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
