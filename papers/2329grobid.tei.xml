<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">USENIX Association 11th USENIX Conference on File and Storage Technologies (FAST &apos;13) 133 MixApart: Decoupled Analytics for Shared Storage Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalin</forename><forename type="middle">Mihailescu</forename><surname>񮽙</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 񮽙</orgName>
								<orgName type="institution" key="instit2">NetApp, Inc. †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Soundararajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 񮽙</orgName>
								<orgName type="institution" key="instit2">NetApp, Inc. †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiana</forename><forename type="middle">Amza</forename><surname>񮽙</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Toronto 񮽙</orgName>
								<orgName type="institution" key="instit2">NetApp, Inc. †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">USENIX Association 11th USENIX Conference on File and Storage Technologies (FAST &apos;13) 133 MixApart: Decoupled Analytics for Shared Storage Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributed file systems built for data analytics and enterprise storage systems have very different functionality requirements. For this reason, enabling analytics on enterprise data commonly introduces a separate analytics storage silo. This generates additional costs, and inefficiencies in data management, e.g., whenever data needs to be archived, copied, or migrated across silos. MixApart uses an integrated data caching and scheduling solution to allow MapReduce computations to analyze data stored on enterprise storage systems. The front-end caching layer enables the local storage performance required by data analytics. The shared storage back-end simplifies data management. We evaluate MixApart using a 100-core Amazon EC2 cluster with micro-benchmarks and production workload traces. Our evaluation shows that MixApart provides (i) up to 28% faster performance than the traditional ingest-then-compute workflows used in enterprise IT analyt-ics, and (ii) comparable performance to an ideal Hadoop setup without data ingest, at similar cluster sizes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We design a novel method for enabling distributed data analytics to use data stored on enterprise storage. Distributed data analytics frameworks, such as MapReduce <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b16">19]</ref> and Dryad <ref type="bibr" target="#b19">[22]</ref>, are utilized by organizations to analyze increasing volumes of information. Activities range from analyzing e-mails, log data, or transaction records, to executing clustering algorithms for customer segmentation. Using such data flow frameworks, data partitions are loaded from an underlying commodity distributed file system, passed through a series of operators executed in parallel on the compute nodes, and the results are written back to the file system <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b19">22]</ref>. The file system component of these frameworks <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b18">21]</ref> is a standalone storage system; the file system stores the data on local server drives, with redundancy (typically, 3-way replication) to provide fault tolerance and data availability.</p><p>While the benefits of these frameworks are well understood, it is not clear how to leverage them for performing analysis of enterprise data. Enterprise data, e.g., corporate documents, user home directories, critical log data, e-mails, need to be secured from tampering, protected from failures, and archived for long-term retention; high-value data demands the enterprise-level data management features provided by enterprise storage.</p><p>Traditional analytics file systems <ref type="bibr" target="#b10">[13]</ref>, however, have been designed for low-value data, i.e., data that can be regenerated on a daily basis; low-value data do not need enterprise-level data management. Consequently, these file systems, while providing high throughput for parallel computations, lack many of the management features essential for the enterprise environment, e.g., support for standard protocols (NFS), storage efficiency mechanisms such as deduplication, strong data consistency, and data protection mechanisms for active and inactive data <ref type="bibr" target="#b23">[26]</ref>.</p><p>These disparities create an environment where two dedicated reliable storage systems (silos) are required for running analytics on enterprise data: a feature-rich enterprise silo that manages the total enterprise data and a high-throughput analytics silo storing enterprise data for analytics. This setup leads to a substantial upfront investment as well as complex workflows to manage data across different file systems. Enterprise analytics typically works as follows: (i) an application records data into the enterprise silo, (ii) an extraction workflow reads the data and ingests the data into the analytics silo, (iii) an analytics program is executed and the results of the analytics are written into the analytics file system, (iv) the results of the analytics are loaded back into the enterprise silo. The workflow repeats as new data arrives.</p><p>To address the above limitations, we design MixApart to facilitate scalable distributed processing of datasets stored in existing enterprise storage systems, thereby displacing the dedicated analytics silo. MixApart replaces the distributed file system component in current analytics frameworks with an end-to-end storage solution, XDFS, that consists of:</p><p>• a stateless analytics caching layer, built out of local server disks, co-located with the compute nodes for data analytics performance,</p><p>• a data transfer scheduler that schedules transfers of data from consolidated enterprise storage into the disk cache, just in time, in a controlled manner, and</p><p>• the enterprise shared storage system for data reliability and data management.</p><p>The design space for MixApart includes two main alternatives: (i) unified caching, by having the XDFS cache interposed on the access path of both the analytics and the enterprise workloads, (ii) dedicated caching, by having the XDFS cache serve analytics workloads only. Moreover, in terms of implementation, for each design option, we can either leverage the existing HDFS <ref type="bibr" target="#b10">[13]</ref> codebase for developing our cache, or leverage existing file caching solutions e.g., for NFS or AFS <ref type="bibr" target="#b5">[8]</ref> file systems. With MixApart we explore the design of a dedicated cache with an implementation based on the existing Hadoop/HDFS codebase. Our main reasons are prototyping speed and fair benchmarking against Hadoop.</p><p>We deployed MixApart on a 100-core Amazon EC2 cluster, and evaluated its performance with microbenchmarks and production workload traces from Facebook. In general, the results show that MixApart provides comparable performance to Hadoop, at similar compute scales. In addition, MixApart improves storage efficiency and simplifies data management. First, the stateless cache design removes the redundancy requirements in current analytics file systems, thus lowering storage demands. Moreover, with classic analytics deployments, ingest is usually run periodically as a way to synchronize two storage systems, independent of job demands. In contrast, our integrated solution provides a dynamic ingest of only the needed data. Second, MixApart eliminates cross-silo data workflows, by relying on enterprise storage for data management. The data in the cache is kept consistent with the associated data in shared storage, thus enabling data freshness for analytics, transparently, when the underlying enterprise data changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MixApart Use Cases</head><p>MixApart allows computations to analyze data stored on enterprise storage systems, while using a caching layer for performance. We describe scenarios where we expect this decoupled design to provide high functional value.</p><p>Analyzing data on enterprise storage: Companies can leverage their existing investments in enterprise storage and enable analytics incrementally. There exist many file-based data such as source-code repositories, e-mails, and log files; these files are generated by traditional applications but currently require a workflow to ingest the data into an analytics filesystem. MixApart allows a single storage back-end to manage and service data for both enterprise and analytics workloads. Data analytics, using the same filesystem namespace, can analyze enterprise data with no additional ingest workflows.</p><p>Cross-data center deployments: MixApart's decoupled design also allows for independent scaling of compute and storage layers. This gives the flexibility of placing the analytics compute tier on cloud infrastructures, such as Amazon EC2 <ref type="bibr">[3]</ref>, while keeping the data onpremise. In this scenario, upfront hardware purchases are replaced by the pay-as-you-go cloud model. Cross-data center deployments benefit from efforts such as AWS Direct Connect <ref type="bibr" target="#b0">[2]</ref> that enable high-bandwidth connections between private data centers and clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MapReduce Workload Analysis</head><p>Data analytics frameworks process data by splitting a user-submitted job into several tasks that run in parallel. In the input data processing phase, e.g., the Map phase, tasks read data partitions from the underlying distributed file system, compute the intermediate results by running the computation specified in the task, and shuffle the output to the next set of operators -e.g., the Reduce phase. The bulk of the data processed is read in this initial phase.</p><p>Recent studies <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b13">16]</ref> of production workloads deployed at Facebook, Microsoft R 񮽙 Bing, and Yahoo! make three key observations: (i) there is high data reuse across jobs, (ii) the input phases are, on average, CPU-intensive, and (iii) the I/O demands of jobs are predictable. Based on the above workload characteristics, we motivate our MixApart design decisions. We then show that, with typical I/O demands and data reuse rates, MixApart can sustain large compute cluster sizes, equivalent to those supported by current deployments using dedicated storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">High Data Reuse across Jobs</head><p>Production workloads exhibit high data reuse across jobs with only 11%, 6%, and 7% of jobs from Facebook, Bing, and Yahoo!, respectively, reading a file once <ref type="bibr" target="#b7">[10]</ref>. Using the job traces collected, Ananthanarayanan et al. estimate that in-memory cache hit rates of 60% are possible by allocating 32GB of memory on each machine, with an optimal cache replacement policy <ref type="bibr" target="#b7">[10]</ref>.</p><p>MixApart employs a distributed cache layer built from local server drives; this disk cache is two orders of magnitude larger than an in-memory cache. For example, Amazon EC2 <ref type="bibr">[3]</ref> instances typically provide about 1TB of local disk space for every 10GB of RAM. Hence, with these very large on-disk caches even a simple LRU policy suffices to achieve near-optimal hit rates, rendering cache thrashing irrelevant. Furthermore, we expect data reuse to increase as computations begin to rely on iterative processing -e.g., Mahout <ref type="bibr" target="#b11">[14]</ref>. Interconnected jobs, such as job pipelines, will naturally exhibit data reuse in MixApart, as the current job input would be the output of the previous job. These trends and observations indicate that by caching data after the first access, MixApart can significantly reduce the number of I/Os issued to shared storage for subsequent accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CPU-intensive Input Phases</head><p>In addition to high data reuse, data operations such as compression/decompression, serialization/deserialization, task setup/cleanup, and the sorting of outputs increase the average time spent on the CPU <ref type="bibr" target="#b6">[9]</ref>. Zaharia et al. show, with 64 MB partition sizes, that the median map task duration is 19s for Facebook's workloads, and 26s for Yahoo!'s workloads <ref type="bibr" target="#b26">[29]</ref>. Higher processing times indicate that a task's effective I/O rate is low, thereby there is ample time to move data from the shared storage to the distributed cache. For instance, a task running for 20s to process a 64 MB input partition implies a task I/O rate of 25 Mbps. A storage server, with a 1 Gbps link, can sustain 40 such map tasks concurrently, even when all data is read from shared storage, with no loss in performance. The distributed cache layer further improves scalability. For example, with a 70% cache hit rate and a task I/O rate of 25 Mbps, more than 130 map tasks can process data in parallel from cache and shared storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Predictable I/O Demands</head><p>Average high data reuse and low task I/O rates confirm the feasibility of MixApart. Individual job patterns that  deviate from the norm, however, could potentially impact scalability, by congesting shared storage when data reuse is low and aggregate job I/O demands are high. Hence, coordination is required to smooth out traffic and ensure efficient storage bandwidth utilization at all times.</p><p>Previous studies observe that production MapReduce workloads have very predictable task times <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b13">16]</ref>. In fact, the analysis of production traces for various Hadoop clusters <ref type="bibr" target="#b13">[16]</ref> shows that processing jobs can be classified into less than 10 bins. In addition, tasks of a job have similar durations <ref type="bibr" target="#b7">[10]</ref>. These two observations, combined with the fact that a task will read all data in its input partition, allow MixApart to coordinate shared storage traffic for individual jobs, and across multiple jobs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Estimating Cluster Sizes Supported</head><p>We expand our analysis to estimate the average compute cluster sizes that can be supported by MixApart based on the workload characteristics introduced above, i.e., the typical data reuse across jobs, the computation to I/O ratio in the workload, and the storage bandwidth utilization. We use a back-of-the-envelope calculation similar to the one in Section 3.2 to derive the number of parallel map tasks under different parameters. <ref type="figure" target="#fig_1">Figure 1</ref> plots the estimated cluster sizes (in number of map tasks). We vary the data reuse ratios from 0 to 0.99, task durations from 0.5s to 40s, and vary the storage bandwidth between 1 Gbps and 10 Gbps. The analysis shows that large analytics clusters can be sustained; for example, as shown in the <ref type="figure">Figure,</ref> with a data reuse ratio of 0.8 and average map task duration of 20s, MixApart can support 2000 parallel tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MixApart Architecture</head><p>MixApart enables scalable and efficient analytics for data stored on shared storage systems. Towards this, it uses the disk space available on each compute node in the analytics cluster as an on-disk caching tier for data on shared storage. <ref type="figure">Figure 2</ref> shows all the components of the MixApart architecture. The architecture consists of two layers: a distributed compute layer and a distributed data storage layer (XDFS), along with the schedulers and metadata managers associated with the two layers.</p><p>The operational goals of MixApart are twofold: (i) preserve the scalability and performance benefit of compute-data co-location, and (ii) ensure efficient utilization of the compute nodes, cache and storage bandwidth. Towards this, MixApart uses per-job task I/O rates and the job scheduling policy to efficiently overlap computation with data fetches, whenever possible, by utilizing two correlated components:</p><p>• Compute Scheduler: a module that schedules tasks according to the XDFS cache contents and a job scheduling policy e.g., FIFO.</p><p>• Data Transfer Scheduler: a module that transfers data from shared storage to caches, as needed by compute tasks, based on job I/O rate predictions.</p><p>Once a job is submitted to the compute layer, this layer pre-processes the job to derive the job tasks, their data requests, and data request I/O rates. This job information is passed to the data transfer scheduler. Guided by the I/O rate prediction, the data transfer scheduler schedules data fetches from shared storage into one or more XDFS cache nodes, just in time, on behalf of job tasks.</p><p>While following a given job scheduling policy, e.g., FIFO, the compute scheduler schedules tasks out of order from its task queue, according to data availability in the cache nodes. Specifically, within each job, the compute scheduler prioritizes tasks with a higher fraction of data already in the cache, or on their way to the cache.</p><p>The logic of the two schedulers is correlated; we employ two scheduler components in order to have separation of concerns, and the flexibility to plug in any policy, independently, at the compute and I/O layers, respectively. For example, the compute scheduler can use FIFO or Fair job scheduling, or real-time job deadlines. On its end, the data transfer scheduler can work in conjunction with shared storage policies for I/O scheduling to achieve better utilization of the shared storage bandwidth. Moreover, for cross-data center deployments of MixApart, i.e., compute on-cloud, data on-premise, the data transfer scheduler can provide end-to-end WAN and shared storage bandwidth management. <ref type="figure">Figure 2</ref> shows an example of the integrated operation of the two schedulers. We see that the compute scheduler assigns tasks T 1 and T 3 to nodes DN 1 and DN 2 , respectively, where the data needed by each of the two tasks is already cached. In parallel with these computations, transfers occur for data expected to be needed next by tasks T 2 and T 4 ; these tasks will be scheduled when their data is in the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">MixApart Compute Layer</head><p>The compute layer is composed of a set of compute nodes and our compute scheduler. Also a part of this layer, a compute manager (see <ref type="figure">Figure 2</ref>) accepts jobs into the system, and maintains job-level information, such as task I/O rates, that our schedulers need. Furthermore, a location handler associated with the compute scheduler keeps track of locations for data blocks currently in the cache and estimates of future data transfers.</p><p>Compute Manager: The MixApart compute manager maintains job-level information, such as job submission time, job priority, job data blocks stored in the cache, and the job-specific average map task I/O rate. When an I/O rate estimate is not available on job submission, the job-level I/O rate can be estimated very fast based on the monitored I/O rate of the first running job map task. Given that tasks within a job are highly homogeneous <ref type="bibr" target="#b7">[10]</ref>, this is expected to be a good predictor of I/O rates of all job tasks.</p><p>Compute Scheduler: Algorithm 1 illustrates the compute scheduler logic. The compute nodes advertise their resource availability by sending periodic heartbeats</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 MixApart Data-Aware Compute Scheduler</head><p>1: if a heartbeat is received from node N then 2:</p><p>if N has a free slot then 3:</p><p>Sort JobQueue based on policy (FIFO, Fair) 4:</p><p>for Job J in JobQueue do 5:</p><p>if J has DataLocal(N) task T then 6:</p><p>Launch T on node N 7:</p><p>else if J has RackLocal(N) task T then 8:</p><p>Launch T on node N 9:</p><p>else if J has DataLocalInProgress(N) task T then 10:</p><p>Launch T on node N 11:</p><p>else if J has RackLocalInProgress(N) task T then 12:</p><p>Launch T on node N 13:</p><p>else if J has CacheLocal task T then 14:</p><p>Launch T on node N 15:</p><p>else if J has CacheLocalInProgress task T then 16:</p><p>Launch T on node N 17:</p><p>end if 18:</p><p>end for 19:</p><p>end if 20: end if to the compute manager (step 1). Upon receiving a resource availability event, the compute scheduler sorts the job scheduling queue based on the policy (3). For example, for a FIFO policy, jobs are sorted based on arrival time and a user-defined job priority; for a Fair policy, jobs are sorted in increasing order of number of running tasks <ref type="bibr" target="#b26">[29]</ref>. The compute scheduler assigns a task from the job at the head of the scheduling queue (4), in decreasing order of data partition locality (5-17), i.e., datalocal for tasks with input data cached on the respective node, rack-local for tasks with input data cached on a different node in the same rack as the current node, and cache-local for tasks with input data anywhere in the XDFS cache. If the data accessed by a task is in the process of data transfer, the compute scheduler considers the future locality benefit and schedules accordingly <ref type="bibr" target="#b6">(9)</ref><ref type="bibr" target="#b7">(10)</ref><ref type="bibr" target="#b8">(11)</ref><ref type="bibr" target="#b9">(12)</ref><ref type="bibr" target="#b12">(15)</ref><ref type="bibr" target="#b13">(16)</ref>.</p><p>To saturate the compute tier, the task scheduler crosses job boundaries when assigning tasks, i.e., the task scheduler is work conserving. Specifically, if the tasks on behalf of jobs currently expected to run do not saturate the compute tier capacity due to lack of data in the cache, the scheduler selects tasks from lower priority jobs for which data is already in the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">XDFS Distributed Storage Layer</head><p>The XDFS on-disk cache layer contains a metadata manager, a set of cache nodes, and a data transfer scheduler.</p><p>XDFS Metadata Manager: The XDFS metadata manager implements both the distributed filesystem functionality as well as MixApart functionality. Namely, the manager implements group membership, failure detection of cache nodes, and interfaces to query the filesystem metadata. It also provides interfaces to query and manage the cache contents.</p><p>The metadata manager stores the data location information to be used by the compute tier for scheduling map tasks. It interfaces with the compute scheduler to assign tasks to nodes where a large fraction of the task's associated input data partition is cached. For every MapReduce job submitted to the system, the compute scheduler queries the metadata node for input data partition locations. For input partitions currently in the cache, the XDFS metadata manager replies with associated cache node locations. For input partitions not present in any cache node, the data transfer scheduler chooses cache nodes to transfer the data to, based on cache node utilization, and coordinates the data transfers from shared storage. The metadata manager notifies the compute scheduler of cache locations as soon as a transfer is initiated; the data locations are used by the compute scheduler to maximize compute-data locality.</p><p>XDFS Data Nodes: The XDFS data nodes store the data needed by MapReduce tasks on local storage. The data nodes copy data blocks into the cache from shared storage, as instructed by the data transfer scheduler. We use a default block granularity of 64 MB for large files; data blocks are stored on the local filesystem, e.g., ext4.</p><p>XDFS Data Consistency: Data consistency in the XDFS cache is maintained through notifications from the shared storage server upon changes to enterprise data. With the NFSv4 protocol, for example, this is achieved through client delegations and server callbacks <ref type="bibr" target="#b4">[7]</ref>. Similarly, the CIFS protocol enables cache consistency through opportunistic locks <ref type="bibr" target="#b1">[4]</ref>. Metadata consistency is maintained through directory delegations and/or periodic checking of directory attributes.</p><p>The XDFS manager holds read delegations for all files in the XDFS cache. The XDFS manager also maintains a list of files that represent inputs of active jobs. For files that are not part of the input of active jobs, the XDFS manager invalidates the corresponding file blocks in the XDFS cache as file update notifications are received from the enterprise storage. However, for files that represent inputs of active jobs, file blocks are marked as invalid after job completion. Invalid file blocks are refetched using the usual data transfer path, upon newer jobs analyzing the respective files.</p><p>Data Transfer block B at IORate(J) 8:</p><p>AvailStorageBandwidth -= IORate(J) 9: end while analytics workloads such that enterprise applications are not impacted. I/O scheduling provides performance isolation between data analytics and enterprise workloads at storage; we use quanta-based scheduling as it minimizes interference effects e.g., due to disk seeks, when different workloads share the same storage subsystem <ref type="bibr" target="#b25">[28]</ref>.</p><p>As shown in Algorithm 2, the data transfer scheduler mimics the job scheduling logic to ensure that the next expected job will have its input data available in the cache (steps 2-3). For example, when the FIFO policy is used at the compute tier, job arrival times and priorities determine the data transfer order. Alternatively, the Fair compute scheduler prioritizes jobs in increasing order of number of running tasks. In this case, the data transfer scheduler selects the next transfer from the job with the lowest number of not-yet-analyzed cached data blocks (including blocks under analysis). Transfer requests are further sorted per-job, by global block demand, i.e., the number of job tasks interested in a data block, across all jobs (steps 4-5). Destination cache nodes are selected based on available node capacity (6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Prototype</head><p>We implemented the MixApart prototype within the Hadoop 0.20.203.0 version. The data-aware compute scheduler is a variant of the default task scheduler in Hadoop; the compute scheduler uses the FIFO job scheduling policy. We reused much of the HDFS codebase and retrofitted it for our purposes. Specifically, the NameNode, acting as XDFS metadata manager, loads shared storage metadata, i.e., NFS metadata, at startup time; data on enterprise storage is made visible through a local mount point on the NameNode. Furthermore, we modified HDFS to support cache blocks in addition to the reliable, persistent blocks. The compute-aware data transfer scheduler is implemented as a separate module at the NameNode (metadata manager) level.</p><p>XDFS can be viewed as HDFS with a redundancy of 1, and with a smart layer that ingests data from NFS into XDFS transparently and on-demand. HDFS, acting as reliable storage, has additional code to handle data loss; XDFS is stateless where a node failure simply results in additional fetches from the shared storage. Being a cache, we have modified XDFS to be aware of data transfers and take advantage of compute times for prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Evaluation</head><p>We evaluate the performance of MixApart in comparison with a dedicated Hadoop setting. We also study the impact MixApart has on enterprise workloads when running concurrently on shared storage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">MixApart vs. Hadoop</head><p>We evaluate MixApart on a 100-core cluster connected to a NFS server, running on Amazon's Elastic Compute Cloud <ref type="bibr">[3]</ref>; the XDFS caches access the shared storage using local mount points. We run micro-benchmarks, as well as production traces from a 3000-machine Hadoop deployment at Facebook. We begin by comparing the performance of MixApart to that of Hadoop with data ingest into the HDFS cluster to show the benefits of overlapping computation with I/O prefetches in MixApart. In subsequent experiments, we consider an ideal version of Hadoop with no data ingest, i.e., all data already placed in HDFS, for comparison against MixApart. Favoring Hadoop in these experiments allows to investigate the upper limit for MixApart's scheduling for concurrency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Testbed</head><p>We evaluate MixApart on Amazon EC2 using three types of EC2 instances. We use 50 "standard-large" instances to deploy the compute nodes and the XDFS data nodes; each instance hosts a pair of compute and data nodes, and is configured with 2 virtual cores, 7.5 GB RAM, 1 Gbps network, and 850 GB of local ephemeral storage. Each compute node provides 2 map slots and 1 reduce slot. The compute manager and the XDFS metadata manager use one "standard-extra-large" instance configured with 4 virtual cores, 15 GB of RAM, 1 Gbps network, and 1690 GB of local storage. The shared storage is provided by a "cluster-compute-extra-large" instance configured with 23 GB RAM, 2 Intel R 񮽙 Xeon R 񮽙 X5570 quadcore CPUs, 10 Gbps network, and 1690 GB of local storage. We configure the shared storage server to store data on 4 EBS (elastic block storage) volumes assembled in a RAID-0 setting; the storage is exposed using NFS.</p><p>We use local instance storage for the XDFS cache; each instance has 850 GB of local storage for a total of 42 TB in the disk cache. The same disks are used for HDFS; HDFS is configured with 3-way replication. We limit the NFS server bandwidth to 1 Gbps in our experiments. This setting also mimics a production environment where enterprise workloads would still be able to use the remaining 9 Gbps. <ref type="figure" target="#fig_4">Figure 4</ref> presents the estimated storage capacity needed for different Hadoop and MixApart configurations <ref type="bibr">1</ref> . We compare standard HDFS (3 copies) with configurations that include enterprise storage either using double parity RAID-6 (NAS+HDFS), or in a disaster-recovery setup (DR-NAS+HDFS). The disaster-recovery setup mirrors the entire storage system to an offsite copy. We estimate the RAID overhead using the 12+2 (17% overhead) configuration. We assume all data is copied from the enterprise storage system into HDFS in the NAS+HDFS and DR-NAS+HDFS schemes; HDFS uses three copies even when a copy is kept on enterprise storage. This results in NAS+HDFS using 4.17 units of space for every unit of data and DR-NAS+HDFS consumes 5.35 units. In contrast, the capacity needed by MixApart varies with the fraction of active (cached) data. The MixApart configurations (NAS+MIX and DR-NAS+MIX) also maintain the entire dataset in enterprise storage and the active data is assumed to have three copies in the caching layer. The analysis shows that MixApart can be more efficient than HDFS; using the NAS+MIX scheme, half of the dataset can be analyzed while using less storage than HDFS. <ref type="bibr">1</ref> An analysis of the reliability and availability of the different configurations is beyond the scope of this paper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">Estimated Capacity</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">Running Individual Jobs</head><p>We use a simple pageview count microbenchmark (derived from the standard grep) that aggregates page views for a given regular expression, from a Wikipedia dataset <ref type="bibr">[1]</ref>. We use 12 days of Wikipedia statistics as the dataset; the data is 82 GB uncompressed (avg. file size is 300 MB) and 23 GB compressed (avg. file size is 85 MB). We run the job on both uncompressed and compressed input. Tasks processing uncompressed data are more I/O intensive than when input is compressed. Namely, for uncompressed, the effective map task I/O rates are roughly 50 Mbps. For compressed, I/O rates are 20 Mbps (due to higher CPU use). We denote runs on uncompressed data as I/O-intensive, and runs on compressed data as CPU-intensive.</p><p>We run each job in isolation, with various ratios of data reuse. The reuse ratio is the fraction of job input data in the XDFS cache at job submission time. HDFS uses 246 GB of storage capacity for uncompressed data, and 69 GB for compressed. MixApart uses 82 GB, and 23 GB for the cache, respectively, when the entire dataset is analyzed. As expected, the cache storage needs for MixApart are one third of HDFS capacity needed. <ref type="figure" target="#fig_2">Figure 3</ref> shows that: (i) overlapping the computation with data ingest improves performance, (ii) data reuse allows MixApart to match the performance of Hadoop+HDFS, and (iii) scheduling data transfers using compute-awareness enables efficient bandwidth use.</p><p>Data Ingest: We compare the performance of MixApart with no data in the cache (denoted MixApart-cold) to Hadoop (denoted Hadoop+ingest) - <ref type="figure" target="#fig_2">Figure 3(a)</ref>. With Hadoop+ingest, data is ingested into HDFS before a job starts. By overlapping the computation and data transfers, MixApart reduces durations by approximately 16% for the I/O-intensive job and by 28% for the CPUintensive job. Having shown that MixApart enables faster computations by overlapping compute and data ingest, in the following experiments, we compare MixApart with an ideal version of Hadoop that has all its input data in HDFS at job submission time.</p><p>Caching and Scheduling: Figures 3(b) and 3(c) show job durations for various data reuse ratios. Durations decrease with higher reuse as a bulk of the data is fetched from local disks, thereby avoiding the NFS bottleneck. Specifically, MixApart with 0.8 reuse matches the performance of Hadoop with all data in HDFS. The compute-aware data transfer scheduler improves performance by scheduling data transfers just-in-time.</p><p>Feasibility Analysis: Furthermore, the results are consistent with the analysis presented in Section 3. For I/O-intensive, the NFS server with 1 Gbps of bandwidth can sustain 20 parallel tasks (map task I/O rates are 50 Mbps). With 0.8 data reuse ratio, 80 parallel tasks can use local disks and 20 parallel tasks can use the NFS server to achieve the same performance as Hadoop with all data in HDFS. For CPU-intensive, lower I/O rates allow MixApart to match stand-alone Hadoop performance starting with 0.6 data reuse ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.4">Running Job Mixes</head><p>Next, we run mixes of two concurrent jobs to show the benefits of MixApart compute and data transfer schedulers. <ref type="table">Table 1</ref> summarizes the job mixes. To show the efficacy of MixApart, we build mixes using variations of the pageview job on different subsets of the Wikipedia dataset. <ref type="figure">Figure 5</ref> shows job durations for the different mixes. We normalize job times to the time taken by Hadoop to complete a job. We do not include the HDFS data ingest time in these experiments. Despite fetching data on-demand into the cache, MixApart compares well to Hadoop.</p><p>Mix-1: <ref type="figure">Figure 5</ref>(a) shows durations when we submit two jobs, A 1 that has high data reuse and is I/O intensive, and B 1 that has low reuse and is CPU intensive. We mark A 1 to be high priority. Hence, A 1 uses the entire compute tier with its data served from the XDFS cache; the data transfer scheduler issues transfer requests for B 1 's input data while A 1 is running. As most data is transferred proactively, job durations are similar. On MixApart, A 1 takes 2% more time and B 1 takes 3% more time.</p><p>Mix-2: This mix is similar to Mix-1 but the two jobs have equal priorities. MixApart schedules A 2 to run in parallel with B 2 . Input data for A 2 is entirely in the cache; for B 2 , the data transfer scheduler coordinates just in time transfers from shared storage -tasks are scheduled as soon as data transfers are initiated. <ref type="figure">Figure 5</ref>(b) shows similar results with both frameworks -A 2 is 6% faster with MixApart than Hadoop and B 2 is 7% slower.</p><p>Mix-3: This mix - <ref type="figure">Figure 5</ref>(c), has jobs A 3 , that has low data reuse and is I/O intensive, and B 3 , with low reuse and CPU intensive. The jobs have equal priorities. MixApart schedules tasks from both jobs to run in parallel as data is being transferred from shared storage. The data transfer scheduler maximizes the number of parallel transfers, given the available storage bandwidth, using per-job I/O rates. As storage bandwidth is the bottleneck, A 3 runs 11% slower in MixApart while the CPU intensive B 3 runs 6% faster in MixApart. Being CPU intensive, the latency to transfer data to the cache is amortized by the higher CPU cost of the computation.</p><p>Mix-4: In the first three mixes, the two jobs have similar run times with Hadoop and MixApart. For MixApart, the data transfer scheduler ensures that tasks have their input data in the cache as needed, by scheduling transfers accordingly. In Mix-4 (shown in <ref type="figure">Figure 5(d)</ref>  <ref type="bibr">Mix-4</ref> Figure 5: Job Mixes. We show job durations for two concurrent MapReduce jobs running on Wikipedia statistics data, under various scenarios.</p><p>Note that the HDFS data has been ingested prior to the experiments; adding the ingest time substantially increases the job time. Despite the handicap, MixApart performs comparably to Hadoop while providing greater storage efficiency. Mix-1 ( <ref type="figure">Figure 5(a)</ref>) runs job A 1 with high data reuse and job B 1 with low data reuse; B 1 waits for A 1 to finish. Mix-2 ( <ref type="figure">Figure 5(b)</ref>) runs A 2 with high reuse and B 2 with low reuse; A 2 and B 2 run in parallel. Mix-3 <ref type="figure">(Fig 5(c)</ref>) runs A 3 and B 3 both with low data reuse; A 3 and B 3 run in parallel. Mix-4 ( <ref type="figure">Figure 5(d)</ref>) runs A 4 that has low data reuse and B 4 that has high reuse; A 4 and B 4 run sequentially with Hadoop and in parallel with MixApart (due to the work-conserving scheduler). with Hadoop, as we ignore the data ingest into HDFS. B 4 , however, runs 37% faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.5">Running Production Traces</head><p>We use the SWIM framework <ref type="bibr" target="#b14">[17]</ref> to evaluate MixApart with traces of MapReduce jobs collected from production environments; SWIM also provides a replay tool to re-run the traces in our environment. The traces preserve workload characteristics such as job input, intermediate, and output data sizes, and job submission patterns. SWIM also provides support for scaling down a trace captured from a large cluster to a smaller cluster, e.g., by scaling down job data sizes, while preserving the job compute to I/O ratios. We use the Facebook trace captured from a 3000-machine Hadoop cluster over a period of 1.5 months in 2010, consisting of 24 intervals of 1 hour job runs, sampled randomly from the original longer trace <ref type="bibr" target="#b13">[16]</ref>. The trace is dominated by small jobs, with more than 90% of the jobs having a single Map task; the remaining jobs' input sizes range from 100s of GBs to TBs. Previous studies have shown this to be an attribute of most Hadoop environments <ref type="bibr" target="#b13">[16]</ref>.  Trace characteristics: <ref type="figure" target="#fig_5">Figure 6</ref> shows that while the data reuse ratio varies over time, there is a moderate amount of reuse throughout the entire trace. We note that the Facebook trace only contains input path information but does not contain output path information, hence, the reuse estimates are conservative. We choose three 1-hour segments to evaluate MixApart: Hour-1 with 0.09 data reuse ratio identified as low reuse; Hour-5 with 0.48 data reuse ratio identified as moderate reuse; and Hour-8 with 0.83 data reuse ratio identified as high reuse. We split each 1-hour trace into two sub-traces: the first 10 minutes are used to warm up the XDFS cache, and the remaining 50 minutes for the actual run. We note that a low percentage of the total data accessed in the actual run is represented by data transferred during warm-up: 1.6% for the low reuse trace, 5.9% for the moderate reuse trace, and 0.8% for the high reuse trace. We further scale the trace attributes, i.e., job sizes, to reflect the lower sized 100-core cluster that we use in our evaluation.</p><p>Results: <ref type="figure">Figures 7(a)-7(c)</ref> show the cumulative distribution function of job durations for Hadoop and MixApart. <ref type="table" target="#tab_5">Table 2</ref> shows job duration averages for each of the three trace segments. The results show that MixApart matches the performance of Hadoop for workloads containing moderate and high data reuse. For the low data reuse trace segment, 35% of the jobs run slower with MixApart - <ref type="figure">Figure 7(a)</ref>. On average, MixApart is about 10% slower for the low reuse trace. The compute tier schedules more than 50 tasks to run in parallel, due to proactive transfers and overall data reuse effects.</p><p>Figures 7(d)-7(f) show the cumulative distribution function of compute tier utilization in number of concurrent map and reduce tasks running at any given time. In general, MixApart matches the number of tasks run in parallel for both the Map and Reduce phases. The only divergence occurs when the workload has low data reuse: Hadoop achieves a higher compute tier utilization by running 90+ map tasks for 5% of the time. As expected, the Reduce phase performance is not affected.</p><p>Detailed Analysis: We gain insights into the performance of MixApart by studying the task concurrency at the compute layer and the data transfer concurrency in the cache layer. MixApart allows a fraction of the tasks to run reusing cached data, while scheduling remaining tasks to be started just in time as data is transferred from shared storage. <ref type="figure">Figure 8</ref> shows the compute layer parallelism; we note that a majority of large jobs have map task I/O rates in the range of 20-30 Mbps, thereby enabling MixApart to schedule 40-50 parallel transfers from shared storage. This allows the compute tier to schedule more than 50 tasks to run in parallel, due to proactive transfers and overall data reuse effects. We also study the data transfer statistics. <ref type="figure">Figures 8(d)</ref>-8(f) show shared storage bandwidth utilization and total data transferred from shared storage over time. The low reuse trace transfers approximately 100 GB from shared storage with most of the data transfers occurring after the 30 minute mark. With higher data reuse and lower workload intensity, the moderate reuse trace transfers about 35 GB from shared storage. The high reuse trace transfers about 50 GB, due to a higher workload intensity.</p><p>Analysis of the Slowdown: We now focus on analyzing the causes of the difference between the performance of MixApart and Hadoop on the low reuse trace, where Hadoop schedules more concurrent map tasks than MixApart for around 5% of the time. We focus on a subset of jobs that create the bottleneck in MixApart. <ref type="figure" target="#fig_8">Figure 9</ref> plots job sizes (as number of tasks) and job durations by job id for this subset of jobs from the low reuse trace. The figure shows that, while most of these jobs are small in size, some are larger class jobs, containing 10s to 100s of tasks. The high degree of task concurrency, coupled with higher than average I/O rates for these tasks cause periods of saturation of the shared storage bandwidth we allocate for MixApart. Periods of limited concurrency at storage have some impact on the available concurrency degree within the compute tier as well. As there is low data reuse across jobs in this trace, most existing tasks for both large and small jobs are backlogged, waiting on I/O requests to be scheduled. The upper part of <ref type="figure" target="#fig_8">Figure 9</ref> shows these queueing effects for both Hadoop and MixApart. However, the queueing effects in Hadoop happen due to saturation of the compute tier, while MixApart's limitation is due to the I/O scheduling policy coupled with low reuse rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Workload Isolation with MixApart</head><p>We study the impact of integrating data analytics and enterprise workloads in a traditional data center setup, focusing on the effects of sharing the storage system. For this purpose, we capture and replay the storage-level I/O requests of the Facebook low reuse trace; the low reuse trace is the most I/O intensive out of the three traces. As enterprise workloads, we generate I/O intensive random and sequential workloads using the flexible I/O tester <ref type="bibr" target="#b2">[5]</ref>. We also replay a segment from enterprise storage traces collected in a production environment at Microsoft <ref type="bibr" target="#b22">[25]</ref>; specifically, we select a 2-hour segment with high I/O intensity relative to the entire Microsoft trace set. The goal of this experiment was simply to validate the isolation between MapReduce workloads and enterprise workloads when running concurrently on shared storage.</p><p>We use quanta-based I/O scheduling, as it has been shown to guarantee performance isolation <ref type="bibr" target="#b25">[28]</ref> the results were similar. <ref type="figure" target="#fig_1">Figure 10</ref> shows the results for a 500 ms quantum and a 90/10 time split for enterprise and analytics workloads, respectively. The random and sequential I/O intensive workloads validate the performance isolation enabled by quanta scheduling. The Microsoft traces are less I/O intensive than the random and sequential microbenchmarks. Hence, running analytics workloads concurrently with the regular MSR storage workload incurs negligible effects on these (MSR) enterprise workloads. We further replay the Microsoft trace segment at artificially generated higher speeds <ref type="bibr">(MSR 6x and MSR 10x)</ref> to increase I/O intensity and show the performance impact of concurrently running data analytics in stress test enterprise situations. As with the random and sequential microbenchmarks, we see that quanta scheduling ensures performance isolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Overhead of Cache Invalidations</head><p>Updates to data in shared storage trigger cache invalidations, potentially reducing the effective cache hit rate for MixApart. The current system is architected for analyzing unstructured data such as logs, and/or corporate documents, produced by append-only enterprise workloads, and/or workloads with low rates of data overwrites. The append-only workloads are similar to workloads at Facebook, i.e., streams of user data, and are therefore backed by the Facebook trace evaluation. Moreover, recent studies of network file system workloads <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b15">18]</ref> corroborate that overwrite rates are low in corporate environments, relative to the total enterprise data. While we do not evaluate the performance effects of cache invalidations, these effects are expected to be negligible for the types of workloads the current MixApart design targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Summary of Results</head><p>The results show that (i) our job trace selection and cluster sizing was appropriate for Hadoop to effectively use the compute tier with all data in HDFS, while allowing us to explore some variety in workload behavior at the same time, (ii) MixApart benefits from high data reuse and from jobs being compute intensive, on average, matching the performance of ideal Hadoop in almost all production scenarios studied, and (iii) the compute parallelism and performance achievable with MixApart is limited only when the I/O demands of all jobs saturate the shared storage bandwidth, and there is low data reuse across jobs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>As the MapReduce paradigm becomes widely adopted within enterprise environments, the drawbacks of a purpose-built filesystem become glaring. Recent work has looked at techniques to interface Hadoop with enterprise filesystems <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b24">27]</ref> as well as studied methods to provide enterprise storage features on top of existing MapReduce frameworks <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b17">20]</ref>. Others have studied the benefits of in-memory caching for MapReduce workloads <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b27">30]</ref>.</p><p>MixApart extends and complements existing work. We leverage the insights of Ananthanarayanan et al. <ref type="bibr" target="#b7">[10]</ref> to argue for a disk caching layer. Moreover, we enable analytics on enterprise filesystems by leveraging a caching layer for acceleration without any changes to existing networked storage systems, while others <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b24">27]</ref> modify enterprise filesystems (i.e., PVFS/GPFS) to support analytics workloads. In this sense, we believe that MixApart allows enterprise customers to leverage the infrastructure that has been deployed without additional hardware costs or downtime to existing storage systems. Modified Cluster Storage Architectures: A body of recent efforts analyzed the aspects of incorporating Hadoop with existing storage systems <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b24">27]</ref>. These works layer Hadoop's MapReduce compute tier on top of clustered filesystems -e.g., by enhancing GPFS/PVFS with large blocks and data location awareness capabilities. In contrast, with MixApart, we enable scalable, decoupled data analytics primarily for data stored in enterprise storage systems, through caching and prefetching. Enhanced MapReduce Distributions: Enhanced distributions of Hadoop aim at incorporating enterprise file system features into HDFS. For instance, the Hadoop distribution from MapR Technologies <ref type="bibr" target="#b3">[6]</ref> offers NFS access to the underlying data. In the same vein, erasure coding within HDFS has been explored recently <ref type="bibr" target="#b17">[20]</ref>. While these attempts provide basic features needed for enterprise environments, more time and effort are required to enhance commodity distributed file systems to the level of their enterprise counterparts. Caching for Analytics: In-memory caching for data analytics workloads has been shown to improve performance, in general, due to data reuse <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b27">30]</ref>. We leverage and extend this observation to showcase the feasibility of MixApart; we improve the caching efficiency by using disk-based caches. By and large, works that optimize performance for specific job classes <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b27">30]</ref>, can be layered on top of MixApart, just as they would be with frameworks such as Hadoop. Location-aware Scheduling: The MixApart data-aware compute scheduler builds on previous work for locationaware compute scheduling <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b26">29]</ref>. In particular, we adapt the Hadoop task scheduler to be work-conserving, and to work in concert with the XDFS data transfer scheduler, by assigning tasks as soon as a transfer of data from shared storage is initiated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusions and Future Work</head><p>MixApart is a flexible data analytics framework that allows enterprise IT to meet its data management needs while enabling decoupled scale-out of the analytics compute cluster. MixApart achieves these goals by using an on-disk caching layer at the compute nodes and intelligent schedulers to utilize the shared storage efficiently.</p><p>We show that MixApart can reduce job durations by up to 28% compared to the traditional ingest-thencompute method. When the ingest phase is ignored for HDFS, MixApart closely matches the performance of Hadoop at similar compute scales. At the same time, MixApart uses a stateless disk cache without data replication within the compute cluster. We expect that the separation of concerns in this simple, decoupled design allows the most functional value in the following two realistic cases. First, MixApart can be used to support the customer option to leverage clouds for analytics, while maintaining the data within their private data center. Second, MixApart can be used to enable selective and transparent cache block refresh when the underlying enterprise data changes; this is an elegant solution for maintaining update consistency for analytics without modifying application semantics or manual interventions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Compute Cluster Size. We show the number of parallel map tasks sustained by the cache + shared storage architecture for MapReduce analytics. Labels represent average map task durations -e.g., 0.5s for I/O intensive tasks. We plot values for cache-storage bandwidths of (a) 1Gbps, and (b) 10Gbps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Individual Jobs. We show job durations for a MapReduce job running on a Wikipedia dataset. The CPU-intensive experiment processes the compressed data; the I/O-intensive processes the uncompressed data. In (a) we run MixApart with a cold cache (MixApart-cold). In (b), (c) we run MixApart with a warm cache. MixApart-DS has the data transfer scheduler disabled, i.e., parallel prefetching based on task I/O rates is not performed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Estimated Storage Capacity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Facebook Trace Characteristics. The data reuse ratio of the trace changes over different time periods. We replay Hour-1, Hour-5 and Hour-8 to cover a broad spectrum of trace characteristics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Facebook Trace. We show job times and task concurrency for the three Facebook trace segments. Figures 7(a)-7(c) show the job running times. Figures 7(d)-7(f) show the task concurrency of both map and reduce tasks. The results show that MixApart matches the performance of Hadoop for the moderate and high data reuse trace segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figures 7(d)-7(f) show the cumulative distribution function of compute tier utilization in number of concurrent map and reduce tasks running at any given time. In general, MixApart matches the number of tasks run in parallel for both the Map and Reduce phases. The only divergence occurs when the workload has low data reuse: Hadoop achieves a higher compute tier utilization by running 90+ map tasks for 5% of the time. As expected, the Reduce phase performance is not affected. Detailed Analysis: We gain insights into the performance of MixApart by studying the task concurrency at the compute layer and the data transfer concurrency in the cache layer. MixApart allows a fraction of the tasks to run reusing cached data, while scheduling remaining tasks to be started just in time as data is transferred from shared storage. Figure 8 shows the compute layer parallelism; we note that a majority of large jobs have map task I/O rates in the range of 20-30 Mbps, thereby enabling MixApart to schedule 40-50 parallel transfers from shared storage. This allows the compute tier to schedule more than 50 tasks to run in parallel, due to proactive transfers and overall data reuse effects. Figures 8(a)-8(c), 7(d)-7(f) also illustrate per-trace job submission rate (workload intensity). The low data reuse trace has moderate workload intensity. The moderate reuse trace has low intensity, with less than 20 concurrent map tasks running for 80% of the time -Figure 7(e). The high reuse trace exhibits high workload intensity, as 30% of the time there are more than 80 map tasks running with both MixApart and Hadoop -Figure 7(f). We also study the data transfer statistics. Figures 8(d)-8(f) show shared storage bandwidth utilization and total data transferred from shared storage over time. The low reuse trace transfers approximately 100 GB from shared storage with most of the data transfers occurring after the 30 minute mark. With higher data reuse and lower workload intensity, the moderate reuse trace transfers about 35 GB from shared storage. The high reuse trace transfers about 50 GB, due to a higher workload intensity. Analysis of the Slowdown: We now focus on analyzing the causes of the difference between the performance of MixApart and Hadoop on the low reuse trace, where Hadoop schedules more concurrent map tasks than MixApart for around 5% of the time. We focus on a subset of jobs that create the bottleneck in MixApart. Figure 9 plots job sizes (as number of tasks) and job durations by job id for this subset of jobs from the low reuse trace. The figure shows that, while most of these jobs are small in size, some are larger class jobs, containing 10s to 100s of tasks. The high degree of task concurrency, coupled with higher than average I/O rates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Low Data Reuse Trace. With many concurrent large I/O intensive tasks, the storage server gets bottlenecked causing subsequent tasks to be backlogged waiting for I/O requests to be scheduled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>by Algorithm 2 XDFS Compute-Aware Data Transfer Scheduler 1: while AvailStorageBandwidth &gt; 0 do 2: Sort JobQueue based on policy (FIFO, Fair) 3</head><label></label><figDesc>Transfer Scheduler: The data transfer sched- uler prefetches data across multiple jobs to utilize the shared storage bandwidth effectively. It allows admin- istrators to specify bounds on bandwidth consumed</figDesc><table>: 
J = head of JobQueue 
4: 
Sort J's BlockQueue based on block demand 
5: 
B = head of BlockQueue 
6: 
D = LocateAvailableDataNode() 
7: 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>), we sub- mit job A 4 that has low data reuse and is CPU intensive, and job B 4 that has high data reuse and is I/O intensive. We mark A 4 to be high priority. A 4 has about 100 map tasks and it uses the entire compute tier with Hadoop; B 4 waits for A 4 to finish. In contrast, with MixApart, A 4 is bottlenecked on prefetching from shared storage, hence there are idle compute slots available; the work- conserving compute scheduler assigns tasks of B 4 to the idle compute slots. A 4 is 43% slower with MixApart than</head><label></label><figDesc></figDesc><table>0 

0.2 

0.4 

0.6 

0.8 

1 

A 1 B 1 
Time (Normalized) 

MixApart 
Hadoop 

(a) Mix-1 

0 

0.2 

0.4 

0.6 

0.8 

1 

A 2 B 2 
Time (Normalized) 

MixApart 
Hadoop 

(b) Mix-2 

0 

0.2 

0.4 

0.6 

0.8 

1 

A 3 B 3 
Time (Normalized) 

MixApart 
Hadoop 

(c) Mix-3 

0 
0.2 
0.4 
0.6 
0.8 
1 
1.2 
1.4 

A 4 B 4 
Time (Normalized) 

MixApart 
Hadoop 

(d) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Facebook Trace. We report average job durations (in sec-

onds) for the three Facebook trace segments. Only for the low data 
reuse segment, MixApart is approximately 10% slower than Hadoop, 
and similar otherwise. 

</table></figure>

			<note place="foot" n="142"> 11th USENIX Conference on File and Storage Technologies (FAST &apos;13) USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the program committee and the anonymous reviewers for their helpful comments. In particular, we thank our shepherd, Kimberly Keeton, for her suggestions for improving this paper. We also thank Ashvin Goel and Bianca Schroeder for their feedback. Finally, we thank Sethuraman Subbiah, Deepak Kenchammana, and the rest of the Advanced Technology Group (ATG) for their comments on the early ideas and the initial paper drafts.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://aws.amazon.com/directconnect" />
	</analytic>
	<monogr>
		<title level="j">Amazon Direct Connect</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<ptr target="http://tools.ietf.org/html/draft-heizer-cifs-v1-spec-00" />
	</analytic>
	<monogr>
		<title level="j">Common Internet File System Protocol</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I/O</forename><surname>Flexible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tester</surname></persName>
		</author>
		<ptr target="http://linux.die.net/man/1/fio" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mapr</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="http://www.mapr.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Network File System v4</title>
		<ptr target="http://tools.ietf.org/html/rfc5661" />
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openafs</surname></persName>
		</author>
		<ptr target="http://www.openafs.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disk-Locality in Datacenter Computing Considered Irrelevant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pacman</surname></persName>
		</author>
		<title level="m">Coordinated Memory Caching for Parallel Jobs. In NSDI&apos;12</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cloud Analytics: Do We Really Need to Reinvent the Storage Stack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tewari</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotCloud&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Hdfs</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/hdfs" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Mahout</surname></persName>
		</author>
		<ptr target="http://mahout.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Haloop</surname></persName>
		</author>
		<title level="m">Efficient Iterative Data Processing on Large Clusters. In VLDB&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Interactive Analytical Processing in Big Data Systems: a Cross-Industry Study of MapReduce Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Case for Evaluating MapReduce Performance Using Workload Suites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Griffith</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MASCOTS&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design Implications for Enterprise Storage Systems via MultiDimensional Trace Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mapreduce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;04</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DiskReduce: RAID for Data-Intensive Scalable Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tantisiriroj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gibson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PDSW&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Google File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gobiof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP&apos;03</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fet-Terly</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Location-aware Cluster Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kozuch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Gass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>O&amp;apos;hallaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krevat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stroucken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Tashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACDC&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Measurement and Analysis of Large-Scale Network File System Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Pasupathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX&apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Write Off-Loading: Practical Power Management for Enterprise Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowstron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Case for Redundant Arrays of Inexpensive Disks (RAID)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patterson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD&apos;88</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the Duality of Data-intensive File System Design: Reconciling HDFS and PVFS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tantisiriroj</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Argon: Performance Insulation for Shared Storage Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wachs</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abd-El-Malek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename></persName>
		</author>
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Delay Scheduling: a Simple Technique for Achieving Locality and Fairness in Cluster Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Spark</surname></persName>
		</author>
		<title level="m">Cluster Computing with Working Sets. In HotCloud&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">faster are trademarks or registered trademarks of NetApp, Inc. in the United States and/or other countries. Windows is a registered trademark of Microsoft Corporation. Intel and Xeon are registered trademarks of Intel Corporation. All other brands or products are trademarks or registered trademarks of their respective holders and should be treated as such</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Go</forename><surname>Netapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Further</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
