<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:26+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-17, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Farshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Roozbeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Farshin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Roozbeh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosti´c</forename><surname>Dejan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosti´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ericsson Research</orgName>
								<orgName type="laboratory">KTH Royal Institute of Technology and Ericsson Research; Gerald Q. Maguire Jr. and Dejan Kostić</orgName>
								<orgName type="institution" key="instit1">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit2">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit3">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit4">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit5">KTH Royal Institute of Technology</orgName>
								<orgName type="institution" key="instit6">KTH Royal Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks Reexamining Direct Cache Access to Optimize I/O Intensive Applications for Multi-hundred-gigabit Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference</title>
						<meeting>the 2020 USENIX Annual Technical Conference						</meeting>
						<imprint>
							<date type="published">July 15-17, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-14-4 Open access to the Proceedings of the 2020 USENIX Annual Technical Conference is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Memory access is the major bottleneck in realizing multi-hundred-gigabit networks with commodity hardware, hence it is essential to make good use of cache memory that is a faster, but smaller memory closer to the processor. Our goal is to study the impact of cache management on the performance of I/O intensive applications. Specifically, this paper looks at one of the bottlenecks in packet processing, i.e., direct cache access (DCA). We systematically studied the current implementation of DCA in Intel® processors, particularly Data Direct I/O technology (DDIO), which directly transfers data between I/O devices and the processor&apos;s cache. Our empirical study enables system designers/developers to optimize DDIO-enabled systems for I/O intensive applications. We demonstrate that optimizing DDIO could reduce the latency of I/O intensive network functions running at 100 Gbps by up to ~30%. Moreover, we show that DDIO causes a 30% increase in tail latencies when processing packets at 200 Gbps, hence it is crucial to selectively inject data into the cache or to explicitly bypass it.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While the computer architecture community continues to focus on hardware specialization, the networking community tries to achieve greater flexibility with Software-defined Networking (SDN) together with Network Function Virtualization (NFV) by moving from specialized hardware toward commodity hardware. However, greater flexibility comes at the price of lower performance compared to specialized hardware. This approach has become more complex due to the end of Moore's law and Dennard scaling <ref type="bibr" target="#b13">[14]</ref>. Furthermore, commercially available 100-Gbps networking interfaces have revealed many challenges for commodity hardware to support packet processing at multi-hundred-gigabit rates. More specifically, the interarrival time of small packets is * Both authors contributed equally to the paper. † This author has made all open-source contributions.</p><p>shrinking to a few nanoseconds (i.e., less than Last Level Cache (LLC) latency). Consequently, any costly computation prevents commodity hardware from processing packets at these rates, thereby causing a tremendous amount of buffering and/or packet loss. As accessing main memory is impossible at these line rates, it is essential to take greater advantage of the processor's cache <ref type="bibr" target="#b80">[81]</ref>. Processor vendors (e.g., Intel®) introduced new monitoring/controlling capabilities in the processor's cache, e.g., Cache Allocation Technology (CAT) <ref type="bibr" target="#b58">[59]</ref>. In alignment with the desire for better cache management, this paper studies the current implementation of Direct Cache Access (DCA) in Intel processors, i.e., Data Direct I/O technology (DDIO), which facilitates the direct communication between the network interface card (NIC) and the processor's cache while avoiding transferring packets to main memory. Our goal is to complete the recent set of studies focusing on understanding the leading technologies for fast networking, i.e., Peripheral Component Interconnect express (PCIe) <ref type="bibr" target="#b55">[58]</ref> and Remote Direct Memory Access (RDMA) <ref type="bibr" target="#b34">[37]</ref>. We believe that understanding &amp; optimizing DDIO is the missing piece of the puzzle to realize high-performance I/O intensive applications. In this regard, we empirically reverse-engineer DDIO's implementation details, evaluate its effectiveness at 100/200 Gbps, discuss its shortcomings, and propose a set of optimization guidelines to realize performance isolation &amp; achieve better performance for multihundred-gigabit rates. Moreover, we exploit a little-discussed feature of Xeon® processors to demonstrate that fine-tuning DDIO could improve the performance of I/O intensive applications by up to ~30%. To the best of our knowledge, we are the first to: (i) systematically study and reveal details of DDIO and (ii) take advantage of this knowledge to process packets more efficiently at 200 Gbps.</p><p>Why DCA matters? Meeting strict Service Level Objectives (SLO) and offering bounded latency for Internet services is becoming one of the critical challenges of data centers while operating on commodity hardware <ref type="bibr" target="#b51">[54]</ref>. Consequently, it is essential to identify the sources of performance variability in commodity hardware and tame them <ref type="bibr" target="#b48">[51]</ref>. In computer systems, one of these sources of variability is the cache hierarchy, which can introduce uncertainty in service times, especially in tail latencies. Additionally, the advent of modern network equipment <ref type="bibr" target="#b81">[82]</ref> enables applications to push costly calculations closer to the network while keeping &amp; performing only stateful functions at the processors <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b35">38]</ref>, thereby making modern network applications ever more I/O intensive. Hence, taming the performance variability imposed by the cache, especially for I/O, is now more crucial than before. Moreover, as CPU core count goes up, it is important to be able to deliver appropriate I/O bandwidth to them. Therefore, we go one level deeper <ref type="bibr" target="#b60">[61]</ref> to investigate the impact of I/O cache management, done by DCA, on the performance of multi-hundred-gigabit networks.</p><p>Contributions. In this paper, we:</p><p>1 Design a set of micro-benchmarks to reveal little-known details of DDIO's implementation * ( §4), 2 Extensively study the characteristics of DDIO in different scenarios and identify its shortcomings * ( §5), 3 Show the importance of balancing load among cores and tuning DDIO capacity when scaling up ( §6), 4 Measure the sensitivity of multiple applications (i.e., Memcached, NVMe benchmarks, NFV service chains) to DDIO ( §7), 5 Demonstrate the necessity and benefits of bypassing cache while receiving packets at 200 Gbps ( §8), 6 Discuss the lessons learned from our study that are essential for optimizing DDIO-enabled systems receiving traffic at multi-hundred-gigabit rates ( §9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Direct Cache Access (DCA)</head><p>A standard method to transfer data from an I/O device to a processor is Direct Memory Access (DMA). In this mechanism, a processor, typically instructed by software, provides a set of memory addresses, aka receive (RX) descriptors, to the I/O device. Later, the I/O device directly reads/writes data from/to main memory without involving the processor. For inbound traffic, the processor can be informed about newly DMA-ed data either by receiving an interrupt or polling the I/O device. Next, the processor fetches the I/O data from main memory to its cache in order to process the data. For outbound traffic, the processor informs the I/O device (via transmit (TX) descriptors) of data that is ready to be DMA-ed from main memory to the device. The main source or destination of traditional DMA transfers is main memory, see <ref type="figure">Fig. 1a</ref>. However, the data actually needs to be loaded into the processor's cache for processing. Therefore, this method is inefficient and costly in terms of (i) number of accesses to main memory <ref type="bibr" target="#b40">[43]</ref> (i.e., 2n + 5 for n cache lines <ref type="bibr" target="#b40">[43]</ref>), (ii) access latency to the I/O data, and (iii) memory bandwidth usage. Moreover, the negative impact of these inefficiencies becomes increasingly severe with higher link * The source code is available at: https://github.com/aliireza/ ddio-bench Figure 1: Different approaches of DMA for transferring data from an I/O device (e.g., NIC). Red arrows show the path that a packet traverses before reaching the processing core.</p><p>speeds. For instance, a server has 6.72 ns to process small packets at 100 Gbps, whereas every access to main memory takes ~100 ns, 15× more expensive. Therefore, placing the I/O data directly in the processor's cache rather than in main memory is desirable. The advent of faster I/O technologies motivated researchers to introduce Direct Cache Access (DCA) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b40">43]</ref>. DCA exploits PCIe Transaction Layer Packet Processing Hint <ref type="bibr">[30]</ref>, making it possible to prefetch portions of I/O data to the processor's cache, see <ref type="figure">Fig. 1b</ref>. Potentially, this overcomes the drawbacks of traditional DMA, thereby achieving maximal I/O bandwidth and reducing processor stall time. Although this way of realizing DCA can effectively prefetch the desired portions of I/O data (e.g., descriptors and packet header), it is still inefficient in terms of memory bandwidth usage since the whole packet is DMAed into main memory. Additionally, this requires operating system (OS) intervention and support from the I/O device, system chipset, and processor <ref type="bibr" target="#b0">[1]</ref>. To address these limitations and avoid ping-ponging data between main memory &amp; the processor's cache, Intel rearchitected the prefetch hint-based DCA, introducing Data Direct I/O technology (DDIO) <ref type="bibr" target="#b26">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Direct I/O Technology (DDIO)</head><p>Intel introduced DDIO technology with the Xeon E5 family. With DDIO, I/O devices perform DMA directly to/from Last Level Cache (LLC) rather than system memory, see <ref type="figure">Fig. 1c</ref>. DDIO is also known as write-allocate-write-updatecapable DCA (wauDCA) <ref type="bibr" target="#b42">[45]</ref>, as it uses this policy to update cache lines in an n-way set associative LLC, where n cache lines form one set. For packet processing applications, NICs can send/receive both RX/TX descriptors and the packets themselves via the LLC, thereby improving applications' response time &amp; throughput † . DDIO works as follows <ref type="bibr" target="#b38">[41]</ref>: Writing packets. When a NIC writes a cache line to LLC via PCIe, DDIO overwrites the cache line if it is already present in any LLC way (aka a PCIe write hit or write update). Otherwise, the cache line is allocated in the LLC and DDIO writes the data into the newly allocated cache line (aka a PCIe write miss or write allocate). In the latter case, DDIO is restricted to use only a limited portion of LLC when allocating cache lines. It is possible to artificially increase this portion by warming up the cache with processor writes to the address of these buffers, then DDIO performs write-updates <ref type="bibr" target="#b15">[16]</ref>. Reading packets. A NIC can read a cache line from LLC if the cache line is present in any LLC way (aka a PCIe read hit). Otherwise, the NIC reads a cache-line-sized chunk from system memory (aka a PCIe read miss).</p><p>To monitor DDIO and its interaction with I/O devices, Intel added uncore performance counters to its processors <ref type="bibr" target="#b27">[29]</ref>. The Intel Performance Counter Monitor (PCM) tool (e.g., pcm-pcie.x * ) <ref type="bibr" target="#b85">[86]</ref> can count the number of PCIe write hits/misses (represented as an ItoM event) and PCIe read hits/misses (represented as a PCIeRdCur event). Next, we discuss the inherent problem of DDIO, which makes it hard to achieve low-latency for multi-hundred-gigabit NICs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">How can DDIO become a Bottleneck?</head><p>Researchers have shown some scenarios in which DDIO cannot provide the expected benefits <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b82">83]</ref>. Two typical cases occur when new incoming packets repeatedly evict the previously DMA-ed packets (i.e., not-yet-processed and already-processed packets) in the LLC. Consequently, the processor has to load not-yet-processed packets from main memory rather than LLC and the NIC needs to DMA the already-processed packets from the main memory, thereby missing the benefits of DDIO. <ref type="bibr">Tootoonchian et al. referred</ref> to this problem as the leaky DMA problem <ref type="bibr" target="#b82">[83]</ref>. To mitigate this problem, they proposed reducing the number of "in-flight" buffers (i.e., descriptors) such that all incoming packets fit in the limited portion of LLC used for I/O. Thus, performance isolation can be done using only CAT (i.e., cache partitioning). Unfortunately, reducing the number of RX descriptors is only a temporary solution due to increasing link speeds. Multihundred-gigabit NICs introduce new challenges, specifically: 1 Packet loss. At sub-hundred-gigabit link speeds reducing the number of RX descriptors may not result in a high packet loss rate, but at ≥100 Gbps packet loss increases due to the tight processing time budget before buffering/queuing happens. For instance, every extra ~7 ns spent stalling or processing/accessing a packet causes another packet to be buffered when receiving 64-B packets at 100 Gbps. When there are insufficient resources for immediate processing, increasing the number of RX descriptors permits packets to be buffered rather than dropped. Delays in processing might occur because of interrupt handling, prolonged processing, or a sudden increase in the packet arrival rate <ref type="bibr" target="#b16">[17]</ref>; therefore, multi-hundred-gigabit networks cannot avoid packet loss without having a sufficiently large number of descriptors. Increasing the number of processing cores can reduce the packet loss rate, but applications that are compute-or memoryintensive require many cores to operate at the speed of the underlying hardware, e.g., <ref type="bibr">Thomas et al. [81]</ref> mention that * The description of events can be found in <ref type="bibr">[27]</ref> and pp. 63-66 of <ref type="bibr" target="#b38">[41]</ref>. a server performing one DRAM access per packet needs 79 cores to process packets at 400 Gbps. 2 TX buffering. One of the scenarios that makes DDIO inefficient is the eviction of already-processed packets. Reducing the number of RX descriptors may solve this problem for systems that require a small number of TX descriptors, but this is not the case for 100-Gbps NICs. Unfortunately, the de facto medium for DMA-ing packets (i.e., PCIe 3.0) induces some transmission limitations <ref type="bibr" target="#b55">[58]</ref>. Consequently, packets often need to be buffered in the computer system for some time before being DMA-ed to the NIC. This buffering can be realized by either a software queue or increasing the number of TX descriptors <ref type="bibr" target="#b32">[35]</ref>. Unfortunately, either of these alternatives increases the probability of eviction of already-processed packets. Therefore, completely solving the leaky DMA problem requires fine-tuning both the size of the software queue and the number of RX &amp; TX descriptors. 3 PAUSE frames. To alleviate packet loss, one can use Ethernet flow control mechanisms (e.g., PAUSE frames) that cause packets to be buffered earlier in the network, i.e., PAUSE frames stop the previous network node from transmitting packets for a short period. However, these mechanisms are costly in terms of latency, making them less desirable than packet loss for time-critical applications. The minimum and maximum pause duration of a 100-Gbps interface are 5.12 ns and 335.5 µs <ref type="bibr" target="#b53">[56]</ref>. Our measurements show that a core that is simply forwarding packets at 100 Gbps with 1024 RX &amp; TX descriptors causes the NIC to send ~179 k PAUSE frames while receiving ~80 M packets. Dynamic reduction. As reducing the number of RX buffers cannot fully solve the problem and it shifts the problem to another part of the network, most probably the previous node; therefore, an alternative is to dynamically reduce the pressure on the LLC when the number of I/O caused cache evictions starts to increase † . These cache evictions can be tracked by monitoring either PCIe events or the length of the software queue. After detecting a problem, the processor should fetch a smaller number of packets from the NIC (i.e., reducing the RX burst size). Thus, the processor passes fewer free buffers to the NIC, reducing the number of DMA transactions. Unfortunately, this approach does not perform well, hence we need a proactive solution, not a reactive one. Is it sufficient to scale up? Due to the demise of the Dennard scaling <ref type="bibr" target="#b13">[14]</ref>, processors are now shipped with more cores rather than higher clock frequencies. Moreover, the per-core cache quota (i.e., LLC slices) has decreased in recent Xeon processors, i.e., the size of LLC slices reduced from 2.5 MiB to 1.375 MiB in the Xeon scalable family (i.e., Skylake) <ref type="bibr" target="#b52">[55]</ref>. This reduction in per-core cache size directly affects the optimal number of descriptors as these are proportional to the limited space for DDIO. For instance, using 18 cores, each having 256 RX descriptors, requires ~6.5 MiB, which is equal to ~26.6% of the LLC in this processor and greater than the available DDIO capacity (see §4.1). Our approach. To overcome these challenges, it is necessary to study and analyze DDIO empirically in order to make the best use of it. A better understanding of DDIO and its implementation can help us optimize current computer systems and enables us to propose a better DCA design for future computer systems that could accommodate the ever-increasing NIC link speeds. For instance, <ref type="figure">Fig. 2</ref> demonstrates that tuning DDIO's capacity makes it possible to achieve a suitable performance while using a large number of descriptors (our approach), as opposed to using a limited number of descriptors (ResQ's approach proposed by Tootoonchian et al. <ref type="bibr" target="#b82">[83]</ref>). Figure 2: Using more DDIO ways ("W") enables 2 cores to forward 1500-B packets at 100 Gbps with a larger number of descriptors while achieving better or similar tail latency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Understanding Details of DDIO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Occupancy</head><p>Initially, Intel announced that DDIO only uses 10% of LLC <ref type="bibr" target="#b26">[28]</ref> and did not mention what part of the LLC is used (i.e., ways, sets, or slices <ref type="bibr" target="#b14">[15]</ref>). Recent Intel technical reports mention that DDIO only uses a subset of LLC ways, by default two ways <ref type="bibr" target="#b38">[41,</ref><ref type="bibr" target="#b70">72]</ref>. However, it is still unclear whether this "subset" is fixed or whether it can be dynamically selected using a variant of Least Recently Used (LRU) policies <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b86">87]</ref>. Knowledge of these details could avoid I/O contention and optimize performance isolation <ref type="bibr" target="#b82">[83]</ref> by performing precise cache management/partitioning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b61">62]</ref> (e.g., way partitioning with CAT <ref type="bibr" target="#b58">[59]</ref>). This issue becomes increasingly critical for newer generations of Xeon processors that have lower LLC set-associativity (e.g., 11 ways in some Skylake processors, as opposed to 20 ways in Haswell processors), thereby using a larger portion ( 2 11 ≈ 18%) of the LLC for I/O. Lower set-associativity makes the cache less flexible when the LLC is divided into multiple partitions, each of which could be used to accommodate different applications' code &amp; data. To clarify this, we assumed that the ways that are used for DDIO are fixed and then try to confirm this with an experiment in which we co-run an I/O and a cachesensitive application. To increase the pressure on the LLC by DMA-ing more cache lines, we used an L2 forwarding DPDK-based application as the I/O intensive application. Specifically, it receives large packets (1024-B) at a high rate (~82 Gbps) using a large number of RX descriptors (4096 RX descriptors). For the cache-sensitive application, we chose water_nsquared from the Splash-3 benchmark suite <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr">69]</ref> since it performs a large number of LLC accesses; hence, it interferes with the I/O application.</p><p>Each application is run on a different core and CAT is used to allocate different cache ways to each core. We allocate two fixed ways to the I/O application and two variable ways to the cache-sensitive application. To avoid memory bandwidth contention, we also used Memory Bandwidth Allocation (MBA) technology <ref type="bibr" target="#b20">[21]</ref> to limit the memory bandwidth of each core to 40%. <ref type="figure" target="#fig_4">Fig. 3a</ref> shows the CAT configuration used in the experiment. We start by allocating the two leftmost ways (i.e., bitmask of 0x600) to the cache-sensitive application and then we keep shifting the allocated ways one to the right until we cover all the LLC ways while measuring the LLC misses of the I/O application. <ref type="figure" target="#fig_4">Fig. 3b</ref> shows the results of this experiment. These results demonstrate that the cache-sensitive application interferes with the I/O application in two regions. The first (see 0x0C0 in <ref type="figure" target="#fig_4">Fig. 3b</ref>) occurs when the cache-sensitive application uses the same ways as the I/O application, due to the code/data interference of the two applications. However, the second (see 0x003 in <ref type="figure" target="#fig_4">Fig. 3b</ref>) cannot be explained with this same argument since the I/O application is limited to using other ways (i.e., 0x0C0). Furthermore, since the CPU socket is isolated, no other application can cause cache misses. CAT only mitigates the contention induced by code/data not DDIO. Therefore, we conclude that the second interference is most probably due to I/O, which means DDIO uses the two rightmost ways in LLC (i.e., bitmask of 0x003). The interference is proportional to the number of received packets per second × average packet size. We expected to see roughly the same amount of cache misses for bitmasks of 0x180 and 0x060, as they are completely symmetrical in terms of way occupancy. However, the undocumented LRU policy of the CPU may affect how the application uses the cache ways.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">I/O Contention</head><p>One of the established mechanisms to ensure performance isolation and mitigate cache contention is CAT, which limits different applications to a subset of LLC ways. However, §4.1 showed that DDIO uses two fixed LLC ways. Therefore, isolating applications using CAT may not fully ensure performance isolation, due to cache contention caused by I/O. Such contention may occur in two common scenarios:</p><p>1 I/O vs. Code/Data. When an application is limited to using those ways which are also used by DDIO, then cache lines allocated in LLC for DDIO may evict the code/data of any application (i.e., either I/O or non-I/O application). This issue was discussed by Tootoonchian et al. <ref type="bibr" target="#b82">[83]</ref>. Their proposed framework, ResQ, uses only 90% of LLC to avoid interfering with DDIO's reserved space, but does not mention which part of LLC is isolated. §4.1 showed the destructive (i.e., ~2.5×) impact on the cache misses of the I/O application due to a cache-hungry application overlapping with DDIO, see the rise in cache misses at the right side of <ref type="figure" target="#fig_4">Fig. 3b</ref>. However, it did not show the impact of contention on the cache-hungry application; therefore, we repeated the experiment and measured the cache misses of the cache-sensitive application while using a lighter configuration. <ref type="figure">Fig. 4</ref> illustrates that the cache misses of the cache-sensitive application were similarly adversely affected. Therefore, overlapping any application with DDIO ways in LLC can reduce the performance of both applications. To tackle this, one can isolate the I/O portion of LLC (e.g., the two ways used for DDIO) by using CAT so that applications share the LLC without overlapping with I/O. Comparing <ref type="figure" target="#fig_4">Fig. 3b</ref> and 4, we see that an unexpected rise (almost 3×) in cache misses occurs in a different region (i.e., bitmask of 0x600 in <ref type="figure">Fig. 4</ref> as opposed to bitmask of 0x003 in <ref type="figure" target="#fig_4">Fig. 3b</ref>) when I/O is evicting code/data. Hence, we speculate that CAT does not use a bijective function to map I/O &amp; code/data to ways, thus f : code/data → Ways is not equivalent to g : I/O → Ways . Specifically, I/O evicts code/data when the latter is located in the two leftmost ways whereas code/data evicts I/O when the latter is using the two rightmost ways. Such information is useful to know, as it will give us an understanding of the eviction policy and the default priority of code/data and I/O.</p><p>2 I/O vs. I/O. When multiple I/O applications are isolated from each other with CAT, they could still unintentionally compete for the fixed ways allocated to DDIO. §8.1 elaborates the negative impact of this type of contention.</p><p>Security implication. Since DDIO uses two fixed ways in LLC, it is possible to extend microarchitectural attacks to extract useful information from I/O data (e.g., <ref type="bibr">NetCAT [44]</ref> and Packet Chasing <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b75">77]</ref>). Furthermore, I/O applications can be vulnerable to performance attacks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">DMA via Remote Socket</head><p>According to Intel <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">32]</ref>, the current implementation of DDIO only affects the local socket. Consequently, if a core accesses I/O data from an I/O device connected to a remote socket, the data has to traverse the inter-core interconnect, i.e., Intel QuickPath Interconnect (QPI) or Intel Ultra path Interconnect (UPI). It was uncertain whether data traversing the inter-core interconnect is loaded into the LLC of the remote socket or not. We clarified this by running the same experiment discussed in §4.2 while the NIC is connected to a remote socket. The result (removed for brevity) showed that cache misses of neither application were affected by the I/O cache lines, hence packets coming through the UPI links do not end up in the local LLC. Additionally, the cache misses of the I/O application dramatically increased to 20× greater than when receiving packets via the local socket without any contention. Thus, DDIO is ineffective for the remote socket and it pollutes the LLC on the socket connected to the NIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tuning Occupancy and Disabling DDIO</head><p>Although <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b70">72]</ref> mention that DDIO uses two ways by default, there is no mention of whether it is possible to increase or decrease the number of ways used by DDIO. A little-discussed Model Specific Register (MSR) called "IIO LLC WAYS" with the address of 0xC8B * is discussed in a few online resources <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b78">79]</ref> and server manuals <ref type="bibr" target="#b71">[73,</ref><ref type="bibr" target="#b72">74]</ref>. For Skylake, the default value of this register is equal to 0x600 (i.e., two bits set). While these bits cannot be unset, it is possible to set additional bits and the maximum value for this register on our CPU is 0x7FF (i.e., 11 bits set: the same as the number of LLC ways). New values for this register follow the same format as CAT bitmasks. On * One can read/write this register via msr-tools (e.g., rdmsr and wrmsr). a processor with the Skylake microarchitecture, these new values should contain consecutive ones, while the Haswell microarchitecture does not require this (i.e., allowing any value in [0x60000, 0xFFFFF]).</p><p>To see whether this MSR register has an effect on performance, we measured the PCIe read/write hit rates (i.e., ItoM and PCIeRdCur events) while using different values for IIO LLC WAYS. We calculate the hit rate based on the number of hits and misses during an experiment where an I/O application processes packets of 1024 B at 100 Gbps while using 4096 RX descriptors. <ref type="figure">Fig. 5</ref> shows that increasing the value of this MSR register leads to a higher PCIe read/write hit rate. This suggests that increasing the value of this register could improve the ability of the system to handle packets at high rates. We believe that the value of this register is positively correlated with the fraction of LLC used by DDIO. Using the technique in §4.1, we could not detect the newly added I/O ways, thus we speculate that the newly added ways follow a different policy (e.g., LRU) than the first two ways used for I/O. Therefore, we assume that the number of bits set specifies the number of ways used by DDIO. Disabling DDIO. DDIO is bundled as a part of Intel Virtualization Technology (Intel VT), hence it is possible to enable/disable it in BIOS for some vendors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b87">88]</ref>. According to <ref type="bibr" target="#b41">[44,</ref><ref type="bibr" target="#b70">72]</ref>, DDIO can be disabled globally (i.e., by setting the Disable_All_Allocating_Flows bit in "iiomiscctrl" register) or per-root PCIe port (i.e., setting bit NoSnoopOpWrEn and unsetting bit Use_Allocating_Flow_Wr in "perfctrlsts_0" register). Some brief discussions of the benefits of disabling DDIO exist <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b77">78]</ref>, but we elaborate this more thoroughly in §7.</p><p>We implemented an element for FastClick, called DDIOTune, which can enable/disable/tune DDIO † .</p><p>This section scrutinizes the performance of DDIO in different scenarios while exploiting the tuning capability of DDIO. The goal is to show where DDIO becomes a bottleneck and when tuning DDIO matters. Therefore, we examined the impact of both system parameters (i.e., #RX descriptors, #cores, and processing time) and workload characteristics (i.e., packet size and rate) on DDIO performance. All of these measurements were done 20 times for both Skylake and Haswell microarchitectures. We observed the same behavior in both cases, but only discuss the Skylake results for the sake of brevity. We initially focus on the performance of an L2 forwarding network function, as an example of an I/O intensive application. Later, we discuss the impact of applications requiring more processing time per packet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Packet</head><p>Size and RX Descriptors §3.1 discussed the negative consequence of a large number of RX descriptors on DDIO performance. This section continues this discussion by looking at the PCIe read/write hit rate metrics for different numbers of RX descriptors and different packet sizes. <ref type="figure">Fig. 6</ref> shows the results of our experiments for PCIe write hit rate. PCIe read hit rates (not included for brevity) demonstrate similar behavior. When packets are &gt;512 B, the PCIe read/write hit rates monotonically decrease with an increasing number of RX descriptors. More specifically, sending 1500-B packets, even with a relatively small number of RX descriptors (i.e., 128), causes 10% misses for both PCIe read and PCIe write hit rates. Furthermore, increasing the number of RX descriptors to 4096 makes DDIO operate at ~40% hit rate, hence 60% of packets require cache allocation and they had to be DMA-ed back to the NIC from main memory rather than LLC. Note that the packet generator is generating packets as fast as possible. Therefore, small packets show the case when the arrival rate is maximal, while large packets demonstrate maximal throughput, see <ref type="figure" target="#fig_7">Fig. 7</ref>  i.e., the number of received/processed packets per second, due to NIC and PCIe limitations. Note that our testbed cannot exceed 90 Gbps when only one core is forwarding packets. Unexpected I/O evictions. In some cases (e.g., 1500-B packets with 128 RX descriptors in <ref type="figure">Fig. 6</ref>), the size of the injected data is smaller than the DDIO capacity (i.e., 187.5 KiB 4.5 MiB). Even taking into account the TX descriptors and the FastClick's software queue, the maximum cache footprint of this workload is ~2 MiB. However, DDIO still experiences ~10% misses. We believe that this behavior may occur when an application cannot use the whole DDIO capacity due to (i) the undocumented cache replacement policy and/or (ii) the cache's complex addressing <ref type="bibr" target="#b14">[15]</ref>, thus multiple buffers may be loaded into the same cache set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Packet Rate and</head><p>Processing Time §5.1 demonstrated that DDIO performs extremely poorly when a core does minimal processing at 100 Gbps. Next, we focus on the worst-case scenario of the previous experiment (i.e., sending 1500-B packets with 4096 RX descriptors) while changing the packet rate. To achieve 100 Gbps, we use two cores. <ref type="figure" target="#fig_8">Fig. 8</ref> shows the PCIe read and PCIe write hit rates. The PCIe read metric results reveal that DDIO performs relatively well until reaching 98 Gbps. However, the PCIe write results indicate that DDIO has to continually allocate cache lines in LLC for 25% of packets at most of these throughputs, due to insufficient space for all of the buffers. Moreover, throughputs above 75 Gbps exacerbate this problem. So far, we analyzed DDIO performance when cores performed minimal processing (i.e., swapping MAC addresses). Now, we analyze DDIO performance for more compute/memory-intensive I/O applications. Memoryintensive applications access memory frequently and execute few instructions per memory access. The time to accessing memory differs depending upon the availability of a cache line in a given part of the memory hierarchy. Therefore, we focus on the number of CPU cycles of the computation; noting that a memory access can be accounted for as given number of cycles. Note that increasing the processing time can change the memory access pattern, as packets continue to be injected by the NIC while some packets are enqueued in the LLC. To see the impact of different packet processing times on the performance of DDIO, we vary the amount of computation per packet by calling the std::mt1993 random number generator multiple times. Ten such calls take ~70 cycles. <ref type="figure">Fig. 9</ref> illustrates the effect of increasing per-packet processing time on the PCIe metrics &amp; achieved throughput. These results demonstrate that increasing processing time slightly improves PCIe read hits rates up to ~60 calls, i.e., 400 cycles. This is expected, as increasing processing makes the application less I/O intensive as the application provides buffers to the NIC at a slower pace. However, increasing processing causes the available processing power (i.e., #cores) to become a bottleneck, substantially decreasing throughput. Similarly, PCIe write hit rates increases after exceeding 60 calls, due to a decrease in throughput &amp; amount of cache injection. Therefore, DDIO performance matters most when an application is I/O bound, rather than CPU/memory bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Numbers of Cores and DDIO Capacity</head><p>When processing power limits an application's performance, the system should scale up/out. However, this scaling can affect DDIO's performance. To see the effect of scaling up, we measured the PCIe metrics while different numbers of cores were forwarding large packets. <ref type="figure">Fig. 10</ref> shows that when an application is I/O intensive, increasing the number   <ref type="figure">Figure 9</ref>: Making an application more compute-intensive results in better PCIe metrics, but lower throughput. In addition to forwarding packets, two cores call a dummy computation, while receiving 1500-B packets with a total of 4096 RX descriptors at 100 Gbps.</p><p>of cores improves the PCIe read/write hit rate, as it enhances the packet transmission rate because of more TX queues and faster consumption of packets enqueued in the LLC.</p><p>To avoid synchronization problems, every queue is bound to one core. However, beyond a certain point (i.e., four cores in our testbed), increasing the number of cores causes more contention in the cache, as every core loads packets independently into the limited DDIO capacity. Furthermore, since newer processors are shipped with more cores, scaling up, even with a small number of RX descriptors, eventually causes the leaky DMA problem-the same problem as having a large number of descriptors (see §3.1). <ref type="figure">Fig. 11</ref> shows PCIe metrics for 1, 2, and 4 cores while changing the number of DDIO ways. Comparing the DDIO performance of different numbers of cores/DDIO ways, we conclude that increasing DDIO capacity leads to similar improvements for PCIe metrics. Therefore, increasing the DDIO capacity rather than the number of cores is beneficial when an application's bottleneck is not processing power or number of TX queues. Unless scaling up happens efficiently, some cores may receive more packets than others, causing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Application-level Performance Metrics</head><p>The previous section focused on the PCIe read/write hit rates and showed that increasing link speed &amp; packet size and the number of descriptors &amp; cores could degrade these metrics. PCIe read/write hit rates represent the percentage of I/O evictions (i.e., the performance of DDIO), but also indirectly affect application performance. The correlation between PCIe metrics and meaningful performance metrics (e.g., latency and throughput) depends on an application's characteristics. For instance, a low PCIe write hit rate can severely affect an application that requires the whole DMA-ed data. Conversely, the impact is much less for an application that needs only a subset of the DMA-ed packet. <ref type="figure">Fig. 2</ref> showed one example of this correlation for the latter case, where the application only accessed the packet header. These results showed that even when an application does not require the whole DMAed data, increasing the number of descriptors (i.e., causing a reduction in PCIe hit rate metrics) could negatively affect the 99 th percentile latency. Note that we observed the same effect at median latency. This section further elaborates this impact in two scenarios where a stateful network function is processing a realistic workload * via 18 cores with a run-tocompletion model <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b92">93]</ref>. The benefits of increasing cache performance are not limited to this model and could be even greater for a pipeline model where fewer cores handle the I/O. Stateful service chain. To evaluate the effect of increasing DDIO capacity, we chose a stateful service chain composed of a router, a network address port translator (NAPT), and a round-robin load balancer (LB) as a suitable chain to exploit hardware offloading capabilities of modern NICs while still keeping state at the processor. In this case, we offload the * We replay the first 400 k packets of a 28-minute campus trace fifty times. The full trace has ~800 M packets with an average size of 981 B.</p><p>routing table of the router to the NIC and only handle the stateful tasks (i.e., NAPT + LB) and the basic functionality of the router in software. We generated 2423 IP filter rules for the campus trace using the GenerateIPFlowDirector element in Metron <ref type="bibr" target="#b35">[38]</ref> and use DPDK's Flow API technology <ref type="bibr" target="#b28">[31]</ref> to offload them into a Mellanox NIC. To examine the impact of load imbalance, we generate two different sets of rules with different load imbalance factors. One distributes the rules among 18 cores in a round-robin manner while the other is load-aware and tries to reduce the flow imbalance in terms of bytes received by every core. We calculated the number of packets received by each core for both cases and the maximum imbalance ratio of a core is 2.78× for the load-aware technique, while the round-robin technique causes 1.69× maximum load imbalance. The load-aware method has a higher load imbalance because we generate rules for the whole trace, but only replay a subset of it. <ref type="figure">Fig. 12</ref> shows the 99 th percentile latency of this chain for different load balancing methods (with different load imbalance ratio), specifically increasing DDIO capacity reduces the 99 th percentile latency by ~21% when the load imbalance is higher. However, when the load imbalance is lower, these improvements reduce to ~2%. A higher load imbalance factor means that a core receives more packets than others, some of which could be evicted while enqueued in the LLC. Hence, it is crucial to realize a good balance to get the most out of DDIO. Furthermore, load imbalance is the root cause of many other performance degradations and is hard to prevent <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Figure 12: DDIO should be carefully tuned when the load imbalance factor is higher. The results shows 99 th percentile latency of a stateful network function while 18 cores are processing mixed-size packets at 100 Gbps. The throughputs were 94 &amp; 97 Gbps for load-ware (higher imbalance) &amp; roundrobin (lower imbalance) experiments, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Is DDIO Always Beneficial?</head><p>The previous section showed that performance could be improved by tuning DDIO for I/O intensive network functions operating at ~100 Gbps. However, these results cannot be generalized, as the improvements are highly dependent on the application's characteristics. Moreover, there may be some applications that do not benefit from DDIO tuning.</p><p>To investigate this, we measure the sensitivity of different applications to DDIO by enabling/disabling it (see §4.4). <ref type="table" target="#tab_4">Table 2</ref> shows the results for four applications/benchmarks: (i) DPDK-based implementation of Memcached developed by Seastar <ref type="bibr" target="#b4">[5]</ref>, (ii) an NVMe benchmarking tool (i.e., fio <ref type="bibr" target="#b3">[4]</ref>), (iii) L2 forwarding application, (iv) a stateful service chain, used in §6, which performs IP filtering in software rather than offloading it to the NIC, and (v) the stateful service chain with round-robin offloading used in §6. We define sensitivity as "Low" if the maximum impact on the performance of an application is ≤ 5%. For Memcached, we use the method recommended by Seastar <ref type="bibr" target="#b1">[2]</ref> with 8 instances of memaslap clients running for 120 s and a Memcached instance with 4 cores. For NVMe benchmarks, we tested a Toshiba NVMe (KXG50PNV1T02) with 4 × 1024-GB SSDs according to <ref type="bibr" target="#b2">[3]</ref>, where we report the average of 10 runs. The L2 forwarding application forwards mixed-size packets, while using 4 cores with a total of 4096 RX descriptors. The stateful service chain without offloading uses RSS to distribute packets among 18 cores (to increase the throughput) with 18 × 256 RX descriptors. The results demonstrate that different applications have different levels of sensitivity to DDIO, which can be exploited by system developers to optimize their system in a multi-tenant environment, where multiple I/O applications co-exist, see §8.1. The most sensitive application is L2 forwarding, which is the most I/O intensive application among these applications and can run at line rate. Some applications (e.g., Memcached) experience less benefit from DDIO, as their performance may be bounded by other bottlenecks. A more detailed sensitivity analysis of different applications remains as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Future Directions for DCA</head><p>Tuning DDIO occupancy was shown to substantially improve the performance of some applications. However, increasing the portion of the cache used for I/O is only a temporary solution for two reasons: (i) I/O is only a part of packet processing and (ii) to achieve suitable performance many networking applications require a large amount of cache memory for code/data. Moreover, many network functions would benefit from performing in-cache flow classification <ref type="bibr" target="#b91">[92]</ref>; hence, there is a trade-off between allocating cache to I/O vs. code/data and this trade-off depends on the application's characteristics &amp; cache size.</p><p>Additionally, since DDIO is way-based, the granularity of partitions is quite coarse in recent Intel processors, due to low set-associativity. Therefore, it is harder to partition the cache fairly between code/data &amp; I/O. These reasons, together with the recent trend in Intel processors of decreasing per-core LLC, eventually make the current implementation of DCA a major bottleneck to achieving low-latency service times. Hence, DCA needs to deliver better performance even with a small fraction of the cache. This makes it necessary to rethink the current DCA designs with an eye toward realizing network services running at multi-hundred gigabits per second. Some possible directions/proposals for future DCA are: 1 Fine-grained placement: adopting CacheDirector <ref type="bibr" target="#b14">[15]</ref> methodology (i.e., sending packets to the appropriate LLC slices) and only sending the relevant parts of these packets to the L2 cache, L1 cache, or potentially CPU registers <ref type="bibr" target="#b25">[26]</ref>; 2 Selective DMA/DCA: only DMA relevant parts of the packet (as required by an application) to the cache and buffer the rest in either main memory, the NIC, or Top-of-Rack switch; and 3 I/O isolation: extend CAT to include I/O prioritization in addition to Code and Data Prioritization (CDP) technology <ref type="bibr" target="#b59">[60]</ref> to alleviate I/O contention. These ideas could be simulated in a cycle accurate simulator (e.g., gem5 <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>), which remains as our future work. Next, we examine one potential solution in the current systems to better take advantage of DDIO.</p><p>8.1 Bypassing Cache §3.1 explained that one way to prevent unnecessary memory accesses and the leaky DMA problem is to reduce the number of descriptors. However, this could increase packet loss and generate more PAUSE frames at high link rates. Unfortunately, both can have a severe impact on the service time as they postpone the service time by at least a couple of microseconds. Taking these consequences into account, we believe future DCA technologies should perform cache injection more effectively: DMA should not be directed to the cache if this would cause I/O evictions; thus, buffering packets in local memory (at a cost of only several hundreds of nanoseconds) is preferable to dropping or enqueuing packets in previous nodes. Additionally, bypassing cache would be beneficial in a multi-tenant scenario where performance isolation is desired. For instance, low-priority and/or low-DDIO-sensitive applications could bypass cache to make room for high-priority and/or high-DDIO-sensitive applications. In addition, one could prioritize <ref type="bibr" target="#b6">[7]</ref> different traffic flows, thus only a subset of received traffic (and hence cores) would use cache for I/O. Implementing a system to prioritize DDIO for different flows either in a programmable switch or modern NICs (e.g., Mellanox Socket Direct Adapters) remains as our future work. Evaluation. To evaluate the benefits of bypassing the cache, we use two methods: (i) disabling DDIO and (ii) exploiting DMA via a remote socket (see §4.3). We set up a 200-Gbps testbed, see <ref type="figure" target="#fig_4">Fig. 13</ref>. We first connect two 100-Gbps NICs to the same socket. Next, we connect one of these NICs to a remote socket. We run two instances of L2 forwarding application located on the first socket, each of which uses 4 cores and one NIC to forward mixedsize packets. We chose four cores per NIC because our earlier experiments (see <ref type="figure">Fig. 10</ref>) showed that DDIO can achieve an acceptable performance while receiving 1500-B packets with four cores. To reduce the contention for cache and memory bandwidth, we apply CAT &amp; MBA to each application (similar to ResQ <ref type="bibr" target="#b82">[83]</ref>). We assume that one of the applications has a higher priority, and we measure its latency in five different scenarios: (i) without the presence of the lowpriority application, (ii) when the low-priority application pollutes the cache via 2-way DDIO (see <ref type="figure" target="#fig_4">Fig. 13a</ref>), (iii) when the low-priority application pollutes the cache via 4-way DDIO, (iv) when the low-priority application bypasses the cache by DMA-ing packets via a remote socket (see <ref type="figure" target="#fig_4">Fig. 13b</ref>), (v) when the low-priority application bypasses the cache via disabled DDIO. <ref type="figure" target="#fig_15">Fig. 14</ref> shows the 99 th percentile latency of the high-priority application-other percentiles show a similar trend with a smaller difference. These results demonstrate that bypassing cache via a remote socket (i.e., case iv) achieves the same latency as when there is no low-priority application (i.e., case i). However, when both applications are receiving traffic via DDIO (i.e., case ii), the 99 th percentile latency degrades ~30%. We observe that bypassing cache has the same benefits as increasing DDIO capacity (i.e., case iii vs. case iv). Furthermore, comparing cases (iv) and (v) indicates that disabling DDIO slightly pollutes the cache (as opposed to bypassing via a remote socket). We speculate that disabling DDIO only affects the packets, not the descriptors. Therefore, we conclude that bypassing cache can result in less variability in performance and potentially better performance isolation. Additionally, it is clearly necessary to tune DDIO capacity when moving toward 200 Gbps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Lessons Learned: Optimization Guidelines</head><p>This section summarizes our key findings, which could help system designers/developers to optimize DDIO for their applications. Furthermore, our study should inspire computer architects to improve DCA's performance by     offering increasing control. Although we focused on packet processing, our work is not limited to network functions. Our investigations could be equally useful in other contexts (e.g., HPC) that require high-bandwidth I/O when transferring data via RDMA and processing with GPUs. We showed that current approaches to avoid DDIO becoming a bottleneck are only temporary solutions and they are inapplicable to multi-hundred-gigabit network applications. We proposed a benchmarking method to understand the unknown &amp; little-discussed details of DDIO. Later, we characterized the performance of DDIO in different scenarios and showed the benefits of bypassing the cache. We concluded that there is no one-size-fits-all approach to utilize DDIO. Our study reveals:</p><p>• The locations of LLC to which DDIO injects data ( §4.1).</p><p>• Co-locating an application's code/data with I/O in the cache could adversely impact its performance ( §4.2).</p><p>• The way that DDIO behavior changes for different system parameters and workload characteristics ( §5).</p><p>• If an application is I/O bound, adding excessive cores could degrade its performance <ref type="figure">(Fig. 10</ref>).</p><p>• If an application is I/O bound, carefully sizing the DDIO capacity can improve its performance and could lead to the same improvements as adding more cores <ref type="figure">(Fig. 11</ref>).</p><p>• If an application starts to become CPU bound, adding more cores can increase its throughput, but then it has to balance load among cores to maximize DDIO benefits <ref type="figure">(Fig. 12</ref>).</p><p>• If an application is truly CPU/memory bound, DDIO tuning is less efficient <ref type="figure">(Fig. 9)</ref>. However, it can be beneficial to buffer in DRAM incoming requests/packets which cannot be processed in time, rather than having the NIC issue PAUSE frames or drop packets.</p><p>• Going beyond ~75 Gbps can cause DDIO to become a bottleneck <ref type="figure" target="#fig_8">(Fig. 8)</ref>. Therefore, it is essential to bypass cache to realize performance isolation. Bypassing cache could be done for low-priority traffic or applications that do not benefit from DDIO ( §8.1).</p><p>• Different applications have different levels of sensitivity to DDIO ( §7). Identifying this level is essential to utilize system resources more efficiently, provide performance isolation, and improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Related Work</head><p>The most relevant work to our study is ResQ <ref type="bibr" target="#b82">[83]</ref>, which we discussed thoroughly in §3.1 and §8.1. This section discusses other efforts relevant to our work. Injecting I/O into the cache. The idea of loading I/O data directly to the processor's cache was initially proposed using cache injection techniques <ref type="bibr" target="#b49">[52,</ref><ref type="bibr" target="#b62">63]</ref>. Later, it was used to enhance network performance on commodity servers and was referred to as DCA <ref type="bibr" target="#b24">[25]</ref>. Amit Kumar et al. <ref type="bibr" target="#b39">[42]</ref> investigated the role of coherency protocol in DCA. Their results indicated that the benefit of DCA would be limited when the network processing rate cannot match the I/O rate. In addition, <ref type="bibr" target="#b73">[75]</ref> showed that DCA could cause cache pollution; hence they proposed an alternative cache injection mechanism to mitigate the problem. A. <ref type="bibr">Kumar et al. [43]</ref> characterized DCA for 10-Gbps Ethernet links. Other works have discussed that DCA is insufficient due to architectural limitations <ref type="bibr" target="#b37">[40,</ref><ref type="bibr" target="#b43">46,</ref><ref type="bibr" target="#b69">71]</ref>. For example, the work in <ref type="bibr" target="#b43">[46]</ref> proposed a new I/O architecture that decouples and offloads I/O descriptor management from the NIC to an on-chip network engine. Similarly, the work in <ref type="bibr" target="#b37">[40]</ref> proposed a flexible network DMA interface which can support DCA. Last but not least, Wen Su et al. <ref type="bibr" target="#b69">[71]</ref> proposed an improvement to combine DCA with an integrated NIC to reduce latency. Efforts toward realizing 100 Gbps. Many have tried to tackle challenges to achieve suitable performance for fast networks, mostly in the context of NFV <ref type="bibr" target="#b46">[49]</ref> and key-value stores <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b42">45]</ref>. Some research has exploited new features in modern/smart/programmable NICs (e.g., <ref type="bibr" target="#b35">[38,</ref><ref type="bibr" target="#b44">47,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b93">94]</ref>) &amp; switches (e.g., <ref type="bibr" target="#b33">[36]</ref>) or proposed new features (e.g., <ref type="bibr" target="#b68">[70]</ref>) to offload costly software processing. A number of works investigate packet processing models (e.g., <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">39,</ref><ref type="bibr" target="#b92">93]</ref>).</p><p>CacheBuilder <ref type="bibr" target="#b79">[80]</ref> and CacheDirector <ref type="bibr" target="#b14">[15]</ref> have discussed the importance of cache management in realizing 100-Gbps networks. HALO <ref type="bibr" target="#b91">[92]</ref> exploited the non-uniform cache architecture (NUCA) characteristics of LLC to perform incache flow classification. Last but not least, IOctopus <ref type="bibr" target="#b67">[68]</ref> proposed a new NIC design and wiring for servers to avoid non-uniform DMA penalties. Our work is complementary to these works. Cache partitioning. Many have tried to overcome cache contention by performing cache partitioning <ref type="bibr" target="#b50">[53]</ref>. These efforts can be split into two main categories: (i) software techniques and (ii) hardware techniques. The former group principally relies on physical addresses to partition cache based on sets <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b66">67]</ref> or slices <ref type="bibr" target="#b14">[15]</ref>. This way of cache partitioning does not require any hardware support, but it is not very commonly used, due to its drawbacks (e.g., OS/App modification and costly re-partitioning). The latter group mostly exploits way-partitioning (e.g., CAT) to partition the cache among different applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91]</ref>. In addition to these techniques, <ref type="bibr">Wang et al. [85]</ref> proposed a hybrid approach that combines both techniques to achieve finer granularity for partitioning. To the best of our knowledge, there are only two works <ref type="bibr">(ResQ [83]</ref> and CacheDirector <ref type="bibr" target="#b14">[15]</ref>) that have specifically tried to exploit cache partitioning techniques to improve packet processing. ResQ proposes to isolate a percentage of LLC that is used for I/O and CacheDirector exploits the NUCA used in Intel processors to distribute I/O more efficiently among different LLC slices. Our work is complementary to these works, as most of them do not consider I/O when partitioning the cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>DCA technologies were introduced to improve the performance of networking applications. However, we systematically showed that the latest implementation of DCA in Intel processors (i.e., DDIO) cannot perform as needed with increasing link speeds. We demonstrated that better I/O management is required to meet the critical latency requirements of future networks. Our main goal is to emphasize that networking is, now more than before, tightly coupled with the capability of the current hardware. Consequently, realizing time-critical multi-hundred-gigabit networks is only possible by (i) increasingly well-documented control over the hardware and (ii) improved holistic system design optimizations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The bitmask used by CAT to allocate LLC ways to the cache-sensitive application (a) CAT configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>0x300 0x180 0x0C0 0x060 0x030 0x018 0x00C 0x006 0x003 Sum of Cache Misses (Million) Ways Allocated by CAT to the Cache-sensitive Application No Contention Contention with I/O App. (Code/Data) Contention with DDIO (b) Sum of cache misses for the I/O application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interference of an I/O and a cache-sensitive application using the parsec_native configuration (to cause a high rate of cache misses) when the cache-sensitive application uses different LLC ways. The rise in the rightmost side shows the contention with DDIO ways.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4: Interference of the cache-sensitive and the I/O applications. Y axis shows the sum of cache misses of the cache-sensitive application. The cache-sensitive application uses a lighter configuration (i.e., ddio_sim), which causes fewer cache misses than the I/O application.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5: Tuning IIO LLC WAYS register increases PCIe read/write hit rates. The achieved throughput is 82-86 Gbps in this experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Increasing the packet size reduces the arrival rate, i.e., the number of received/processed packets per second, due to NIC and PCIe limitations. Note that our testbed cannot exceed 90 Gbps when only one core is forwarding packets. Unexpected I/O evictions. In some cases (e.g., 1500-B packets with 128 RX descriptors in Fig. 6), the size of the injected data is smaller than the DDIO capacity (i.e., 187.5 KiB 4.5 MiB). Even taking into account the TX descriptors and the FastClick's software queue, the maximum cache footprint of this workload is ~2 MiB. However, DDIO still experiences ~10% misses. We believe that this behavior may occur when an application cannot use the whole DDIO capacity due to (i) the undocumented cache replacement policy and/or (ii) the cache's complex addressing [15], thus multiple buffers may be loaded into the same cache set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Increasing packet rates negatively impact the PCIe metrics, when 2 cores forward 1500-B packets with 4096 RX descriptors. The PCIe write metric is more degradation-prone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Increasing the number of cores does not always improve PCIe metrics for an I/O intensive application. Different numbers of cores are forwarding 1500-B packets at 100 Gbps with 256 RX descriptors per core.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Receiver setup to achieve 200 Gbps. On the right setup, the second NIC is connected to the remote socket. It sends packets through UPI link directly to the main memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Bypassing cache and tuning DDIO at 200 Gbps mitigate I/O contention and improve the tail latency of the high-priority application up to 30%. Scenarios: (i) 100 Gbps with no contention; (ii) contention at 200 Gbps; (iii) tuning DDIO at 200 Gbps; (iv) bypassing cache via a remote socket; and (v) bypassing cache via disabled DDIO. The total achieved throughput of the receiver is written on the bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 : Details of our testbed. In each case, the NIC is a Mellanox ConnectX-5 VPI.</head><label>1</label><figDesc></figDesc><table>Machine 

Configuration 
Intel Xeon Processor 
Memory 
Last Level Cache (LLC) 
Model 
Frequency #Cores 
Size 
Associativity 
Packet generator (Skylake) 
Gold 6134 
3.2 GHz 
8 
512 GiB 18×1.375 MiB 
11 
Server (Skylake) 
Gold 6140 
2.3 GHz 
18 
256 GiB 18×1.375 MiB 
11 
Server (Haswell) 
E5-2667 v3 
3.2 GHz 
8 
128 GiB 
8×2.5 MiB 
20 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 : DDIO sensitivity changes for different applications.</head><label>2</label><figDesc></figDesc><table>Application 

DDIO 
Enabled 
Disabled 
Sensitivity 
Throughput 
Median (µs) Avg (µs) 
99 th (µs) 
Throughput 
Median (µs) Avg (µs) 
99 th (µs) 
Memcached (TCP) 
1003058 TPS 
N/A 
477.62 
N/A 
994387 TPS 
N/A 
481.62 
N/A 
Low 
Memcached (UDP) 
638763 TPS 
N/A 
750.12 
N/A 
631354 TPS 
N/A 
758.75 
N/A 
Low 
NVMe (Full Write) 
4427.2 MiB/s 
44879.4 
44437.6 
46452.4 
4434.2 MiB/s 
44827 
44374.68 46452.4 
Low 
NVMe (Random Read) 
3372.4 MiB/s 
582 
589.67 
765.7 
3233.7 MiB/s 
601.8 
614.46 
805.7 
High 
NVMe (Random Write) 
1498.3 MiB/s 
1307.8 
1324.73 
1991.2 
1499.9 MiB/s 
1309.5 
1323.38 
1971.4 
Low 
L2 Forwarding 
98.01 Gbps 
500.82 
662 
1055.98 
87.02 Gbps 
1058.15 
862 
1229.62 
High 
Stateful Service Chain 
63.92 Gbps 
665 
657 
923 
63.25 Gbps 
672 
666 
931 
Low 
(without offloading) 
Stateful Service Chain 
97.35 Gbps 
499 
505 
595 
87.46 Gbps 
531 
924 
1981 
High 
(with round-robin offloading) 

</table></figure>

			<note place="foot">† We will use the terms I/O device and NIC interchangeably.</note>

			<note place="foot">† Our implementation is available at: https://github.com/ tbarbette/fastclick/tree/DMAdynamic</note>

			<note place="foot">† The element is available at: https://github.com/tbarbette/ fastclick/wiki/DDIOTune 678 2020 USENIX Annual Technical Conference USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our shepherd, Mark Silberstein, and anonymous reviewers for their insightful comments. We are grateful to Tom Barbette for helping us with his NPF tool. This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The work was also funded by the Swedish Foundation for Strategic research (SSF). This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 770889).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Direct Cache Access (DCA)</title>
		<ptr target="ftp://supermicro.com/ISO_Extracted/CDR-X8-" />
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Memcached</forename><surname>Benchmark</surname></persName>
		</author>
		<ptr target="https://github.com/scylladb/seastar/wiki/Memcached-Benchmark" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2019" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fio</forename><surname>Benchmarking -Benchmarking Linux With Sysbench</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioping</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unixbench</forename></persName>
		</author>
		<ptr target="https://wiki.mikejung.biz/Benchmarking" />
		<title level="m">Lots of Examples</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I/O</forename><surname>Flexible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tester</surname></persName>
		</author>
		<ptr target="https://fio.readthedocs.io/en/latest/fio_doc.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seastar</surname></persName>
		</author>
		<ptr target="http://seastar.io/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data Direct I/O Characterization for Future I/O System Exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myoungsoo</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://yifanyuan3.github.io/publication/ddio_gem5" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2020" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Method, apparatus, and system for allocating cache using traffic class</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James A</forename><surname>Arellano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coleman</surname></persName>
		</author>
		<idno>App. 14/866</idno>
		<imprint>
			<date type="published" when="2017-03-30" />
			<biblScope unit="page">862</biblScope>
		</imprint>
	</monogr>
<note type="report_type">US Patent</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RSS++: Load and State-Aware Receive Side Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><forename type="middle">P</forename><surname>Katsikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Kosti´ckosti´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Emerging Networking Experiments And Technologies, CoNEXT &apos;19</title>
		<meeting>the 15th International Conference on Emerging Networking Experiments And Technologies, CoNEXT &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="318" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast Userspace Packet Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Soldani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Mathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM/IEEE Symposium on Architectures for Networking and Communications Systems, ANCS &apos;15</title>
		<meeting>the Eleventh ACM/IEEE Symposium on Architectures for Networking and Communications Systems, ANCS &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A High-Speed Load-Balancer Design with Guaranteed Per-Connection-Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Kosti´ckosti´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Papadimitratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Chiesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-02" />
			<biblScope unit="page" from="667" to="683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A case for effective utilization of Direct Cache Access for big data workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsha</forename><surname>Basavaraj</surname></persName>
		</author>
		<ptr target="https://escholarship.org/uc/item/0fr3735b" />
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
		<respStmt>
			<orgName>UC San Diego</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradford</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arkaprava</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somayeh</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rathijit</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Korey</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilay</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Gem5 Simulator. SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">KPart: A Hybrid Cache PartitioningSharing Technique for Commodity Multicores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>El-Sayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="page" from="104" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dark silicon and the end of multicore scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Amant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="365" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Make the Most out of Last Level Cache in Intel Processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Farshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Roozbeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Kosti´ckosti´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys &apos;19</title>
		<meeting>the Fourteenth EuroSys Conference 2019, EuroSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="https://software.intel.com/en-us/articles/financial-services-industry-fsi-frequently-asked-questions" />
		<title level="m">Financial Services Industry (FSI) -Frequently Asked Questions</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Intel Ethernet X520 to XL710 -Tuning the buffers: a practical guide to reduce or avoid packet loss in DPDK applications</title>
		<ptr target="https://etherealmind.com/wp-content/uploads/2017/01/X520_to_XL710_Tuning_The_Buffers.pdf" />
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
	<note>Intel Forum</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ginseng: Market-Driven LLC Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liran</forename><surname>Funaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Orna Agmon Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 USENIX Annual Technical Conference (USENIX ATC 16)</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scale-out ccNUMA: Exploiting Skew with Strongly Consistent Caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasilis</forename><surname>Gavrielatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Katsarakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference, EuroSys &apos;18</title>
		<meeting>the Thirteenth EuroSys Conference, EuroSys &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Intel Xeon Processor E5 Family: Architecture, Power Efficiency, and Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Rowland</surname></persName>
		</author>
		<ptr target="https://www.hotchips.org/wp-content/uploads/hc_archives/hc24/HC24-8-" />
		<imprint>
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Introduction to Memory Bandwidth Allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Herdrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khawar</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Cornu</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/introduction-to-memory-bandwidth-allocation" />
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CAMA: A Predictable Cache-Aware Memory Allocator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Haupenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reineke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 23rd Euromicro Conference on Real-Time Systems</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">How to disable Data Direct I/O (DDIO) on Intel Xeon E5?</title>
		<ptr target="https://forums.intel.com/s/question/0D50P0000490NFhSAM/how-to-disable-data-direct-io-ddio-on-intel-xeon-e5?language=en_US" />
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Introduction to Receive Side Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Hudek</surname></persName>
		</author>
		<ptr target="https://docs.microsoft.com/en-us/windows-hardware/drivers/network/introduction-to-receive-side-scaling" />
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="2019" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Direct cache access for high bandwidth network I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huggahalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tetrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Case for a Network Fast Path to the CPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Workshop on Hot Topics in Networks, HotNets &apos;19</title>
		<meeting>the 18th ACM Workshop on Hot Topics in Networks, HotNets &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intel Data Direct I/O Technology Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/io/data-direct-i-o-technology-brief.html" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Intel Xeon Processor Scalable Memory Family Uncore Performance Monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/programmable/documentation/lbl1415123763821.html#lbl1453336559194" />
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
	<note>Intel Arria 10 Avalon-ST Interface with SR-IOV PCIe Solutions User Guide</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Intel Ethernet Flow Director and Memcached Performance</title>
		<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/intel-ethernet-flow-director.pdf" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<ptr target="https://software.intel.com/en-us/vtune-amplifier-cookbook-io-issues-remote-socket-accesses" />
		<title level="m">IO Issues: Remote Socket Accesses</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Power Management of the Third Generation Intel Core Micro Architecture formerly codenamed Ivy Bridge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Jahagirdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varghese</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inder</forename><surname>Sodhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Wells</surname></persName>
		</author>
		<ptr target="https://bit.ly/2LKVfZr" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">High Performance Cache Replacement Using Re-reference Interval Prediction (RRIP)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">B</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture, ISCA &apos;10</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture, ISCA &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data Plane Development Kit: Performance Optimization Guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthurajan</forename><surname>Jayakumar</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/dpdk-performance-optimization-guidelines-white-paper" />
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">NetCache: Balancing Key-Value Stores with Fast In-Network Caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhou</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Soulé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeongkeun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles, SOSP &apos;17</title>
		<meeting>the 26th Symposium on Operating Systems Principles, SOSP &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="121" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Design Guidelines for High Performance RDMA Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 USENIX Annual Technical Conference (USENIX ATC 16)</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="437" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Metron: NFV Service Chains at the True Speed of the Underlying Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Katsikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Barbette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Kosti´ckosti´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Steinert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Conference on Networked Systems Design and Implementation (NSDI 18), NSDI&apos;18</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="171" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SNF: synthesizing high performance NFV service chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Katsikas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Enguehard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><forename type="middle">Q</forename><surname>Ku´zniarku´zniar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejan</forename><surname>Maguire</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kosti´ckosti´c</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">High Performance Packet Processing with FlexNIC. SIG-PLAN Not</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnamurthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Benchmarking and Analysis of Software Data Planes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciek</forename><surname>Konstantynowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><forename type="middle">M</forename><surname>Shah</surname></persName>
		</author>
		<ptr target="https://fd.io/wp-content/uploads/sites/34/2018/01/performance_analysis_sw_data_planes_dec21_2017.pdf" />
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="2019" to="2026" />
			<pubPlace>Cisco, Intel Corporation, FD.io</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Impact of Cache Coherence Protocols on the Processing of Network Traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huggahalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007)</title>
		<imprint>
			<date type="published" when="2007-12" />
			<biblScope unit="page" from="161" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Characterization of Direct Cache Access on multi-core systems and 10GbE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huggahalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Makineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE 15th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2009-02" />
			<biblScope unit="page" from="341" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">NetCAT: Practical Cache Attacks from the Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Andriesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristiano</forename><surname>Giuffrida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Razavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2020-05" />
		</imprint>
	</monogr>
	<note>Intel Bounty Reward</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">FullStack Architecting to Achieve a Billion-Requests-PerSecond Throughput on a Single Key-Value Store Server Platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Seongil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukhan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<idno>5:1-5:30</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A new server I/O architecture for high speed networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Znu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bnuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 17th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">MICA: A holistic approach to fast in-memory key-value storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Networked Systems Design and Implementation (NSDI 14)</title>
		<meeting><address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="429" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gaining insights into multicore cache partitioning: Bridging the gap between simulation and real systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingda</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sadayappan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 14th International Symposium on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2008-02" />
			<biblScope unit="page" from="367" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Survey of Performance Acceleration Techniques for Network Function Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Linguaglossa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pontarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rétvári</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bifulco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jarschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="746" to="764" />
			<date type="published" when="2019-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Performance Considerations for Packet Processing on Intel Architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://fdio-vpp.readthedocs.io/en/latest/events/Summits/FDioMiniSummit/OSS_2017/2017_05_10_performanceconsideration.html" />
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Taming Performance Variability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Maricq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Duplyakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Maltzahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="409" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The cache injection/cofetch architecture: initial performance evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Milutinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sheaffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Fifth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems</title>
		<meeting>Fifth International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems</meeting>
		<imprint>
			<date type="published" when="1997-01" />
			<biblScope unit="page" from="63" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Survey of Techniques for Cache Partitioning in Multicore Processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nines are Not Enough: Meaningful Metrics for Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Workshop on Hot Topics in Operating Systems (HoTOS)</title>
		<meeting>17th Workshop on Hot Topics in Operating Systems (HoTOS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Intel Xeon Processor Scalable Family Technical Overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mulnix</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview" />
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">What is the potential impact of PAUSE frames on a network connection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netapp</surname></persName>
		</author>
		<ptr target="https://ntap.com/2RpAx1Q" />
		<imprint>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Network Performance Framework</surname></persName>
		</author>
		<ptr target="https://github.com/tbarbette/npf" />
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolf</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Antichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Fernando Zazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>López-Buedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding PCIe Performance for End Host Networking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">Conference of the ACM Special Interest Group on Data Communication, SIGCOMM &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Introduction to Cache Allocation Technology in the Intel Xeon Processor E5 v4 Family</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khang</forename><surname>Nguyen</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/introduction-to-cache-allocation-technology" />
		<imprint>
			<date type="published" when="2016-02" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Code and Data Prioritization -Introduction and Usage Models in the Intel® Xeon® Processor E5 v4 Family</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khang T Nguyen</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/introduction-to-code-and-data-prioritization-with-usage-models" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Always Measure One Level Deeper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="74" to="83" />
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">CoPart: Coordinated Partitioning of Last-Level Cache and Memory Bandwidth for Fairness-Aware Workload Consolidation on Commodity Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsu</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongbeom</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woongki</forename><surname>Baek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys &apos;19</title>
		<meeting>the Fourteenth EuroSys Conference 2019, EuroSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Ramakrishnan Rajamony. Method and apparatus for accelerating input/output processing using cache injections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazim Shafi Patrick Joseph</forename><surname>Bohrer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-03" />
			<biblScope unit="volume">6711650</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title/>
		<ptr target="https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/741386" />
	</analytic>
	<monogr>
		<title level="j">PCIe Bandwidth Drops on Skylake-SP</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive Insertion Policies for High Performance Caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Moinuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aamer</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Patt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Annual International Symposium on Computer Architecture, ISCA &apos;07</title>
		<meeting>the 34th Annual International Symposium on Computer Architecture, ISCA &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="381" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Splash-3: A Properly Synchronized Benchmark Suite for Contemporary Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sakalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leonardsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2016-04" />
			<biblScope unit="page" from="101" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Reducing Cache Misses Using Hardware and Software Page Placement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Supercomputing, ICS &apos;99</title>
		<meeting>the 13th International Conference on Supercomputing, ICS &apos;99<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">IOctopus: Outsmarting Nonuniform DMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Smolyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Markuze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Pismenny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Eran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerd</forename><surname>Zellweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Bolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liran</forename><surname>Liss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
		<meeting>the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="101" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Loom: Flexible and Efficient NIC Packet Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Stephens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019-02" />
			<biblScope unit="page" from="33" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Using Direct Cache Access Combined with Integrated NIC Architecture to Accelerate Network Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE 14th International Conference on High Performance Computing and Communication 2012 IEEE 9th International Conference on Embedded Software and Systems</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Hardware-Level Performance Analysis of Platform I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Sudarikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lu</surname></persName>
		</author>
		<ptr target="https://dpdkprcsummit2018.sched.com/event/EsPa/hardware-level-performance-analysis-of-platform-io" />
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<ptr target="https://www.supermicro.com/manuals/superserver/1U/MNL-1886.pdf" />
		<title level="m">Supermicro. 1028UX-LL1-B8, 1028UX-LL2-B8, and 1028-LL3-B8 User&apos;s Manual</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">6028UX-TR4 User&apos;s Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Supermicro</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">DMA cache: Using on-chip storage to architecturally separate I/O data from CPU data for improving I/O performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA -16 2010 The Sixteenth International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Packet Chasing: Spying on Network Packets over a Cache Side-Channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadkazem</forename><surname>Taram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Tullsen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1909.04841.pdf" />
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2019" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Packet Chasing: Observing Network Packets over a Cache Side-Channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadkazem</forename><surname>Taram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th</title>
		<meeting>the 47th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<title level="m">International Symposium on Computer Architecture, ISCA &apos;20</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Enabling Efficient RDMAbased Synchronous Mirroring of Persistent Memory Transactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Tavakkol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aasheesh</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanko</forename><surname>Novakovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaveh</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Gómez-Luna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasan</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Barthels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singla</surname></persName>
		</author>
		<idno>abs/1810.09360</idno>
	</analytic>
	<monogr>
		<title level="m">Pratap Subrahmanyam, and Onur Mutlu</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<ptr target="https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/600913" />
		<title level="m">Temporary PCIe Bandwidth Drops on Haswell-v3</title>
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dark Packets and the End of Network Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelby</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Symposium on Architectures for Networking and Communications Systems, ANCS &apos;18</title>
		<meeting>the 2018 Symposium on Architectures for Networking and Communications Systems, ANCS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">CacheCloud: Towards Speed-of-light Datacenter Communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shelby</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 18)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The Case For InNetwork Computing On Demand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Tokusashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pedone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Soulé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zilberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference 2019, EuroSys &apos;19</title>
		<meeting>the Fourteenth EuroSys Conference 2019, EuroSys &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">ResQ: Enabling SLOs in Network Function Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurojit</forename><surname>Amin Tootoonchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Argyraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18)</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018-04" />
			<biblScope unit="page" from="283" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Lynx: A SmartNIC-Driven Accelerator-Centric Architecture for Network Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maroun</forename><surname>Tork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Maudlej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Silberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20</title>
		<meeting>the TwentyFifth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="117" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">SWAP: Effective Fine-Grain Management of Shared Last-Level Caches with Minimum Hardware Support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Setter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Martínez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017-02" />
			<biblScope unit="page" from="121" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Intel Performance Counter Monitor -A Better Way to Measure CPU Utilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Willhalm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Dementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Fay</surname></persName>
		</author>
		<ptr target="https://software.intel.com/en-us/articles/intel-performance-counter-monitor" />
		<imprint>
			<date type="published" when="2017-01" />
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Intel Ivy Bridge Cache Replacement Policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Wong</surname></persName>
		</author>
		<ptr target="http://blog.stuffedcow.net/2013/01/ivb-cache-replacement/" />
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xeon</surname></persName>
		</author>
		<ptr target="https://forums.intel.com/s/question/0D50P0000490VP0SAM/xeon-e5-disable-ddio-in-os?language=en_US" />
		<imprint>
			<biblScope unit="page" from="2019" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">DCAPS: Dynamic Cache Allocation with Partial Sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaocheng</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenlin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference, EuroSys &apos;18</title>
		<meeting>the Thirteenth EuroSys Conference, EuroSys &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">dCat: Dynamic Cache Management for Efficient, Performance-sensitive Infrastructure-as-a-service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthick</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wesley</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference, EuroSys &apos;18</title>
		<meeting>the Thirteenth EuroSys Conference, EuroSys &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">vCAT: Dynamic Cache Management Using CAT Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Thi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Real-Time and Embedded Technology and Applications Symposium (RTAS)</title>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">HALO: Accelerating Flow Classification for Scalable Packet Processing in NFV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture, ISCA &apos;19</title>
		<meeting>the 46th International Symposium on Computer Architecture, ISCA &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="601" to="614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A Closer Look at NFV Execution Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Asia-Pacific Workshop on Networking 2019, APNet &apos;19</title>
		<meeting>the 3rd Asia-Pacific Workshop on Networking 2019, APNet &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">NetFPGA SUME: Toward 100 Gbps as Research Commodity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zilberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Audzevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="41" />
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
