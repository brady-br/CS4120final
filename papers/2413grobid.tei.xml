<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Neural Machine Translation in English-Japanese Task</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Zhu</surname></persName>
							<email>chugen.shu@weblio.co.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Weblio Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Neural Machine Translation in English-Japanese Task</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we evaluate Neural Machine Translation (NMT) models in English-Japanese translation task. Various network architectures with different recurrent units are tested. Additionally , we examine the effect of using pre-reordered data for the training. Our experiments show that even simple NMT models can produce better translations compared with all SMT baselines. For NMT models, recovering unknown words is another key to obtaining good translations. We describe a simple workaround to find missing translations with a back-off system. To our surprise, performing pre-reordering on the training data hurts the model performance. Finally, we provide a qualitative analysis demonstrates a specific error pattern in NMT translations which omits some information and thus fail to preserve the complete meaning.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the last two decades, Statistical Machine Translation (SMT) with log-linear models in the core has shown promising results in the field. However, as stated in <ref type="bibr">(Duh and Kirch- hoff, 2008)</ref>, log-linear models may suffer from the underfitting problem and thus give poor performance. While for recurrent neural networks (RNNs), as demonstrated in ( <ref type="bibr" target="#b15">Mikolov et al., 2010)</ref>, they brought significant improvement in Natural Language Processing tasks. In their research, RNNs are shown to be capable of giving more prediction power compared with conventional language models when large training data is given. Using these neural language models to rescore SMT outputs generally gives better translation results <ref type="bibr" target="#b2">(Auli and Gao, 2014</ref>). Other approaches rescore with RNNs that predict the next word by taking the word in current step and S as inputs <ref type="bibr">(Kalch- brenner and Blunsom, 2013;</ref><ref type="bibr">Cho, Merrien- boer, et al., 2014</ref>). Here, S is a vector representation summarizes the whole input sentence.</p><p>Neural machine translation is a brand-new approach that samples translation results directly from RNNs. Most published models involve an encoder and a decoder in the network architecture <ref type="bibr" target="#b18">(Sutskever, Vinyals, and Le, 2014)</ref>, called Encoder-Decoder approach. <ref type="figure" target="#fig_0">Fig- ure 1</ref> gives a general overview of this approach. In <ref type="figure" target="#fig_0">Figure 1</ref>, the vector output S of the encoder RNN represents the whole input sentence. Hence, S contains all information required to produce the translation. In order to boost up the performance, <ref type="bibr" target="#b18">(Sutskever, Vinyals, and Le, 2014</ref>) used stacked Long Short-Term Memory (LSTM) units for both encoder and decoder, their ensembled models outperformed phrasebased SMT baseline in English-French translation task. Recently, by scaling up neural network models and incorporating some techniques during the training, the performance of NMT models have already achieved the state-of-the-art in English-French translation task ( <ref type="bibr" target="#b13">Luong et al., 2015</ref>) and English-German translation task ( <ref type="bibr" target="#b10">Jean et al., 2015)</ref>.</p><p>In this paper, we describe our works on applying NMT to English-Japanese translation task. The main contributions of this work are detailed as follows:</p><p>• We examined the effect of using different network architecture and recurrent units for English-Japanese translation</p><p>• We empirically evaluated NMT models trained on pre-reordered data</p><p>• We demonstrate a simple solution to recover unknown words in the translation results with a back-off system</p><p>• We provide a qualitative analysis on the translation results of NMT models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recurrent neural networks</head><p>Recurrent neural network is the solution for modeling temporal data with neural networks. The framework of widely used modern RNN is introduced by Elman <ref type="bibr" target="#b7">(Elman, 1990)</ref>, it is also known as Elman Network or Simple Recurrent Network. At each time step, RNN updates its internal state h t based on a new input x t and the previous state h t−1 , produces an output y t . Generally, they are computed recursively by applying following operations:</p><formula xml:id="formula_0">h t = f (W i x t + W h h t−1 + b h )<label>(1)</label></formula><formula xml:id="formula_1">y t = f (W o h t + b o )<label>(2)</label></formula><p>Where f is an element-wise non-linearity, such as sigmoid or tanh. Figures 2 illustrates the computational graph of a RNN. Solid lines in the figure mark out the Affine transformations followed with a non-linear activation. Dashed lines indicate that the result of previous computation is just a parameter of next operation. The bias term b h is omitted in the illustration.</p><p>RNN can be trained with Backpropagation Through Time (BPTT), which is a gradientbased technique that unfolds the network through time so as to compute the actual gradients of parameters in each time step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Long short-term memory</head><p>For RNN, as the internal state h t is completely changed in each time step, BPTT algorithm dilutes error information after each step of computation. Hence, RNN suffers from the problem that it is difficult to capture longterm dependencies.</p><p>Long short-term memory units (Hochreiter and Schmidhuber, 1997) incorporate some gates to control the information flow. In addition to the hidden units in RNN, memory cells are used to store long-term information, which is updated linearly. Empirically, LSTM can preserve information for arbitrarily long periods of time. <ref type="figure" target="#fig_2">Figure 3</ref> gives an illustration of the computational graph of a basic LSTM unit. In which, input gate i t , forget gate f t and output gate o t are marked with rhombuses. "×" and "+" are element-wise multiplication and elementwise addition respectively. The computational steps follows <ref type="bibr" target="#b8">(Graves, 2013)</ref>, 11 weight parameters are involved in this model, compared with only 2 weight parameters in RNN. We can see from <ref type="figure" target="#fig_2">Figure 3</ref> that the memory cells c t can keep unchanged when f t outputs 1 and i t outputs 0. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gated recurrent unit</head><p>Gated recurrent unit (GRU) is originally proposed in <ref type="bibr" target="#b4">(Cho, Merrienboer, et al., 2014</ref>). Similarly to LSTM unit, GRU also has gating units to control the information flow. While LSTM unit has a separate memory cell, GRU unit only maintains one kind of internal states, thus reduces computational complexity. The computational graph of a GRU unit is demonstrated in <ref type="figure">Figure 4</ref>. As shown in the figure, 6 weight parameters are involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: An illustration of a GRU unit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Network architectures of Neural Machine Translation</head><p>A basic architecture of NMT is called EncoderDecoder approach <ref type="bibr" target="#b18">(Sutskever, Vinyals, and Le, 2014)</ref>, which encodes the input sequence into a vector representation, then unrolls it to generate the output sequence. Then softmax function is applied to the output layer in order to compute cross-entropy. Instead of using one-hot embeddings for the tokens in the vocabulary, trainable word embeddings are used. As a pre-processing step, "&lt;eos&gt;" token is appended to the end of each sequence. When translating, the token with the highest probability in the output layer is sampled and input back to the neural network to get next output. This is done recursively until "&lt;eos&gt;" is observed. <ref type="figure" target="#fig_3">Figure 5</ref> gives a detailed illustration of this architecture when using stacked multilayer recurrent units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Soft-attention models in NMT</head><p>As stated in <ref type="bibr" target="#b5">(Cho, Merriënboer, et al., 2014</ref>), two critical drawbacks exist in the basic Encoder-Decoder approach: (1) the performance degrades when the input sentence gets longer, (2) the vocabulary size in the target size is limited.</p><p>Attentional models are first proposed in the field of computer vision, which allows the recurrent network to focus on a small portion in the image at each step. The internal state is updated only depends on this glimpse. Softattention first evaluates the weights for all possible positions to attend, then make a weighted summarization of all hidden states in the encoder. The summarized vector is finally used to update the internal state of the decoder. Contrary to hard-attention mechanism which selects only one location at each step and thus has to be trained with reinforce learning techniques, soft-attention mechanism makes the computational graph differentiable and thus able to be trained with standard backpropagation.</p><p>Figure 6: The recurrent network using softattention mechanism to predict next output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>63</head><p>The application of soft-attention mechanism in machine translation is firstly described in ( <ref type="bibr" target="#b3">Bahdanau, Cho, and Bengio, 2014</ref>), which is referred as "RNNsearch" in this paper. The computational graph of a soft-attention NMT model is illustrated in <ref type="figure">Figure 6</ref>. In which, the encoder is replaced by a bi-directional RNN, the hidden states of two RNNs is finally concatenated in each input position. At each time step of decoding, an alignment weight a i is computed based on the previous state of the decoder and the concatenated hidden state of position i in the encoder. The alignment weights are finally normalized by softmax function. The weighted summarization of the hidden states in the encoder is then fed into the decoder. Hence, the internal state of the decoder is updated based on 3 inputs: the previous state, weighted summarization of the encoder and the target-side input token.</p><p>The empirical results in ( <ref type="bibr" target="#b3">Bahdanau, Cho, and Bengio, 2014)</ref> show that the performance of RNNsearch does not degrade severely like normal Encoder-Decoder approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Solutions of unknown words</head><p>A critical practical problem of NMT is the fixed vocabulary size in the output layer. As the output layer uses dense connections, enlarging it will significantly increase the computational complexity and thus slow down the training.</p><p>According to existing publications, two kinds of approaches are used to tackle this problem: model-specific and translationspecific approach.</p><p>Well known modelspecific approaches are noise-contrastive training (Mnih and Kavukcuoglu, 2013) and classbased models ( <ref type="bibr" target="#b15">Mikolov et al., 2010</ref>). In ( <ref type="bibr" target="#b10">Jean et al., 2015)</ref>, another model-specific solution is proposed by using only a small set of target vocabulary at each update. By using a very large target vocabulary, they were able to outperform the state-of-the-art system in English-German translation task.</p><p>Solutions of Translation-specific approach usually take advantage of the alignment of tokens in both sides. For examples, the proposed method in ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref> annotates the unknown target words with "unkpos i " instead of "unk". Where the subscript i is the position of the aligned source word for the unknown target word. The alignments can be obtained by conventional aligners. The purpose of this processing step put some cues for recovering missing words into the output. By applying this approach, they were able to surpass the state-of-the-art SMT system in English-French translation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment setup</head><p>In our experiments, we are curious to see how NMT models work in English-Japanese translation and how well the existing approaches for unknown words fit into this setting. As Japanese language drastically differs from English in terms of word order and grammar structure. NMT models must capture the semantics of long-range dependencies in a sentence in order to translate it well.</p><p>We use Japanese-English Scientific Paper Abstract Corpus (ASPEC-JE) as training data and focus on evaluating the models for English-Japanese translation task. In order to make the training time-efficient, we pick 1.5M sentences according to similarity score then filter out long sentences with more than 40 words in either English or Japanese side. This processing step gives 1.1M sentences for training. We randomly separate out 1,280 sentences as valid data.</p><p>As almost zero pre-knowledge of NMT experiments in English-Japanese translation can be found in publications, our purpose is to conduct a thorough experiment so that we can evaluate and compare different model architectures and recurrent units. However, the limitation of computational resource and time disallows us to massively test various models, training schemes, and hyper-parameters.</p><p>In our experiments, we evaluated four kinds of models as follow:</p><p>• LSTM Search: Soft-attention model with LSTM recurrent units Most of the details of these models are common. The recurrent layers of all the models contain 1024 neurons each. The size of word embedding is 1000. We truncate the sourceside and target-side vocabulary sizes to 80k and 40k respectively. For all models, we insert a dense layer contains 600 neurons immediately before the output layer. We basically use SGD with learning rate decay as optimization method, the batch size is 60 and initial learning rate is 1. The gradients are clipped to ensure L2 norm lower than 3. Although we sort the training data according to the input length, the order of batches is shuffled before training. For LSTM units, we set the bias of forget gate to 1 before training <ref type="bibr" target="#b11">(Jozefowicz, Zaremba, and Sutskever, 2015)</ref>. During the translation, we set beam size to 20, if no valid translation is obtained, then another trail with beam size of 1000 will be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating models by perplexity</head><p>For our in-house experiments, the evaluation of our models mainly relies on the perplexity measured on valid data, as a strong correlation between perplexity and translation performance is observed in many existing publications ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref>. The changing perplexities of the models described in Section 5.1 are visualized in <ref type="figure" target="#fig_5">Figure 7</ref>. In <ref type="figure" target="#fig_5">Figure 7</ref>, we can see that soft-attention models with LSTM unit constantly outperforms the muti-layer Encoder-Decoder model. This matches our expectation as the alignment between English and Japanese is too complicated thus it is difficult for simple EncoderDecoder models to capture it correctly. Another observation is that the performance of the soft-attention model with GRU unit is significantly lower than that with LSTM unit. As this is conflict with the results reported in other publications <ref type="bibr" target="#b11">(Jozefowicz, Zaremba, and Sutskever, 2015)</ref>, one possible explanation is that some implementation issues exist and further investigation is required.</p><p>One surprising observation is that using prereordered data to train soft-attention models does not benefit the perplexity, but degrades the performance by a small margin. We show that the same conclusion can be drawn by measuring translation performance directly in latter sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Replacing unknown words</head><p>Initially, we adapt the solution described in ( <ref type="bibr" target="#b13">Luong et al., 2015)</ref>, which annotate the unknown words with "unkpos i ", where i is the position of aligned source word. We find this require source-side and target-side sentences roughly aligned. When testing on the softattention model with pre-reordered training data, we found this method can correctly point out the rough aligned position of a missing word. This allows us to recover the missing output words with a dictionary or SMT systems.</p><p>However, for the training data in natural order, the position of aligned words in two languages differs drastically. The solution described above can hardly be applied as it annotates the unknown words with relative positions. Here, we propose a simple workaround for recovering the unknown words with a back-off system. We translate the input sentence using both a NMT system and a baseline SMT system. Assume the translation results are similar, then if we observe an unknown word in the result of the NMT system, then it is reasonable to infer that the rarest word in the baseline result which is missing in the NMT result should be this unknown translation. This is demonstrated in <ref type="figure" target="#fig_6">Figure 8</ref>, the rarest word in the baseline result is picked out to replace the unknown word in the NMT result. Practically, the assumption will not be true, the results of NMT systems and conventional SMT systems differ tremendously. Hence, some incorrect word replacements are introduced. This method can be generalized to recover multiple unknown words by selecting the rarest word in a near position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluating translation performance</head><p>In this section, we describe our submitted systems and report the evaluation results in the English-Japanese translation task of The 2nd Workshop on Asian Translation 1 (Nakazawa et al., 2015). We train these models with AdaDelta for 5 epochs. Then, we fine-tune the model with AdaGrad using an enlarged training data, that each sentence contains no more than 50 words. With this fine-tuning step, we are able to achieve perplexity of 1.76 in valid data. The automatic evaluation results are shown in <ref type="table" target="#tab_0">Table 1</ref>. Three SMT baselines are picked for comparison. In the middle of the table, we list two single soft-attention NMT models with LSTM unit. The results show that training models on pre-reordered corpus leads to degrading of translation performance, where the pre-reordering step is done using the model described in <ref type="bibr" target="#b19">(Zhu, 2014)</ref>.</p><p>Our submitted systems are basically an ensemble of two LSTM Search models trained on natural-order data, as shown in the bottom of <ref type="table" target="#tab_0">Table 1</ref>. After we replaced unknown words with the technique described in 5.3, we gained 0.8 BLEU on test data. This is our first submitted system, marked with "S1".</p><p>We also found it is useful to perform a sys-1 Our team ID is "WEBLIO MT" tem combination based on perplexity scores. We evaluate the perplexity for all outputs produced by a baseline system and the NMT model. These two sets of perplexity score are normalized by mean and standard deviation respectively. Then for each NMT result, we rescore it with the difference of perplexity against the baseline system. Intuitively, if the NMT result is better than the baseline result, the new score shall be a positive number. In our experiment, we pick the system described in (Zhu, 2014) as baseline system. We pick top-1000 results from NMT and the rest from the baseline system, this gives us a gain of 1.8 in BLEU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>30.000</head><p>Submitted system 1 (S1)</p><p>43.500</p><p>Submitted system 2 (S2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>53.750</head><p>Finally, we added 3 pre-reordered LSTM Search models to the ensemble, results in a 5-model ensemble. During the translation, these three models receive pre-reordered input, another two LSTM Search models receive input in natural order. We gain 0.24 BLEU with this setting, and this is the second submitted system, marked with "S2". Human evaluation results of our submitted systems are shown in <ref type="table" target="#tab_1">Table 2</ref>. As we already know that pre-reordering does not help improving translation performance, a natural choice is to train more normal LSTM Search models and put into the ensemble. We failed to do it because of insufficient time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative analysis</head><p>To find some insights in the translation results of NMT systems, we performed qualitative analysis on a proportion of held-out development data. During the inspection, we found many errors share the same pattern. It turns out that NMT model tends to make a perfect translation by omitting some information during the translation. In this case, the output tends to be a valid sentence, but the meaning is partially lost. One example of this phenomenon is shown in the following snippet:</p><p>Input: this paper discusses some systematic uncertainties including casimir force , false force due to electric force , and various factors for irregular uncertainties due to patch field and detector noise . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we performed a systematic evaluation of various kinds for NMT models in the setting of English-Japanese translation. Based on the empirical evaluation results, we found soft-attention NMT models can already make good translation results in English-Japanese translation task. Their performance surpasses all SMT baselines by a substantial margin according to RIBES scores. We also found that NMT models can work well without extra data processing steps such as pre-reordering. Finally, we described a simple workaround to recover unknown words with a back-off system. However, a sophisticated solution for dealing with unknown words is still an open question in the English-Japanese setting. As some patterns of mistakes can be observed from the translation results, there exists some space for further improvements.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Basic neural network architecture in Encoder-Decoder approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the computational graph of a RNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of a basic LSTM unit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of a basic neural network architecture for NMT with stacked multi-layer recurrent units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>• pre-reordered LSTM Search:</head><label></label><figDesc>Same as LSTM Search, but the model is trained on pre-reordered corpus • GRU Search: Soft-attention model with GRU recurrent units • LSTM Encoder-Decoder: Basic Encoder-Decoder model with 4 stacked LSTM layers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of the training for different models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Illustration of replacing unknown words with a back-off system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Automatic evaluation results in WAT2015</head><label>1</label><figDesc></figDesc><table>Model 
BLEU RIBES 

PB basline 

29.80 0.691 

HPB baseline 

32.56 0.746 

T2S baseline 

33.44 0.758 

Single LSTM Search 

32.19 0.797 

Pre-reordered LSTM Search 

30.97 0.779 

Ensemble of 2 LSTM Search 

33.38 0.800 

+ UNK replacing (S1) 

34.19 0.802 

+ System combination 

35.97 0.807 
+ 3 pre-reordered ensembles (S2) 36.21 0.809 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Human evaluation results for submit-
ted system in WAT2015 

Model 
HUMAN 

T2S baseline 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-16" />
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Copyright is held by the author(s)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Decoder integration and expected bleu training for recurrent neural network language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">References</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL&apos;14)</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics (ACL&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="136" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Properties of Neural Machine Translation: Encoder-Decoder Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Syntax, Semantics and Structure in Statistical Translation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">103</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond log-linear models: boosted minimum error rate training for N-best Re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers. Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="37" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive science 14</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation 9</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On Using Very Large Target Vocabulary for Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note>Long Pa-67 pers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Empirical Exploration of Recurrent Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recurrent Continuous Translation Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1700" to="1709" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Addressing the Rare Word Problem in Neural Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">China</forename><surname>Beijing</surname></persName>
		</author>
		<title level="m">Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">In: IN-TERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-09-26" />
			<biblScope unit="page" from="1045" to="1048" />
			<pubPlace>Makuhari, Chiba, Japan</pubPlace>
		</imprint>
	</monogr>
	<note>Recurrent neural network based language model</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>C.J.C. Burges et al.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overview of the 2nd Workshop on Asian Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Nakazawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</title>
		<meeting>the 2nd Workshop on Asian Translation (WAT2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weblio Prereordering Statistical Machine Translation System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Asian Translation (WAT2014)</title>
		<meeting>the 1st Workshop on Asian Translation (WAT2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
