<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FAST: Quick Application Launch on Solid-State Drives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongsoo</forename><surname>Joo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhee</forename><surname>Ryu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangsoo</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<postCode>2260, 48109</postCode>
									<settlement>Hayward St., Ann Arbor</settlement>
									<region>MI</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">†Ewha Womans University</orgName>
								<address>
									<addrLine>11-1 Daehyun-dong Seodaemun-gu</addrLine>
									<postCode>120-750</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">‡Seoul National University</orgName>
								<address>
									<addrLine>599 Kwanak-Gu Kwanak Rd</addrLine>
									<postCode>151-744</postCode>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FAST: Quick Application Launch on Solid-State Drives</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Application launch performance is of great importance to system platform developers and vendors as it greatly affects the degree of users&apos; satisfaction. The single most effective way to improve application launch performance is to replace a hard disk drive (HDD) with a solid state drive (SSD), which has recently become affordable and popular. A natural question is then whether or not to replace the traditional HDD-aware application launchers with a new SSD-aware optimizer. We address this question by analyzing the inefficiency of the HDD-aware application launchers on SSDs and then proposing a new SSD-aware application prefetching scheme, called the Fast Application STarter (FAST). The key idea of FAST is to overlap the computation (CPU) time with the SSD access (I/O) time during an application launch. FAST is composed of a set of user-level components and system debugging tools provided by the Linux OS (operating system). In addition, FAST uses a system-call wrapper to automatically detect application launches. Hence, FAST can be easily deployed in any recent Linux versions without kernel recompilation. We implemented FAST on a desktop PC with a SSD running Linux 2.6.32 OS and evaluated it by launching a set of widely-used applications, demonstrating an average of 28% reduction of application launch time as compared to PC without a prefetcher.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Application launch performance is one of the important metrics for the design or selection of a desktop or a laptop PC as it critically affects the user-perceived performance. Unfortunately, application launch performance has not kept up with the remarkable progress of CPU performance that has thus far evolved according to Moore's law. As frequently-used or popular applications get "heavier" (by adding new functions) with each new release, their launch takes longer even if a new, powerful machine equipped with high-speed multi-core CPUs and several GBs of main memory is used. This undesirable trend is known to stem from the poor random access performance of hard disk drives (HDDs). When an application stored in a HDD is launched, up to thousands of block requests are sent to the HDD, and a significant portion of its launch time is spent on moving the disk head to proper track and sector positions, i.e., seek and rotational latencies. Unfortunately, the HDD seek and rotational latencies have not been improved much over the last few decades, especially compared to the CPU speed improvement. In spite of the various optimizations proposed to improve the HDD performance in launching applications, users must often wait tens of seconds for the completion of launching frequently-used applications, such as Windows Outlook.</p><p>A quick and easy solution to eliminate the HDD's seek and rotational latencies during an application launch is to replace the HDD with a solid state drive (SSD). A SSD consists of a number of NAND flash memory modules, and does not use any mechanical parts, unlike disk heads and arms of a conventional HDD. While the HDD access latency-which is the sum of seek and rotational latencies-ranges up to a few tens of milliseconds (ms), depending on the seek distance, the SSD shows a rather uniform access latency of about a few hundred microseconds (us). Replacing a HDD with a SSD is, therefore, the single most effective way to improve application launch performance.</p><p>Until recently, using SSDs as the secondary storage of desktops or laptops has not been an option for most users due to the high cost-per-bit of NAND flash memories. However, the rapid advance of semiconductor technology has continuously driven the SSD price down, and at the end of 2009, the price of an 80 GB SSD has fallen below 300 US dollars. Furthermore, SSDs can be installed in existing systems without additional hardware or software support because they are usually equipped with the same interface as HDDs, and OSes see a SSD as a block device just like a HDD. Thus, end-users begin to use a SSD as their system disk to install the OS image and applications.</p><p>Although a SSD can significantly reduce the application launch time, it does not give users ultimate satisfaction for all applications. For example, using a SSD reduces the launch time of a heavy application from tens of seconds to several seconds. However, users will soon become used to the SSD launch performance, and will then want the launch time to be reduced further, just as they see from light applications. Furthermore, users will keep on adding functions to applications, making them heavier with each release and their launch time greater. According to a recent report <ref type="bibr" target="#b23">[24]</ref>, the growth of software is rapid and limited only by the ability of hardware. These call for the need to further improve application launch performance on SSDs.</p><p>Unfortunately, most previous optimizers for application launch performance are intended for HDDs and have not accounted for the SSD characteristics. Furthermore, some of them may rather be detrimental to SSDs. For example, running a disk defragmentation tool on a SSD is not beneficial at all because changing the physical location of data in the SSD does not affect its access latency. Rather, it generates unnecessary write and erase operations, thus shortening the SSD's lifetime.</p><p>In view of these, the first step toward SSD-aware optimization may be to simply disable the traditional optimizers designed for HDDs. For example, Windows 7 disables many functions, such as disk defragmentation, application prefetch, Superfetch, and Readyboost when it detects a SSD being used as a system disk <ref type="bibr" target="#b26">[27]</ref>. Let's consider another example. Linux is equipped with four disk I/O schedulers: NOOP, anticipatory, deadline, and completely fair queueing. The NOOP scheduler almost does nothing to improve HDD access performance, thus providing the worst performance on a HDD. Surprisingly, it has been reported that NOOP shows better performance than the other three sophisticated schedulers on a SSD <ref type="bibr" target="#b10">[11]</ref>.</p><p>To the best of our knowledge, this is the first attempt to focus entirely on improving application launch performance on SSDs. Specifically, we propose a new application prefetching method, called the Fast Application STarter (FAST), to improve application launch time on SSDs. The key idea of FAST is to overlap the computation (CPU) time with the SSD access (I/O) time during each application launch. To achieve this, we monitor the sequence of block requests in each application, and launch the application simultaneously with a prefetcher that generates I/O requests according to the a priori monitored application's I/O request sequence. FAST consists of a set of user-level components, a system-call wrapper, and system debugging tools provided by the Linux OS. FAST can be easily deployed in most recent Linux versions without kernel recompilation. We have implemented and evaluated FAST on a desktop PC with a SSD running Linux 2.6.32, demonstrating an average of 28% reduction of application launch time as compared to PC without a prefetcher.</p><p>This paper makes the following contributions:</p><p>• Qualitative and quantitative evaluation of the inefficiency of traditional HDD-aware application launch optimizers on SSDs;</p><p>• Development of a new SSD-aware application prefetching scheme, called FAST; and</p><p>• Implementation and evaluation of FAST, demonstrating its superiority and deployability.</p><p>While FAST can be also applied to HDDs, its performance improvements are only limited to high I/O requirements of application launches on HDDs. We observed that existing application prefetchers outperformed FAST on HDDs by effectively optimizing disk head movements, which will be discussed further in Section 5.</p><p>The paper is organized as follows. In Section 2, we review other related efforts and discuss their performance in optimizing application launch on SSDs. Section 3 describes the key idea of FAST and presents an upper bound for its performance. Section 4 details the implementation of FAST on the Linux OS, while Section 5 evaluates its performance using various real-world applications. Section 6 discusses the applicability of FAST to smartphones and Section 7 compares FAST with traditional I/O prefetching techniques. We conclude the paper with Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Application Launch Optimization</head><p>Application-level optimization. Application developers are usually advised to optimize their applications for fast startup. For example, they may be advised to postpone loading non-critical functions or libraries so as to make applications respond as fast as possible <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>. They are also advised to reduce the number of symbol relocations while loading libraries, and to use dynamic library loading. There have been numerous case studies-based on in-depth analyses and manual optimizations-of various target applications/platforms, such as Linux desktop suite platform <ref type="bibr" target="#b7">[8]</ref>, a digital TV <ref type="bibr" target="#b16">[17]</ref>, and a digital still camera <ref type="bibr" target="#b32">[33]</ref>. However, such an approach requires the experts' manual optimizations for each and every application. Hence, it is economically infeasible for generalpurpose systems with many (dynamic) application programs.</p><p>Snapshot technique. A snapshot boot technique has also been suggested for fast startup of embedded systems <ref type="bibr" target="#b18">[19]</ref>, which is different from the traditional hibernate shutdown function in that a snapshot of the main memory after booting an OS is captured only once, and used repeatedly for every subsequent booting of the system. However, applying this approach for application launch is not practical for the following reasons. First, the page cache in main memory is shared by all applications, and separating only the portion of the cache content that is related to a certain application is not possible without extensive modification of the page cache. Furthermore, once an application is updated, its snapshot should be invalidated immediately, which incurs runtime overhead. Prediction-based prefetch.</p><p>Modern desktops are equipped with large (up to several GBs) main memory, and often have abundant free space available in the main memory. Prediction-based prefetching, such as Superfetch <ref type="bibr" target="#b27">[28]</ref> and Preload <ref type="bibr" target="#b11">[12]</ref>, loads an application's code blocks in the free space even if the user does not explicitly express his intent to execute that particular application. These techniques monitor and analyze the users' access patterns to predict which applications to be launched in future. Consequently, the improvement of launch performance depends strongly on prediction accuracy. Sorted prefetch. The Windows OS is equipped with an application prefetcher <ref type="bibr" target="#b35">[36]</ref> that prefetches application code blocks in a sorted order of their logical block addresses (LBAs) to minimize disk head movements. A similar idea has also been implemented for Linux OS <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>. We call these approaches sorted prefetch. It monitors HDD activities to maintain a list of blocks accessed during the launch of each application. Upon detection of an application launch, the application prefetcher immediately pauses its execution and begins to fetch the blocks in the list in an order sorted by their LBAs. The application launch is resumed after fetching all the blocks, and hence, no page miss occurs during the launch. Application defragmentation. The block list information can also be used in a different way to further reduce the seek distance during an application launch. Modern OSes commonly support a HDD defragmentation tool that reorganizes the HDD layout so as to place each file in a contiguous disk space. In contrast, the defragmentation tool can relocate the blocks in the list of each application by their access order <ref type="bibr" target="#b35">[36]</ref>, which helps reduce the total HDD seek distance during the launch. Data pinning on flash caches. Recently, flash cache has been introduced to exploit the advantage of SSDs at a cost comparable to HDDs. A flash cache can be integrated into traditional HDDs, which is called a hybrid HDD <ref type="bibr" target="#b36">[37]</ref>. Also, a PCI card-type flash cache is available <ref type="bibr" target="#b25">[26]</ref>, which is connected to the mother board of a desktop or laptop PC. As neither seek nor rotational latency is incurred while accessing data in the flash cache, we can accelerate application launch by storing the code blocks of frequently-used applications, which is called a pinned set. Due to the small capacity of flash cache, how to determine the optimal pinned set subject to the capacity constraint is a key to making performance improvement, and a few results of addressing this problem have been reported <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22]</ref>. We expect that FAST can be integrated with the flash cache for further improvement of performance, but leave it as part of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SSD Performance Optimization</head><p>SSDs have become affordable and begun to be deployed in desktop and laptop PCs, but their performance characteristics have not yet been understood well. So, researchers conducted in-depth analyses of their performance characteristics, and suggested ways to improve their runtime performance. Extensive experiments have been carried out to understand the performance dynamics of commercially-available SSDs under various workloads, without knowledge of their internal implementations <ref type="bibr" target="#b6">[7]</ref>. Also, SSD design space has been explored and some guidelines to improve the SSD performance have been suggested <ref type="bibr" target="#b9">[10]</ref>. A new write buffer management scheme has also been suggested to improve the random write performance of SSDs <ref type="bibr" target="#b19">[20]</ref>. Traditional I/O schedulers optimized for HDDs have been revisited in order to evaluate their performance on SSDs, and then a new I/O scheduler optimized for SSDs has been proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Launch Optimization on SSDs</head><p>As discussed in Section 2.1, various approaches have been developed and deployed to improve the application launch performance on HDDs. On one hand, many of them are effective on SSDs as well, and orthogonal to FAST. For example, application-level optimization and prediction-based prefetch can be used together with FAST to further improve application launch performance.</p><p>On the other hand, some of them exploit the HDD characteristics to reduce the seek and rotational delay during an application launch, such as the sorted prefetch and the application defragmentation. Such methods are ineffective for SSDs because the internal structure of a SSD is very different from that of a HDD. A SSD typically consists of multiple NAND flash memory modules, and does not have any mechanical moving part. Hence, unlike a HDD, the access latency of a SSD is irrelevant to the LBA distance between the last and the current block   requests. Thus, prefetching the application code blocks according to the sorted order of their LBAs or changing their physical locations will not make any significant performance improvement on SSDs. As the sorted prefetch has the most similar structure to FAST, we will quantitatively compare its performance with FAST in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Application Prefetching on SSDs</head><p>This section illustrates the main idea of FAST with examples and derives a lower bound of the application launch time achievable with FAST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Cold and Warm Starts</head><p>We focus on the performance improvement in case of a cold start, or the first launch of an application upon system bootup, representing the worst-case application launch performance. <ref type="figure" target="#fig_1">Figure 1</ref>(a) shows an example cold start scenario, where s i is the i-th block request generated during the launch and n the total number of block requests. After s i is completed, the CPU proceeds with the launch process until another page miss takes place. Let c i denote this computation.</p><p>The opposite extreme is a warm start in which all the code blocks necessary for launch have been found in the page cache, and thus, no block request is generated, as shown in <ref type="figure" target="#fig_1">Figure 1</ref>(b). This occurs when the application is launched again shortly after its closure. The warm start represents an upper-bound of the application launch performance improvement achievable with optimization of the secondary storage.</p><p>Let the time spent for s i and c i be denoted by t(s i ) and t(c i ), respectively. Then, the computation (CPU) time, t cpu , is expressed as</p><formula xml:id="formula_0">t cpu = n ∑ i=1 t(c i ),<label>(1)</label></formula><p>and the SSD access (I/O) time, t ssd , is expressed as</p><formula xml:id="formula_1">t ssd = n ∑ i=1 t(s i ).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Proposed Application Prefetcher</head><p>The rationale behind FAST is that the I/O request sequence generated during an application launch does not change over repeated launches of the application in case of cold-start. The key idea of FAST is to overlap the SSD access (I/O) time with the computation (CPU) time by running the application prefetcher concurrently with the application itself. The application prefetcher replays the I/O request sequence of the original application, which we call an application launch sequence. An application launch sequence S can be expressed as (s 1 , . . . , s n ). <ref type="figure" target="#fig_1">Figure 1</ref>(c) illustrates how FAST works, where t cpu &gt; t ssd is assumed. At the beginning, the target application and the prefetcher start simultaneously, and compete with each other to send their first block request to the SSD. However, the SSD always receives the same block request s 1 regardless of which process gets the bus grant first. After s 1 is fetched, the application can proceed with its launch by the time t(c 1 ), while the prefetcher keeps issuing the subsequent block requests to the SSD. After completing c 1 , the application accesses the code block corresponding to s 2 , but no page miss occurs for s 2 because it has already been fetched by the prefetcher. It is the same for the remaining block requests, and thus, the resulting application launch time t launch becomes which represents a lower bound of the application launch time achievable with FAST. However, FAST may not achieve application launch performance close to Eq. (5) when there is a significant variation of I/O intensiveness, especially if the beginning of the launch process is more I/O intensive than the other. <ref type="figure">Figure 2</ref> illustrates an extreme example of such a case, where the first half of this example is SSD-bound and the second half is CPU-bound. In this example, t cpu is equal to t ssd , and thus the expected launch time t expected is given to be t ssd + t(c 8 ), according to Eq. (4). However, the actual launch time t actual is much larger than t expected . The CPU usage in the first half of the launch time is kept quite low despite the fact that there are lots of remaining CPU computations (i.e., c 5 , . . . , c 8 ) due to the dependency between s i and c i . We will provide a detailed analysis for this case using real applications in Section 5.</p><formula xml:id="formula_2">t launch = t(s 1 ) + t cpu .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We chose the Linux OS to demonstrate the feasibility and the superior performance of FAST. The implementation of FAST consists of a set of components: an application launch manager, a system-call profiler, a disk I/O profiler, an application launch sequence extractor, a LBAto-inode reverse mapper, and an application prefetcher generator. <ref type="figure" target="#fig_4">Figure 3</ref> shows how these components interact with each other. In what follows, we detail the implementation of each of these components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Application Launch Sequence</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Disk I/O Profiler</head><p>The disk I/O profiler is used to track the block requests generated during an application launch. We used Blktrace <ref type="bibr" target="#b2">[3]</ref>, a built-in Linux kernel I/O-tracing tool that monitors the details of I/O behavior for the evaluation of I/O performance. Blktrace can profile various I/O events: inserting an item into the block layer, merging the item with a previous request in the queue, remapping onto another device, issuing a request to the device driver, and a completion signal from the device. From these events, we collect the trace of device-completion events, each of which consists of a device number, a LBA, the I/O size, and completion time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Application Launch Sequence Extractor</head><p>Ideally, the application launch sequence should include all of the block requests that are generated every time the application is launched in the cold start scenario, without including any block requests that are not relevant to the application launch. We observed that the raw block request sequence captured by Blktrace does not vary from one launch to another, i.e., deterministic for multiple launches of the same application. However, we observed that other processes (e.g., OS and application daemons) sometimes generate their own I/O requests simultaneously with the application launch. To handle this case, the application launch sequence extractor collects two or more raw block request sequences to extract a common sequence, which is then used as a launch sequence of the corresponding application. The implementation of the application launch sequence extractor is simple: it searches for and removes any block requests appearing in some of the input sequences. This procedure makes all the input sequences the same, so we use any of them as an application launch sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LBA-to-Inode Map</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">LBA-to-Inode Reverse Mapper</head><p>Our goal is to create an application prefetcher that generates exactly the same block request sequence as the obtained application launch sequence, where each block request is represented as a tuple of starting LBA and size. Since the application prefetcher is implemented as a user-level program, every disk access should be made via system calls with a file name and an offset in that file. Hence, we must obtain the file name and the offset of each block request in an application launch sequence.</p><p>Most file systems, including EXT3, do not support such a reverse mapping from LBA to file name and offset. However, for a given file name, we can easily find the LBA of all of the blocks that belong to the file and their relative offset in the file. Hence, we can build a LBA-to-inode map by gathering this information for every file. However, building such a map of the entire file system is time-consuming and impractical because a file system, in general, contains tens of thousands of files and their block locations on the disk change very often.</p><p>Therefore, we build a separate LBA-to-inode map for each application, which can significantly reduce the overhead of creating a LBA-to-inode map because (1) the number of applications and the number of files used in launching each application are very small compared to the number of files in the entire file system; and (2) most of them are shared libraries and application code blocks, so their block locations remain unchanged unless they are updated or disk defragmentation is performed.</p><p>We implement the LBA-to-inode reverse mapper that receives a list of file names as input and creates a LBAto-inode map as output. A LBA-to-inode map is built using a red-black tree in order to reduce the search time. Each node in the red-black tree has the LBA of a block as its key, and a block type as its data by default. According to the block type, different types of data are added to the node. A block type includes a super block, a group descriptor, an inode block bitmap, a data block bitmap, an inode table, and a data block. For example, a node for a data block has a block type, a device number, an inode number, an offset, and a size. Also, for a data block, a table is created to keep the mapping information between an inode number and its file name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">System-Call Profiler</head><p>The system-call profiler obtains a full list of file names that are accessed during an application launch, 1 and passes it to the LBA-to-inode reverse mapper. We used strace for the system-call profiler, which is a debugging tool in Linux. We can specify the argument of strace so that it may monitor only the system calls that have a file name as their argument. As many of these system calls are rarely called during an application launch, we monitor only the following system calls that frequently occur during application launches: open(), creat(), execve(), stat(), stat64(), lstat(), lstat64(), access(), truncate(), truncate64(), statfs(), statfs64(), readlink(), and unlink().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Application Prefetcher</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Application Prefetcher Generator</head><p>The application prefetcher is a user-level program that replays the disk access requests made by a target appli-1 Files mounted on pseudo file systems such as procfs and sysfs are not processed because they never generate any disk I/O request. 3. Depending on the type of block, generate an appropriate system call using the converted disk access information.</p><p>4. Repeat Steps 1-3 until processing all s i . <ref type="table" target="#tab_1">Table 1</ref> shows the kind of system calls used for each block type. There are two system calls that can be used to replay the disk access for data blocks of a regular file. If we use read(), data is first moved from the SSD to the page cache, and then copying takes place from the page cache to the user buffer. The second step is unnecessary for our purpose, as the process that actually manipulates the data is not the application prefetcher but the target application. Hence, we chose posix fadvise() that performs only the first step, from which we can avoid the overhead of read(). We use the POSIX FADV WILLNEED parameter, which informs the OS that the specified data will be used in the near future. When to issue the corresponding disk access after posix fadvise() is called depends on the OS implementation. We confirmed that the current version of Linux we used issues a block request immediately after receiving the information through posix fadvise(), thus meeting our need. A symbolic-linked file name is stored in data block pointers in an inode entry when the length of the file name is less than or equal to 60 bytes (c.f., the space of data block pointers is 60 bytes, 4*12 for direct, 4 for single indirect, another 4 for double indirect, and last 4 for triple indirect data block pointer). If the length of linked file name is more than 60 bytes, the name is stored in the data blocks pointed to by data block pointers in the inode entry. We use readlink() to replay the data block access of symbolic-link file names that are longer than 60 bytes.  <ref type="figure" target="#fig_3">Figure 4</ref> is an example of automatically-generated application prefetcher. Unlike the target application, the application prefetcher successively fetches all the blocks as soon as possible to minimize the time between adjacent block requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Implicitly-Prefetched Blocks</head><p>In the EXT3 file system, the inode of a file includes pointers of up to 12 data blocks, so these blocks can be found immediately after accessing the inode. If the file size exceeds 12 blocks, indirect, double indirect, and triple indirect pointer blocks are used to store the pointers to the data blocks. Therefore, requests for indirect pointer blocks may occur in the cold start scenario when the application is accessing files larger than 12 blocks. We cannot explicitly load those indirect pointer blocks in the application prefetcher because there is no such system call. However, the posix fadvise() call for a data block will first make a request for the indirect block when needed, so it can be fetched in a timely manner by running the application prefetcher.</p><p>The following types of block request are not listed in <ref type="table" target="#tab_1">Table 1</ref>: a superblock, a group descriptor, an inode entry bitmap, a data block bitmap. We found that requests to these types of blocks seldom occur during an application launch, so we did not consider their prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Application Launch Manager</head><p>The role of the application launch manager is to detect the launch of an application and to take an appropriate action. We can detect the beginning of an application launch by monitoring execve() system call, which is implemented using a system-call wrapper. There are three phases with which the application launch manager A counter to record the number of application launches done in the initial launch phase n pre f A counter to record the number of launches done in the application prefetch phase after the last check of the miss ratio of the application prefetcher N rawseq The number of raw block request sequences that are to be captured at the launch profiling phase N chk</p><p>The period to check the miss ratio of the application prefetcher R miss A threshold value for the prefetcher miss ratio that is used to determine if an update of the application or shared libraries has taken place T idle A threshold value for the idle time period that is used to determine if an application launch is completed T timeout The maximum amount of time allowed for the disk I/O profiler to capture block requests deals: a launch profiling phase, a prefetcher generation phase, and an application prefetch phase. The application launch manager uses a set of variables and parameters for each application to decide when to change its phase. These are summarized in <ref type="table" target="#tab_2">Table 2</ref>.</p><p>Here we describe the operations performed in each phase: (1) Launch profiling. If no application prefetcher is found for that application, the application launch manager regards the current launch as the first launch of this application, and enters the initial launch phase. In this phase, the application launch manager performs the following operations in addition to the launch of the target application:</p><p>1. Increase n init of the current application by 1.</p><p>2. If n init = 1, run the system call profiler.</p><p>3. Flush the page cache, dentries (directory entries), and inodes in the main memory to ensure a cold start scenario, which is done by the following command: 5. If n init = N rawseq , enter the prefetcher generation phase after the current launch is completed.</p><p>(2) Prefetcher generation. Once application launch profiling is done, it is ready to generate an application prefetcher using the information obtained from the first phase. This can be performed either immediately after the application launch is completed, or when the system is idle. The following operations are performed:</p><p>1. Run the application launch sequence extractor.</p><p>2. Run the LBA-to-inode reverse mapper.</p><p>3. Run the application prefetcher generator.</p><p>4. Reset the values of n init and n pre f to 0.</p><p>(3) Application prefetch. If the application prefetcher for the current application is found, the application launch manager runs the prefetcher simultaneously with the target application. It also periodically checks the miss ratio of the prefetcher to determine if there has been any update of the application or shared libraries. Specifically, the following operations are performed:</p><p>1. Increase n pre f of the current application by 1.</p><p>2. If n pre f = N chk , reset the value of n pre f to 0 and run the disk I/O profiler. Its termination conditions are the same as those in the first phase.</p><p>3. Run the application prefetcher simultaneously with the target application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">If a raw block request sequence is captured, use it to</head><p>calculate the miss ratio of the application prefetcher. If it exceeds R miss , delete the application prefetcher.</p><p>The miss ratio is defined as the ratio of the number of block requests not issued by the prefetcher to the total number of block requests in the application launch sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Experimental platform. We used a desktop PC equipped with an Intel i7-860 2.8 GHz CPU, 4GB of PC12800 DDR3 SDRAM and an Intel 80GB SSD (X25-M G2 Mainstream). We installed a Fedora 12 with Linux kernel 2.6.32 on the desktop, in which we set NOOP as the default I/O scheduler. For benchmark applications, we chose frequently used user-interactive applications, for which application launch performance matters much. Such an application typically uses graphical user interfaces and requires user interaction immediately after completing its launch. Applications like gcc and gzip are not included in our set of benchmarks as launch performance is not an issue for them. Our benchmark set consists of the following Linux applications: Acrobat reader, Designer-qt4, Eclipse, F-Spot, Firefox, Gimp, Gnome, Houdini, Kdevdesigner, Kdevelop, Konqueror, Labview, Matlab, OpenOffice, Skype, Thunderbird, and XilinxISE. In addition to these, we used Wine <ref type="bibr" target="#b0">[1]</ref>, which is an implementation of the Windows API running on the Linux OS, to test Access, Excel, Powerpoint, Visio, and Word-typical Windows applications. Test scenarios. For each benchmark application, we measured its launch time for the following scenarios.</p><p>• Cold start: The application is launched immediately after flushing the page cache, using the method described in Section 4.4. The resulting launch time is denoted by t cold .</p><p>• Warm start: We first run the application prefetcher only to load all the blocks in the application launch sequence to the page cache, and then launch the application. Let t warm denote the resulting launch time.</p><p>• Sorted prefetch: To evaluate the performance of the sorted prefetch <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref> on SSDs, we modify the application prefetcher to fetch the block requests in the application launch sequence in the sorted order of their LBAs. After flushing the page cache, we first run the modified application prefetcher, then immediately run the application. Let t sorted denote the resulting launch time.</p><p>• FAST: We flush the page cache, and then run the application simultaneously with the application prefetcher. The resulting launch time is denoted by t FAST .</p><p>• Prefetcher only: We flush the page cache and run the application prefetcher. The completion time of the application prefetcher is denoted by t ssd . It is used to calculate a lower bound of the application launch time t bound = max(t ssd ,t cpu ), where t cpu = t warm is assumed.</p><p>Launch-time measurement. We start an application launch by clicking an icon or inputting a command, and can accurately measure the launch start time by monitoring when execve() is called. Although it is difficult to clearly define the completion of a launch, a reasonable definition is the first moment the application becomes responsive to the user <ref type="bibr" target="#b1">[2]</ref>. However, it is difficult to accurately and automatically measure that moment. So, as an alternative, we measured the completion time of the last block request in an application launch sequence using Blktrace, assuming that the launch will be completed very soon after issuing the last block request. For the warm start scenario, we executed posix fadvise() with POSIX FADV DONTNEED parameter to evict the last block request from the page cache. For the sorted prefetch and the FAST scenarios, we modified the application prefetcher so that it skips prefetching of the last block request. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>Application launch sequence generation. We captured 10 raw block request sequences during the cold start launch of each application. We ran the application launch sequence extractor with a various number of input block request sequences, and observed the size of the resulting application launch sequences. <ref type="figure" target="#fig_5">Figure 5</ref> shows that for all the applications we tested, there is no significant reduction of the application launch sequence size while increasing the number of inputs from 2 to 10. Hence, we set the value of N rawseq in <ref type="table" target="#tab_2">Table 2</ref> to 2 in this paper. We used the size of the first captured input sequence as the number of inputs one in <ref type="figure" target="#fig_5">Figure 5</ref> (the application launch sequence extractor requires at least two input sequences). For some applications, there are noticeable differences in size between the number of inputs one and two. This is because the first raw input request sequence includes a set of bursty I/O requests generated by OS and user daemons that are irrelevant to the application launch. <ref type="figure" target="#fig_5">Fig- ure 5</ref> shows that such I/O requests can be effectively excluded from the resulting application launch sequence using just two input request sequences. The second and third columns of <ref type="table">Table 3</ref> summarize the total number of block requests and accessed blocks of the thus-obtained application launch sequences, respectively. The last column shows the total number of files used during the launch of each application. Testing of the application prefetcher. Application prefetchers are automatically generated for the benchmark applications using the application launch sequences in <ref type="table">Table 3</ref>. In order to see if the application prefetchers fetch all the blocks used by an application, we first flushed the page cache, and launched each application immediately after running the application prefetcher. During the application launch, we captured all the block requests generated using Blktrace, and counted the number of missed block requests. The average number of missed block requests was 1.6% of the number of block requests in the application launch sequence, but varied among repeated launches, e.g., from 0% to 6.1% in the experiments we performed.</p><p>By examining the missed block requests, we could categorize them into three types: (1) files opened by OS daemons and user daemons at boot time; (2) journaling data or swap partition accesses; and (3) files dynamically created or renamed at every launch (e.g., tmpfile()). The first type occurs because we force the page cache to be flushed in the experiment. In reality, they are highly likely to reside in the page cache, and thus, this type of misses will not be a problem. The second type is irrelevant to the application, and observed even during idle time. The third type occurs more or less often, depending on the application. FAST does not prefetch this type of block requests as they change at every launch.</p><p>Experiments for the test scenarios. We measured the launch time of the benchmark applications for each test scenario listed in Section 5.1. <ref type="figure">Figure 6</ref> shows that the average launch time reduction of FAST is 28% over the cold start scenario. The performance of FAST varies considerably among applications, ranging from 16% to 46% reduction of launch time. In particular, FAST shows performance very close to t bound for some applications, such as Eclipse, Gnome, and Houdini. On the other hand, the gap between t bound and t FAST is relatively larger for such applications as Acrobat reader, Firefox, OpenOffice, and Labview.</p><p>Launch time behavior. We conducted experiments to see if the application prefetcher works well as expected when it is simultaneously run with the application. We  chose Firefox because it shows a large gap between t bound and t FAST . We monitored the generated block requests during the launch of Firefox with the application prefetcher, and observed that the first 12 of the entire 1566 block requests were issued by Firefox, which took about 15 ms. As the application prefetcher itself should be launched as well, FAST cannot prefetch these block requests until finishing its launch. However, we observed that all the remaining block requests were issued by FAST, meaning that they are successfully prefetched before the CPU needs them.</p><p>CPU and SSD usage patterns. We performed another experiment to observe the CPU and SSD usage patterns in each test scenario. We chose two applications, Eclipse and Firefox, representing the two groups of applications of which t FAST is close to and far from t bound , respectively. We modified the OS kernel to sample the number of CPU cores having runnable processes and to count the number of cores in the I/O wait state. <ref type="figure" target="#fig_6">Figure 7</ref> shows the CPU and SSD usage of the two applications, where the entire CPU is regarded as busy if at least one of its cores is active. Similarly, the SSD is assumed busy if there are one or more cores in the I/O wait state. In the cold start scenario, there is almost no overlap between CPU computation and SSD access for both applications. In the warm start scenario, the CPU stays fully active until the launch is completed as there is no wait. One exception we observed is the time period marked with Circle (a), during which the CPU seems to be in the eventwaiting state. FAST is shown to be successful in overlapping CPU computation with SSD access as we intended. However, CPU usage is observed to be low at the beginning of launch for both applications, which can be explained with the example in <ref type="figure">Figure 2</ref>. As Eclipse shows a shorter such time period (Circle (b)) than Firefox (Circle (c)), t FAST can reach closer to t bound . In the case of Firefox, however, the ratio of t cpu to t ssd is close to 1:1, allowing FAST to achieve more reduction of launch time for Firefox than for Eclipse.</p><p>Performance of sorted prefetch. <ref type="figure">Figure 6</ref> shows that the sorted prefetch reduces the application launch time by an average of 7%, which is less efficient than FAST, but non-negligible. One reason for this improvement is the difference in I/O burstiness between the cold start and the sorted prefetch. Most SSDs (including the one we used) support the native command queueing (NCQ) feature, which allows up to 31 block requests to be sent to a SSD controller. Using this information, the SSD controller can read as many NAND flash chips as possible, effectively increasing read throughput. The average queue depth in the cold start scenario is close to 1, meaning that for most of time there is only one outstanding request in case of SSD. In contrast, in the sorted prefetch scenario, the queue depth will likely grow larger than 1 because the prefetcher may successively issue asynchronous I/O requests using posix fadvise(), at small inter-issue intervals.</p><p>On the other hand, we could not find a clear evidence that sorting block requests in their LBA order is advantageous in case of SSD. Rather, the execution time of the sorted prefetcher was slightly longer than its unsorted version for most of the applications we tested. Also, the sorted prefetch shows worse performance than the cold start for Excel, Powerpoint, Skype, and Word. Although these observations were consistent over repeated tests, a further investigation is necessary to understand such a behavior. Simultaneous launch of applications. We performed experiments to see how well FAST can scale up for launching multiple applications. We launched multiple applications starting from the top of <ref type="table">Table 3</ref>, adding five at a time, and measured the launch completion time of all launched applications 2 . <ref type="figure" target="#fig_7">Figure 8</ref> shows that FAST could reduce the launch completion time for all the tests, whereas the sorted prefetch does not scale beyond 10 applications. Note that the FAST improvement decreased from 20% to 7% as the number of applications increased from 5 to 20. Runtime and space overhead. We analyzed the runtime overhead of FAST for seven possible combinations of running processes, and summarized the results in Table 4. Cases 2 and 3 belong to the launch profiling phase, which was described in Section 4.4. During this phase, Case 2 occurs only once, and Case 3 occurs N rawseq times. Case 4 corresponds to the prefetcher generation phase (the right side of <ref type="figure" target="#fig_4">Figure 3</ref>), and shows a relatively long runtime. However, we can hide it from users by running it in background. Also, since we primarily focused on functionality in the current implementation, there is room for further optimization. Cases 5, 6, and 7 belong to the application prefetch phase, and repeatedly occur until the application prefetcher is invalidated. Cases 6 and 7 occur only when n pre f reaches N chk , and Case 7 can be run in background.</p><p>FAST creates temporary files such as system call log files and I/O traces, but these can be deleted after FAST completes creating application prefetchers. However, the generated prefetchers occupy disk space as far as application prefetching is used. In addition, application launch sequences are stored to check the miss ratio of the corresponding application prefetcher. In our experiment, the total size of the application prefetchers and application launch sequences for all 22 applications was 7.2 MB.</p><p>FAST applicability. While previous examples clearly demonstrated the benefits of FAST for a wide range of applications, FAST does not guarantee improvements for all cases. One such a scenario is when a target application is too small to offset the overhead of loading the prefetcher. We tested FAST with the Linux utility uname, which displays the name of the OS. It generated 3 I/O requests whose total size was 32 KB. The measured t cold was 2.2 ms, and t FAST was 2.3 ms, 5% longer than the cold start time.</p><p>Another possible scenario is when the target application experiences a major update. In this scenario, FAST may fetch data that will not be used by the newly updated application until it detects the application update and enters a new launch profiling phase. We modified the application prefetcher so that it fetches the same size of data from the same file but from another offset that is not used by the application. We tested the modified prefetcher with Firefox. Even in this case, FAST reduced application launch time by 4%, because FAST could still prefetch some of the metadata used by the application. Assuming most of the file names are changed after the update, we ran Firefox with the prefetcher for Gimp, which fetches a similar number of blocks as Firefox. In this experiment, the measured application launch time was 7% longer than the cold start time, but the performance degradation was not drastic due to the internal parallelism of the SSD we used (10 channels).</p><p>Configuring application launch manager. The application launch manager has a set of parameters to be configured, as shown in <ref type="table" target="#tab_2">Table 2</ref>. If N rawseq is set too large, users will experience the cold-start performance during the initialization phase. If it is set too small, unnecessary blocks may be included in the application prefetcher. <ref type="figure" target="#fig_5">Figure 5</ref> shows that setting it between 2 and 4 is a good choice. The proper value of N chk will depend on the runtime overhead of Blktrace; if FAST is placed in the OS kernel, the miss ratio of the application prefetcher may be checked upon every launch (N chk = 1) without noticeable overhead. Also, setting R miss to 0.1 is reasonable, but it needs to be adjusted after gaining enough experience in using FAST. To find the proper value of T idle , we investigated the SSD's maximum idle time during the cold-start of applications, and found it to range from 24 ms (Thunderbird) to 826 ms (Xilinx ISE). Hence, setting T idle to 2 seconds is proper in practice. As the maximum cold-start launch time is observed to be less than 10 seconds, 30 seconds may be reasonable for T timeout . All these values may need to be adjusted, depending on the underlying OS and applications. Running FAST on HDDs. To see how FAST works on a HDD, we replaced the SSD with a Seagate 3.5" 1 TB HDD (ST31000528AS) and measured the launch time of the same set of benchmark applications. Although FAST worked well as expected by hiding most of CPU computation from the application launch, the average launch time reduction was only 16%. It is because the application launch on a HDD is mostly I/O bound; in the cold start scenario, we observed that about 85% of the application launch time was spent on accessing the HDD. In contrast, the sorted prefetch was shown to be more effective; it could reduce the application launch time by an average of 40% by optimizing disk head movements.</p><p>We performed another experiment by modifying the sorted prefetch so that the prefetcher starts simultaneously with the original application, like FAST. However, the resulting launch time reduction was only 19%, which is worse than that of the unmodified sorted prefetch. The performance degradation is due to the I/O contention between the prefetcher and the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Applicability of FAST to Smartphones</head><p>The similarity between modern smartphones and PCs with SSDs in terms of the internal structure and the usage pattern, as summarized below, makes smartphones a good candidate to which we can apply FAST:</p><p>• Unlike other mobile embedded systems, smartphones run different applications at different times, making application launch performance matter more;  <ref type="figure">Figure 9</ref>: Measured application launch time on iPhone 4 (CPU: 1 GHz, SDRAM: 512 MB, NAND flash: 32 GB).</p><p>• Smartphones use NAND flash as their secondary storage, of which the performance characteristics are basically the same as the SSD; and</p><p>• Smartphones often use slightly customized (if not the same) OSes and file systems that are designed for PCs, reducing the effort to port FAST to smartphones.</p><p>Furthermore, a smartphone has the characteristics that enhance the benefit of using FAST as follows:</p><p>• Users tend to launch and quit applications more frequently on smartphones than on PCs;</p><p>• Due to relatively smaller main memory of a smartphone, users will experience cold start performance more frequently; and</p><p>• Its relatively slower CPU and flash storage speed may increase the absolute reduction of application launch time by applying FAST.</p><p>Although <ref type="bibr">we</ref> have not yet implemented FAST on a smartphone, we could measure the launch time of some smartphone applications by simply using a stopwatch. We randomly chose 14 applications installed on the iPhone 4 to compare their cold and warm start times, of which the results are plotted in <ref type="figure">Figure 9</ref>. The average cold start time of the smartphone applications is 6.1 seconds, which is more than twice of the average cold start time of the PC applications (2.4 seconds) shown in <ref type="figure">Fig- ure 6</ref>. <ref type="figure">Figure 9</ref> also shows that the average warm start time is 63% of the cold start time (almost the same ratio as in <ref type="figure">Figure 6</ref>), implying that we can achieve similar benefits from applying FAST to smartphones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Comparison of FAST with Traditional Prefetching</head><p>FAST is a special type of prefetching optimized for application launch, whereas most of the traditional prefetching schemes focus on runtime performance improvement. We compare FAST with the traditional prefetching algorithms by answering the following three questions that are inspired by previous work <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">What to Prefetch</head><p>FAST prefetches the blocks appeared in the application launch sequence. While many prediction-based prefetching schemes <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> suffer from the low hit ratio of the prefetched data, FAST can achieve near 100% hit ratio. This is because the application launch sequence changes little over repeated launches of an application, as observed by previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>. Sequential pattern detection schemes like readahead <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref> can achieve a fairly good hit ratio when activated, but they are applicable only when such a pattern is detected. By contrast, FAST guarantees stable performance improvement for every application launch.</p><p>One way to enhance the prefetch hit ratio for a complicated disk I/O pattern is to analyze the application source code to extract its access pattern. Using the thusobtained pattern, prefetching can be done by either inserting prefetch codes into the application source code <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> or converting the source code into a computation thread and a prefetch thread <ref type="bibr" target="#b39">[40]</ref>. However, such an approach does not work well for application launch optimization because many of the block requests generated during an application launch are not from the application itself but from other sources, such as loading shared libraries, which cannot be analyzed by examining the application source code. Furthermore, both require modification of the source code, which is usually not available for most commercial applications. Even if the source code is available, modifying and recompiling every application would be very tedious and inconvenient. In contrast, FAST does not require application source code and is thus applicable for any commercial application.</p><p>Another relevant approach <ref type="bibr" target="#b5">[6]</ref> is to deploy a shadow process that speculatively executes the copy of the original application to get hints for the future I/O requests. It does not require any source modification, but consumes non-negligible CPU and memory resources for the shadow process. Although it is acceptable when CPU is otherwise stalled waiting for the I/O completion, employing such a shadow process in FAST may degrade application launch performance as there is not enough CPU idle period as shown in Figure 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">When to Prefetch</head><p>FAST is not activated until an application is launched, which is as conservative as demand paging. Thus, unlike prediction-based application prefetching schemes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>, there is no cache-pollution problem or additional disk I/O activity during idle period. However, once activated, FAST aggressively performs prefetching: it keeps on fetching subsequent blocks in the application launch sequence asynchronously even in the absence of page misses. As the prefetched blocks are mostly (if not all) used by the application, the performance improvement of FAST is comparable to that of the predictionbased schemes when their prediction is accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">What to Replace</head><p>FAST does not modify the replacement algorithm of page cache in main memory, so the default page replacement algorithm is used to determine which page to evict in order to secure free space for the prefetched blocks.</p><p>In general, prefetching may significantly affect the performance of page replacement. Thus, previous work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35]</ref> emphasized the need for integrated prefetching and caching. However, FAST differs from the traditional prefetching schemes since it prefetches only those blocks that will be referenced before the application launch completes (e.g., in next few seconds). If the page cache in the main memory is large enough to store all the blocks in the application launch sequence, which is commonly the case, FAST will have minimal effect on the optimality of the page replacement algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We proposed a new I/O prefetching technique called FAST for the reduction of application launch time on SSDs. We implemented and evaluated FAST on the Linux OS, demonstrating its deployability and performance superiority. While the HDD-aware application launcher showed only 7% of launch time reduction on SSDs, FAST achieved a 28% reduction with no additional overhead, demonstrating the need for, and the utility of, a new SSD-aware optimizer. FAST with a well-designed entry-level SSD can provide end-users the fastest application launch performance. It also incurs fairly low implementation overhead and has excellent portability, facilitating its wide deployment in various platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Various application launch scenarios (n = 4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 (Figure 2 :Figure 3 :</head><label>123</label><figDesc>Figure 1(d) shows another possible scenario where t cpu &lt; t ssd . In this case, the prefetcher cannot complete fetching s 2 before the application finishes computation c 1 . However, s 2 can be fetched by t(c 1 ) earlier than that of the cold start, and this improvement is accumulated for all of the remaining block requests, resulting in t launch :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example application prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>echo 3 &gt;</head><label>3</label><figDesc>/proc/sys/vm/drop_caches 4. Run the disk I/O profiler. Terminate the disk I/O profiler when any of the following conditions are met: (1) if no block request occurs during the last T idle seconds or (2) the elapsed time since the start of the disk I/O profiler exceeds T timeout seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The size of application launch sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Usage of CPU and SSD (sampling rate = 1 KHz).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Simultaneous launch of multiple applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 : System calls to replay access of blocks in an application launch sequenceone-by-one from S of the target application. 2. Convert s i into its associated data items stored</head><label>1</label><figDesc></figDesc><table>Block type 
System call 
Inode table 

open() 

Data block: a directory 
opendir() and readdir() 
Data block: a regular file 
read() or posix_fadvise() 
Data block: a symbolic 
link file 

readlink() 

cation. We implemented the application prefetcher gen-
erator to automatically create an application prefetcher 
for each target application. It performs the following op-
erations. 

1. Read s i in the 
LBA-to-inode map, e.g., 
(dev,LBA,size)→(datablk,filename,offset,size) or 

(dev,LBA,size)→(inode,start_inode,end_inode). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Variables and parameters used by the applica- tion launch manager</head><label>2</label><figDesc></figDesc><table>Type 
Description 
n init 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Runtime overhead (application: Firefox)</head><label>4</label><figDesc></figDesc><table>Running processes 
Runtime (sec) 
1. Application only (cold start scenario) 
0.86 
2. strace + blktrace + application 
1.21 
3. blktrace + application 
0.88 
4. Prefetcher generation 
5.01 
5. Prefetcher + application 
0.56 
6. Prefetcher + blktrace + application 
0.59 
7. Miss ratio calculation 
0.90 

</table></figure>

			<note place="foot">t ssd . In this case, the prefetcher cannot complete fetching s 2 before the application finishes computation c 1 . However, s 2 can be fetched by t(c 1 ) earlier than that of the cold start, and this improvement is accumulated for all of the remaining block requests, resulting in t launch : t launch = t ssd + t(c n ). (4) Note that n ranges up to a few thousands for typical applications, and thus, t(s 1 ) t cpu and t(c n ) t ssd . Consequently, Eqs. (3) and (4) can be combined into a single equation as: t launch ≈ max(t ssd ,t cpu ), (5)</note>

			<note place="foot" n="2"> Except for Gnome that cannot be launched with other applications, and Houdini whose license had expired.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We deeply appreciate Prof. Heonshik Shin for his support and providing research facility. We also thank our shepherd Arkady Kanevsky, and the anonymous reviewers for their invaluable comments that improved this paper. This research was supported by WCU (World Class University) program through National Research Foundation of Korea funded by the Ministry of Education, Science and Technology (R33-10085), and RP-Grant 2010 of Ewha Womans University. Sangsoo Park is the corresponding author (email: sangsoo.park@ewha.ac.kr).</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wine</forename><surname>User Guide</surname></persName>
		</author>
		<ptr target="http://www.winehq.org/docs/wineusr-guide/index" />
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Launch Time Performance Guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apple</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="http://developer.apple.com/documentation/Performance/Conceptual/LaunchTime/LaunchTime.pdf" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axboe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Block</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tracing</surname></persName>
		</author>
		<ptr target="http://www.kernel.org/git/?p=linux/kernel/git/axboe/blktrace.git;a=blob;f=README" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Block-reORGanization for self-optimizing storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhadkamkar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liptak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hristidis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Borg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FAST</title>
		<meeting>FAST</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A study of integrated prefetching and caching strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIG-METRICS</title>
		<meeting>SIG-METRICS</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic I/O hint generation through speculative execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gibson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding intrinsic characteristics and system implications of flash memory based solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMETRICS</title>
		<meeting>SIGMETRICS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="181" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Analyzing and improving GNOME startup time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colitti</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SANE</title>
		<meeting>SANE</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Practical prefetching via data compression. SIGMOD Rec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curewitz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitter</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The performance of PC solid-state disks (SSDs) as a function of bandwidth, concurrency, device architecture, and system organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCA</title>
		<meeting>ISCA</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="279" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A new I/O scheduler for solid state devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dunn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, Texas A&amp;M University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep. TAMU-ECE-2009-02</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Preload-An adaptive prefetching daemon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esfahbod</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Toronto, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Graduate Department of Computer Science, University of</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the design of a new Linux readahead framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengguang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hongsheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="75" to="84" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequential prefetching in adaptive replacement cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gill</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Modha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Sarc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX</title>
		<meeting>USENIX</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="293" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On faster application startup times: Cache stuffing, seek profiling, adaptive preloading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OLS (2005)</title>
		<meeting>OLS (2005)</meeting>
		<imprint>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="http://www.intel.com/design/flash/nand/turbomemory/index.htm" />
		<title level="m">INTEL. Intel Turbo Memory with User Pinning. Intel</title>
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing the startup time of embedded systems: A case study of digital TV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">O</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maeng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consumer Electron</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="2242" to="2247" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving application launch times with hybrid disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CODES+ISSS</title>
		<meeting>CODES+ISSS</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="373" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving Linux startup time using software resume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaminaga</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OLS (2006)</title>
		<meeting>OLS (2006)</meeting>
		<imprint>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BPLRU: A buffer management scheme for improving random writes in flash storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FAST</title>
		<meeting>FAST</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disk schedulers for solid state drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMSOFT</title>
		<meeting>EMSOFT</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="295" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">I/O performance optimization techniques for hybrid hard disk-based mobile consumer devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consumer Electron</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="1469" to="1476" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Practical prefetching techniques for parallel file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PDIS</title>
		<meeting>PDIS</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="182" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spending Moore&apos;s dividend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="62" to="69" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Linux solution for prefetching necessary data during application and system startup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichota</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prefetch</surname></persName>
		</author>
		<ptr target="http://code.google.com/p/prefetch/" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonvolatile disk caches in the storage hierarchy of mainstream computer systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthews</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hensgen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grimsrud</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Intel R Turbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memory</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Support</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solid-State</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drives</surname></persName>
		</author>
		<ptr target="http://blogs.msdn.com/e7/archive/2009/05/05/support-and-q-a-for-solid-state-drives-and.aspx" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Microsoft</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Windows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Accelerators</surname></persName>
		</author>
		<ptr target="http://www.microsoft.com/whdc/system/sysperf/perfaccel.mspx" />
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automatic compiler-inserted I/O prefetching for out-of-core applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mowry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Demke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krieger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Reducing Application Startup Time in the Solaris 8 OS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neelakanth</forename><surname>Nadgir</surname></persName>
		</author>
		<ptr target="http://developers.sun.com/solaris/articles/reducingapp.html" />
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Linux 2.6 performance improvement through readahead optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pulavarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OLS</title>
		<meeting>OLS</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="105" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Energy efficient prefetching and caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Papathanasiou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX</title>
		<meeting>USENIX</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="22" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Linux bootup time reduction for digital still camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OLS</title>
		<meeting>OLS</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Out-of-band detection of boot-sequence termination events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parush</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pelleg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ta-Shma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICAC</title>
		<meeting>ICAC</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="71" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Informed prefetching and caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patterson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Ginting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stodol-Sky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelenka</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOSP</title>
		<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="79" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russinovich</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<title level="m">Microsoft Windows Internals</title>
		<imprint>
			<publisher>Microsoft Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="458" to="462" />
		</imprint>
	</monogr>
	<note>4th ed</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<ptr target="http://www.samsung.com/global/business/semiconductor/support/brochures/downloads/hdd/hddatasheet200708.pdf" />
		<title level="m">SAMSUNG SEMICONDUCTOR. Samsung Hybrid Hard Drive</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Reducing seek overhead with application-directed prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vandebogart</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX</title>
		<meeting>USENIX</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A cost-benefit scheme for high performance predictive prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vellanki</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chervenak</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SC</title>
		<meeting>SC</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A decoupled architecture for application-specific file prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiueh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX</title>
		<meeting>USENIX</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
