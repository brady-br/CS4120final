<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST &apos;16). Open access to the Proceedings of the 14th USENIX Conference on File and Storage Technologies is sponsored by USENIX Mitigating Sync Amplification for Copy-on-write Virtual Disk Mitigating Sync Amplification for Copy-on-write Virtual Disk</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 22-25, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghai</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>University</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingshu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yubin</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Samsung Electronics</orgName>
								<orgName type="laboratory">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University † Samsung Electronics Co., Ltd</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 14th USENIX Conference on File and Storage Technologies (FAST &apos;16). Open access to the Proceedings of the 14th USENIX Conference on File and Storage Technologies is sponsored by USENIX Mitigating Sync Amplification for Copy-on-write Virtual Disk Mitigating Sync Amplification for Copy-on-write Virtual Disk</title>
					</analytic>
					<monogr>
						<title level="m">USENIX Association 14th USENIX Conference on File and Storage Technologies (FAST &apos;16)</title>
						<imprint>
							<biblScope unit="page">241</biblScope>
							<date type="published">February 22-25, 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Copy-on-write virtual disks (e.g., qcow2 images) provide many useful features like snapshot, de-duplication, and full-disk encryption. However, our study uncovers that they introduce additional metadata for block organization and notably more disk sync operations (e.g., more than 3X for qcow2 and 4X for VMDK images). To mitigate such sync amplification, we propose three optimizations, namely per virtual disk internal journal-ing, dual-mode journaling, and adaptive-preallocation, which eliminate the extra sync operations while preserving those features in a consistent way. Our evaluation shows that the three optimizations result in up to 110% performance speedup for varmail and 50% for TPCC.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One major benefit of virtualization in the cloud environment is the convenience of using image files as virtual disks for virtual machines. For example, by using the copy-on-write (CoW) feature provided by virtual disks in the qcow2 format, a cloud administrator can provide an image file as a read-only base file, and then overlay small files atop the base file for virtual machines <ref type="bibr" target="#b15">[16]</ref>. This can significantly ease tasks like VM deployment, backup, and snapshot, and bring features such as image size growing, data de-duplication <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>, and full-disk encryption. Thus, CoW virtual disks have been widely used in major cloud infrastructures like OpenStack.</p><p>However, the convenience also comes at a cost. We observe that with such features being enabled, the performance of some I/O intensive workloads may degrade notably. For example, running varmail on virtual disks with the qcow2 format only gets half the throughput of running on the raw formats. Our analysis reveals that the major reason is a dramatic increase of sync operations (i.e., sync amplification ) under qcow2, which is more than 3X compared to the raw format.</p><p>The extra sync operations are used to keep the consistency of the virtual disk. A CoW image (e.g., qcow2) contains much additional metadata for block organization, such as the mapping from virtual block numbers to physical block numbers, which should be kept consistent * ‡Corresponding authors to prevent data loss or even disk corruption. Thus, the qcow2 manager heavily uses the fdatasync system call to ensure the order of disk writes. This, however, causes notable performance slowdown as a sync operation is expensive <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b0">1]</ref>. Further, since a sync operation triggers disk flushes that force all data in the write cache to be written to the disk <ref type="bibr" target="#b24">[25]</ref>, it reduces the effectiveness of the write cache in I/O scheduling and write absorption. For SSD, sync operations can result in additional writes and subsequent garbage collection. Our evaluation shows that SSD has a 76% performance speedup for random write workload after disabling write cache flushing. A workload with frequent syncs may also interfere with other concurrent tasks. Our experiment shows that sequential writes suffer from 54.5% performance slowdown if another application calls fdatasync every 10 milliseconds. Besides, we found that other virtual disk formats like VMDK share similar sync amplification issues.</p><p>One way to mitigate the sync amplification problem is disabling the sync operations. For example, the VirtualBox (version 4.3.10) just ignores the guest sync requests for high performance <ref type="bibr" target="#b22">[23]</ref>. Besides, VMware Workstation (version 11) provides an option to enable write cache <ref type="bibr" target="#b23">[24]</ref>, which ignores guest sync operations as well. This, however, is at the risk of data inconsistency or even corruption upon crashes <ref type="bibr" target="#b5">[6]</ref>.</p><p>To enjoy the rich features of CoW virtual disks with low overhead, this paper describes three optimizations, per virtual disk internal journaling, dual-mode journaling and adaptive preallocation, to mitigate sync amplification while preserving metadata consistency.</p><p>Per virtual disk journaling leverages the journaling mechanism to guarantee the consistency of virtual disks. Qcow2 requires multiple syncs to enforce ordering, which is too strong according to our observation. To address this issue, we implement an internal journal for each virtual disk, where metadata/data updates are first logged in a transaction, which needs only one sync operation to put them to disk consistently. Such a journaling mechanism, however, requires data to be written twice, which is a waste of disk bandwidth. We further introduce dual-mode journaling which monitors each modification to the virtual disk and only logs metadata (i.e., reference table, lookup table) when there is no data overwriting.</p><p>Adaptive preallocation allocates extra blocks for a virtual disk image when the disk is growing. The preallocated blocks can be used directly in the following expansion of virtual disks. This saves the image manager from requesting the host file system for more free blocks, and thus avoids extra flush operations.</p><p>We have implemented the optimizations for qcow2 in QEMU-2.1.2. Our optimizations result in up to 50% performance speedup for varmail and 30% speedup for tpcc for a mixture of different workloads. When we run varmail and tpcc with a random workload, they can achieve 110% and 50% speedup, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>We use the qcow2 format as an example to describe the organization of a VM image and the causes of sync amplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The qcow2 Format</head><p>A qcow2 virtual disk contains an image header, a twolevel lookup table, a reference table, and data clusters, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The image header resembles the superblock of a file system, which contains the basic information of the image file such as the base address of the lookup table and the reference   Note that, each step in the whole appending process should not be reordered; otherwise, it may cause metadata inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sync Amplification</head><p>The organization of qcow2 format requires extra efforts to retain crash consistency such that the dependencies between the metadata and data are respected. For example, a data cluster should be flushed to disk before updating the lookup table; otherwise, the entry in the lookup table may point to some garbage data. The reference table should be updated before updating the lookup table; otherwise, the lookup table may point to some unallocated data cluster.</p><p>We use two simple benchmarks in QEMU-2.1.2 to compare the number of sync operations in the guest VM and the host: 1) "overwrite benchmark", which allocates blocks in advance in the disk image (i.e., the qcow2 image size remains the same before and after the test); 2) "append benchmark", which allocates new blocks in the disk image during the test (i.e., the image size increases after the test). The test writes 64KB data and calls fdatasync every 50 iterations. We find that the virtual disks introduce more than 3X sync operations for qcow2 and 4X for VMDK images, as shown in <ref type="figure">Figure 2</ref>. As shown in <ref type="figure">Figure 3</ref>, a fdatasync of the user application can cause a transaction commit in the file system. This requires two flushes (in guest VM) to preserve its atomicity, which are then translated into two set of writes in QEMU. The first write puts the data and the journal metadata of the VM to the virtual disk, which in the worst case, causes its size to grow.</p><p>To grow the virtual disk in QEMU, a data block must be allocated, and the corresponding reference table block should be set strictly before other operations. This necessitates the first flush. After that, the L2 data block must be updated strictly before the remaining operations. This necessitates the second flush. (In some extreme cases where the L1 data block should be updated as well, it introduces even more flushes  <ref type="figure">Figure 3</ref>: Illustration of sync amplification: it shows how the number of sync operations increases after the user issues fdatasync. The fdatasync with red color is necessary to impose the write ordering, while fdatasync with gray color are solely because of the flawed implementation of qcow2. J M is for journal of metadata, J C is for journal of commit block, Re f is for reference table, L2 is for L2 lookup table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Other Virtual Disk Formats</head><p>Other virtual disks have a similar structure to qcow2 virtual disk. They have an image header to describe the basic information of the virtual disk, a block map table to translate virtual block address to physical block address, and many data blocks to store the guest VM's data. For example, the grain directory and grain table in the VMDK consist of a two-level table to do address translation. The VMDK also keeps two copies of the grain directories and grain tables on disk to improve the virtual disk's resilience. FVD <ref type="bibr" target="#b21">[22]</ref> even has a bitmap for the copy-on-read feature. In summary, the organization of virtual disks will translate one update operation in the guest into several update operations in the host. Besides, the virtual disks should carefully schedule the update order to preserve crash consistency, which introduces more sync operations. Actually, our evaluation shows that VMDK has more severe sync amplification than qcow2, as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OPTIMIZATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Per Virtual Disk Internal Journaling</head><p>According to our analysis, we found that the cause of the extra sync operations is the overly-constrained semantics imposed during the modification of virtual disks. This is because the underlying file system cannot preserve the internal consistency of a virtual disk, in which certain data serves as metadata to support the rich features. As a result, the virtual disk has to impose strong ordering while being updated for the sake of crash consistency. Based on this observation, we designed a per virtual disk internal journaling mechanism. As the name suggests, each virtual disk maintains an independent and internal journal, residing in a preallocated space of the virtual disk to which the journal belongs. The per virtual disk internal journal works in a manner similar to the normal file system journal, with the exception that it only logs the modification of the content of a single virtual disk.</p><p>On a virtual disk update, the metadata (e.g., reference table and lookup table) as well as the changed data are first logged into the preallocated virtual disk journaling area, which is treated as a transaction. At the end of this update, the journal commit block of a virtual disk is appended to the end of the transaction, indicating this transaction is committed. If any failures occur before the commit block is made durable, the whole transaction is canceled, and the virtual disk is still in a consistent state. We also leverage the checksum mechanism: by calculating a checksum of all the blocks in the transaction, we reduce the number of flushes to one per transaction.</p><p>Like other journaling mechanisms, we should install the modifications logged in the journal to their home place, i.e., checkpoint. To improve the performance of checkpoint, we delay the checkpoint process for batching and write absorbing. After a checkpoint, the area took up by this transaction can be reclaimed and reused in the future.</p><p>The per virtual disk journaling relaxes the original overly-constrained ordering requirement to an all-ornothing manner, which reduces the number of flushes while retaining crash consistency.   <ref type="table">1 lookup table, L2 means level-2 lookup table, and Ref  means reference table)</ref> Dual-Mode Journaling: Journaling mechanism requires logged data to be written twice. This, under certain circumstances, severely degrades performance due to wasted disk bandwidth. To this end, we apply an optimization similar to Prabhakaran et al. <ref type="bibr" target="#b17">[18]</ref> and Shen et al. <ref type="bibr" target="#b20">[21]</ref>. This optimization, namely dual-mode journaling, drives the per virtual disk internal journaling to dynamically adapt between two modes according to the virtual disk's access pattern. Specifically, it only logs all the data when there is an overwriting of the original data; otherwise, only metadata (reference table and lookup table) is journaled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ref</head><p>It should be noted that dual-mode journaling differs from prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref> in that it leverages different journaling mode for a single file, and thus it does not cause any inconsistency issues discussed in <ref type="bibr" target="#b20">[21]</ref>. This is because our journaling mechanism only deals with a single file while prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref> needs to deal with a complete file system. The two modes used are described as follows.</p><p>No-data Journaling Mode: Instead of writing the data into the journaling, the no-data journaling mode simply calculates a checksum of the data and puts only the checksum into the journal. The data is written to the "home" place. The process is shown in <ref type="figure" target="#fig_2">Figure 4</ref>-b.</p><p>The Overwriting Problem and Full Journaling Mode: The no-data journaling mode, however, can render the recovery process inconsistent upon data overwriting. This is because the correctness of recovery relies on the checksum of the blocks in the transaction, which does not necessarily reside in the journaling area since no-data mode journaling does not log data blocks. The content of those data blocks can be arbitrarily affected by the following overwriting operations; the whose checksum, of course, can be different from the time when it was committed.</p><p>Consequently, if a transaction is not fully checkpointed when its following transaction aborts, and at the same time the data of the previous transaction is partially overwritten, the recovery process will erroneously consider the already committed transaction as a broken one. Therefore, if a disk write transaction needs to overwrite the data which has not been checkpointed, the system will switch to full-mode journaling and put the entire data into the journal, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>-c.</p><p>Crash Recovery: During crash recovery, we scan the journal to find the first transaction that has not been checkpointed. Then, we calculate the checksum of the journal data and other related data in this transaction. If the calculated checksum is the same as the checksum recorded in the current journal transaction, we apply the logged modifications for the data and metadata to their home place and do recovery for the next transaction. It the two checksums are not equal, it means that the transaction is not completely written, and we reach the end of the journal. We abort the transaction and finish crash recovery.</p><p>Other Implementation Issues: The current suboptimal implementation of qcow2 image format involves some unnecessary sync operations (as shown in grey boxes in <ref type="figure">Figure 3</ref>). We just remove these sync operations without affecting the consistency.</p><p>With the above operations, we can reduce the number of syncs to one for each flush request from the guest VM. In another word, if the guest OS issues a flush operation, there will be only one sync on the host.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adaptive Preallocation</head><p>We further diagnose the behavior of the host file system when handling guest VM's sync requests, and find that the actual number of disk flushes are usually more than the number of sync requests. This is because, if a write from the guest VM increases the size of its image file, the host file system will trigger a journal commit, which flushes the disk twice, the first for the data and the second for the journal commit block.</p><p>More specifically, Linux provides three syscalls (msync, fsync, fdatasync) for the sync operations. The fsync transfers all modified in-memory data in the file referred to by the file descriptor (fd) to the disk. The fdatasync is similar to fsync, but it does not flush metadata unless that metadata is needed in order to allow a subsequent data retrieval to be correctly handled (i.e., the file size). The msync is used for memory-mapped I/O. In qcow2, it uses fdatasync to make data persistent on the disk. Thus, if a guest VM's sync request does not increase the disk image's size, there will be only one flush operation for the data; but if the image size changes, the host file system will commit a transaction and cause an extra disk flush.</p><p>We leverage an adaptive preallocation approach to reducing the number of journal commits in the host. When the size of the image file needs to grow, we append more blocks than those actually required. A future virtual disk write operation which originally extends the image file can now be transformed into an "overwrite" operation. In this case, fdatasync will not force a journal commit in the host, which can reduce the latency of sync operation.</p><p>Specifically, we compare the position of the write operation with the image size. If the position of the write operation exceeds the image size, we will do the preallocation. Currently, the size of preallocated space is 1MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We implemented the optimizations in QEMU-2.1.2, which comprise 1300 LoC. This section presents our evaluation of the optimized qcow2 from two aspects: consistency and performance.</p><p>We conducted the experiments on a machine with a 4-core Intel Xeon E3-1230 V2 CPU (3.3GHz) and 8GB memory. We use 1 TB WDC HDD and 120G Samsung 850 EVO SSD as the underlying storage devices. The host OS is Ubuntu 14.04; the guest OS is Ubuntu 12.04. Both guest and host use the ext4 file system. We use KVM <ref type="bibr" target="#b9">[10]</ref> and configure each VM with 1 VCPU, 2GB memory, and 10GB disk. The cache mode of each VM is writeback, which is the default setting. It has good performance while being safe as long as the guest VM correctly flushes necessary disk caches <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Consistency</head><p>To validate the consistency properties, we implement a trace record and replay framework. We run a test workload in the guest and record the write operations and sync operations in the host. Then, we randomly choose a sync operation in the trace and replay all the I/O operations above this sync operation on a backup image (the image is a copy of the guest image before running the test workload). After that, we choose some write operations between this sync and the next sync operation, and apply these writes to the backup image Finally, we use the qemu-img tool to check the consistency of virtual disk image.</p><p>We record two traces, one for the append workload and the other for append + overwrite workload. In the append workload, we append 64k data and then call fdatasync in the VM. In the append + overwrite workload, we append 64k data, call fdatasync, overwrite the 64k data and then call fdatasync. We simulated 200 crash scenarios for each workload. We divide data in the virtual disk image into four types: guest data, metadata (i.e., Lookup table), journal data (i.e., the journal record for metadata's modification) and journal commit block. The 200 crash scenarios for each workload contain all four types of data loss. The result shows that optimized qcow2 can recover correctly and get a consistent state for all test cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance</head><p>We first present the performance of synchronous overwrite and append workloads. For overwrite workload, we generate a base image and allocate space in advance, then overlay a small image atop the base image. For append workload, we do the experiment directly on a newly allocated image. We also run the Filebench <ref type="bibr" target="#b14">[15]</ref> and TPCC <ref type="bibr" target="#b3">[4]</ref> workload. Filebench's varmail simulates a mail server workload and will frequently call sync operations. TPCC simulates a complete environment where a population of terminal operators executes against a database. We run the experiments under three configurations. For the first configuration (raw), we only boot one VM and run the tests. For the second configuration (seq), we boot two VMs, one for the experiments and the other is doing sequential I/O all the time. For the third configuration (ran), we also have two VMs, one for the experiments and the other is doing random I/O all the time. Figure 5: Micro-benchmark result on 2 storage media. Each separate optimization as well as the overall result is showed. "ori" refers to the original system; "flaw" refers to the system which removes unnecessary sync operations caused by qcow2 flawed implementation; "pre" refers to the adaptive prealloacation; "jou" refers to the per virtual disk internal journal; "opt" refers to the overall result. Figure 5 and <ref type="figure" target="#fig_4">Figure 6</ref> show the performance results on both HDD (hard disk driver) and SSD. For overwrite workload, our system improves the throughput by 200% for disk and 100% for SSD. For varmail, our system achieves 110% speedup when running varmail together with a random workload on HDD. The performance gain for varmail on SSD is about 50% when running varmail together with a sequential workload. <ref type="figure">Figure 7</ref> compares the TPCC transaction latency between our system and the original system. The results show that our system has lower latency, and the latency even decreased by 40% when TPCC is running together with a random workload. We also evaluate the multiple VM configurations. We run four VMs with varmail, TPCC, fileserver and webserver workloads respectively. Varmail and TPCC workloads issue syncs frequently, while fileserver and webserver have fewer sync operations. <ref type="figure">Figure 8</ref> shows the evaluation results for multiple VMs test. On HDD, the performance of varmail and tpcc improves 50% and 34%, respectively. On SSD, the performance gain is 30% and 20%, respectively. Besides, fileserver and webserver on the optimized system have similar performance to those on the original system. This is because fileserver and webserver have few sync operations and do not update qcow2 metadata frequently. Qcow2, VMDK and VDI support big clusters (1MB or more). Big clusters decrease the frequency of metadata operations and can mitigate the sync amplification. <ref type="figure" target="#fig_6">Figure 9</ref> shows using big clusters may mitigate sync amplification as our approach for the sync write workload. However, it results in 50% performance degradation for the direct write workload. This is because bigger clusters increase the cost of allocating a new cluster <ref type="bibr" target="#b7">[8]</ref>. Besides, bigger clusters reduce the compactness of sparse files. In contrast, our optimizations mitigate sync amplification without such side effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Reducing sync operations has been intensively studied <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. For example, adaptive journaling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b17">18]</ref> is a common approach to reducing journaling incurred sync operations. OptFS <ref type="bibr" target="#b0">[1]</ref> advocates separation of durability and ordering and split the sync operation into dsync and osync for file systems accordingly. However, it requires the underlying device to provide asynchronous durability notification interface. Our work focuses on the virtual disk image format and is transparent to the guest VM. NoFS <ref type="bibr" target="#b1">[2]</ref> eliminates all the ordering points and uses the backpointer to maintain crash consistency; but it is hard to implement atomic operations, such as rename. Xsyncfs <ref type="bibr" target="#b16">[17]</ref> also aimed to improve sync performance. It delays sync operations until an external observer reads corresponding data. By delaying the sync operations, there is more space for I/O scheduler to batch and absorb write operations.</p><p>Improving file system performance for the virtual machine is also a hot research topic <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19]</ref>. Le <ref type="bibr" target="#b10">[11]</ref> analyzed the performance of nested file systems in virtual machines. Li <ref type="bibr" target="#b13">[14]</ref> proposed to accelerate guest VM's sync operation by saving the syncing data in host memory and returning directly without writing to disk. FVD <ref type="bibr" target="#b21">[22]</ref> is a high-performance virtual disk format. It supports many features, such as copy-on-read and prefetching. QED <ref type="bibr" target="#b18">[19]</ref> is designed to avoid some of the pitfalls of qcow2 and is expected to be more performant than qcow2. All these work did not address the sync amplification problem.</p><p>Our work leverages several prior techniques such as checksum <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> and pre-allocation <ref type="bibr" target="#b12">[13]</ref>, but applies them to solve a new problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper uncovered the sync amplification problem of copy-on-write virtual disks. It then described three optimizations to minimize sync operations while preserving crash consistency. The evaluation showed that the optimizations notably boost some I/O intensive workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENT</head><p>We thank our shepherd Nisha Talagala and the anonymous reviewers for their constructive comments. This work is supported in part by China National Natural Science Foundation (61572314, 61303011), a research grant from Samsung, Inc., National Youth Top-notch Talent Support Program of China, Zhangjiang Hi-Tech program <ref type="bibr">(No. 201501-YP-B108-012)</ref>, and Singapore NRF (CRE-ATE E2S2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The orginization of qcow2 disk format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: #sync operations observed from inside/outside of the VM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The dual-mode journaling optimization: it shows the process of data updating on a qcow2 virtual disk, as mentioned in section 2.1. (L1 means level-1 lookup table, L2 means level-2 lookup table, and Ref means reference table)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Macro-benchmark on 2 storage media.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Latency for TPCC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: "Sync write" means calling fdatasync after each write operation; "Direct write" means opening the file with O DIRECT flag; "2M cluster" means the cluster size is 2M; The cluster size for "ori" and "opt" is 64k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table . The image file is organized at the granularity of cluster, and the size of the cluster is stored in the image header. The lookup table is used for address translation. A virtual block address (VBA) a in the guest VM is split into three parts, i.e., a=(a 1 , a 2 , a 3 ): a 1 is used as the L1 table's index to locate the corresponding L2 table; a 2 is used as the L2 table's index to locate the corresponding data cluster; a 3 is the offset in the data cluster. The reference table is used to track each cluster used by snapshots. The refcount in ref- erence table is set to 1 for a newly allocated cluster, and its value grows when more snapshots use the cluster.</head><label>.</label><figDesc></figDesc><table>Data 
Cluster 

Data 
Cluster 

L2 Table Addr 
Cluster Addr 

L1 Table 
L2 Table 

L2 Table 

Reference 
Table 

a1 
a2 
a3 

Image Header 

Guest (Virtual) Block Address 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>) . The third flush is used to update the base image's reference table. When creating a new image based on the base image, the refcount in the reference table of the base image will be increased by one to indicate that another image uses the base image's data. When updating the new image, qcow2 will copy data from the base image to a new place and do updates.at most.</head><label>.</label><figDesc>The new image will use the COW data and will not ac- cess the old data in the base image, so the refcount in the base image should be decreased by one. The third flush is used to make the reference table of the base image durable. The fourth flush is introduced solely because of the suboptimal implementation in QEMU. The sec- ond write is the same as the first one, which needs four flushes. Consequently, we need around eight flushes for one guest fdatasync</figDesc><table>Ref 
Data 
L2 

VM 

HOST 

APPLICATION 

FILE SYSTEM 

Data 

Data 

JM 
JC 

fdatasync 

Flush 
Flush 

fdatasync 
fdatasync 
Ref 

(a) User application issues a fdatasync 

(b) Journaling in guest FS 

Ref 
Data 
L2 
Ref 
fdatasync 
fdatasync 
fdatasync 
fdatasync 
fdatasync 
fdatasync 
Unnecessary 
Unnecessary 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimistic crash consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidambaram</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="228" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Consistency without ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidambaram</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A cryptographic checksum for integrity protection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cohen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="505" to="510" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Oltp-bench: An extensible testbed for benchmarking relational databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difallah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cudre-Mauroux</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="277" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ext4 metadata checksums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ext4</forename><surname>Wiki</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Problem for virtualbox doesn&apos;t flush</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forums</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename></persName>
		</author>
		<ptr target="https://forums.virtualbox.org/viewtopic.php?t=13661" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajnoczi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="http://www.linux-kvm.org/images/d/d6/Kvm-forum-2013-toward_qcow2_deduplication.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">cluster size parameter on qcow2 image format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajnoczi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<ptr target="http://lists.gnu.org/archive/html/qemu-devel/2012-02/msg03164.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The effectiveness of deduplication on virtual machine disk images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SYSTOR 2009: The Israeli Experimental Systems Conference</title>
		<meeting>SYSTOR 2009: The Israeli Experimental Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">kvm: the linux virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kivity</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Laor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lublin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liguori</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding performance implications of nested file systems in a virtualized environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Waldio: eliminating the filesystem journaling in resolving the journaling of journal anomaly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 USENIX Annual Technical Conference (USENIX ATC 15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="235" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Buffer and i/o resource pre-allocation for implementing batching and buffering techniques for video-on-demand systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golubchik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 13th International Conference on</title>
		<meeting>13th International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="344" to="353" />
		</imprint>
	</monogr>
	<note>Data Engineering</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new disk i/o model of virtualized cloud environment. Parallel and Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1129" to="1138" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcdougall</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Filebench</surname></persName>
		</author>
		<ptr target="http://sourceforge.net/apps/mediawiki/filebench/index.php?title=Filebench" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The qcow2 image format</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcloughlin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<ptr target="https://people.gnome.org/~markmc/qcow-image-format.html" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethink the sync</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nightingale</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flinn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Analysis and evolution of journaling file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabhakaran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="105" to="120" />
		</imprint>
	</monogr>
	<note>General Track</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Qed specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qemu</forename><surname>Wiki</surname></persName>
		</author>
		<ptr target="http://wiki.qemu.org/Features/QED/Specification" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Qemu emulator user documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qemu</forename><surname>Wiki</surname></persName>
		</author>
		<ptr target="http://qemu.weilnetz.de/qemu-doc.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Journaling of journal is (almost) free</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="287" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fvd: A high-performance virtual machine image format for cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Responding to guest ide/sata flush requests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virtual</forename><surname>Box Manual</surname></persName>
		</author>
		<ptr target="https://www.virtualbox.org/manual/ch12.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Performance best practices for vmware workstation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vmware</forename><surname>Support</surname></persName>
		</author>
		<ptr target="www.vmware.com/pdf/ws7_performance.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wikipedia</forename><surname>Diskbuffer</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Disk_buffer" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
