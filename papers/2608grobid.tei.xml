<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Using R for Iterative and Incremental Processing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
							<email>shivaram@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">HP Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Indrajit</forename><surname>Roy</surname></persName>
							<email>indrajitr@hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">HP Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Auyoung</surname></persName>
							<email>alvina@hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">HP Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
							<email>rob.schreiber@hp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC Berkeley</orgName>
								<orgName type="institution" key="instit2">HP Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Using R for Iterative and Incremental Processing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>It is cumbersome to write complex machine learning and graph algorithms in existing data-parallel models like MapReduce. Many of these algorithms are, by nature, iterative and perform incremental computations, neither of which are efficiently supported by current frameworks. We argue that array-based languages, like R [1], are ideal to express these algorithms, and we should extend these languages for processing in the cloud. In this paper we present the challenges and abstractions to extend R. Early results show that many computations are an order of magnitude faster than processing in Hadoop.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Randomness combined with linear algebra is a powerful tool for modern problems. -Ravi Kannan, FCRC 2011 <ref type="bibr" target="#b7">[8]</ref> Many real-world analytics applications are best formulated as iterative linear algebra operations. For example, PageRank and anomaly detection on graphs calculate eigenvectors of large matrices <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> and recommendation systems implement matrix decomposition <ref type="bibr" target="#b16">[17]</ref>. Even graph algorithms (shortest path, spanning tree, strongly connected components, etc.) primarily involve array manipulation and can be expressed with linear algebra operators such as matrix multiply <ref type="bibr" target="#b8">[9]</ref>.</p><p>Existing frameworks for large-scale processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> are both ill-suited to express and inefficient to implement many applications of this type. For example, it is known that most large-scale frameworks are not efficient for matrix operations <ref type="bibr" target="#b13">[14]</ref>. Linear algebraic algorithms execute on structured data, such as matrices, while MapReducelike systems do not retain this structure.</p><p>An additional challenge of implementing these applications is that many are incremental in nature, and refine their underlying mathematical models by analyzing newly arriving data. For example, user recommendations in Netflix and Amazon should be updated as new ratings appear, PageRank recalculated as Web pages change, and spammers in a social network detected as they add spurious relationships. Current systems either don't support incremental computations or, if they do, are inefficient for linear algebra (Incoop <ref type="bibr" target="#b1">[2]</ref>).</p><p>A major challenge of supporting incremental computations is creating appropriate bindings between the storage system and the processing framework. Caching data in memory <ref type="bibr" target="#b15">[16]</ref> can speed up iterative programs, but current systems do not have primitives to maintain consistency in the presence of updates to the storage systems.</p><p>As our reliance upon linear algebra for analyzing massive data increases, we need large-scale systems that can express and efficiently implement such analysis.</p><p>We argue that array-based languages such as R provide a more appropriate programming model to express many machine learning and graph algorithms. The core construct of arrays make these languages ideal to represent vectors and matrices, and perform matrix operations. R has traditionally been the software of choice for machine learning users and statisticians, albeit for small problem sizes. It does not support scalable, fault-tolerant computations and was not built for incremental processing.</p><p>In this paper, we embrace R and extend it for large scale incremental processing. Our primary focus is on machine learning and graph algorithms. We hypothesize that by extending R, programmers will not only express the algorithms in a natural, linear algebraic formulation, but that the execution of the underlying implementation will also be significantly more efficient.</p><p>We present a prototype called Presto that extends R to run on a cluster and supports incremental processing. Early results show that several algorithms can be expressed in fewer than 140 lines of Presto code and are an order of magnitude faster than Hadoop implementations.</p><p>2 Challenges of using R While R has over 2,000 packages for analysis, it is primarily used as a single threaded, single machine installation. R is not scalable nor does it support incremental processing. We list the challenges of using R, some of which are due to incremental processing while others arise because of the array-based programming model. Structure and Scalability. Scaling R to run on a cluster has its challenges. Unlike MapReduce, Spark and others, where only one record is addressed at a time, the ease of array-based programming is due to a global view of data. R programs maintain the structure of data by mapping data to arrays and manipulating them. For example, graphs are represented as adjacency matrices and outgoing edges of a vertex are obtained from the corresponding row (used in PageRank, shortest path, etc). In contrast, MapReduce-like programs don't act on the global structure of data which results in inefficiencies: Pregel observes that MapReduce has to pass the entire state of the graph between steps <ref type="bibr" target="#b9">[10]</ref>. Ideally, after scaling R, programmers will still be able to address and manipulate distributed arrays. This goal forces us to ask: what kind of memory management and runtime support is required for scaling an array-based language like R?</p><p>Sparse datasets. Most real-world datasets are sparse. For example, the Netflix prize dataset is a matrix with 480K users (rows) and 17K movies (cols) but only 100 million of the total possible 8 billion ratings are available. Similarly, very few of the total edges are present in Web graphs. It is important to store and manipulate such data as sparse matrices and retain only non-zero entries.</p><p>How do we efficiently handle distributed sparse arrays and assign tasks to process them? Without careful task assignment performance can suffer from load imbalance: certain tasks may process partitions containing many non-zero elements and end up slowing down the whole system. Load imbalance is not a problem in MapReduce based systems where mappers scan any of the equal-size data partitions but the algorithm pays the additional price of sorting to send the right data to the reducers. Supporting incremental updates is also challenging as array partitions which were previously sparse may become dense and vice-versa. How do we handle such imbalance in the data partitions and perform efficient scheduling and straggler mitigation?</p><p>Incremental processing. In incremental processing, if a programmer writes y = f (x), then y is recomputed automatically whenever x changes. Incremental processing raises many challenging questions. Only a few portions of the input may change; hence only the affected parts of the algorithm should be re-executed. How do we express such partial computations without scanning the whole dataset? Since new data continuously enters the system how should we enforce that distributed algorithms run only on a consistent view of the data?</p><p>Storage. Our target environment is one where data resides in distributed storage and multiple parallel programs incrementally process the data. What are the storage requirements and interfaces for an array-based system? How do we maintain consistency between the inmemory data of the program and the data stored on disk?</p><p>3 Background on R In this section we briefly review R and the syntax for arrays <ref type="bibr" target="#b0">[1]</ref>. R uses interpreted conditional execution (if), loops (for, while, repeat), and commonly uses procedures written in C, C++ and FORTRAN for better performance. Line 1 in <ref type="figure" target="#fig_0">Figure 1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Presto: New R abstractions</head><p>Presto extends R with new language extensions and a runtime to manage distributed execution. In addition to scalability, these extensions add parallel execution and incremental processing. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, programmers use these extensions to write a Presto program and then submit it to a master node. The runtime at the master node is in charge of the overall execution. It fault tolerantly executes the Presto program as distributed tasks across worker nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Storage driver</head><p>The storage driver in Presto is used to read input data, handle incremental updates, and save output data. While Presto can be ported to different storage systems using a driver program, it is natural to use it with a distributed table store such as HBase or HP Vertica. Using a table store makes it easier to map the complete data or subsets of it to arrays. In this paper we will assume that HBase is the underlying storage layer. Multiple programs, including MapReduce, SQL queries, and Presto programs may execute on the same HBase data. The Presto storage driver exports an interface that allows programs to register callbacks on tables (similar to database triggers). Callbacks are needed to notify the Presto program when data enters the store or is modified during incremental processing.  The storage layer should support atomic multi-row updates which is available in databases and can be built on top of HBase <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Distributed arrays</head><p>Presto solves the problem of structure and scalability by introducing distributed arrays in R. Distributed arrays (darray) provide a shared, in-memory view of multidimensional data stored across multiple machines. Distributed arrays have the following characteristics:</p><p>Partitioned. Distributed arrays can be partitioned into chunks of rows, columns or blocks. Users can either specify the size of the partitions or let the runtime determine it. Locally, sparse partitions are stored in a compressed sparse column format. Partitions help programmers specify coarse-grained parallelism by assigning tasks to operate on partitions. Partitions can be referred to by the splits function.</p><p>Shared. Distributed arrays can be read-shared by multiple concurrent tasks. The runtime can leverage user hints to cache and co-locate data. Such hints reduce the overhead of remote copying during computation. Concurrent writes to array partitions are not allowed in the system. This is a conscious choice as Presto targets linear algebra operators which perform transformations from an input vector space to an output vector space. In such operations, each output element is calculated and written only once. Hence, Presto supports single-writer, multiple-reader consistency semantics.</p><p>Dynamic. Distributed arrays can be directly constructed from data in the storage layer. The Presto storage driver supports parallel loading of array partitions. If the array registered a callback on the storage table then whenever the data is changed the array will be notified and updated by the driver. Distributed arrays can also be created, re-sized and updated in the form of intermediate data during a computation. Thus, distributed arrays are dynamic; both the contents and the size of the distributed arrays can change as data is incrementally updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Distributed parallelism</head><p>Presto provides programmers with the foreach construct to execute deterministic functions in parallel. The Presto runtime creates tasks on worker nodes for parallel execution of the loop body. By default, there is an implicit barrier at the end of the loop to ensure that all parallel tasks finish before statements after the loop are executed. Programmers can set the loop parameter wait to false to remove the barrier and create tasks that continue to run without synchronizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Incremental computations</head><p>Presto introduces onchange and update to execute incremental algorithms on a consistent view of data. Programmers express dependencies by waiting on updates to distributed arrays. For example, onchange(A){..} implies that the embedded statements will be executed whenever array A is updated. A can also be a list of distributed arrays or just an array partition.</p><p>Programmers use the update construct to propagate changes to data. Programmers have the flexibility to determine what constitutes a change. For example, a PageRank calculation (Section 4.5) may batch multiple changes to the whole Web graph matrix, M, and then call update(M), or it may absorb changes corresponding to only the top 100 Websites and call update on the submatrix, update(M <ref type="bibr">[v]</ref>). In both cases the runtime will invoke the corresponding tasks that are waiting for the changes.</p><p>By calling update, programmers not only trigger the corresponding onchange tasks but also bind the tasks to the data that they should process. The update construct creates a version vector that succinctly describes the state of the array, including the versions of partitions that may be distributed across machines. This version vector is sent to all waiting tasks. Each task fetches the data corresponding to the version vector and, thus, executes on a programmer-defined, consistent view of data. <ref type="figure" target="#fig_2">Figure 3</ref> shows the code for incremental PageRank. The PageRank of a Web page measures its relative importance in the Web. The graph is represented as an adjacency matrix M. PageRank is the principal eigenvector of the matrix and is calculated in parallel (lines 5-12) using the power method <ref type="bibr" target="#b2">[3]</ref>. In line 1, M is partitioned and loaded in parallel from the HBase table. The vector pgr is the initial PageRank vector and is partitioned similar to M. The PageRank calculation code is embedded inside the onchange clause in line 4. Therefore, whenever M is updated the changes are propagated to the PageRank task waiting on the onchange clause. For clarity we have simplified the code; the actual PageRank calculation occurs on the transition matrix and not the adjacency matrix. Therefore, changes to the adjacency matrix triggers recalculation of the transition matrix, which in turn causes re-computation of PageRank. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Example: PageRank</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Design and implementation sketch</head><p>The Presto master acts as the control thread for program execution. New tasks are created on workers whenever foreach loops are encountered in the program. The master keeps a map of the variables and their physical location which is used by workers to exchange information using pairwise communication. Presto reuses results from previous computations using task level memoization. For example, in the matrix multiplication C = A × B, if only a few rows of A change then only the corresponding blocks in C will be re-calculated. We briefly describe other mechanisms used in Presto.</p><p>Dynamic partitioning. Partitioning a sparse matrix uniformly by rows or columns may lead to an uneven distribution of non-zero elements. Since it is important to distribute the computation evenly across tasks, the Presto runtime checks the size of partitions and if required, divides them further to reduce load imbalance. As the input for our programs are from a table in HBase, we reuse the HBase region boundaries to create the initial sparse matrix partitions. For iterative algorithms, we refine the partitions based on the execution of the first few iterations. Although repartitioning may involve copying data, it is expensive to calculate optimal partitions statically and static partitions do not help in cases where the data is incrementally updated. Dynamic partitioning also provides the flexibility to increase or decrease the amount of parallelism (tasks) in the program at runtime.</p><p>Versioning. Presto uses versioning to ensure correctness when arrays are updated across iterations or data is incrementally added from external sources. For example, write conflicts may arise if tasks read share an array which is also written to within that iteration. To avoid conflicts, each partition of a distributed array has a version. The version of a distributed array is a concatenation of the versions of its partitions, similar in spirit to vector clocks. Writes to array partitions create a new version of the partition. This version update ensures that concurrent readers of previous versions still have access to data. By versioning arrays Presto can safely execute iterative algorithms and multiple concurrent onchange tasks.</p><p>Co-location and caching. Presto workers execute functions which generally require multiple array partitions including remote ones. Presto uses two mechanisms to reduce communication overhead: partition co-location and caching. Partitions which are accessed and modified together in the same function are co-located on the same worker. Further, Presto automatically caches remote arrays partitions that are fetched during task execution. Workers use the version vector to make sure the cached arrays are valid and use the least recently used policy to evict older entries. Due to automatic caching, Presto does not need to provide explicit directives such as broadcast variables <ref type="bibr" target="#b15">[16]</ref>.</p><p>Handling dependences. Programmers can use the onchange clause to express dependence on multiple arrays. Presto resolves a multi-dependence as a logical conjunction. Statements embedded in onchange are invoked only when update has been called on all the arrays present in the dependence. Presto uses task queues to store tasks that will be invoked when data dependences have been satisfied. Task queues store a handle to functions embedded in the onchange clause. The onchange construct registers callbacks on distributed arrays that are specified in the clause. Whenever update is called on an array, Presto notifies the registered onchange task. The notification includes the version vector of the array which the onchange task should process. When all the dependences of an onchange task are satisfied, it is executed by the runtime.</p><p>Fault tolerance. Presto uses primary-backup replication to withstand failures of the master node. Only the meta-data information like the symbol table, program execution state, and worker information is replicated. When workers fail they are restarted and the corresponding functions are re-executed. Presto uses re-execution to transitively reconstruct lost partitions. Partitions are periodically made durable on HBase for faster recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We have implemented six algorithms in Presto: matrix multiply, PageRank, Netflix recommendation <ref type="bibr" target="#b16">[17]</ref>, Twitter anomaly detection (calculates eigenvalues using iterative Lanczos) <ref type="bibr" target="#b6">[7]</ref>, vertex centrality in a graph, and Smith Waterman alignment used in gene sequencing. Each of these algorithms is implemented in fewer than 140 lines of code. Our comparison with Apache Mahout shows that Presto can be 20× faster than Hadoop. We are implementing other graph algorithms in Presto and plan to compare against recent systems like Spark. Y-axis is log scale.</p><p>1.2B edge ClueWeb09 1 graph. With 64 workers, each iteration in Hadoop takes about 161 seconds while Presto takes only 8 seconds on average. The first iteration in Presto takes 175 seconds because it incurs the overhead of disk loading and has data imbalance; the first partition has one-sixth of the total data. <ref type="figure">Figure 4</ref>(b) shows the convergence time when 1M Web links (0.1% of the Web graph) are updated in a day. We compare the time to incrementally compute PageRank as the number of updates vary versus running one calculation from scratch at the end of a day (Static). The plot shows that convergence time increases with number of updates; from 220 seconds for 100K updates to 327 seconds for 1M updates. Processing each update is considerably faster than performing the Static computation, which takes 602 seconds. However, incremental processing means computations occur more often: if the batch size is 100K then 10 re-computations are triggered and the total processing time (the middle bar) exceeds that of Static computation. Thus, there is a tradeoff between freshness of results and overall resource usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related work</head><p>Programming models such as MapReduce, DryadLINQ, and others do not export a global view of data. They are inefficient to express linear algebraic formulations, and don't support incremental processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b10">11]</ref>. Piccolo is closest to Presto in terms of programming model as it is based on partitioned tables but uses a key-value interface instead of matrices and does not focus on linear algebra <ref type="bibr" target="#b13">[14]</ref>. Piccolo currently does not support incremental processing. Parallel MATLAB and certain extensions of R execute distributed array programs but don't handle faults, load imbalance or incremental processing. MadLINQ <ref type="bibr" target="#b14">[15]</ref> provides a platform on Dryad specifically for matrix computations. Similar to Presto, MadLINQ can reuse existing matrix libraries on local partitions, is fault tolerant and distributed. However MadLINQ does not efficiently handle sparse datasets or support dynamic partitioning.</p><p>Incoop uses memoization for incremental computa-1 http://lemurproject.org/clueweb09.php tions but works only for MapReduce jobs thus inheriting its inefficiencies <ref type="bibr" target="#b1">[2]</ref>. DryadInc supports a limited form of incremental processing and does not handle dynamic data or arbitrary task dependences <ref type="bibr" target="#b12">[13]</ref>. Percolator <ref type="bibr" target="#b11">[12]</ref> is Google's incremental processing system that applies multi-row transactional updates to Bigtable <ref type="bibr" target="#b3">[4]</ref>. Presto can reuse Percolator's storage abstractions such as atomic multi-row updates and notifications for HBase and HP Vertica. Unlike Percolator, Presto also provides the language abstractions and mechanisms to handle fine-grained incremental processing such as dynamic partitioning, consistent updates, and memory management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper advocates the use of array-based languages, such as R, for large-scale machine learning and graph processing. We list the challenges of using R and propose abstractions to extend it for such computations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example array use in R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Presto architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Incremental PageRank on a dynamic Web graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 (Figure 4 :</head><label>44</label><figDesc>Figure 4: PageRank experiment (a) Comparison with Hadoop (b) Convergence time as the Web links are updated. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Presto Program = R + (darray, foreach, splits, onchange, update) In-memory</head><label></label><figDesc></figDesc><table>Presto Worker 
Presto Worker 
Presto Worker 

Presto Master 

(Symbol table, Task Scheduler, Memory Manager, Fault tolerance) 

R environment 

Presto Worker 

R environment 

Presto Worker 

R environment 

Presto Worker 

Storage layer (HBASE, Vertica) 

Presto storage driver (callback, atomic) 

8/26/2011 
40 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.r-project.org" />
		<title level="m">The R project for statistical computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Incoop: Mapreduce for incremental computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhatotia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">A</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pasquin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOCC&apos;11</title>
		<meeting>SOCC&apos;11<address><addrLine>Cascais, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual Web search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh international conference on World Wide Web, WWW7</title>
		<meeting>the seventh international conference on World Wide Web, WWW7</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bigtable: A distributed storage system for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI 2006</title>
		<meeting>OSDI 2006</meeting>
		<imprint>
			<date type="published" when="2006-11" />
			<biblScope unit="page" from="205" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dryad: Distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spectral Analysis for Billion-Scale Graphs: Discoveries and Implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Meeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD (2)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Algorithms: Recent highlights and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISCA&apos;11</title>
		<meeting>of ISCA&apos;11<address><addrLine>San Jose, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph Algorithms in the Language of Linear Algebra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kepner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fundamentals of Algorithms. SIAM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pregel: A system for large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CIEL: A universal execution engine for distributed data-flow computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;11</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-scale incremental processing using distributed transactions and notifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dabek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;10</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DryadInc: Reusing work in large-scale computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotCloud&apos;09</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piccolo: Building fast, distributed programs with partitioned tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;10, Vancouver</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MadLINQ: large-scale distributed matrix computation for the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moscibroda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM european conference on Computer Systems, EuroSys &apos;12</title>
		<meeting>the 7th ACM european conference on Computer Systems, EuroSys &apos;12<address><addrLine>Bern, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="197" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotCloud&apos;10</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LargeScale Parallel Collaborative Filtering for the Netflix Prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAIM &apos;08</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
