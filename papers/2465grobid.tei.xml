<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open access to the Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST &apos;19) is sponsored by ScaleCheck: A Single-Machine Approach for Discovering Scalability Bugs in Large Distributed Systems This paper is included in the Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST &apos;19). ScaleCheck: A Single-Machine Approach for Discovering Scalability Bugs in Large Distributed Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 25-28, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><forename type="middle">A</forename><surname>Stuardo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">America</forename><forename type="middle">;</forename><surname>Riza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Suminto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Lukman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Chuang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cloudera</forename><forename type="middle">;</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><forename type="middle">A</forename><surname>Stuardo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Leesatapornwongsa</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Samsung Research America † Cloudera</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riza</forename><forename type="middle">O</forename><surname>Suminto</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ke</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Lukman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Chuang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">Tanakorn Leesatapornwongsa</orgName>
								<orgName type="institution" key="instit3">Samsung Research</orgName>
								<orgName type="institution" key="instit4">University of Chicago</orgName>
								<orgName type="institution" key="instit5">University of Chicago</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open access to the Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST &apos;19) is sponsored by ScaleCheck: A Single-Machine Approach for Discovering Scalability Bugs in Large Distributed Systems This paper is included in the Proceedings of the 17th USENIX Conference on File and Storage Technologies (FAST &apos;19). ScaleCheck: A Single-Machine Approach for Discovering Scalability Bugs in Large Distributed Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February 25-28, 2019</date>
						</imprint>
					</monogr>
					<note>https://www.usenix.org/conference/fast19/presentation/stuardo</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present SCALECHECK, an approach for discovering scalability bugs (a new class of bug in large storage systems) and for democratizing large-scale testing. SCALECHECK employs a program analysis technique, for finding potential causes of scalability bugs, and a series of colocation techniques , for testing implementation code at real scales but doing so on just a commodity PC. SCALECHECK has been integrated to several large-scale storage systems, Cassan-dra, HDFS, Riak, and Voldemort, and successfully exposed known and unknown scalability bugs, up to 512-node scale on a 16-core PC.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Being a critical backend of many today's applications and services, storage systems must be highly reliable. Decades of research address a variety of storage dependability issues, including availability <ref type="bibr" target="#b37">[44,</ref><ref type="bibr" target="#b48">55]</ref>, consistency <ref type="bibr" target="#b34">[41,</ref><ref type="bibr" target="#b71">77]</ref>, durability <ref type="bibr" target="#b44">[51,</ref><ref type="bibr" target="#b66">72]</ref>, integrity <ref type="bibr" target="#b28">[36,</ref><ref type="bibr" target="#b49">56]</ref>, security <ref type="bibr" target="#b46">[53,</ref><ref type="bibr" target="#b65">71]</ref>, and reliability <ref type="bibr" target="#b67">[73,</ref><ref type="bibr" target="#b68">74]</ref>.</p><p>The dependability challenge grows as storage systems continue to scale in large distributed manners, especially in the last couple of years where the field witnesses a phenomenal deployment scale; Netflix runs tens of 500-node Cassandra clusters <ref type="bibr" target="#b26">[34]</ref>, Apple deploys a total of 100,000 Cassandra nodes <ref type="bibr" target="#b1">[2]</ref>, Yahoo! revealed the largest Hadoop/HDFS cluster with 4500 nodes <ref type="bibr" target="#b27">[35]</ref>, and Cloudera's customers deploy Spark on 1000 nodes <ref type="bibr" target="#b19">[24,</ref><ref type="bibr" target="#b22">27]</ref>.</p><p>Is scale a friend or a foe <ref type="bibr" target="#b62">[68]</ref>? On the positive side, scale surpasses the limit of a single machine in meeting increasing demands of compute and storage. On the negative side, this new era of "cloud-scale" storage systems has given birth to a new class of bug, scalability bugs, as defined in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>From our in-depth study of scalability bugs ( §2), we identified two challenges. First, scalability bugs are not easy to discover; their symptoms only surface in large deployment scales (e.g., N &gt;100 nodes). Protocol algorithms might seem scalable in design sketch, but until real deployment takes place, some bugs remain unforeseen (i.e., there are specific e.g., N&gt;100</p><p>Scalability bugs: Latent bugs that are scale dependent, whose symptoms surface in large-scale deployments (e.g., N &gt;100 nodes), but not necessarily in small/mediumscale (e.g., N &lt;100) deployments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples:</head><p>"obvious symptom in 1000 nodes" <ref type="bibr">[Cassandra bug #6127]</ref>, "with &gt;500 nodes, ... trouble" <ref type="bibr">[# 6409]</ref>; "16800 maps <ref type="bibr">[recovery]</ref> was slow" [Hadoop #3711], "1900 nodes, [namenode's] queue overflowed" <ref type="bibr">[#4061]</ref>; "with &gt;200 nodes, it doesn't work" <ref type="bibr">[HBase #12139</ref>]. implementation choices whose impacts at scale are unpredictable). Last but not least, their root causes are often hidden in the rarely tested background and operations protocols.</p><p>Second, the common practice of debugging scalability bugs is arduous, slow and expensive. For example, when customers report scalability issues, the developers might not have direct access to the same cluster scale and must wait for a "higher-level" budget approval for using large test clusters. As it stands today, many developers are heavily reliant on test clusters operated by large companies to do scale testing and only accessible to expert developers <ref type="bibr" target="#b21">[26]</ref>.</p><p>These realities raise the following question: how to discover latent scalability bugs and democratize large-scale testing? To this end, we introduce SCALECHECK, a concept that emphasizes the need to scale-check distributed system implementations at real scales, but do so cheaply on just one machine, hence empowering more developers to perform large-scale testing and debugging.</p><p>We design SCALECHECK with two components (SFIND and STEST) to address the two challenges. First, to reveal hidden scalability bugs, we build SFIND, a program analysis support for finding "scale-dependent loops." This strategy is based on our findings that the common root cause of scalability bugs is loops that iterate on data structures that grow as the system scales out (e.g., an O(N 3 ) loop that iterates through lists of node descriptors). Such loops can span across multiple functions and classes and iterate a va-riety of data structures, hence the need for an automated approach. With SFIND output, developers can setup the necessary workloads that will exercise the loops and reveal any potential impacts to performance or availability.</p><p>Next, to democratize large-scale testing, we build STEST, a single-machine scale-testing framework. We target one machine because arguably the most popular testing practice is via unittests, which only requires a PC. Developers already invest a significant effort on unittests; their LOC can reach 20% of the system's code itself. However, current distributed systems and their unittests are not built with single-machine scale-testing in mind. For example, naively packing nodes as processes/VMs onto one machine quickly hits a colocation limit of 50 nodes/machine and we found no way to achieve a high colocation factor with black-box methods (no target system modification). Thus, we introduce novel colocation techniques such as global-event driven architecture (GEDA) in single-process cluster and processing illusion (PIL) with non-intrusive modification.</p><p>To show the generality and effectiveness of SCALECHECK, we have integrated SCALECHECK to a variety of large-scale storage systems, Cassandra <ref type="bibr" target="#b51">[58]</ref>, HDFS <ref type="bibr">[18]</ref>, Riak <ref type="bibr" target="#b23">[30]</ref>, and Voldemort <ref type="bibr">[29]</ref>, across a total of 15 earlier and newer releases. We scale-checked a total of 18 protocols (bootstrap, rebalance, add/decommission nodes, etc.), reproduced 10 known bugs and discovered 4 unknown critical scalability bugs (in Cassandra and HDFS). By only modifying the target systems in 179 to 918 LOC (and with a generic STEST library), we can colocate up to 512 nodes on a 16-core 32-GB commodity PC with high result accuracy (i.e., observe a similar behavior as in the real-scale deployment).</p><p>SCALECHECK is unique compared to related work. For example, scalability simulation <ref type="bibr" target="#b32">[39,</ref><ref type="bibr" target="#b50">57]</ref> only checks models, but SCALECHECK checks implementation code. Extrapolation from "mini clusters" <ref type="bibr" target="#b50">[57,</ref><ref type="bibr" target="#b69">75,</ref><ref type="bibr" target="#b74">80]</ref> does not work if the bug symptoms do not surface in small deployments, but SCALECHECK checks at real scales. Finally, emulation "tricks" run implementation code at real scale but in a smaller emulated environment <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">48,</ref><ref type="bibr" target="#b72">78]</ref> (the same category SCALECHECK can be put in), however existing techniques have limitations such as not addressing CPU contention and not finding potential causes automatically (more in §7). We also acknowledge many other works in improving storage scalability <ref type="bibr" target="#b35">[42,</ref><ref type="bibr" target="#b64">70]</ref>, while our work emphasizes on scalability faults.</p><p>In summary, scalability bugs are new-generation bugs to combat in modern cloud-scale storage. Finding them without dependence of large clusters is a new research area to explore. In fact, this problem was discussed in a recent large meeting of Hadoop committee <ref type="bibr" target="#b21">[26]</ref>. Currently, many new features in the alpha releases of Hadoop/HDFS still "sit on the shelf," i.e., it is hard to test alpha (or even beta) releases at real scales as large production systems are not always ac- cessible for testing. Some new features are still pushed and deployed but without much confidence. With this unideal reality, the committee agrees on the need for this new research, that it will increase their confidence on new releases <ref type="bibr" target="#b21">[26]</ref>. Some companies began to invest in building scale-testing frameworks. For example, LinkedIn just released their scaletesting framework this year <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> but it only emulates storage space specifically for HDFS. For interested readers, we provide a supplemental file <ref type="bibr" target="#b0">[1]</ref>. In the following sections, we present an extended motivation ( §2), SCALECHECK design, application and implementation, and evaluation ( §3-5) discussion, related work, and conclusion ( §6-8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Scalability Bugs</head><p>Scalability bugs are not a well-understood problem. To the best of our knowledge, we provide the first in-depth look at scalability bugs in scale-out systems.</p><p>(a) What is an example of scalability bugs? In Cassandra issue #c6127 in <ref type="figure">Figure 2</ref>  <ref type="bibr" target="#b6">[7]</ref>, the bug surfaced when bootstrapping a large cluster. Here, every node receives gossips from peer nodes (with their ring views), then find any difference to synchronize their views of the ring. The root cause is that during bootstrapping with many view changes, the gossip processing is scale-dependent, O(N 3 ), as it iterates through the node's and peer's ring data structures and uses a list-copy mechanism. When N is large, this CPUintensive process creates a backlog of new gossips, hence many nodes are inadvertently declared dead (and then alive after the gossips arrive). This repeating process leads to a cluster instability with thousands of "flappings" as N grows; a "flap" is when a node marks a peer as down and alive again. More detailed examples are presented in §5.1.</p><p>(b) Do they exist in many scalable systems? We have collected a total of 55 bugs in many modern distributed systems <ref type="bibr">(13 in Cassandra, 5 in Couchbase, 6 in Hadoop, 13 in HBase, 16 in HDFS, 1 in Riak, and 1 in Voldemort)</ref>. This is an arduous process due to the lack of searchable keywords for "scalability bugs"; we might have missed some other bugs. We post the full list in Section 2 of <ref type="bibr" target="#b0">[1]</ref>. All the bugs were reported from large deployments (100-1900 nodes). We emphasize again that all these bugs can only be reproduced at scale.</p><p>(c) What are the root causes? We study the buggy code, patches, and developer discussions and find that the majority (52) of the bugs are caused by scale-dependent loops, which iterate scale-dependent data structures (e.g., list of nodes); the rest is about logic bugs that can be caught with singlefunction testing. We break them down to three categories: (1) CPU-intensive loops (15 bugs); <ref type="figure">Figure 2</ref> shows an example. (2) Disk IO loops (26 bugs); the pattern is similar to <ref type="figure">Figure 2</ref> but the nested-loops contain disk IOs. (3) Lockingrelated loops (11 bugs); they can be in the form of locks inside the loops or vice versa. These patterns suggest that this problem lends itself to program analysis ( §3.1).</p><p>(d) Where are they located? The bugs are within the user-facing read/write calls (12 bugs) and operational protocols (40 bugs) such as block report, bootstrap, consistency repair, decommission, de-replication, distributed fsck, heartbeat, job recovery, log cleaning, rebalance, and region assignment. This suggests that scalability correctness is not merely about the user-facing paths. Large systems are full of operational paths that must be scale-tested as well.</p><p>(e) When do they happen? User-facing read/write protocols run "all the time" in deployment, hence are continuously tested. Operational protocols, however, are not frequently exercised. In a stable-looking cluster, scalability bugs can linger silently until the buggy operational protocols are triggered (akin to buggy error handling). For the bugs in userfacing calls, most were triggered by unique workloads such as large deletions or writes after decommission.</p><p>(f) How do scalability bugs impact users? Scalability bugs can cause both performance and availability problems. Although many of the bugs are in the operational protocols, they can cascade to user-visible impacts. For example, when nodes are incorrectly declared dead, some data become unreachable; or scale-dependent operations in the master node (e.g., in HDFS) can cause global lock contention, hence longer time to process user read/write requests.</p><p>(g) Why were the bugs not found before? First, the workloads and the necessary scales to cover the buggy protocols are not captured in the unittests as creating a scalable test platform is not straightforward <ref type="bibr" target="#b21">[26]</ref>. Second, protocols might be scalable in design, but not in practice. Related to c6127 <ref type="figure">(Figure 2</ref>), the failure detector/gossiper <ref type="bibr" target="#b43">[50]</ref> was adopted for its "scalable" design <ref type="bibr" target="#b51">[58]</ref>. However, the design does not account for the gossip processing time during bootstrap/cluster-changes, which can be long, and the subsequent backlogs. To debug, the developers tried to "do the <ref type="bibr">[simple]</ref> math" but failed <ref type="bibr" target="#b6">[7]</ref>. Specific implementation choices such as overloading gossips with many other purposes (e.g., announcing boot/rebalance changes) deviate from the original design sketch, hence the need for scaletesting the implementation code at real scales.  (h) Are scalability bugs easy to debug and fix? The bugs took 1 month to fix on average with tens of back-and-forth discussions. One big factor of delayed fixes is the lack of budget for large test clusters as such luxury tends to only be accessible in large companies, but not to open-source developers <ref type="bibr" target="#b21">[26]</ref>. Another factor is that debugging and fixing are not a single-iteration task; developers must repeatedly instrument the system and re-run at scale to pinpoint the root cause and test the patch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SCALECHECK</head><p>We now present the design of SCALECHECK, which is composed of two parts to achieve two goals: SFIND ( §3.1), a program analysis that exposes scale-dependent loops to developers, and STEST ( §3.2), a set of colocation techniques that enable hundreds of nodes to be colocated on one machine for testing. While STEST produces accurate bug symptoms in most cases, it does not deliver accurate results when all nodes are CPU intensive. For this, we introduce PIL ( §3.3), an emulation technique that provides processing illusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SFIND</head><p>The first challenge to address is: how to find scale-dependent loops? Unfortunately, it is not trivial as such loops can span multiple functions and iterate many scale-dependent collections (iterable data-structure instances such as list). In <ref type="figure" target="#fig_2">Fig- ure 3</ref>, the O(N 3 ) loops span 1000+ LOC, 3 classes, and 10 functions and iterate 3 scale-dependent collections. This difficulty motivates SFIND, a generic program analysis that helps developers pinpoint scale-dependent loops. Below are the three main steps of SFIND. For space, the pseudo-code can be found in our supplement, Section 3.1 of <ref type="bibr" target="#b0">[1]</ref>.</p><p>(1) Auto-tagging of scale-dependent collections: SFIND first automatically tags scale-dependent collections. This is done by growing the cluster and data sizes (e.g., add nodes and add files/blocks) in steps. After each step, we record the size of each instantiated collection. When all the steps are done, we check each collection's growth tendency and mark as scale dependent those whose size increases as the cluster/data size grows.</p><p>This, however, is insufficient due to two reasons. First, there are collections that only grow when background/operational tasks are triggered ( §2d); thus, we must also run all non-foreground tasks. Second, there are "ephemeral" collections (e.g., messages) whose content are scale-dependent but might have been garbage collected by the runtime. Given that the measurements are taken in steps, garbage collection can happen in between them so these collections will not be detected consistently, thus this phase must be iterated multiple times to remove such noise.</p><p>For Java systems, we track heap objects and map them to their instance names by writing around 1042 LOC of analysis on top of Java language supports such as JVMTI <ref type="bibr" target="#b61">[67]</ref> and Reflection <ref type="bibr" target="#b17">[22]</ref>. This phase also performs a dataflow analysis to taint all other variables derived from scale-dependent collections. In our experience, by scaling out to just 30 nodes (30 steps), which can be done easily on one machine, scaledependent collections can be clearly observed (though not the symptoms). This phase found 32 scale-dependent collections in Cassandra (three in <ref type="figure" target="#fig_2">Figure 3</ref>) and 12 in HDFS.</p><p>(2) Finding scale-dependent loops: With the tagging, SFIND then automatically searches for scale-dependent loops, specifically by tainting loops (for, while) as well as recursive functions that iterate through the scale-dependent collections, performing a control-flow analysis to construct the nested Big O complexity of each loop, and identifying the loop contents (CPU/instructions only, IOs, or locks). With these steps, in <ref type="figure" target="#fig_2">Figure 3</ref> for example, SFIND can mark applyStateLocally as an O(N 3 ) function.</p><p>We also cover a special "implicit loop" -a synchronized (locking) function in a node that is being called by all the peer nodes. A common example is in the master-worker architecture where all the N worker nodes RPC into a master's lock-protected function. When N grows, there is a potential of lock contention (congestion) to the function (examples are in §5.1). SFIND also handles such scenarios by tagging RPC classes and searching for functions called by the peer nodes.</p><p>(3) Reporting and triaging: SFIND finds 131 scaledependent loops in Cassandra and 92 in HDFS, hence the need for triaging. For example, if a function g has lower complexity than f , and g is within the call path of f , then testing f can be prioritized. For every nested loop to test, SFIND reports the relevant control-and data-flows from the outer-most to inner-most loop, along with the entry points (either client/admin RPCs or background daemon threads). The entry points are finally ranked by counting the number of spanned scale-dependent lines of code, the theoretical complexity (in terms of scale-dependent data structures), the number of IO operations (including reads/writes) and the number of blocking operations (including locking and operations that block waiting for a future result) in that path. The theoretical complexity is not by itself a complete indicator of potential bottlenecks. For example, an entry point reported with high complexity ,e.g. O(N 3 ), but with no IO/Blocking operations on its code path might not be as bottleneck prone as one reported with less complexity, e.g. O(N ), but many IO/Blocking operations on its code path. This ranking helps developers prioritize and create the necessary test workloads. For example, in <ref type="figure" target="#fig_2">Figure 3</ref>, the O(N 3 ) path is only exercised if the cluster bootstraps from scratch when peers do not know about each other (hinted from the "if(!localStateMap.get())", "onChange()", "state==STATUS" and "val==NORMAL"). SFIND reports that this entry point spans over 6700 scale-dependent lines of code and performs over 20N IO and 4N blocking operations, which implies that it is likely to become a bottleneck as the cluster size grows and should be prioritized.</p><p>Creating test workloads from SFIND report is a manual process. Automated test generation is possible for singlemachine programs/libraries <ref type="bibr" target="#b31">[38]</ref>, however, we are not aware of any work that automates such process in the context of real-world, complex, large-scale distributed systems. We put our work in the context of DevOps culture <ref type="bibr" target="#b56">[62]</ref> where developers are testers and vice versa, which (hopefully) simplifies test workload creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STEST</head><p>The next challenge is: how to test scale-dependent loops at real scales (hundreds of nodes) on one machine? Many scale-dependent loops were unfortunately not subjected to testing because existing unittest frameworks do not scale. Below we describe the hurdles to achieve a high colocation factor. Starting in Section 3.2.1, we began with black-box methods (no/small target system modification).</p><p>Unfortunately, we found that existing systems are not built with single-machine scale-testing in mind (the theme of this section); we faced many colocation bottlenecks (memory/CPU contentions and context switching delays) that limit large colocation. In Section §3.2.2, we will describe our solutions to achieve single-machine scale-testable systems with minimal changes. All the methods we use are summarized in <ref type="table">Table 1</ref> using Cassandra as an example. Abbreviations of our methods (e.g., NP, SPC, GEDA) are added for ease of reference in the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Black-Box Approaches</head><p>• Naive Packing (NP): The easiest setup is (naively) packing all nodes as processes on a single machine. However, we did not reach a large colocation factor, which is caused by the following reasons.</p><p>(a) Memory bottlenecks: Many distributed systems today are implemented in managed languages (e.g., Java, Erlang) whose runtimes consume non-negligible memory overhead. Java and Erlang VMs, for example, use around 70 and 64 MB of memory per process respectively. We also tried running nodes as Linux KVM VMs and using KSM (kernel samepage merging) tool. Interestingly, the tool does not find many duplicate pages even though the VMs/processes are supposed to be similar (as reported elsewhere <ref type="bibr" target="#b59">[65]</ref>). Overall, including Cassandra's memory usage, per-node memory consumption reaches 100 MB. Thus, a 32-GB machine can only colocate around 300 nodes.</p><p>(b) Process context switches: Before we hit the memory bottleneck (e.g., reach 300 nodes), we observed that the target systems' "inaccuracy" is already high when we colocate just 50 nodes. For measuring inaccuracy, we measure several application-level metrics; for example, in Cassandra, if gossips should be sent every 1 second, but are sent every 1.3 second, then the inaccuracy is 30%. We use 10% as the maximum acceptable inaccuracy/event lateness. We noticed high inaccuracies even before we hit the CPU bottlenecks (i.e., CPU has not reached 90% utilization). We suspected that the process context switches could be the reasons.</p><p>(c) Managed-language VM limitations: We also found that managed-language VMs are backed by advanced services. For example, Erlang VMM contains a DNS service that sends heartbeat messages among connected VMs. When hundreds of Erlang VMs (one for each Riak node) run on one Erlang VMM, the heartbeat messages cause a "network" overflow that undesirably disconnects Erlang VMs (also reported in <ref type="bibr" target="#b33">[40]</ref>). Naive packing is infeasible.</p><p>• Single-Process Cluster (SPC) + Network Stub: To address the bottlenecks above, we deployed all nodes as threads in a single process. Surprisingly, our target systems are not easy to run in this "single-process cluster." For example, Cassandra developers bemoan the fact that their gossip/faultdetector protocols are not adequately scale-tested <ref type="bibr" target="#b11">[15,</ref><ref type="bibr">28]</ref> because Cassandra (and many other systems) uses "singleton" design pattern for simplicity (but bad for modularity) <ref type="bibr">[32]</ref>. That is, most global states are static variables that cannot be modularized to per-node isolated variables.</p><p>Our strawman attempt was a redesign to a more modular one, which costs us almost 3000 LOC (and no longer a black-box method); Cassandra developers also attempted a similar method to no avail <ref type="bibr" target="#b11">[15,</ref><ref type="bibr">28]</ref>. We found another way: leveraging class loader isolation support from the language runtime <ref type="bibr" target="#b18">[23]</ref>, which is rarely used but fits SPC purpose. In Java systems, we can manipulate the class loader hierarchy such that a node's main thread (and all child threads) use an isolated set of Java class resources, not shared with those belonged to other nodes, hence no target system modification.</p><p>Very recently, we found that Cassandra developers also begin to develop a similar method to address this problem <ref type="bibr" target="#b7">[8]</ref>.</p><p>By SPC-ing Cassandra, we now hit a colocation limit of 70 nodes <ref type="table">(Table 1b)</ref>, but still have not reached the memory or CPU bottlenecks. We suspected thread and/or user-kernel context switching as a root cause. We removed the latter by creating a generic network stub that (de)marshalls inter-node messages and skips the OS. This stub is also helpful in reducing network memory footprints under higher colocation. For example, in Voldemort, the nodes communicate via Java NIO <ref type="bibr" target="#b20">[25]</ref> which is fast but contains buffers and connection metadata that take up memory space and prevent &gt;200-node colocation (more in §5.4). For Cassandra, the network stub allows up to 120-node colocation <ref type="table">(Table 1c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">A White-Box Approach</head><p>Adding network stub is our last black-box approach as we found no other way to reduce thread context switching in a black-box way. In fact, we observed a massive thread context switching issue. In P2P systems such as Cassandra, each node spawns a thread to listen from a peer. Thus, just for messaging, there are N 2 threads to manage for the whole cluster. This can be solved by using select()-like system call <ref type="bibr" target="#b16">[21]</ref>, which would reduce the problem to N threads. However, we still observed around N ×26 active threadseach node still runs multiple service stages (gossiper, failure detector, etc.), each can be multi-threaded. A high colocation factor will spawn thousands of threads.</p><p>• Global Event Driven Arch. (GEDA): To address the problem, we must redesign the target system, but with minimal changes. We leverage the staged event-driven architecture (SEDA) <ref type="bibr" target="#b73">[79]</ref>  <ref type="figure" target="#fig_3">(Figure 4a</ref>), common in server code, in which each service/stage (in each node) exclusively has an event queue and a thread pool. In STEST mode, we convert SEDA to a global-event driven architecture (GEDA; <ref type="figure" target="#fig_3">Figure 4b</ref>). That is, for every stage, there is only one queue and one thread pool for the whole cluster. As an example, let's consider a periodic gossip service. With 500-node colocation, there are 500 threads in SPC, each sending a gossip every second. With GEDA, we only deploy a few threads (matched with the number of available cores) shared among all the nodes for sending gossips. As another example, for gossip processing stage, there is only one global gossip-receiving queue shared among all the nodes.</p><p>GEDA works with a minimal code change to the target system. Logically, as events are about to be enqueued into the original per-node event queues ( 1 in <ref type="figure" target="#fig_3">Figure 4</ref>), we redirect them to GEDA-level event queues, to be later processed by GEDA worker threads. This only requires ∼10 LOC change per stage (as we use aspect-oriented programming <ref type="bibr" target="#b2">[3]</ref>). While simple, care must be taken for singlethreaded/serialized stage. For example, Cassandra's gossip processing is intentionally single-threaded to prevent concurrency issues. This is illustrated in case 2 in <ref type="figure" target="#fig_3">Figure 4</ref> where the per-node stage is serialized (i.e., y must be processed after x). Here, if the events are forwarded down during enqueue, GEDA's multiple threads will break the program semantic (e.g., x and y can be processed concurrently). Thus, for single-threaded/serialized stage, we must interpose at dequeue time ( 3 in <ref type="figure" target="#fig_3">Figure 4</ref>), which costs ∼50 LOC change per stage (details in §3.2 of <ref type="bibr" target="#b0">[1]</ref>). Thus, by default we interpose at enqueue (small changes) and at dequeue for singlethreaded stage (more changes).</p><p>Adding GEDA to Cassandra only costs us 581 LOC (Table 1d) and is simple; the same 10-50 LOC method above is simply repeated across all the stages. Overall, GEDA does not change the logic of the target systems, but successfully removes some delays that should have never existed in the first place, as if the nodes run exclusively on independent machines. For HDFS tests, GEDA enables 512-node colocation ( §5.4) but for some Cassandra tests, it only enables around 130-node colocation <ref type="table">(Table 1d)</ref>, which we elaborate in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Processing Illusion (PIL)</head><p>Finally, the last challenge we address is: how to produce accurate results (i.e., the same bug symptoms observed in real-scale deployment) when colocating hundreds of CPUintensive nodes? We found that STEST is sufficient for accurately revealing bug symptoms in scale-dependent lockrelated loops or IO serializations, as these root causes do not contend for CPUs. For CPU-intensive loops, STEST is also sufficient for master-worker architecture where only one node is CPU intensive (e.g., HDFS master).</p><p>However, for CPU-intensive loops in P2P systems such as Cassandra, where all nodes are busy, the bug symptoms reported by STEST are not accurate. For example, for Cassandra issue #c6127 ( §2a), in 256-node real deployment, we observed around 2000 flappings (the bug symptom) but 21,000 flappings in STEST. The inaccuracy gets worse as we scale; with N CPU-intensive nodes on a C-core machine, roughly N/C nodes contend on a given core.</p><p>To address this, we need to emulate CPU-intensive processing by supplementing STEST with processing illusion (PIL), an approach that replaces an actual processing with sleep(). For example, for c6127, we can replace the expensive gossip/stage-changes processing (see <ref type="figure" target="#fig_2">Figures 2 and 3)</ref>, with sleep(t) where t is an accurate timing of how long the processing takes.</p><p>The intuition behind PIL is similar to the intuition behind other emulation techniques. For example, Exalt provides an illusion of storage space; their insight was "how data is processed is not affected by the content of the data being written, but only by its size" <ref type="bibr" target="#b72">[78]</ref>. Similarly, PIL provides an illusion of compute processing; our insight is that "the key to computation is not the intermediate results, but rather the execution time and eventual output." In other words, with PIL, we will still observe the overall timing behaviors and the corresponding impacts accurately.</p><p>PIL might sound outrageous, but it is feasible as we address the following concerns: how a function (or code block) can be safely replaced with sleep() without changing the whole processing semantic ( §3.3.1) and how we can produce the output and predict the timing "t" if the actual compute is skipped ( §3.3.2)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">PIL-Safe Functions</head><p>Our first challenge is to ensure that functions (or code blocks) can be safely replaced with sleep(), but still retain the cluster-wide behavior and unearth the bug symptoms. We name such functions as "PIL-safe functions." We identify two main characteristics of such functions: (1) Memoizable output: a PIL-safe function must have a memoizable (deterministic) output based on the input of the function. (2) Nonpertinent IOs: if a function performs local/remote disk IOs that are not pertinent to the correctness of the corresponding protocol, the function is PIL-safe. For example, in c6127, there is a ring-table checkpoint (not shown) needed for fault tolerance but is irrelevant (never read) during bootstrapping.</p><p>We extend SFIND to SFIND P IL , which includes a static analysis that finds code blocks in scale-dependent loops that can be safely PIL-ed. SFIND P IL analyzes the content of each loop in functions related to the relevant cluster state and checks for two cases: (1) The loop performs operations that affect the cluster state, so we need to insert pre-memoization and replay code to record/reconstruct the cluster state <ref type="bibr">[1, §3.3]</ref>. We consider all variables involved in the execution of a target protocol as relevant states. While our static analysis tool eases the identification of these variables, programmer intervention can help for additional verification. In (2), the loop performs non-pertinent operations only (such as IO). In this case, we can automatically replace the loop with a sleep call without affecting the behavior of the protocol.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Pre-Memoization (with Determinism)</head><p>As PIL-safe functions no longer perform the actual computation, the next question to address is: how do we manufacture the output such that the global behavior is not altered (e.g., rebalancing protocol should terminate successfully)?. For functions with no pertinent outputs, we just need to do time profiling but not output recording. For functions with pertinent outputs, our solution is pre-memoization, which records input-output pairs and the processing time, specifically a tuple of three items (ByteString in, out, long nanoSec) indexed by hash(in)), which represent the to-bemodified variables before and after the function is executed and the processing time, respectively <ref type="figure" target="#fig_4">(Figure 5b)</ref>.</p><p>Another challenge encountered is non-determinism: the state of each node (the input) depends on the order of arriving messages (which are typically random). Let's consider Riak's bootstrap+rebalance protocol where eventually all nodes own a similar number of partitions. A node initially has an unbalanced partition table, receives another partition table from a peer node, then inputs it to a rebalance function, and finally sends the output to a random node via gossiping. Every node repeats the same process until the cluster is balanced. In a Riak cluster with N =256 and P =64, there are in total 2489 rebalance iterations with a set of specific inputs in one run. Another run of the protocol will result in a different set of inputs due to gossip randomness. Our calculation shows that there are (N N P ) 2 possible inputs.</p><p>To address this, during pre-memoization, we also record non-determinism such as message orderings such that order determinism is enforced during replay. For example, across different runs, a Riak node now receives gossips from the same sequence of nodes. With order determinism, prememoization and SCALECHECK work as follow: (1) We first run the whole cluster on a real deployment and interpose sleep-safe functions. (2) When sleep-safe functions are executed, we record the inputs and corresponding outputs to a memoization database (SSD-backed files). (3) During this pre-memoization phase, we record message nondeterminism (e.g., gossip send-receive pairs and their timings). (4) After pre-memoization completes, we can repeatedly run SCALECHECK wherein order determinism is enforced (e.g., no randomness), sleep-safe functions replaced with PIL, and their outputs retrieved from the memoization database. Note that steps 1-3 are the only steps that require real deployment.</p><p>Other than this, similar to the theme in the previous section that existing systems are not amenable to single-machine testing, we found similar issues such as the use of wallclock time which essentially incapacitates memoization and replay. Here, we convert wall-clock time to "cluster start time + elapse time" in 296 LOC <ref type="table">(Table 1e</ref>). For test workloads that show CPU busyness in all nodes, SFIND P IL finds PIL-safe functions and inserts our prememoization library calls. Next, STEST now works in two parts. c STEST mez (without PIL) will run the test on a real cluster, but just one time, to pre-memoize PIL-safe functions and store the tuples to a SSD-backed database file. d STEST P IL (with PIL) will then run by having SFIND P IL remove the pre-memoization library calls, replace the expensive PIL-safe function with sleep(t), and insert our code that constructs the memoized output data. SCALECHECK also records message ordering during STEST mez and replays the same order in STEST P IL (not shown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Putting It All Together</head><p>As another benefit, SCALECHECK can also ease real-scale debugging efforts. First, the only step that consumes more time is the no-PIL pre-memoization phase <ref type="figure" target="#fig_4">(Figure 5c</ref>), up to 6x longer time than real-deployment testing ( §5.5). However, this is only a one-time overhead. Most importantly, developers can repeatedly re-run STEST P IL <ref type="figure" target="#fig_4">(Figure 5d</ref>) as many times as needed (tens of iterations) until the bug behavior is completely understood. In STEST P IL , the protocol under test runs in a similar duration as if all the nodes run on independent machines.</p><p>Second, some fixes can be tested by only re-running the last step; for example, fixes such as changing the failure detector Φ algorithm (for c6127), caching slow methods (c3831), changing lock management (c5456), and enabling parallel processing (v1212). However, if the fixes involve a complete redesign (e.g., optimized gossip processing in c3881, decentralized to centralized rebalancing in r3926), STEST mez must be repeated.   <ref type="bibr" target="#b0">[1]</ref>. We will release our code publicly.  <ref type="bibr" target="#b23">[30]</ref>, and Voldemort <ref type="bibr">[29]</ref>. The major system-specific change is achieving "STEST-able systems" (i.e., supporting SPC and GEDA), which range between 179 to 918 LOC (less than 1 % of the target code size). This is analogous to how file systems code are modified to make them "friendlier" to fsck <ref type="bibr" target="#b45">[52,</ref><ref type="bibr" target="#b57">63]</ref>. The rest is the generic SFIND and STEST library code (pre-memoization, auto PIL insertion, message order determinism support, AspectJ utilities). SFIND was built with Eclipse AST Parser <ref type="bibr">[11]</ref> to support Java programs. We leave porting to Erlang's parser <ref type="bibr" target="#b10">[12,</ref><ref type="bibr">13]</ref> as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application and Implementation</head><p>Generality: We show the generality of SCALECHECK with two major efforts. First, we scale-checked a total of 18 protocols: 8 Cassandra (e.g., bootstrap, scale-out, decommission), 8 HDFS (e.g., decommission, block reports, snapshot), 1 Riak (rebalance), and 1 Voldemort (rebalancing) protocols (full list in §4 of <ref type="bibr" target="#b0">[1]</ref>). A protocol can be built on top of other protocols (e.g., bootstrap on gossip and failure detection protocols). Second, for exposing known bugs, we applied SCALECHECK to a total of 10 earlier releases: 4 Cassandra, 4 HDFS, 1 Riak, and 1 Voldemort old releases. For finding unknown bugs, we also ran SCALECHECK on recent releases of the four systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We now evaluate SCALECHECK: Is SCALECHECK effective in exposing scalability bugs ( §5.1-5.2), accurate ( §5.3), scalable and efficient ( §5.4-5.5)? We compare SCALECHECK with real deployments of 32 to 512 nodes, deployed on at most 128 machines (testbed group limit), each has 16-core AMD Opteron(tm) with 32-GB DRAM.Our target protocols only make at most 2 busy cores per node, which justifies why we pack 8 nodes per one 16-core machine for the real deployment. <ref type="table" target="#tab_7">Table 3</ref> lists the 10 real-world bugs we use for benchmarking SCALECHECK. We chose these 10 bugs (among the 55 bugs we studied) because the reports contain detailed descriptions of the bugs, which is important for us to create the "input" (i.e., the test cases). <ref type="figure" target="#fig_6">Figure 6</ref> shows the accuracy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Exposing Scalability Bugs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bug# N Protocol Metric</head><p>Tm T pil c6127 <ref type="bibr" target="#b6">[7]</ref> ≥256 Bootstrap #flaps 2h 15m c3831 <ref type="bibr" target="#b5">[6]</ref> ≥256 Decomm. #flaps 17m 9m c3881 <ref type="bibr" target="#b4">[5]</ref> ≥64 Add nodes #flaps 7m 5m c5456 <ref type="bibr" target="#b3">[4]</ref> ≥256 Add nodes #flaps 16m 4m r3926 <ref type="bibr" target="#b24">[31]</ref> ≥128 Rebalance TComp 6h 2h v1212 <ref type="bibr" target="#b25">[33]</ref> ≥128 Rebalance TComp 22h -h9198 <ref type="bibr" target="#b14">[19]</ref> ≥256 Blk. report QSize 8m -h4061 <ref type="bibr" target="#b13">[17]</ref> ≥256 Decomm.</p><p>T Lock 6h -h1073 <ref type="bibr" target="#b12">[16]</ref> ≥512 Pick nodes TComp 1m -h395 <ref type="bibr" target="#b15">[20]</ref> ≥512 Blk. report TComp 5m - of SCALECHECK in exposing the 10 bugs using the "bugsymptom" metrics in <ref type="table" target="#tab_7">Table 3</ref> (the first bug c6127 will be shown later in Section 5.3 and the last bug h395 is omitted in <ref type="figure" target="#fig_6">Figure 6</ref> for space).</p><p>Results summary: First, SCALECHECK is effective and accurate in exposing scalability bugs, some of which only surface in 256+ nodes. As shown, for Cassandra and Riak bugs where all nodes are CPU intensive, PIL is needed for accuracy (SCk+PIL vs. Real lines in <ref type="figure" target="#fig_6">Figures 6a-d)</ref>, but for the rest, STEST suffices (SCk vs. Real in 6e-f). Second, SCALECHECK can help developers prevent recurring bugs; the series of Cassandra bugs (as described later below) involves the same protocols (gossip, rebalance, and failure detector) and create the same symptom (high #flaps). As code evolves, it can be continuously scale-checked with SCALECHECK.</p><p>Third, different systems of the same type (e.g., key-value stores, master-worker file systems) implement similar protocols. The effectiveness of SCALECHECK methods in scalechecking the different protocols above can be useful to many other distributed systems.</p><p>Bug descriptions: We now briefly describe the bugs. Longer descriptions can be found in Section 5.1 of <ref type="bibr" target="#b0">[1]</ref>.</p><p>(a) <ref type="figure" target="#fig_6">Figure 6a</ref>: In Cassandra c3831 <ref type="bibr" target="#b5">[6]</ref> when a node X is removed, all other nodes must own X's key-partitions. This scale-dependent, CPU-intensive "pending keyrange calculation" cause cluster-wide flapping (the y-axis), observable in 256+ nodes. The fix caches the outputs of slow methods.</p><p>(b) <ref type="figure" target="#fig_6">Figure 6b</ref>: c3881 <ref type="bibr" target="#b4">[5]</ref> is similar to the previous bug (c3831), but the fix was obsolete as the concept of multi-  <ref type="table" target="#tab_7">Table 3</ref>. The x-axis represents the number of nodes (N ). The figure title describes the y-axis, i.e., the bug symptom metrics as recorded in "Real" deployment vs. SCALECHECK. For Cassandra and Riak bugs (a-d), where all nodes are CPU-intensive, the bug symptoms are inaccurate without PIL ("SCk" lines). However, with PIL ("SCk+PIL" lines), the bug symptoms are relatively accurate as in the real deployment scenarios. For Voldemort and HDFS bugs (e-h), where there is no concurrent CPU busyness, PIL is not needed.</p><p>ple key-partitions per node was added. The calculation is now scale-dependent on N ×P . This causes CPU spikes and massive flapping during scaling out; the bug surfaced in 64+ nodes (when 32+ new nodes are added to existing 32+ nodes). The bug was fixed with a complete redesign of the pending keyrange calculation. (c) <ref type="figure" target="#fig_6">Figure 6c</ref>: Interestingly, c5456 <ref type="bibr" target="#b3">[4]</ref> is a bug in the same protocol as above. The previous fix was obsolete again as pending range calculation is now multi-threaded; range calculations can happen concurrently. However, this new design introduces a new coarse-grained lock that can block gossip processing for a long time, thus introduces flapping (in 256+ nodes). The fix changed the lock management.</p><p>(d) <ref type="figure" target="#fig_6">Figure 6d</ref>: In r3926 <ref type="bibr" target="#b24">[31]</ref>, Riak's rebalancing algorithm employed 3 complex stages (claim-target, claim-hole, fullrebalance) to converge to a perfectly balanced ring. Each node runs this CPU-intensive algorithm on every bootstrapgossip received. The larger the cluster, the longer time the perfect balance is achieved (a high y value in 128+ nodes). (f) <ref type="figure" target="#fig_6">Figure 6f</ref>: In h9198 <ref type="bibr" target="#b14">[19]</ref>, incremental block reports (IBRs) from HDFS datanodes to the namenode acquire the global master lock (i.e., a special worker-to-master "loop" as explained in §3.1). As N grows, more IBR calls acquire the lock. The IBR requests quickly backlog the namenode's IPC queue; with 256 nodes, the IPC queue hits the max of 1000 pending requests; y=1 (×1000). When this happens, user requests are undesirably dropped by the namenode. The fix batches the IBR request processing. In HDFS, to emulate large blocks, we reuse the "TinyDataNode" class (1KB blocks) that the developers already use in the unit tests.</p><p>(g) <ref type="figure" target="#fig_6">Figure 6g</ref>: In h4061 <ref type="bibr" target="#b13">[17]</ref>, when D datanodes are decommissioned, the blocks must be replicated to the other N −D nodes. Every 5 minutes, the DecommissionMonitor thread in the namenode iterates all the block descriptors to check if the D nodes can be safely decommissioned (when all data replications complete). This thread, unfortunately, must hold the global file system lock. When N is 256+, this process can hold the lock (i.e., stall user requests) for more than 10 seconds (y&gt;10). The fix used a dedicated thread to manage decommissioning and refined the algorithm.</p><p>(h) <ref type="figure" target="#fig_6">Figure 6h</ref>: In h1073 <ref type="bibr" target="#b12">[16]</ref>, for a new file creation, the namenode calls a chooseTarget function to sort a list of target datanodes from their distances from the writer and choose the best nodes. When N and the replication factor are large, it can take more than one second to choose. The fix modified the sorting algorithm.</p><p>(i) Finally, in h395 <ref type="bibr" target="#b15">[20]</ref> (figure not shown for space), datanodes send block reports too frequently and when N &gt;512 nodes, the namenode spends more time in this background process as opposed to serving users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Discovering Unknown Bugs</head><p>We also integrated SCALECHECK to recent stable versions of Cassandra, HDFS, Riak, and Voldemort, and found 1 unknown bug in Cassandra and 3 bugs in HDFS.</p><p>For Cassandra, SFIND pointed us to another nested scaledependent loop. We created the corresponding test case and SCALECHECK showed that cluster-wide flapping resurfaces again but only in 512-node deployment. As an example, decommissioning just only one node already caused almost 100,000 flaps. The developers confirmed that the bug is related to a design problem. To prevent flappings, the devel- opers suggested us to add/remove node one at a time with 2-minute separation, which means scaling-out/down 100 nodes will take over 3 hours (i.e., this bug impedes instant elasticity). The developers recently started a new initiative for designing "Gossip 2.0" to scale to 1000+ nodes <ref type="bibr">[14]</ref>.</p><formula xml:id="formula_0">a) #flaps = f ( Φ &gt; 8 ) b) Φ = f ( TavgGossip, T lastGossip ) TavgGossip = avg. of last 1000 T lastGossip c) T lastGossip = f ( #hops, TgossipExec ) #hops = log(N ) on average TgossipExec = T stateU</formula><p>For Riak and Voldemort, we found that their latest-stable bootstrap/rebalance protocols do not exhibit any scalability bug, up to 512 nodes.</p><p>For HDFS, we found 3 instances of scale-dependent loops that hold the entire namenode read/write lock (also confirmed by the developers). Specifically, SFIND reports the following number of lines executed: Here, "B" represents the number of blocks per datanode (e.g., 10,000). The first function, getSnapshotDiff, contains a bug that the HDFS developers were hunting for 4 weeks, as the unresponsive-namenode impact recently affected a customer. In this path, there is a recursive function iterating on a list of files and blocks and a conditional path that makes ACL lookups which causes the namenode to be unresponsive for more than 40 seconds in at least a 512-node deployment. Similar symptoms were also reproduced for the second and third bugs (refreshDatanodes and metaSave). The developers say these bugs are dangerous because if the namenode is paused for 45 seconds, it will cause a heavy failover. They also say these bugs are hard to find in a million-plus lines of code. More details/graphs are in §5.2 of [1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Accuracy</head><p>The goal of our next evaluation is to show that PILinfused SCALECHECK mimics similar behaviors as in realdeployment testing and is accurate not only in the final bugsymptom metric but also in the detailed internal metrics. For this, we collected roughly 18 million values. For space, we only focus on c6127 <ref type="bibr" target="#b6">[7]</ref> (see §2a).  <ref type="figure" target="#fig_8">Figure 7</ref>, measured in real deployment ("Real") and in SCALECHECK ("SCk") with different cluster sizes <ref type="bibr">(32, 64, 128, 256, and 512</ref> in the x-axis). The y-axes (the metrics) are described in the figure titles.</p><p>without PIL ("SCk") and STEST P IL with PIL ("SCk+PIL"), respective to the real-deployment testing ("Real").</p><p>(a) <ref type="figure" target="#fig_10">Figure 8a</ref> shows the total number of flaps (aliveto-dead transitions) observed in the whole cluster during bootstrapping. STEST by itself will not be accurate if all nodes are CPU intensive ( §3.3). However, with PIL, SCALECHECK closely mimics real deployment scenarios. Next, <ref type="figure" target="#fig_8">Figure 7a</ref> defines that #flaps depends on Φ <ref type="bibr" target="#b43">[50]</ref>. Every node A maintains a Φ for a peer B (a total of N ×(N −1) variables to monitor).</p><p>(b) <ref type="figure" target="#fig_10">Figure 8b</ref> shows the maximum Φ values observed for every peer node; for graph clarity, from here on we only show with-PIL results. For example, for the 512-node setup, the whisker plots show the distribution of the maximum Φ values observed for each of the 512 nodes. As shown, the larger the cluster, more Φ values exceeds the threshold value of 8, hence the flapping. <ref type="figure" target="#fig_8">Figure 7b</ref> points that Φ depends on the average inter-arrival time of when new gossips about B arrives at A (T avgGossip ) and the time since A heard the last gossip about B (T lastGossip ). The point is that T lastGossip should not be much higher than T avgGossip .</p><p>(c) <ref type="figure" target="#fig_10">Figure 8c</ref> shows the whisker plots of gossip interarrival times (T lastGossip ) that we collected for every A-B pair (millions of gossips as a gossip message contains N gossips of the peer nodes). The figure shows that in larger clusters, new gossips do not arrive as fast as in smaller clusters, especially at high percentiles. <ref type="figure" target="#fig_8">Figure 7c</ref> shows that T lastGossip depends on how far B's new gossips propagate through other nodes to A (#hops) and the gossip processing time in each hop (T gossipExec ). The latter (T gossipExec ) is essentially the state-update processing time (T stateUpdate ), triggered whenever there are state changes.</p><p>(d) <ref type="figure" target="#fig_10">Figure 8d</ref> (in log scale) shows the whisker plots of the state-update processing time (T stateUpdate ). In the 512-node setup, we measured around 25,000 state-update invocations. The figure shows that at high percentiles, T stateUpdate is scale dependent (the culprit). As shown in <ref type="figure" target="#fig_8">Figure 7d</ref>, T stateUpdate complicatedly depends on a scale-dependent 2-dimensional input (Size ringT able and Size newStates ). A node's Size ringT able depends on how many nodes it knows, including the partition arrangement (≤N ×P ) and Size newStates (≤N ), which increases as cluster size grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Colocation Factor</head><p>This section shows the maximum colocation factor SCALECHECK can achieve as each technique is added one at a time on top of the other. To recap, the techniques are: single-process cluster (SPC), network stub (Stub), global event driven architecture (GEDA), and processing illusion (PIL). The results are based on a 16-core machine. 1</p><p>Maximum colocation factor ("MaxCF"): A maximum colocation factor is reached when the system behavior in SCALECHECK mode starts to "deviate" from the real deployment behavior. Deviation happens when one or more of the following bottlenecks are reached: (1) high average CPU utilization (&gt;90%), (2) memory exhaustion (nodes receive outof-memory exceptions and crash), and (3) high event "lateness."</p><p>Queuing delays from thread context switching can make events late to be processed, although the CPU utilization is not high. We instrument our target systems to measure event lateness of relevant events (as described in §3.2.2). We use 10% as the maximum acceptable event lateness. Note that the residual limiting bottlenecks come from the main logic of the target protocols, not removable with general methods.</p><p>Results and observations: <ref type="figure" target="#fig_11">Figure 9</ref> shows different sequences of integration to our four target systems and the resulting maximum colocation factors. We make several important observations from this figure.</p><p>First, when multiple techniques are combined, they collectively achieve a high colocation factor (up to 512 nodes for the three systems respectively). For example, in <ref type="figure" target="#fig_11">Figure  9a</ref>, without using PIL in Cassandra, MaxCF only reaches 136. But with PIL, MaxCF significantly jumps to 512. When we increased the colocation factor (+100 nodes) beyond the maximum, we hit the residual bottlenecks mentioned before; at this point, we did not measure MaxCF with small increments (e.g., +1 node) due to time limitation.</p><p>Second, distributed systems are implemented in different ways. Thus, integrations to different systems face different sequences of bottlenecks. To show this, we tried different sequences of integration sequences. For example, in Cassandra <ref type="figure" target="#fig_11">(Figure 9a</ref>), our integration sequence is +SPC, +Stub, +GEDA, and +PIL (as we hit context switching overhead before CPU). For Riak <ref type="figure" target="#fig_11">(Figure 9b</ref>), we began with PIL as we hit CPU limitation first before hitting Erlang VMM network overflow which requires SPC ( §3.2.1), and Riak does not require GEDA because Erlang, as an event-driven language, manages thread executions as events (more in Section 5.4 of <ref type="bibr" target="#b0">[1]</ref>). For Voldemort <ref type="figure" target="#fig_11">(Figure 9c</ref>), we began with SPC and then network stub to reduce Java VM and Java NIO memory overhead respectively, and PIL so far is not needed as the tested workload does not involve parallel CPU-intensive operations. For HDFS <ref type="figure" target="#fig_11">(Figure 9d</ref>), we only need SPC and GEDA but not PIL as only the master node that is CPU intensive (but not the datanodes). Finally, it is the combination of all techniques that make SCALECHECK effective. For example, while in <ref type="figure" target="#fig_11">Figure 9a</ref> we apply the sequence of SPC+Stub+GEDA+PIL resulting in PIL as the dominant factor, in another experiment we applied a different sequence PIL+SPC+Stub and failed to hit 512 nodes, not until GEDA is added and becomes the dominant factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Pre-Memoization and Replay Time</head><p>The "T m " and "T pil " columns in <ref type="table" target="#tab_7">Table 3</ref> on page quantifies the duration of the pre-memoization (STEST mez ) and PIL-based replay (STEST P IL ) stages when N ≥256. For example, for CPU-intensive bugs such as c6127, the prememoization time takes 2 hours while the PIL-based replay is only 15 minutes (similar to the real-deployment test); for r3926, it is 6 vs. 2 hours. Pre-memoization does not necessarily take N × longer time because one node only consumes 2 cores (while the machine has 16 cores) and also not every node is busy all the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Test Coverage</head><p>SFIND labeled 32 collections in Cassandra and 12 in HDFS as scale dependent. From these, SFIND identified 131 and 92 scale-dependent loops in Cassandra and HDFS (out of more than 1500 and 1900 total loops) respectively. So far, we have tested 57 (44%) and 64 (69%) of the loops in Cassandra and HDFS. The time-consuming factor is the manual creation of new test cases that will exercise the loops (see end of §3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association 17th USENIX Conference on File and Storage Technologies 369</head><p>We emphasize that SFIND is not a bug-finding tool, hence the reason why we do not report false positives. A more complete picture of SFIND's output can be found in Section 5.6 of our supplemental document <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>At the moment, our work focuses on scale-dependent CPU/processing time ( §2c), and the "scale" here implies the scale of cluster size. However, there are other scaling problems that lead to IO and memory contentions <ref type="bibr" target="#b39">[46,</ref><ref type="bibr" target="#b63">69,</ref><ref type="bibr" target="#b70">76]</ref>, usually caused by the scale of load <ref type="bibr" target="#b30">[37,</ref><ref type="bibr" target="#b40">47]</ref> or data size <ref type="bibr" target="#b58">[64]</ref>. For emulating data size, we are only aware of one work, Exalt <ref type="bibr" target="#b72">[78]</ref>, which is orthogonal to SCALECHECK (more in §7).</p><p>In our bug study, we learn that some load or data-size related bugs can be addressed with accurate modeling <ref type="bibr" target="#b40">[47]</ref> (e.g., d dead nodes will add d/(N −d) load to every live node) and some others can already be reproduced with a single machine (e.g., loading as much file metadata to check the limit of HDFS memory bottleneck <ref type="bibr" target="#b70">[76]</ref>). Nevertheless, we will continue our study of these other scaling dimensions, especially as scaling bugs in datacenter distributed systems is not a well-understood problem.</p><p>So far, SCALECHECK is limited as a single-machine framework, which integrates well to the de-facto unit-test style. To increase colocation factor, a higher-end machine can be used. Another approach is to extend SCALECHECK to run on multiple machines. However, this means that we need to enable back the networking library, which originally already caused a colocation bottleneck. We also acknowledge as a limitation that adding new code will also add new maintenance costs. In future work, we intend to approach zero-effort emulation.</p><p>Finally, SFIND by itself is not sufficient to reveal scalability bugs. Building a program analysis that covers all paths and understands the cascading impacts is challenging. Not all scale-dependent loops imply buggy code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In Section 1, we briefly discussed related work in four categories: real-scale testing/benchmarking (direct, but not economical) <ref type="bibr" target="#b21">[26,</ref><ref type="bibr" target="#b52">59]</ref>, large-scale simulation (easy to run, but rarely used for server infrastructure code) <ref type="bibr" target="#b32">[39,</ref><ref type="bibr" target="#b47">54,</ref><ref type="bibr" target="#b50">57]</ref>, extrapolation (easy to run, but missing bugs in small training scale) <ref type="bibr" target="#b50">[57,</ref><ref type="bibr" target="#b55">61,</ref><ref type="bibr" target="#b69">75,</ref><ref type="bibr" target="#b74">80]</ref>, and emulation. SCALECHECK falls in this category and below discuss three closely related works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b41">48,</ref><ref type="bibr" target="#b72">78]</ref>.</p><p>Exalt <ref type="bibr" target="#b72">[78]</ref> targets IO-intensive (Big Data) scalability problems where storage capacity is the colocation bottleneck. Exalt's library (Tardis) compresses users' data to zero bytes on disk. With this, Exalt can co-locate 100 space-emulated HDFS datanodes per machine. As the authors stated, their approach "may not discover scalability problems that arise at the nodes that are being emulated" <ref type="bibr" target="#b72">[78]</ref>. Thus, it cannot cover P2P systems where the scale-dependent code is in all the nodes. However, as Exalt targets storage space emulation and SCALECHECK addresses processing time emulation, we believe they complement each other. LinkedIn's Dynamometer is similar to Exalt <ref type="bibr" target="#b9">[10]</ref>.</p><p>DieCast <ref type="bibr" target="#b41">[48]</ref>, invented for network emulation, can colocate processes/VMs on a single machine as if they run individually, by "dilating" time. The trick is adding a "time dilation factor" (TDF) support <ref type="bibr" target="#b42">[49]</ref> into the VMM. For example, TDF=5 implies that for every second of wall-clock time, each emulated VM believes that time has advanced by only 200 ms (1/TDS second). DieCast was only evaluated with a colocation factor (TDF) of 10 as the testing time significantly increases proportionally to the TDF; colocating 500 nodes will increase testing time by 500 times. DieCast was introduced for answering "what if the network is much faster?", but not specifically for single-machine scale-testing. Another significant difference is that both Exalt and DieCast papers do not present an in-depth bug study.</p><p>In terms of related work in the static/program analysis space, Clarity <ref type="bibr" target="#b60">[66]</ref> and Speed <ref type="bibr" target="#b38">[45]</ref> use static analysis to look for potential performance bottlenecks by focusing on redundant traversals and precise complexity bounding. Both approaches are evaluated in libraries. However, for distributed systems, real-scale testing can help reveal unintended complex component interactions, and not all scale-dependent loops cause problems.</p><p>Finally, a recent work also highlights the urgency of combating scalability bugs <ref type="bibr" target="#b53">[60]</ref>. The work, however, does not employ methodical and incremental changes, only suggests a manual approach, and reproduces only 4 bugs in 1 system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>Technical leaders of a large cloud provider emphasized that "the most critical problems today is how to improve testing coverage so that bugs can be uncovered during testing and not in production" <ref type="bibr" target="#b36">[43]</ref>. It is now evident that scalability bugs are new-generation bugs to combat, that existing large-scale testing is arduous, expensive, and slow, and that today's distributed systems are not single-machine scaletestable. Our work addresses these contemporary issues and will hopefully spur more solutions in this new area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Scalability bugs. Definition and quotes from scalability bug reports. Detailed examples are in §2a and §5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: O(N 3 ) scale-depended loops ( §3.1). The partial code segment above depicts the O(N 3 ) loops in Figure 2. SFIND automatically tags epStateMap, affected-Ranges, and map as scale-dependent collections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure (4b) GEDA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: SCALECHECK complete automated flow (Section 3.4). "SCk" represents SCALECHECK. The left-most figure illustrates testing in real deployments, where testing time is fast (T ) but requires N machines. Stages (a) to (d) reflect the automated SCALECHECK process as described in Section 3.4. STESTmez in stage (c) runs on one machine but will take some time (&gt;T ). STESTP IL in stage (d) still runs on one machine but only consumes a similar time as in deployment testing (T +e) and can be replayed numerous times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5a -</head><label>5a</label><figDesc>Figure 5a-d summarizes the complete four stages of SCALECHECK: a SFIND searches for scale-dependent loops which helps developers create test workloads. b For test workloads that show CPU busyness in all nodes, SFIND P IL finds PIL-safe functions and inserts our prememoization library calls. Next, STEST now works in two parts. c STEST mez (without PIL) will run the test on a real cluster, but just one time, to pre-memoize PIL-safe functions and store the tuples to a SSD-backed database file. d STEST P IL (with PIL) will then run by having SFIND P IL remove the pre-memoization library calls, replace the expensive PIL-safe function with sleep(t), and insert our code that constructs the memoized output data. SCALECHECK also records message ordering during STEST mez and replays the same order in STEST P IL (not shown). As another benefit, SCALECHECK can also ease real-scale debugging efforts. First, the only step that consumes more time is the no-PIL pre-memoization phase (Figure 5c), up to 6x longer time than real-deployment testing ( §5.5). However, this is only a one-time overhead. Most importantly, developers can repeatedly re-run STEST P IL (Figure 5d) as many times as needed (tens of iterations) until the bug behavior is completely understood. In STEST P IL , the protocol under test runs in a similar duration as if all the nodes run on independent machines. Second, some fixes can be tested by only re-running the last step; for example, fixes such as changing the failure detector Φ algorithm (for c6127), caching slow methods (c3831), changing lock management (c5456), and enabling parallel processing (v1212). However, if the fixes involve a complete redesign (e.g., optimized gossip processing in c3881, decentralized to centralized rebalancing in r3926), STEST mez must be repeated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: SCALECHECK effectiveness in exposing scalability bugs (Section 5.1). "SCk" represents SCALECHECK. The bugs are listed in Table 3. The x-axis represents the number of nodes (N ). The figure title describes the y-axis, i.e., the bug symptom metrics as recorded in "Real" deployment vs. SCALECHECK. For Cassandra and Riak bugs (a-d), where all nodes are CPU-intensive, the bug symptoms are inaccurate without PIL ("SCk" lines). However, with PIL ("SCk+PIL" lines), the bug symptoms are relatively accurate as in the real deployment scenarios. For Voldemort and HDFS bugs (e-h), where there is no concurrent CPU busyness, PIL is not needed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(e) Figure 6e: In v1212 [33], Voldemort's rebalancing was not optimized for large clusters; it led to more stealer-donor partition transitions as the cluster size grows (128+ nodes). The fix changed the stealer-donor transition algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Cassandra internal metrics ( §5.3). Above are the metrics we measured within the Cassandra bootstrap protocol for measuring SCALECHECK accuracy (Figure 8). "f" represents "a function of" (i.e., an arbitrary function).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Accuracy in exposing c6127 ( §5.3). The figures represent the metrics presented in Figure 7, measured in real deployment ("Real") and in SCALECHECK ("SCk") with different cluster sizes (32, 64, 128, 256, and 512 in the x-axis). The y-axes (the metrics) are described in the figure titles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Maximum colocation factor (Section 5.4). The colocation factor reached as each technique is added.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Z) mark Y as dead.</head><label></label><figDesc></figDesc><table>(Y 5 , map[N]) { 
O(N 3 ) 
} 
node X 

f 

Y 5 
Y 9 Y 7 
... 

backlog 
gossip(Y 1 ) 

gossips 
node Z 

... 

Y=dead 

(no new 
gossip) 
a 

b 

c 
d 
e 

Figure 2: An example bug (Section 2a). (a) Every second 

every node gossips to its peers its ring view and version number 
(e.g., Y gossiped up to version Y9), (b) the receiving node (e.g., X) 
executes "f ()" to synchronize the view, (c) when N is large, this 
O(N 3 ) scale-dependent process creates a backlog of new gossips, 
(d) thus X keeps gossiping only the latest (old) versions (e.g., Y1), 
(e) as Y 's recent gossips are not propagated on time, other nodes 
(e.g., </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>pre-memoization 1 Machine PIL Replay with PIL Auto PIL infusion by SFinder T+e sec STest PIL SCk</head><label></label><figDesc></figDesc><table>in = modVars; 
t = getTime(in); 
sleep(t); 
// F(); 
modVars = getOut(in); 

list L1, L2, L3; 
scale-dep list 

func F(){ 
for(...L1) 
for(...L2) 
for(...L3) 
...; } 

in = modVars; 
t1 = time(); 
F(); 
t 
= time()-t1; 
out = modVars; 
store(in,out,t); 

&gt; T sec 

1 2 .. N 1 2 .. N 

Auto instrumentation of 
memoization library 

zzz 

N 
N 
.. 
.. 
2 
2 
1 
1 

zzz 

SFinder 

a 
b 
c 
d 

O(N 3 ) 

SFinder PIL 
STest 

1 

2 

.. 

N 

#Machines 

T sec 

Testing in real 
deployment 

Mez 

Single-machine testing 
w/ vs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 : Integrations LOC (Section 4). More explanations</head><label>2</label><figDesc></figDesc><table>are in Section 4 of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 2 quantifies</head><label>2</label><figDesc></figDesc><table>the application of SCALECHECK tech-
niques to a variety of distributed systems, Cassandra [58], 
HDFS [18], Riak </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :TComp, T Lock , and QSize denote computation time, lock time, and queue size, respectively. The "Tm" and " T pil " columns quantify the duration of the pre-memoization (STESTmez) and PIL replay (STESTP IL) stages when N ≥256, as discussed in §5.5. "-" implies PIL is unnecessary.</head><label>3</label><figDesc></figDesc><table>Bug benchmark ( §5.1). The table lists the scal-

ability bugs we use for benchmarking SCALECHECK. "c" stands 
for Cassandra, "h" for HDFS, "r" for Riak, and "v" for Volde-
mort. The "N" column represents the #nodes for the bug symptoms 
to surface. The "Metric" column lists the quantifiable metrics of 
the bug symptoms; </table></figure>

			<note place="foot" n="366"> 17th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="1"> So far, we consistently use the same testbed, but a higher-end machine can be used in the future.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgments</head><p>We thank Cheng Huang, our shepherd, and the anonymous reviewers for their tremendous feedback and comments. This material was supported by funding from NSF (grant Nos. CNS-1350499, CNS-1526304, CNS-1405959, and CNS-1563956) as well as generous donations from Dell EMC,Google, Huawei, and NetApp, and CERES center.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Anonymized document for ScaleCheck supplementary materials (also submitted to HotCRP), for interested reviewers</title>
		<ptr target="https://tinyurl.com/sck-supp-mat" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Apache_Cassandra" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspectj</surname></persName>
		</author>
		<ptr target="www.eclipse.org/aspectj" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-5456" />
		<title level="m">Cassandra bug: Large number of bootstrapping nodes cause gossip to stop working</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-3881" />
		<title level="m">Cassandra bug: reduce computational complexity of processing topology changes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cassandra bug: scaling to large clusters in GossipStage impossible due to calculatePendingRanges</title>
		<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-3831" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-6127" />
		<title level="m">Cassandra bug: vnodes don&apos;t scale to hundreds of nodes</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Make it possible to run multi-node coordinator/replica tests in a single JVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandra</forename><surname>Feature</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-14821" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dynamometer Github Repository</surname></persName>
		</author>
		<ptr target="https://github.com/linkedin/dynamometer" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<ptr target="https://engineering.linkedin.com/blog/2018/02/dynamometer--scale-testing-hdfs-on-minimal-hardware-with-maximum" />
		<title level="m">Dynamometer: Scale Testing HDFS on Minimal Hardware with Maximum Fidelity</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<ptr target="https://github.com/inaka/elvis" />
	</analytic>
	<monogr>
		<title level="j">Elvis: Erlang Style Reviewer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-9100" />
		<title level="m">Gossip is inadequately tested</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/HADOOP-1073" />
		<title level="m">Hadoop bug: DFS Scalability: high CPU usage in choosing replication targets and file open</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/HADOOP-4061" />
		<title level="m">Hadoop bug: Large number of decommission freezes the Namenode</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/HDFS-9198" />
		<title level="m">HDFS bug: Coalesce IBR processing in the NN</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="https://issues.apache.org/jira/browse/HDFS-395" />
		<title level="m">HDFS bug: DFS Scalability: Incremental block reports</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nio</forename><surname>Java</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selector</surname></persName>
		</author>
		<ptr target="http://tutorials.jenkov.com/java-nio/selectors.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<ptr target="https://docs.oracle.com/javase/8/docs/technotes/guides/reflection/index.html" />
	</analytic>
	<monogr>
		<title level="j">Java Reflection API</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="http://www.mastertheboss.com/jboss-server/jboss-as-7/jboss-as-7-classloading" />
	</analytic>
	<monogr>
		<title level="j">JBoss AS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Meet Cloudera&apos;s Apache Spark Committers</title>
		<ptr target="http://blog.cloudera.com/blog/2015/09/meet-clouderas-apache-spark-committers/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<ptr target="https://groups.google.com/forum/#!topic/project-voldemort/J7ADKefjR50" />
		<title level="m">NIO in Voldemort: Non-heap memory usage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Chuang of Cloudera and Uma Maheswara Rao Gangumalla of Intel; they are also part of Apache Hadoop Project Management Committee (PMC) members</title>
		<editor>Personal Communication from Andrew Wang and Wei-Chiu</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Personal Communication from Imran Rashid (Software Developer at Cloudera)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riak</surname></persName>
		</author>
		<ptr target="http://basho.com/products/riak-kv" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<ptr target="http://lists.basho.com/pipermail/riak-users_lists.basho.com/2011-April/003895.html" />
		<title level="m">Riak bug: Large ring creation size</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<ptr target="https://groups.google.com/forum/#!msg/project-voldemort/3vrZfZgQp2Y/Uqt8NgJHg4AJ" />
		<title level="m">Number of partitions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Running Netflix on Cassandra in the Cloud</title>
		<ptr target="https://www.youtube.com/watch?v=97VBdgIgcCU" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Why the world&apos;s largest Hadoop installation may soon become the norm</title>
		<ptr target="http://www.techrepublic.com/article/why-the-worlds-largest-hadoop-installation-may-soon-become-the-norm/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lakshmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">R</forename><surname>Bairavasundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Remzi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An Analysis of Data Corruption in the Storage Stack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 6th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Characterizing, Modeling, and Generating Workload Spikes for Stateful Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armando</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 1st ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawson</forename><forename type="middle">R</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 8th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using Automated Performance Modeling to Find Scalability Bugs in Complex Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Calotoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Poke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on High Performance Computing, Networking, Storage and Analysis (SC)</title>
		<meeting>International Conference on High Performance Computing, Networking, Storage and Analysis (SC)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving the network scalability of Erlang</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Chechina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Ghaffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Trindera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="page" from="22" to="34" />
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Optimistic Crash Consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanumalayan</forename><surname>Sankaranarayana Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LazyBase: trading freshness for performance in a scalable database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Cipar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">B</forename><surname>Morrey</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">A N</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alistair</forename><forename type="middle">C</forename><surname>Veitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 EuroSys Conference (EuroSys)</title>
		<meeting>the 2012 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Uncovering Bugs in Distributed Storage Systems during Testing (Not in Production!)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pantazis</forename><surname>Deligiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mccutchen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><forename type="middle">F</forename><surname>Donaldson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Mudduluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaz</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfram</forename><surname>Schulte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 14th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Availability in Globally Distributed Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franis</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentina</forename><forename type="middle">I</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van-Anh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carrie</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quinlna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 9th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SPEED: precise and efficient static estimation of program computational complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><forename type="middle">K</forename><surname>Mehra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trishul</forename><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</title>
		<meeting>the 36th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">What Bugs Live in the Cloud? A Study of 3000+ Issues in Cloud Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiratat</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Patana-Anake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffry</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adityatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kurnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agung</forename><surname>Eliazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Laksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincentius</forename><surname>Lukman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anang</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 5th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Failure Recovery: When the Cure Is Worse Than the Disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Mcdirmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bergan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madan</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 14th Workshop on Hot Topics in Operating Systems (HotOS XIV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">DieCast: Testing Distributed Systems with an Accurate Scale Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diwaker</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashi</forename><surname>Venkatesh Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 5th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">To Infinity and Beyond: Time-Warped Network Emulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diwaker</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenmeth</forename><surname>Yocum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Mcnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Snoeren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 3rd Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Phi Accrual Failure Detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naohiro</forename><surname>Hayashibara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Defago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Yared</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Katayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 23rd Symposium on Reliable Distributed Systems (SRDS)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cloud Storage FUD: Failure and Uncertainty and Durability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Henry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 7th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chunkfs: Using divide-and-conquer to improve file system reliability and repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Val</forename><surname>Henson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjan</forename><surname>Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Gud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2nd Workshop on Hot Topics in System Dependability (HotDep)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">StrongBox: Confidentiality, Integrity, and Performance using Stream Ciphers for Full Drive Encryption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Dickens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><forename type="middle">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 23rd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fireflies: A secure and scalable membership and gossip service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Håvard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robbert</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ymir</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Vigfusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Designing for disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cipriano</forename><forename type="middle">A</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 3rd USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">High Performance Metadata Integrity Protection in the WAFL Copy-on-Write File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harendra</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuvraj</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Kesavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumith</forename><surname>Makam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 15th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Laguna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bronis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>De Supinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">L</forename><surname>Gamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Zhezhe Chen, and Feng Qin. Debugging High-Performance Computing Applications at Massive Scales</title>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">58</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cassandra -A Decentralized Structured Storage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware (LADIS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The Case for Drill-Ready Cloud Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 5th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><forename type="middle">A</forename><surname>Stuardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riza</forename><forename type="middle">O</forename><surname>Suminto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Lukman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bugs: When 100-Node Testing is Not Enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalability</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th Workshop on Hot Topics in Operating Systems (HotOS XVII</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">PCatch: Automatically Detecting Performance Cascading Bugs in Cloud Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xicheng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EuroSys Conference (EuroSys)</title>
		<meeting>the 2018 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">LISA &apos;11 ThemeDevOps: New Challenges, Proven Values. USENIX ;login: Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Limoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hughe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-08" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">ffsck: The Fast File System Checker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dragga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 11th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Yak: A High-Performance Big-Data-Friendly Garbage Collector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Demsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanazsadat</forename><surname>Alamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Increasing the transparent page sharing in java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazunori</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamiya</forename><surname>Onodera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Static Detection of Asymptotic Performance Bugs in Collection Traversals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswaldo</forename><surname>Olivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isil</forename><surname>Dillig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<meeting>the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Tool Interface version 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oracle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jvmtm</surname></persName>
		</author>
		<ptr target="https://docs.oracle.com/javase/8/docs/platform/jvmti/jvmti.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Is Scale Your Enemy, Or Is Scale Your Friend?: Technical Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM (CACM)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Making Sense of Performance in Data Analytics Frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 12th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Scale and Concurrency of GIGA+: File System Directories with Millions of Files</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swapnil</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 9th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">AONT-RS: Blending Security and Performance in Dispersed Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">K</forename><surname>Resch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Plank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 9th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">FlashTier: a Lightweight, Consistent and Durable Storage Cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 EuroSys Conference (EuroSys)</title>
		<meeting>the 2012 EuroSys Conference (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Understanding Latent Sector Errors and How to Protect Against Them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Damouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipa</forename><surname>Gill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 8th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Flash Reliability in Production: The Expected and the Unexpected</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Lagisetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arif</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 14th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Evaluating Scalability Bottlenecks by Workload Extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</title>
		<meeting>the IEEE International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Konstantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shvachko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalability</surname></persName>
		</author>
		<title level="m">The Limits to Growth. USENIX ;login</title>
		<imprint>
			<date type="published" when="2010-04" />
			<biblScope unit="volume">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Consistency-Based Service Level Agreements for Cloud Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">B</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Kotla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Abu-Libdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Exalt: Empowering Researchers to Evaluate Large-Scale Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Kapritsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lara</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Alvisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Dahlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 11th Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">SEDA: An Architecture for Well-Conditioned, Scalable Internet Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Welsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Culler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 18th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Vrisha: Using Scaling Properties of Parallel Programs for Bug Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milind</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Bagchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th IEEE International Symposium on High Performance Distributed Computing (HPDC)</title>
		<meeting>the 20th IEEE International Symposium on High Performance Distributed Computing (HPDC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
