<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaling Guest OS Critical Sections with eCS Scaling Guest OS Critical Sections with eCS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 11-13. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Tech</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>Changwoo Min</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Virginia Tech; Taesoo Kim</orgName>
								<orgName type="institution" key="instit2">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaling Guest OS Critical Sections with eCS Scaling Guest OS Critical Sections with eCS</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18)</title>
						<meeting>the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18) <address><addrLine>Boston, MA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 11-13. 2018</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2018 USENIX Annual Technical Conference is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Multi-core virtual machines (VMs) are now a norm in data center environments. However, one of the well-known problems that VMs suffer from is the vCPU scheduling problem that causes poor scalability behaviors. More specifically, the symptoms of this problem appear as preemption problems in both under-and over-committed scenarios. Although prior research efforts attempted to alleviate these symptoms separately, they fail to address the common root cause of these problems: the missing semantic gap that occurs when a guest OS is preempted while executing its own critical section, thereby leading to degradation of application scalability. In this work, we strive to address all preemption problems together by bridging the semantic gap between guest OSes and the hypervisor: the hypervisor now knows whether guest OSes are running in critical sections and a guest OS has hypervisor&apos;s scheduling context. We annotate all critical sections by using the lightweight para-virtualized APIs, so we called enlightened critical sections (eCS), that provide scheduling hints to both the hypervisor and VMs. The hypervisor uses the hint to reschedule a vCPU to fundamentally overcome the double scheduling problem for these annotated critical sections and VMs use the hypervisor provided hints to further mitigate the blocked-waiter wake-up problem. Our evaluation results show that eCS guarantees the forward progress of a guest OS by 1) decreasing preemption counts by 85-100% while 2) improving the throughput of applications up to 2.5× in an over-committed scenario and 1.6× in an under-committed scenario for various real-world workloads on an 80-core machine.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Virtualization is now the backbone of every cloud-based organization to run and scale applications horizontally on demand. Recently, this scalability trend is also extending towards vertical scaling <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, i.e., a virtual machine (VM) has up to 128 virtual CPUs (vCPUs) and 3.8 TB of memory to run large in-memory databases <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b36">35]</ref> and data processing engines <ref type="bibr" target="#b48">[47]</ref>. At the same time, cloud providers strive to oversubscribe their resources to improve hardware utilization and reduce energy consumption, without imposing any permissible overhead on the application <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b47">46]</ref>. However, over subscription requires multiplexing of physical CPUs among VMs to equally distribute physical CPU cycles. Thus, the multiplexing of these VMs introduces the double scheduling Figure 1: Impact of exposing some of the semantic information from the VM to the hypervisor and vice-versa, which leads to better scalability of Psearchy and Apache web server benchmark, in a scenario in which two VMs are running with the same benchmark. Here, PVM and HVM denote with and without para-virtualization support, while eCS represents our approach. Psearchy mostly suffers from LHP and BWW. Similarly, Apache suffers from LHP and ICP.</p><p>problem <ref type="bibr" target="#b41">[40]</ref>: 1) the guest OS schedules processes on vCPUs and 2) the hypervisor schedules vCPUs on physical CPUs. Some of the prior works address this problem by adopting co-scheduling approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b46">45]</ref>, which can suffer from priority inversion, CPU fragmentation, and may mitigate the double scheduling symptoms <ref type="bibr" target="#b41">[40]</ref>. Such symptoms, that have mostly been addressed individually, are lock-holder preemption (LHP) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b45">44]</ref>, lock-waiter preemption (LWP) <ref type="bibr" target="#b45">[44]</ref>, and blocked-waiter wakeup (BWW) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">39]</ref>, problems. The root cause of this double scheduling phenomenon is a semantic gap between a hypervisor and guest OSes, in which the hypervisor is agnostic of not only the scheduling of VMs but also guest OS-specific critical code that deter the scalability of applications. Furthermore, LHP/LWP are not only limited to spinlocks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">42]</ref>, but are also possible in blocking primitives such as mutex and rwsem as well as readers of the rwsem. Moreover, because of their non work-conserving nature, these blocking primitives inherently suffer from the BWW problem (refer Psearchy in <ref type="figure">Figure 1 (a)</ref>). Besides these, none of the prior works have identified the preemption of an interrupt context that happens in interrupt-intensive applications such as Apache web-server <ref type="figure">(Figure 1 (b)</ref>). We define this problem as interrupt context preemption (ICP).</p><p>Our key observation is that these symptoms occur because 1) the hypervisor is scheduling out a vCPU at a time when the vCPU is executing a critical code, and 2) a vCPU, waiting to acquire a lock, is either uncooperative or sleeping <ref type="bibr" target="#b15">[16]</ref>, leading to LWP and BWW issues. Thus, we propose an alternative perspective, i.e., instead of devising a solution for each symptom, we use four key ideas that allows a VM to hint the hypervisor for mak-ing an effective scheduling decision to allow its forward progress. First, we consider all of the locks and interrupt contexts as critical components. Second, we devise a set of para-virtualized APIs that annotate these critical components as enlightened critical sections (eCS). These APIs are lightweight in nature and notify a hypervisor from the VM and vice-versa with memory operations via shared memory, while avoiding the overhead of hypercall and interrupt injection. Third, the hypervisor now can figure out whether a vCPU is executing an eCS and can reschedule it. We empirically found that an extra schedule (one millisecond <ref type="bibr" target="#b28">[27]</ref>) is sufficient as it decreases preemptions by 85-100%; and these critical sections are shorter (in µs <ref type="bibr" target="#b43">[42]</ref>) than one schedule. However, by rescheduling a vCPU, we introduce unfairness in the system. We tackle this issue with the OS's fair scheduling policy <ref type="bibr" target="#b28">[27]</ref>, which compensates for that additional schedule by allowing other tasks to run for extra time, thereby maintaining the eventual fairness in the system. Lastly, we leverage our APIs to design a virtualized schedule-aware spinning strategy (eSchdSpin) that enables lock waiters to be work conserving as well as cooperative inside a VM. That is, a vCPU now cooperatively spins for the lock, if a physical CPU is under-committed, else it yields the vCPU.</p><p>Thus, our approach improves the scalability of realworld applications by 1.2-1.6× in an under-committed case. Moreover, our eCS annotation, combined with eSchdSpin, avoids preemption by 85-100% while improving the scalability of applications by 1.4-2.5× in an over-committed scenario on an 80-core machine.</p><p>In summary, we make the following contributions:</p><p>• We identify similarities among various subproblems that stem from the double scheduling phenomenon. Moreover, we identify three new problems: 1) LHP in blocking locks, 2) readers preemption (RP) in readwrite locks and semaphores, and 3) vCPU preemption while processing an interrupt context (ICP).</p><p>• We address these subproblems with eCS, which we annotate with six new APIs that bridge the semantic gap between a hypervisor and a VM, and even among vCPUs inside a VM.</p><p>• Our annotation approach, along with eSchdSpin, improves the scalability of applications in both under-and over-committed scenarios up to 2.5× with only 0-15% preemptions, while maintaining eventual fairness with merely one extra schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>We first describe the problem of double scheduling and highlight its implications. Later, we summarize the prior attempts to solve this problem, and then motivate our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Symptoms of Double Scheduling</head><p>In a virtualized environment, a hypervisor multiplexes the hardware resources for a VM, such as assigning vCPUs to physical CPUs (pCPUs). In particular, it runs a vCPU to execute by its fair share <ref type="bibr" target="#b28">[27]</ref>, which is a general policy of commodity OSes such as Linux, and preempts it because of either vCPUs of other VM or of the intermittent processes of the OS and bookkeeping tasks of the hypervisor such as I/O threads. Hence, there is a possibility that the hypervisor can preempt a vCPU while executing some critical task inside a VM that leads to an application performance anomaly, which we enumerate below:</p><p>Lock holder preemption (LHP) problem occurs when a vCPU holding a lock gets preempted and all waiters waste CPU cycles for the lock. Most of the prior works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b45">44]</ref> have focused on non-blocking primitives such as spinlocks. <ref type="bibr" target="#b0">1</ref> On the other hand, LHP also occurs in blocking primitives such as mutex <ref type="bibr" target="#b29">[28]</ref> and rwsem <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b32">31]</ref>, which the prior works have not identified. However, LHP accounts up to 90% preemptions for blocking primitives in some of the memory intensive applications that have short critical sections.</p><p>Lock waiter preemption (LWP) problem stems when the very next waiter is preempted just before acquiring the lock, which occurs due to the strict FIFO ordering of spinlocks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">42]</ref>. Fortunately, this problem has been mostly mitigated in existing spinlock design <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, as the current implementation allows waiters to steal the lock before joining the waiter queue. We do not see such a problem in blocking primitives because the current implementation is based on the test-and-set (TAS) lockan unfair lock, which inherently mitigates LWP.</p><p>Blocked-waiter wakeup (BWW) problem occurs mostly for blocking primitives in which the latency to wake up a waiter to pass the lock is quite high. This issue severely degrades the throughput of applications running on a high core count <ref type="bibr" target="#b15">[16]</ref>, even in a native environment. Moreover, it is evident in both under-and over-committed VM scenarios. For example, the BWW problem degrades the application scalability up to 1.6× (refer <ref type="figure">Figure 6</ref>).</p><p>Readers preemption (RP) problem is a new class of problem that occurs when a vCPU holding a read lock among multiple readers gets preempted. This problem impedes the forward progress of a VM and also increases the latency of the write lock. For instance, various memoryintensive workloads have sub-optimal throughput as RP accounts to at most 20% of preemptions. We observe this issue in various read-dominated memory-intensive workloads in which the readers are scheduled out.</p><p>problem that occurs when an RCU reader is preempted, while holding the RCU read lock <ref type="bibr" target="#b34">[33]</ref>. Because of RRP, the guest OS suffers from an increased quiescence period. This issue can increase the memory footprint of the application, and is responsible for 5% of preemptions.</p><p>Interrupt context preemption (ICP) problem happens when a vCPU that is executing an interrupt context gets preempted. In particular, this problem is different from prior works that focus on interrupt delivery <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">43]</ref> rather than interrupt handling. This issue occurs in cases such as TLB shootdowns, function call interrupts, rescheduling interrupts, IRQ work interrupts, etc. in every commodity OS. For example, we found that Apache web server, an interrupt-intensive workload, suffers from the ICP problem as it accounts to almost 18% of preemptions for evaluated workloads (refer <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prior Approaches</head><p>Some of the prior studies mitigate LHP and LWP problems by relaxed co-scheduling <ref type="bibr" target="#b46">[45]</ref>, balancing vCPUs to physical CPUs <ref type="bibr" target="#b42">[41]</ref> with IPIs as a heuristic <ref type="bibr" target="#b16">[17]</ref>, or using hardware features <ref type="bibr" target="#b35">[34]</ref>. Meanwhile, others designed a para-virtualized interface <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b45">44]</ref> to only tackle the LHP and LWP problem for spinlocks. Besides these, one radical design focused on scheduling VM's processes than vCPUs by hot plugging vCPUs on the basis of load on the VM <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">40]</ref>. Unfortunately, all of these prior works address the double scheduling problem either partially that misses other preemption problems, or take a radical path that is not only difficult to adopt in practice but can have significant overhead, in terms of scaling for machines with almost 100 physical cores. Because their approach involves 1) the detection of response to the double scheduling in the form of hypercalls and interrupt injection <ref type="bibr" target="#b2">[3]</ref>, and 2) explicit task migration from idle vCPUs to active vCPUs. On the contrary, our approach does simple memory operations and exploits the vCPU scheduling boundary to notify the hypervisor for scheduling decisions without any explicit task and vCPU migration: a lightweight approach even at high core count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Case for An Extra Schedule</head><p>As mentioned before, OS critical sections are the ones that define the forward progress of an application for which the OS is responsible. For instance, let us take an example of two threads competing to acquire a lock to update contents of a file. If the lock holder, which is updating the file, is preempted, the other waiter will waste CPU cycles. There are several critical operations that affect the application scalability <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">25]</ref>, and OS performs such operations either by acquiring a lock or executing an interrupt context (I/O processing, TLB shootdowns, etc.). In particular, a delay in processing of these critical sections can result in a severe performance anomaly such as a convoy effect <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16]</ref>, or decreased network through- put for applications such as web servers (refer <ref type="figure">Figure 1)</ref>. Hence, unlike prior approaches, we propose a simple and an intuitive approach, i.e., now a VM hints the hypervisor about a critical section that enables the hypervisor to let a vCPU execute for a pre-defined time slot (schedule). This extra schedule is sufficient to complete a critical section because 1) most critical sections are very fine-grained, and have a time granularity of several microseconds <ref type="bibr" target="#b43">[42]</ref>, while 2) the granularity of a single schedule is in the order of milliseconds, which is sufficient enough to complete a critical section. For instance, an extra schedule decreases the preemption count by 85-100% ( <ref type="figure">Figure 3</ref>). This approach is not only practical but also critical to apply on machines with large core count. However, the extra schedule introduces unfairness in the system, which we address by designing a simple, zero-overhead schedule penalization algorithm that tries to maintain the eventual fairness in the system by leveraging the CFS <ref type="bibr" target="#b28">[27]</ref> that tries to maintain fairness in the system. Increase the eCS count for a vCPU with cpu id by 1 for a non-preemptable task void deactivate_non_preemptable_ecs(cpu_id)</p><p>Decrease the eCS count for a vCPU with cpu id by 1 for a non-preemptable task void activate_preemptable_ecs(cpu_id)</p><p>Increase the eCS count for a vCPU with cpu id by 1 for a preemptable task void deactivate_preemptable_ecs(cpu_id)</p><p>Decrease the eCS count for a vCPU with cpu id by 1 for a preemptable task</p><formula xml:id="formula_0">Hypervisor → VM bool is_vcpu_preempted(cpu_id) †</formula><p>Return whether a vCPU with cpu id is preempted by the hypervisor</p><formula xml:id="formula_1">bool is_pcpu_overcommitted(cpu_id)</formula><p>Return whether a physical CPU, running a vCPU with cpu id, is over-committed <ref type="table">Table 1</ref>: Set of para-virtualized APIs exposed by the hypervisor to a VM for providing hints to the hypervisor to mitigate double scheduling. These APIs provide hints to the hypervisor and VM via shared memory. A vCPU relies on the first four APIs to ask for an extra schedule to overcome LHP, LWP, RP, RRP, and ICP. Meanwhile, a vCPU gets hints from the hypervisor by using the last two APIs to mitigate LWP and BWW problems. The cpu_id is the core id that is used by tasks running inside a guest OS. † Currently, is_vcpu_preempted() is already exposed to the VM in Linux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design</head><p>A hypervisor can mitigate various preemption problems, if it is aware of a vCPU executing a critical section. We denote such a hypervisor-aware critical section as an enlightened critical section (eCS), that can be executed for one more schedule. eCS is applicable to all synchronization primitives and mechanisms such as RCU and interrupt contexts. We now present our lightweight APIs that act as a cross-layer interface for annotating an eCS and later focus on our notion of an extra schedule and our approach to maintain eventual fairness in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Lightweight Para-virtualized APIs</head><p>We propose a set of six lightweight para-virtualized APIs to bridge the semantic gap that both VM and hypervisor use for conveying information between them. These APIs rely on four variables (refer <ref type="figure" target="#fig_2">Figure 2</ref>) that are local to each vCPU. They are exposed via shared memory between the hypervisor and a VM and the notification happens via simple read and write memory operations. A simple memory read is sufficient for the hypervisor to decide on scheduling because 1) it tries to execute each vCPU on a separate pCPU, 2) and it requires knowing about an eCS only at the schedule boundary, thereby removing the cost of polling and other synchronous notifications <ref type="bibr" target="#b2">[3]</ref>.</p><p>To consider an OS critical section as an eCS, we mark the start and unmark the end of a critical section, which lets the hypervisor know about an eCS. However, a process in an OS can be of two types. First is the non-preemptable process that can never be scheduled out. Such a process is either an interrupt or a kernel thread running after acquiring a spinlock. Another one is the preemptable task such as a user process or a process with blocking lock. Hence, we introduce four APIs (VM → Hypervisor) to separately handle these two types of tasks. The last two APIs (Hypervisor → VM) provide the hypervisor context to the VM, which a lock waiter can use to mitigate the LWP problem or yield the vCPU to other hypervisor tasks or vCPUs in an over-committed scenario. <ref type="figure" target="#fig_2">Figure 2</ref> illustrates those four states:</p><p>• non_preemptable_ecs_count maintains the count of active non-preemptable eCSs, such as nonblocking locks, RCU reader, and interrupt contexts.</p><p>It is similar to the preemption count of the OS.</p><p>• preemptable_ecs_count is similar to the preemption count variable of the OS, but it only maintains the count of active preemptable eCSs, such as blocking primitives, namely, mutex and rwsem.</p><p>• vcpu_preempted denotes whether a vCPU is running.</p><p>It is useful for handling the BWW problem in both under-and over-committed scenarios.</p><p>• pcpu_overloaded denotes whether a physical CPU, executing that particular vCPU, is over-committed.</p><p>Lock waiters can use this information to address the BWW problem in an over-committed scenario.  i.e., entering an eCS, shares information with the hypervisor. During entry ( 1 ), vCPU 2 first updates its corresponding state (non_preemptable_ecs_count or preemptable_ecs_count) ( 2 ) and continues to execute its critical section. Meanwhile, the hypervisor, before scheduling out vCPU 2 , checks vCPU 2 's eCS states ( 3) and allows it to run for extra time if certain criteria are fulfilled ( §3.2); otherwise, it schedules out vCPU 2 with other waiting tasks. When vCPU 2 exits the eCS, it decreases the eCS state count, denoting the end of critical section. <ref type="figure" target="#fig_2">Figure 2</ref> (b) illustrates another scenario that addresses the BWW problem. in which the hypervisor updates the eCS states:</p><p>pcpu_overloaded and vcpu_preempted while scheduling in and out vCPU 2 , respectively, at each schedule boundary ( 1 ). We devise a simple approach-virtualized scheduling-aware spinning (eSchdSpin)-that enables efficient scheduling aware waiting for both blocking and non-blocking locks ( §4). That is, vCPU 2 reads both states ( 2) and decides whether to keep spinning until the lock is acquired if the pCPU is not overloaded ( 3 ), else it yields, which allows the other vCPU (in VM 2 ) or a hypervisor's task to progress forward by doing some useful task, thereby mitigating the double scheduling problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Eventual Fairness with Selective Scheduling</head><p>As mentioned before, the hypervisor relies on its scheduler to figure out whether a vCPU is executing an eCS. That is, when a vCPU with a marked eCS is about to be scheduled out, the hypervisor scheduler checks the value of eCS count variables <ref type="figure" target="#fig_2">(Figure 2</ref>). If any of these values are greater than zero, the hypervisor lets the vCPU run for an extra schedule. However, vCPU rescheduling introduces two problems: 1) How does the hypervisor handles a task with eCS, which the guest OS can preempt or schedule out? 2) How does it ensure the system fairness?</p><p>We handle an eCS preemptable task with preemptable_ecs_count counter APIs, which differentiate between a preemptable task and a non-preemptable task. We do so because the guest OS can schedule out a preemptable task. In this case, the hypervisor should avoid rescheduling that vCPU because 1) it will result in false rescheduling, and 2) it can hamper the VM performance. We address this issue inside the guest OS,</p><p>i.e., before scheduling out an eCS-marked task inside a guest OS, we save the value of preemptable_ecs_count to a task-specific structure and reset the counter to zero. Later, when the task is rescheduled again by the guest OS, we restore the preemptable_ecs_count with the saved value from the task-specific structure, thereby mitigating the false scheduling.</p><p>With vCPU rescheduling, we introduce unfairness at two levels: 1) An eCS marked vCPU will always ask for rescheduling on every schedule boundary. <ref type="bibr" target="#b1">2</ref> 2) By rescheduling a vCPU, the hypervisor is unfair to other tasks in the system. We resolve the first issue by allowing the hypervisor to reschedule an eCS-marked vCPU only once during that schedule boundary as rescheduling extends the boundary. At the end of schedule boundary, the hypervisor schedules other tasks to avoid the starving other tasks or VMs and addresses indefinite rescheduling. In addition, the hypervisor also keeps track of this extra reschedule information and runs other vCPUs for longer duration and inherently balances the running time, an equivalent to vCPU penalization. Thus, our approach selectively reschedules and penalizes a vCPU rather than balancing the extra reschedule information across all cores, which will result in an unnecessary overhead of synchronizing all runtime information of rescheduling. We call our approach as the local CPU penalization approach, as we only penalize a vCPU that executed an eCS, thereby ensuring eventual fairness in the system. Moreover, our local vCPU scheduling is a form of selective-relaxed coscheduling of vCPUs depending on what kind of tasks are being executed, while without maintaining any synchronization among vCPUs, unlike prior approaches <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b46">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Use Case</head><p>The double scheduling phenomenon introduces the semantic gap in three places: 1) from a vCPU to a physical CPU that results in LHP, RP, and ICP problems; 2) from a 2 Such a VM can be either an I/O or an interrupt-intensive VM that spends most of its time in the kernel, or even a compromised VM.   <ref type="table" target="#tab_1">Table 2</ref>).</p><formula xml:id="formula_2">API LHP RP RRP ICP LWP BWW activate_non_preemptable_vcs() ✓ ✓ ✓ ✓ - - deactivate_non_preemptable_vcs() ✓ ✓ ✓ ✓ - - activate_preemptable_vcs() ✓ ✓ - - - - deactivate_preemptable_vcs() ✓ ✓ - - - - is_vcpu_preempted() - - - - ✓ ✓ is_pcpu_overcommitted() - - - - - ✓</formula><p>LWP and BWW problem. The LWP problem occurs in the case of FIFO-based locks such as MCS and Ticket locks <ref type="bibr" target="#b22">[23]</ref>. However, unfair locks, such as qspinlock <ref type="bibr" target="#b5">[6]</ref>, mutex <ref type="bibr" target="#b30">[29]</ref>, and rwsem <ref type="bibr" target="#b38">[37]</ref>, do not suffer from this problem, and are currently used in Linux. The reason is that they allow other waiters to steal the lock, while suffering from the issue of starvation. On the other hand, all of these locks suffer from the BWW problem because the cost to wake up a sleeping in a virtualized environment varies from 4,000-10,000 cycles. as a wake-up call results in a VMexit, which adds an extra overhead to notify a vCPU to wake up a process. This problem is severe for blocking primitives because they are non-work conserving in nature <ref type="bibr" target="#b15">[16]</ref>, i.e., the waiters schedule out themselves, even if a single task is present in the run queue of the guest OS. We partially mitigate this issue by allowing the waiters to spin rather than sleep if a single task is present in the run queue of the guest scheduler (SchdSpin). However, this approach is non-cooperative when multiple VMs are running. Thus, to avoid unnecessary spinning of waiters, we rely on our is_pcpu_overcommitted() API that notifies a waiter to only spin if the pCPU is not over-committed.</p><p>We call this approach the virtualized scheduling-aware spinning approach (eSchdSpin).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>We realized the idea of eCS by implementing it on the Linux kernel version 4.13. Besides annotating various locks and interrupt contexts with eCS, we specifically modified the scheduler and the para-virtual interface of the KVM hypervisor. Our changes are portable enough to apply on the Xen hypervisor too. The whole modification consists of 1,010 lines of code (see <ref type="table" target="#tab_2">Table 3</ref>).</p><p>Lightweight para-virtualized APIs. We share the information between the hypervisor and a VM with a shared memory between them, which is similar to the kvm_steal_time <ref type="bibr" target="#b3">[4]</ref> implementation. For instance, each VM maintains a per-core eCS states, and the hypervisor maintains per-vCPU eCS states for each VM.</p><p>Scheduler extension.</p><p>We extend a scheduler-totask notification mechanism, preempt_notifier <ref type="bibr" target="#b17">[18]</ref>, for identifying an eCS-marked vCPU at the schedule boundary. Our extension allows the scheduler to know about the task scheduling requirement and decide scheduling strategy at the schedule boundary. For example, in our case, the extension reads the non_preemptable_ecs_count and preemptable_ecs_count to decide the scheduling strategy for the vCPU. Besides this, we rely on the notifier's in and out APIs to set the value of vcpu_preempted and pcpu_overloaded variables.</p><p>We implemented our vCPU rescheduling decision in the schedule_tick function <ref type="bibr" target="#b37">[36]</ref>. The schedule_tick function performs two tasks: 1) It does the bookkeeping of the task runtime, which is used for ensuring the fairness in the system. 2) It also is responsible for setting the rescheduling flag (TIF_NEED_RESCHED) if there is more than one task on that run queue, which is used by the scheduler to schedule out the task if the reschedule flag is set. We implemented the rescheduling strategy by bypassing the setting up of the reschedule flag in case the preempt_notifier check function returned true, meanwhile updating the runtime statistics of the vCPU.</p><p>Annotating locks for eCS. We mark eCS by using the non-preemptable APIs for non-blocking primitives, preemptable ones for mutex and rwsem. Our annotation comprises only 60 LoC that covers around 12,000 lock instances with 85,000 lock API calls in the Linux kernel that has 10 million LoC for the kernel version 4.13. Experimental setup. We extended VBench <ref type="bibr" target="#b12">[13]</ref> for our evaluation. We chose four benchmarks: Apache web server <ref type="bibr" target="#b6">[7]</ref>, Metis <ref type="bibr" target="#b20">[21]</ref>, Psearchy from Mosbench, and Pbzip2 <ref type="bibr" target="#b8">[9]</ref>. The Apache web server serves a 300 bytes static page for each request that is generated by WRK <ref type="bibr" target="#b9">[10]</ref>. Both of them are running inside the VM to remove the network wire overhead and only stress the VM's kernel components. We choose Apache to stress the interrupt handler to emphasize the importance of eCS for an interrupt context. Metis is a map-reduce library for a single multi-core server that mostly stresses the memory allocator (spinlock) and the page-fault handler (rwsem) of the OS. Similar to Metis, Psearchy is an in-memory parallel search and indexer that stresses the writer side of the rwsem design. In addition, we also choose Pbzip2-a parallel compression and decompression program-because we wanted to use a minimally kernel-intensive application. Moreover, none of these workloads suffer from performance degradation from any known user space bottleneck in a non-virtualized environment. We use memory-based file system, tmpfs, to isolate the effect of I/O. We further pin the cores to circumvent vCPU migration at the hypervisor level to remove the jitter from our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate our eCS approach against the following configurations: 1) PVM is a para-virtualized VM that includes unfair qspinlock implementation, which mitigates LWP and BWW issues, and it is the default configuration since Linux v4.5. 2) HVM is the one without paravirtualization support and also includes unfair qspinlock implementation. Both PVM and HVM are not eCS annotated. Note that we could not compare other prior works because they are not open sourced <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">45]</ref> and are very specific to the Xen hypervisor <ref type="bibr" target="#b43">[42]</ref>. We evaluate these configuration on an eight socket, 80-core machine with Intel E7-8870 processors. Another point is that the current version of KVM partially addresses the BWW problem that can occur from the user space <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overhead of eCS</head><p>We evaluate the cost of our lightweight para-virtualized APIs on various blocking and non-blocking locks, and RCU. <ref type="table">Table 4</ref> enumerates the overhead of the sole API cost including the cost of executing a critical section with a simple microbenchmark that executes an empty critical section to quantify the impact of eCS API on these primitives in both lowest (1 core) and highest contention (80 core) scenarios. 1 core denotes that a thread is trying to acquire a critical section, whereas 80 core denotes that 80 threads are competing. We observe that eCS adds an overhead of almost 0.9-18.4 ns in low contention, whereas negligible overhead in high contention scenario, except RCU. For RCU, the empty critical section  <ref type="table">Table 4</ref>: Cost of using our lightweight para-virtualized APIs with various synchronization primitives and mechanism. 1 core and 80 core denote the time (in ns) to execute an empty critical section with one and 80 threads, respectively. Although, our approach slightly adds an overhead on a single core count, there is no performance degradation for our evaluated workloads.</p><p>suffers from almost twice the overhead because both RCU's lock/unlock operations do a single memory update on the preempt_count variable for a preemptable kernel. Even though our APIs add an overhead in the low contended scenario, we do not observe any performance degradation for any of our evaluated workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance in an Over-committed Scenario</head><p>We evaluate the performance of the aforementioned workloads in an over-committed scenario by running two VMs in which each vCPU from both VMs share a physical CPU. <ref type="figure">Figure 3 (i)</ref> shows the throughput of these workloads for PVM, HVM, and eCS; (ii) shows the number of unavoidable preemptions that we capture while running these workloads when a vCPU is about to be scheduled out for eCS; and (iii) represents the percentage of types of observed preemptions, namely, LHP for blocking (B-LHP) and non-blocking (NB-LHP) locks, RP, RRP, ICP problems that we observe for the eCS configuration, including both avoided and unavoided preemptions.</p><p>Apache. eCS outperforms both PVM and HVM by 1.2× and 1.6×, respectively (refer (t:a) in <ref type="figure">Figure 3</ref>). Moreover, our approach reduces the number of possible preemptions by 85.8-100% (refer (n:a)) because of our rescheduling approach. We cannot completely avoid all preemptions because of our schedule penalization approach, as some of the preemptions occur consecutively. Even though eCS adds overhead, especially to RCU, it still does not degrade the scalability for four reasons: 1) We address the BWW problem, which allows for more opportunities to acquire the lock on time; 2) both hypervisor → VM APIs allow cooperative co-scheduling of the VMs; 3) our extra schedule approach avoids 85.8-100% of captured preemptions with the help of our VM → hypervisor APIs; and 4) the APIs overhead partially mitigates the highly contended system at higher core count by acting as a back-off mechanism. Another interesting observation is that we observe almost every type of preemption (refer <ref type="figure">Figure 3</ref> (p:a)) because of serving the static pages, which involves blocking locks for the socket connection and softirq and spinlocks use for the interrupts processing. In particular, the number of preemptions is dominated by LHP for non-blocking and blocking locks, followed by ICP and then RP. We believe that the ICP problem will further exacerbate with optimized interrupt delivery mechanisms <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">43]</ref>. PVM is 1.36× faster than HVM at 80 cores because of the support of para-virtualized spinlock (qspinlock <ref type="bibr" target="#b19">[20]</ref>) as well as the asynchronous page fault mechanism that decreases the contention <ref type="bibr" target="#b31">[30]</ref>. The major bottleneck for this workload is the interrupt injection, which can be mitigated by proposed optimized methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">43]</ref>. In addition, <ref type="figure" target="#fig_7">Figure 4</ref> (b) presents the latency CDF for the Apache workload at 80 cores in both under-and over-compression case. We observe that eCS not only maintains almost equivalent latency as that of PVM in an under-committed case, but also decreases in the over-committed case by 10.3-17% and 9.5-27.9% against PVM and HVM, respectively.</p><p>Psearchy mostly stresses the writer side of rwsem as it performs 20,000 small and large mmap/munmap operations along with stressing the memory allocator for inode operations, which mostly idles the guest OS because of the non-work conserving blocking locks <ref type="bibr" target="#b15">[16]</ref>. <ref type="figure">Figure 3</ref> (t:b) shows the throughput, in which eCS outperforms both PVM and HVM by 2.3× and 1.7×, respectively. The reason is that we 1) partially mitigate the BWW problem with our eSchdSpin approach, and 2) decrease the number of preemptions by 95.7-100% with an extra schedule (refer (n:b)). In addition, our eSchdSpin approach decreases the idle time from 65.4% to 45.2%, as it allows waiters to spin than schedule out themselves, which severely degrades the scalability in a virtualized environment, as observed for both PVM and HVM. This workload is dominated by mostly blocking and non-blocking locks, as they account to almost 98% preemptions (refer (p:b)). We also observe that HVM outperforms PVM by 1.33× because the asynchronous page fault mechanism introduces more BWW issue as it schedules out a vCPU if the page is not available, which does not happen for HVM.</p><p>Metis is a mix of both page fault and mmap operations that stress both the reader and the writer of the rwsem. Hence, it also suffers from the BWW problem, as we observe in <ref type="figure">Figure 3 (t:c)</ref>. eCS outperforms PVM and HVM by 1.3× at 80 cores because of the reduced BWW problem and decreased preemptions that account to 91.4-99.5% <ref type="figure">(Fig- ure 3 (n:c)</ref>). Note that the reader preemptions are 20%, thereby illustrating that readers preemptions is possible for read-dominated workloads, which has not been observed by any prior works. We do not observe any difference in the throughput of HVM and PVM.</p><p>Pbzip2 is an efficient compression/decompression work-   <ref type="figure">Figure 3</ref>: Analysis of real-world workloads in an over-committed scenario, i.e., two instances of VM are executing the same workload. Column (i) represents the scalability of selected workloads in three settings: PVM, HVM, and with eCS annotations. Column (ii) represents the number of preemptions caught and prevented by the hypervisor with our APIs. Column (iii) represents the type of preemptions caught by the hypervisor (refer <ref type="table">Table 4</ref>). By allowing an extra schedule, our approach reduces preemptions by 85-100% and improve scalability of applications by up to 2.5×, while observing almost all types of preemptions for each workload.   <ref type="table">Table 1</ref>) on Psearchy in both under-and over-committed scenarios.</p><p>load that spends only around 5% of the time in the kernel space. <ref type="figure">Figure 3 (t:d)</ref> shows that the performance of eCS is similar to PVM and HVM, while decreasing the number of preemptions by 98.4-100% (refer (n:d)). We do not observe any performance gain in this scenario because 1) these preemptions may not be too critical to affect the application scalability, and 2) the overhead of our APIs, which do not provide any gains even after decreasing the preemptions. Similar to the other workloads, LHP dominates the preemption, followed by RP, ICP, and RRP. In summary, our APIs not only reduce preemptions by 85-100%, but also improve the scalability of applications that use these synchronization primitives up to 2.5×, while no observable overhead on these applications. Moreover, we found that these preemptions occur for almost every type of primitives, specifically in the case of blocking synchronization primitives, read locks (Metis and Pbzip2), and interrupts (e.g., TLB operations, packet processing etc.). In addition, most of the workloads still suffer from the BWW problem because of them being nonwork conserving. We partially address this problem with the help of our eSchdSpin approach. One point to note is that we do not observe too many preemptions, as shown by prior works <ref type="bibr" target="#b43">[42]</ref>, because the current Linux kernel has dropped the FIFO-based Ticket spinlock and has replaced it with a highly optimized unfair queue-based lock <ref type="bibr" target="#b19">[20]</ref> that mitigates the problem of LHP and LWP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance in an Under-committed Case</head><p>We evaluate our eCS approach against PVM and HVM configurations in which a VM is running to show the impact of both APIs and eSchdSpin approach. We also include bare-metal configuration (Host) as a baseline ( <ref type="figure">Figure 5</ref>). We observe that eCS addresses the BWW problem, and outperforms both PVM and HVM in the case of Apache (1.2× and 1.2×), Psearchy (1.6× and 1.9×), Metis (1.2× and 1.3×), and Psearchy (1.2× and 1.4×), while having almost similar latency for the Apache workload <ref type="figure" target="#fig_7">(Fig- ure 4 (a)</ref>). Likewise, eCS performance is similar to that of bare-metal, except for the Psearchy workload.</p><p>For Apache, our APIs act as a back-off mechanism to improve its scalability, as the system is heavily contended. The throughput degrades after 30 cores because of the overhead of process scheduling, socket overhead, and inefficient kernel packet processing. Besides this, both Psearchy and Metis suffer from the BWW problem, which we improve with our eSchdSpin approach that results in better scalability as well as reduction in the idling of VMs. In particular, we decrease the idle time of Psearchy and Metis by 25% and 20%, respectively, by using our approach. One point to note is that blocking locks are based on the TAS lock, whose throughput severely degrades with increasing core count because of the increase cache-line contention, which we observe after 40 cores for Psearchy for all configurations. We also find that the Host is still 1.4× faster than eCS because eSchdSpin only partially mitigates the BWW problem, while introducing excessive cache-line contention, which we can circumvent with NUMA-aware locks <ref type="bibr" target="#b15">[16]</ref>. For Pbzip2, we observe that eCS performs equivalent to the Host, while outperforming PVM and HVM after 60 cores, because Pbzip2 spends the least amount of time in the kernel space (5%), and starts to suffer from the BWW problem only after 60 cores, which our eSchdSpin easily tackles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Addressing BWW Problem via eCS</head><p>We evaluate the impact of the BWW problem on Psearchy in both under-and over-committed scenarios. <ref type="figure">Figure 6</ref> (a) shows that our scheduling-aware spinning approach (marked as eCS + SchdSpin) improves the throughput of Psearchy by 1.5× and 1.2× at 40 and 80 cores, respectively, in an under-committed scenario. SchdSpin approach allows a blocking waiter, both reader and writer, to actively spin for the lock if the number of tasks in the run queue is one, else the task schedules itself out. This approach is similar to the scheduling-aware parking/wake-up strategy <ref type="bibr" target="#b15">[16]</ref>, which we applied to the stock mutex and rwsem. As mentioned before, the reason for such an improvement is that the current design is not scheduling aware, as the waiter parks itself if it is unable to acquire the lock. With our approach, we try to mitigate this performance anomaly and allow the applications to scale further. Unfortunately, the scheduling-aware approach is inefficient in the case of the over-committed scenario, as shown in <ref type="figure">Figure 6</ref> (b). The reason is that current waiters are guest OS agnostic, which leads to wasting CPU resources and resulting in more LHP and LWP problems, thereby degrading the scalability by almost 4.4× (marked eCS + SchdSpin in (b)) against a simple eCS configuration that still suffers from the BWW problem. We overcome this issue by using our is_pcpu_overcommitted() API that allows the SchdSpin approach to spin only when there is no active task on the pCPU's run queue; otherwise, the waiter is scheduled out when more than one task are in the run queue of the pCPU. By using our API (marked eCS + eSchdSpin), we outperform the baseline eCS approach by 1.8× and the eCS + SchdSpin approach by 8×. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">System Eventual Fairness</head><p>We now evaluate whether we are able to achieve eventual fairness while allowing eCS annotated VMs to obtain an extra schedule followed by local vCPU penalization. To evaluate the fairness, we run a simple micro-benchmark in two VMs (marked VM 1 and VM 2 ). VM 1 is a non-annotated VM, whereas VM 2 is an eCS annotated one. This microbenchmark indefinitely reads the content of a file that stresses the read side of the rwsem and spends around 99% of the time in the kernel without scheduling out the task, thereby prohibiting the guest OS from doing any halt exits. <ref type="figure" target="#fig_9">Figure 7</ref> (a) shows the time difference between two VM runtimes that we measure at every 100 ms window for each VM as well as the number of preemptions for VM 2 in that window. <ref type="figure" target="#fig_9">Figure 7 (b)</ref> shows the cumulative runtime of the VMs. We observe from <ref type="figure" target="#fig_9">Figure 7 (</ref>  <ref type="figure">)</ref>. Hence, the extra schedule approach followed by our local vCPU penalization ensures that none of the tasks running on that particular physical CPU suffers from the fairness issue, also referred as eventual fairness. Moreover, <ref type="figure" target="#fig_9">Figure 7</ref> (b) shows that both VMs get almost equivalent runtime in a lockstep fashion with both VMs getting almost 4.95 seconds at the end of 10 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Our eCS approach addresses the problem of preemptions and BWW in both under-and over-committed scenarios by annotating all synchronization primitives and mechanisms in the kernel space. However, besides these primitives, kernel developers have to manually annotate a critical section if they want to avoid the preemptions while introducing their own primitives. One approach could be that the hypervisor can read the instruction pointer (IP) to figure out an eCS, but the guest OS must provide a guest OS symbol table to resolve the IP. In addition, the current design of eCS only targets the kernel space of a guest OS, and it is still agnostic of the user space critical sections such as pthread locks. Hence, we would like to extend our approach to the user space critical sections to further avoid the preemption problem, as we believe that eCS is a natural fit for multi-level scheduling. However, we need to communicate the scheduling hint down to the lowest layer effectively, which requires designing of the eCS composability extensions.</p><p>Our annotation approach does not open any security vulnerability because our approach is based on the paravirtualized VM, and it is similar to other approaches that share the information with the hypervisor <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. By using our virtualized scheduling-aware spinning ap-proach (eSchdSpin), we partially mitigate the BWW problem. However, our Hypervisor → VM APIs expose scheduling information of the pCPU, but they only tell if a pCPU is overloaded or a vCPU is preempted. In addition, a VM cannot misuse this information as it will be later penalized by the hypervisor. There is also very slight possibility of priority inversion problem with our extra schedule approach. However, the window of that hypervisor-granted extra schedule is too small to incur priority inversion and performance, unlike co-scheduling approaches <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b46">45]</ref> in which the scheduling window is in the order of several milliseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related work</head><p>The double scheduling phenomenon is a recurring problem in the domain of virtualization, which seriously impacts the performance of a VM. There have been comprehensive research efforts to mitigate this problem.</p><p>Synchronization primitives in VMs. <ref type="bibr">Uhlig et al. [44]</ref> demonstrated the spinlock synchronization issue in a virtualized environment, which he addressed with synchronous hints to the hypervisor, and was later replaced by para-virtual hooks for the spinlock <ref type="bibr" target="#b7">[8]</ref> for notifying the hypervisor to block the vCPU after it has exhausted its busy wait threshold. Meanwhile, other problems such as <ref type="bibr">LWP [32]</ref>, the BWW problem <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">39]</ref>, and RCU readers preemption problems were found. Gleaner <ref type="bibr" target="#b4">[5]</ref> that addressed the BWW problem implemented a user space solution to handle tasks among a varying number of vCPUs, by manipulating tasks' processor affinity in the user space, which is difficult to maintain at runtime as it must accurately track each task launch and deletion. However, our eSchdSpin approach is user agnostic and mitigates the problem to certain extent for large core count.</p><p>Taebe et al. <ref type="bibr" target="#b43">[42]</ref> addressed the LHP/LWP issue by exposing the time window from the hypervisor to the guest OS, which leverages this information that enables a waiter to either spin or join the waiting queue. However, their solution is not applicable to CFS <ref type="bibr" target="#b28">[27]</ref> scheduler of Linux as it does not expose the scheduling window information. Their solution is orthogonal to our approach as we want the hypervisor to take a decision than the VM. Waiman Long <ref type="bibr" target="#b19">[20]</ref> designed and implemented qspinlock that inherently overcomes the problem of LWP by exploiting the property of the TAS lock in the queue-based lock. It works by allowing the other waiters to steal the lock before joining the queue without disrupting the waiters' queue. However, qspinlock is still prone to LHP. Meanwhile, by annotating various locks as eCS, we confirm these problems, and further identify new sets of problems such as RP and ICP, and provide a simple solution to address the double scheduling phenomenon.</p><p>Partial handling of scheduling overhead in VMs. There have been several studies on virtualization overhead because of the software-hardware redirection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b40">39]</ref> and co-scheduling issues <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b46">45]</ref>. For example, VMware relies on relaxed co-scheduling <ref type="bibr" target="#b46">[45]</ref> to mitigate double scheduling problem, in which vCPUs are scheduled in batches and the stragglers are synchronized within a predefined threshold. Besides this, other works have proposed balanced vCPU scheduling <ref type="bibr" target="#b42">[41]</ref> or even IPI based demand scheduling <ref type="bibr" target="#b16">[17]</ref>. However, these coscheduling approaches suffer from CPU fragmentation. On the contrary, our approach neither introduces any CPU fragmentation nor it needs to synchronize the global scheduling information for all the vCPU of a VM because each vCPU is locally penalized by the hypervisor rather than synchronizing them among other vCPUs.</p><p>Song et al. <ref type="bibr" target="#b41">[40]</ref> proposed the idea of dynamically adjusting vCPUs according to available CPU resources, while allowing guest OS to schedule its tasks. They used the approach of vCPU ballooning, which avoided the problem of double scheduling and was later extended by <ref type="bibr">Cheng et al. [3]</ref> by designing a lightweight hotplug vCPU mechanism. Although their approach is effective in case of small VMs, it is complementary to our approach and may not scale effectively for large SMP VMs because of the overhead of migrating tasks from one vCPU to another as well as the frequent rescheduling of the targeted vCPUs. eCS, on the other hand, does not suffer from any explicit IPI and migration-specific tasks, as it only adds an overhead of a simple memory operations for a scheduling decision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Double scheduling phenomenon is a well-known problem in the domain of virtualization that leads to several symptoms in the form of LHP, LWP, and BWW. We identify that it not only is limited to non-blocking locks, but also is applicable to blocking locks and reader side of locks. We present a single shot solution with our key insight: if a certain key component of a guest OS is allowed to proceed further, the guest OS will make forward progress. We identify these critical components as synchronization primitives and mechanism such as spinlocks, mutex, rwsem, RCU, and even interrupt context, which we call enlightened critical sections (eCS). We annotate eCS with our lightweight APIs that expose whether a VM is executing a critical section, which the hypervisor uses to provide an extra schedule at the scheduling boundary, thereby allowing the guest OS to progress forward. In addition, by leveraging the hypervisor scheduling context, a VM mitigates the effect of BWW problem with our simple virtualized spinning-aware spinning strategy. With eCS, we not only decrease the spurious preemptions by 85-100% but also improve the throughput of applications up to 1.6× and 2.5× in an under-and over-committed scenario, respectively.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Hint Lightweight Para-virtualized API Description VM → Hypervisor void activate_non_preemptable_ecs(cpu_id)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 presents</head><label>2</label><figDesc>two scenarios in which the sched- ule context information is shared between a vCPU and the hypervisor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 (</head><label>2</label><figDesc>a) shows how a vCPU,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>We evaluate our approaches by answering the following questions: • What is the overhead of an eCS annotation and the scheduler overhead to read the values? ( §6.1) • Does eCS helps in an over-committed case? ( §6.2) • How does eCS impact the scalability of a VM? ( §6.3) • How do our APIs address the BWW problem? ( §6.4) • Does our schedule penalization approach maintain the eventual fairness of the system? ( §6.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CDF of the latency of requests for the Apache web server workload in both under-and over-committed scenarios at 80 cores. It clearly shows the impact of eCS in the over-committed scenario, while having minimal impact in the under-committed case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Performance of real-world workloads when running on the bare metal (Host), and inside a VM with three configurations: PVM, HVM, and with eCS annotations. In this scenario, only one VM is running. We use Host as the baseline for the comparison because we consider Host to have almost optimal performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Fairness in eCS. Running time of a vCPU of two co-scheduled VMs (VM 1 and VM 2 ) with eCS annotations for a period of 10 seconds with 100 ms window granularity while executing a kernel intensive task (reading the contents of a file) that involves read side of rwsem. (a) shows the difference in running time of vCPU per window granularity as well as the number of preemptions occurring per window, while (b) illustrates the cumulative running time, and shows that the hypervisor maintains eventual fairness in the system, even if VM 2 is allowed extra schedules. Both VMs get 4.95 seconds to run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>non_preemptable_ecs_count eCS State vCPU 1 vCPU 2 vCPU 3 eCS states eCS states eCS stateseCS states eCS states eCS states</head><label></label><figDesc></figDesc><table>preemptable_ecs_count 

vcpu_preempted 

pcpu_overloaded 

Hypervisor 

❷ 

❸ 

(a) VM Hypervisor 

... 

VM1 

eCS 
states 

states states states 

eCS 
states 
eCS 
states 

vCPU 1 vCPU 2 vCPU 3 

... 

non_preemptable_ecs_count 

eCS State 

vCPU 1 vCPU 2 vCPU 3 

preemptable_ecs_count 

vcpu_preempted 

pcpu_overloaded 

Hypervisor 

... 

VM2 

eCS 
states 

states states states 

eCS 
states 
eCS 
states 

vCPU 1 vCPU 2 vCPU 3 

... 

(b) Hypervisor VM 

. 
. 
. 
. 
. 
. 

❶ 
❸ 

❷ 

write flow 
read flow 

❶ 

Figure 2: Overview of the information flow between a VM 
and a hypervisor. Each vCPU has a per-CPU state that is shared 
with the hypervisor, denoted as eCS state. Figure (a) shows how 
the vCPU 2 relays information about an eCS to the hypervisor. 
On entering a critical section or an interrupt context ( 1), vCPU 2 
updates the non_preemptable_ecs_count ( 2 ). After a while, 
before scheduling out vCPU 2 , the hypervisor reads its eCS state 
( 3 ), and allows it run for one more schedule to mitigate any 
of the double scheduling problems. Figure (b) shows how the 
hypervisor shares the information whether a vCPU is preempted 
or a physical CPU is overloaded, at the schedule boundary. For 
instance, the hypervisor marks vcpu_preempted, while schedul-
ing out a vCPU; or updates pcpu_overloaded flag to one if the 
number of active tasks on that physical CPU is more than one. 
Both try to further mitigate LWP and BWW problems. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Applicability of our six lightweight para-virtualized APIs that strive to address the symptoms of double scheduling.</head><label>2</label><figDesc></figDesc><table>Component 
Lines of code 

eCS annotation 
60 

eCS infrastructure 
800 
Scheduler extension 
150 

Total 
1,010 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>eCS requires small modifications to the existing Linux 
kernel, and the annotation effort is also minimal: 60 LoC 
changes to support the 10 million LoC Linux kernel that has 
around 12,000 of lock instances with 85,000 lock invocations. 

pCPU to a vCPU; and 3) from one vCPU to another in a VM, 
both suffer from LWP and BWW problems. Table 2 shows 
how to use our APIs to address these problems. 

LHP, RP, RRP, and ICP problem. To circumvent these 
problems, we rely on the VM → hypervisor notifica-
tion because a vCPU running any spinlocks, read-write 
locks, mutex, rwsem, or an interrupt context is already 
inside the critical section. Thus, we call activate_*() 
and deactivate_*() APIs for annotating critical sections. 
For example, the first two APIs are applicable to spin-
locks, read-write locks, RCU, and interrupts, and the next 
two are for mutex and rwsem. (refer </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Comparison of Software and Hardware Techniques for x86 Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Agesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII</title>
		<meeting>the 12th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS XII<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Amazon Web Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">vScale: Automatic and Efficient Processor Scaling for SMP Virtual Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 11th European Conference on Computer Systems (EuroSys)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Steal time for KVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Costa</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/449657/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gleaner: Mitigating the Blocked-waiter Wakeup Problem for Virtualized Multicore Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference, USENIX ATC&apos;14</title>
		<meeting>the 2014 USENIX Conference on USENIX Annual Technical Conference, USENIX ATC&apos;14<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Paravirtualized Spinlocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fitzhardinge</surname></persName>
		</author>
		<ptr target="http://lwn.net/Articles/289039/" />
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A S</forename></persName>
		</author>
		<ptr target="https://httpd.apache.org/" />
	</analytic>
	<monogr>
		<title level="j">Foundation. APACHE HTTP Server Project</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">How to Deal with Lock-Holder Preemption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Friebel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>Xen Summit</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Parallel BZIP2 (PBZIP2), Data Compression Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilchrist</surname></persName>
		</author>
		<ptr target="http://compression.ca/pbzip2/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">wrk -a HTTP benchmarking tool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Glozer</surname></persName>
		</author>
		<ptr target="https://github.com/wg/wrk" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Compute Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bare-metal performance for i/o virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Har&amp;apos;el</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), ASPLOS XVII</title>
		<meeting>the 17th ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), ASPLOS XVII<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-03" />
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vbench</surname></persName>
		</author>
		<ptr target="https://github.com/sslab-gatech/vbench" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalability In The Clouds! A Myth Or Reality?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Asia-Pacific Workshop on Systems (APSys)</title>
		<meeting>the 6th Asia-Pacific Workshop on Systems (APSys)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-07" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opportunistic Spinlocks: Achieving Virtual Machine Scalability in the Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scalable NUMA-aware Blocking Synchronization Primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<idno>978-1-931971-38-6</idno>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference (USENIX ATC 17)</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="603" to="615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Demand-based Coordinated Scheduling for SMP VMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS</title>
		<meeting>the Eighteenth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="369" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">sched: add notifier for process migration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kivity</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/356536/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">locking/qspinlock: Enhance pvqspinlock &amp; introduce queued unfair lock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/650776/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">qspinlock: a 4-byte queue spinlock with PV support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2015/4/24/631" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optimizing MapReduce for Multicore Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Kaashoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>MIT CSAIL</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Message Passing Workloads in KVM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matlack</surname></persName>
		</author>
		<ptr target="http://www.linux-kvm.org/images/a/ac/02x03-Davit_Matalack-KVM_Message_passing_Performance.pdf" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Algorithms for Scalable Synchronization on Shared-memory Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<publisher>ACM</publisher>
			<pubPlace>Trans</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Server</surname></persName>
		</author>
		<ptr target="http://www.microsoft.com/en-us/server-cloud/products/sql-server/features.aspx" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Understanding Manycore Scalability of File Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="71" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Linux rwsem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="http://www.makelinux.net/ldd3/chp-5-sect-3" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/230501/" />
		<title level="m">Modular Scheduler Core and Completely Fair Scheduler</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Generic Mutex Subsystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bueso</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/locking/mutex-design.txt" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Generic Mutex Subsystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bueso</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/locking/mutex-design.txt" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">KVM: Add asynchronous page fault for PV guest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Naptov</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/359842/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linux percpu-rwsem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nesterov</surname></persName>
		</author>
		<ptr target="http://lxr.free-electrons.com/source/include/linux/percpu-rwsem.h" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Preemptable Ticket Spinlocks: Improving Consolidated Performance in the Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Lange</surname></persName>
		</author>
		<idno>978-1-4503-1266-0</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE &apos;13</title>
		<meeting>the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The RCU-Reader Preemption Problem in VMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Mckenney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2017 USENIX Annual Technical Conference (ATC)<address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="265" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Enabling Intel Virtualization Technology Features and Benefits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Righini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sap</forename><surname>Sap Hana</surname></persName>
		</author>
		<idno>2.0 SPS 02</idno>
		<ptr target="http://hana.sap.com/abouthana.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Process Scheduling in Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seeker</surname></persName>
		</author>
		<ptr target="https://www.prism-services.io/pdf/linux_scheduler_notes_final.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">PATCH] rwsem: steal writing sem for better performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2013/2/5/309" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Demystifying CPU Ready (%RDY) as a Performance Metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Software</surname></persName>
		</author>
		<ptr target="http://www.actualtechmedia.com/wp-" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Characterizing the Performance and Scalability of Many-core Applications on Virtualized Platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Fudan University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Schedule Processes, Not VCPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Asia-Pacific Workshop on Systems, APSys &apos;13</title>
		<meeting>the 4th Asia-Pacific Workshop on Systems, APSys &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Is Co-scheduling Too Expensive for SMP VMs?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sukwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 6th European Conference on Computer Systems (EuroSys)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-04" />
			<biblScope unit="page" from="257" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The Lock Holder and the Lock Waiter Pre-emption Problems: Nip Them in the Bud Using Informed Spinlocks (I-Spinlock)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Teabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tchana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hagimont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 12th European Conference on Computer Systems (EuroSys)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="286" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Comprehensive Implementation and Evaluation of Direct Interrupt Delivery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Chiueh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE &apos;15</title>
		<meeting>the 11th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards Scalable Multiprocessor Virtual Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dannowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Conference on Virtual Machine Research And Technology Sym</title>
		<meeting>the 3rd Conference on Virtual Machine Research And Technology Sym<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="4" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The CPU Scheduler in VMware ESX 4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vmware</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>VMware</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Best Practices for Oversubscription of CPU, Memory and Storage in vSphere Virtual Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vmware</surname></persName>
		</author>
		<ptr target="https://communities.vmware" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Spark: Cluster Computing with Working Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud&apos;10</title>
		<meeting>the 2Nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud&apos;10<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="10" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
