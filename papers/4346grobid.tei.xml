<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries ARTIFACT EVALUATED PASSED Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fnu</forename><surname>Suya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fnu</forename><surname>Suya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Chi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Virginia</orgName>
								<orgName type="institution" key="instit2">University of Virginia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries ARTIFACT EVALUATED PASSED Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note type="submission">In 29 th USENIX Security Symposium, August 2020 (Accepted: June 2019; This version: December 2, 2019)</note>
					<note>This paper is included in the Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We study adversarial examples in a black-box setting where the adversary only has API access to the target model and each query is expensive. Prior work on black-box adversarial examples follows one of two main strategies: (1) transfer attacks use white-box attacks on local models to find candidate adversarial examples that transfer to the target model, and (2) optimization-based attacks use queries to the target model and apply optimization techniques to search for adversarial examples. We propose hybrid attacks that combine both strategies, using candidate adversarial examples from local models as starting points for optimization-based attacks and using labels learned in optimization-based attacks to tune local models for finding transfer candidates. We empirically demonstrate on the MNIST, CIFAR10, and ImageNet datasets that our hybrid attack strategy reduces cost and improves success rates. We also introduce a seed prioritization strategy which enables attackers to focus their resources on the most promising seeds. Combining hybrid attacks with our seed prioritization strategy enables batch attacks that can reliably find adversarial examples with only a handful of queries.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning (ML) models are often prone to misclassifying inputs, known as adversarial examples (AEs), that are crafted by perturbing a normal input in a constrained, but purposeful way. Effective methods for finding adversarial examples have been found in white-box settings, where an adversary has full access to the target model <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b39">39]</ref>, as well as in black-box settings, where only API access is available <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43]</ref>. In this work, we aim to improve our understanding of the expected cost of black-box attacks in realistic settings. For most scenarios where the target model is only available through an API, the cost of attacks can be quantified by the number of model queries needed to find a desired number of adversarial examples. Black-box attacks often require a large number of model queries, and each query takes time to execute, in addition to incurring a service charge and exposure risk to the attacker.</p><p>Previous black-box attacks can be grouped into two categories: transfer attacks <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b36">36]</ref> and optimization attacks <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b43">43]</ref>. Transfer attacks exploit the observation that adversarial examples often transfer between different models <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b41">41]</ref>. The attacker generates adversarial examples against local models using white-box attacks, and hopes they transfer to the target model. Transfer attacks use one query to the target model for each attempted candidate transfer, but suffer from transfer loss as local adversarial examples may not successfully transfer to the target model. Transfer loss can be very high, especially for targeted attacks where the attacker's goal requires finding examples where the model outputs a particular target class rather than just producing misclassifications.</p><p>Optimization attacks formulate the attack goal as a blackbox optimization problem and carry out the attack using a series of queries to the target model <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b43">43]</ref>. These attacks require many queries, but do not suffer from transfer loss as each seed is attacked interactively using the target model. Optimization-based attacks can have high attack success rates, even for targeted attacks, but often require many queries for each adversarial example found.</p><p>Contributions. Although improving query efficiency and attack success rates for black-box attacks is an active area of research for both transfer-based and optimization-based attacks, prior works treat the two types of attacks independently and fail to explore possible connections between the two approaches. We investigate three straightforward possibilities for combining transfer and optimization-based attacks (Section 3), and find that only one is generally useful (Section 4): failed transfer candidates are useful starting points for optimization attacks. This can be used to substantially improve black-box attacks in terms of both success rates and, most importantly, query cost. Compared to transfer attacks, hybrid attacks can significantly improve the attack success rate by adopting optimization attacks for the non-transfers, which increases per-sample query cost. Compared to optimization attacks, hybrid attacks significantly reduce query complexity when useful local models are available. For example, for both MNIST and CIFAR10, our hybrid attacks reduce the mean query cost of attacking normally-trained models by over 75% compared to state-of-the-art optimization attacks. For ImageNet, the transfer attack only has 3.4% success rate while the hybrid attack approaches 100% success rate.</p><p>To improve our understanding of resource-limited blackbox attacks, we simulate a batch attack scenario where the attacker has access to a large pool of seeds and is motivated to obtain many adversarial examples using limited resources. Alternatively, we can view the batch attacker's goal as obtaining a fixed number of adversarial examples with fewest queries. We demonstrate that the hybrid attack can be combined with a novel seed prioritization strategy to dramatically reduce the number of queries required in batch attacks (Section 5). For example, for ImageNet, when the attacker is interested in obtaining 10 adversarial examples from a pool of 100 candidate seeds, our seed prioritization strategy can be used to save over 70% of the queries compared to random ordering of the seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>In this section, we overview the two main types of black-box attacks which are combined in our hybrid attack strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Transfer Attacks</head><p>Transfer attacks take advantage of the observation that adversarial examples often transfer across models. The attacker runs standard white-box attacks on local models to find adversarial examples that are expected to transfer to the target model. Most works assume the attacker has access to similar training data to the data used for the target model, or has access to pretrained models for similar data distribution. For attackers with access to pretrained local models, no queries are needed to the target model to train the local models. Other works consider training a local model by querying the target model, sometimes referred to as substitute training <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b36">36]</ref>. With naïve substitute training, many queries are needed to train a useful local model. Papernot et al. adopt a reservoir sampling approach to reduce the number of queries needed <ref type="bibr" target="#b36">[36]</ref>. Li et al. use active learning to further reduce the query cost <ref type="bibr" target="#b27">[27]</ref>. However, even with these improvements, many queries are still needed and substitute training has had limited effectiveness for complex target models.</p><p>Although adversarial examples sometimes transfer between models, transfer attacks typically have much lower success rates than optimization attacks, especially for targeted attacks.</p><p>In our experiments on ImageNet, the highest transfer rate of targeted attacks observed from a single local model is 0.2%, while gradient-based attacks achieve nearly 100% success. Liu et al. improve transfer rates by using an ensemble of local models <ref type="bibr" target="#b29">[29]</ref>, but still only achieve low transfer rates (3.4% in our ImageNet experiments, see <ref type="table">Table 3</ref>).</p><p>Another line of work aims to improve transferability by modifying the white-box attacks on the local models. Dong et al. adopt the momentum method to boost the attack process and leads to improved transferability <ref type="bibr" target="#b15">[15]</ref>. Xie et al. improve the diversity of attack inputs by considering image transformations in the attack process to improve transferability of existing white-box attacks <ref type="bibr" target="#b45">[45]</ref>. Dong et al. recently proposed a translation invariant optimization method that further improves transferability <ref type="bibr" target="#b16">[16]</ref>. We did not incorporate these methods in our experiments, but expect they would be compatible with our hybrid attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimization Attacks</head><p>Optimization-based attacks work by defining an objective function and iteratively perturbing the input to optimize that objective function. We first consider optimization attacks where the query response includes full prediction scores, and categorize those ones that involve estimating the gradient of the objective function using queries to the target model, and those that do not depend on estimating gradients. Finally, we also briefly review restricted black-box attacks, where attackers obtain even less information from each model query, in the extreme, learning just the label prediction for the test input.</p><p>Gradient Attacks. Gradient-based black-box attacks numerically estimate the gradient of the target model, and execute standard white-box attacks using those estimated gradients. <ref type="table">Table 1</ref> compares several gradient black-box attacks.</p><p>The first attack of this type was the ZOO (zeroth-order optimization) attack, introduced by Chen et al. <ref type="bibr" target="#b10">[10]</ref>. It adopts the finite-difference method with dimension-wise estimation to approximate gradient values, and uses them to execute a Carlini-Wagner (CW) white-box attack <ref type="bibr" target="#b8">[8]</ref>. The attack runs for hundreds to thousands of iterations and takes 2D queries per CW optimization iteration, where D is the dimensionality. Hence, the query cost is extremely high for larger images (e.g., over 2M queries on average for ImageNet).</p><p>Following this work, several researchers have sought more query-efficient methods for estimating gradients for executing black-box gradient attacks. Bhagoji et al. propose reducing query cost of dimension-wise estimation by randomly grouping features or estimating gradients along with the principal components given by principal component analysis (PCA) <ref type="bibr" target="#b4">[4]</ref>. Tu et al.'s AutoZOOM attack uses two-point estimation based on random vectors and reduces the query complexity per CW iteration from 2D to 2 without losing much accuracy on estimated gradients <ref type="bibr" target="#b43">[43]</ref>. Ilyas et al.'s NES attack <ref type="bibr" target="#b21">[21]</ref> uses a natural evolution strategy (which is in essence still random vector-based gradient estimation) <ref type="bibr" target="#b44">[44]</ref>, to estimate the gradients for use in projected gradient descent (PGD) attacks <ref type="bibr" target="#b32">[32]</ref>.</p><p>Ilyas et al.'s Bandits TD attack incorporates time and data dependent information into the NES attack <ref type="bibr" target="#b22">[22]</ref>. Al-Dujaili et al.'s SignHunter adopts a divide-and-conquer approach to estimate the sign of the gradient and is empirically shown to be superior to the Bandits TD attack in terms of query efficiency</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Estimation</head><p>Queries per Iteration White-box Attack</p><formula xml:id="formula_0">ZOO [10] ˆ g = {ˆg{ˆg i , ˆ g 2 , ..., ˆ g D }, ˆ g i ≈ f (x+δe i )− f (x−δe i ) δ 2D CW [8]</formula><p>Bhagoji et. al <ref type="bibr" target="#b4">[4]</ref> ZOO + random feature group or PCA ≤ 2D FGSM <ref type="bibr" target="#b17">[17]</ref>, PGD <ref type="bibr" target="#b32">[32]</ref> AutoZOOM <ref type="bibr" target="#b43">[43]</ref> </p><formula xml:id="formula_1">u i ∼ U, ˆ g = 1 N N i f (x+δu i )− f (x) δ u i N + 1 CW [8] NES [21] u i ∼ N(0, I), ˆ g = 1 N N i f (x+δu i ) δ u i N PGD Bandits TD [22] NES + time/data dependent info N PGD SignHunter [1] Gradient sign w/ divide-and-conquer method 2 log(D)+1 PGD Cheng et al. [13] u i ∼ U, ˆ g = 1 N N i ( √ λ · v + √ 1 − λ · (I−vv T )u i ||(I−vv T )u i || 2 )</formula><p>N PGD <ref type="table">Table 1</ref>: Gradient attacks. These attacks use some method to estimate gradients and then leverage white-box attacks. D is data dimension, e i denotes standard basis, N is the number of gradient averages. f (x) denotes prediction confidence of image x: for targeted attacks, it denotes the confidence of target class; for untargeted attacks, it denotes the confidence of original class. δ is a small constant. v is the local model gradient. λ is a constant controlling the strength of local and target model gradients. <ref type="table">Table 2</ref>: Gradient-free attacks. These attacks define an objective function and obtain the AE by solving the optimization problem. Q denotes a set of orthonormal candidate vectors, l(x ) denotes the cross-entropy loss of image x with original label (untargeted attack) or target label (targeted attack). π(x |θ) denotes the distribution of x parameterized by θ, V denotes ground set of all pixel locations. Variables with * are locally-optimal solutions obtained by solving the corresponding optimization problems. and attack success rate <ref type="bibr" target="#b1">[1]</ref>. Cheng et al. recently proposed improving the Bandits TD attack by incorporating gradients from surrogate models as priors when estimating the gradients <ref type="bibr" target="#b13">[13]</ref>.</p><formula xml:id="formula_2">Attack Applicable Norm Objective Function Solution Method Sim-BA [18] L 2 , L ∞ min x f (x ) Iterate: sample q from Q, first try q, then −q NAttack [28] L 2 , L ∞ min θ l(x )π(x |θ)dx Compute θ * , then sample from π(x | θ * ) Moon et al. [43] L ∞ max S⊆V f (x + i∈S e i − iS e i ) Compute S * , then x + i∈S * e i − iS * e i</formula><p>For our experiments (Section 4.2), we use AutoZOOM and NES as representative state-of-the-art black-box attacks. 1</p><p>Gradient-free Attacks. Researchers have also explored search-based black-box attacks using heuristic methods that are not based on gradients, which we call gradient-free attacks. One line of work directly applies known heuristic black-box optimization techniques, and is not competitive with the gradient-based black-box attacks in terms of query efficiency. Alzantot et al.</p><p>[2] develop a genetic programming strategy, where the fitness function is defined similarly to CW loss <ref type="bibr" target="#b8">[8]</ref>, using the prediction scores from queries to the black- <ref type="bibr" target="#b1">1</ref> We also tested Bandits TD on ImageNet, but found it less competitive to the earlier attacks and therefore, do not include the results in this paper. We have not evaluated SignHunter and the attack of Cheng et al. <ref type="bibr" target="#b13">[13]</ref>, but plan to include more results in the future versions and have released an open-source framework to enable other attacks to be tested using our methods. box model. A similar genetic programming strategy was used to perform targeted black-box attacks on audio systems <ref type="bibr" target="#b40">[40]</ref>. <ref type="bibr">Narodytska et al. [34]</ref> use a local neighbor search strategy, where each iteration perturbs the most significant pixel. Since the reported query efficiency of these methods is not competitive with results for gradient-based attacks, we did not consider these attacks in our experiments.</p><p>Several recent gradient-free black-box attacks (summarized in <ref type="table">Table 2</ref>) have been proposed that can significantly outperform the gradient-based attacks. Guo et al.'s Sim-BA <ref type="bibr" target="#b18">[18]</ref> iteratively adds or subtracts a random vector sampled from a predefined set of orthonormal candidate vectors to generate adversarial examples efficiently. <ref type="bibr">Li et al.'s NAttack [28]</ref> formulates the adversarial example search process as identifying a probability distribution from which random samples are likely to be adversarial. Moon et al. formulate the L ∞ -norm black-box attack with perturbation as a problem of selecting a set of pixels with + perturbation and applying the − perturbation to the remaining pixels, such that the ob-jective function defined for misclassification becomes a set maximization problem. Efficient submodular optimization algorithms are then used to solve the set maximization problem efficiently <ref type="bibr" target="#b33">[33]</ref>. These attacks became available after we started our experiments, so are not included in our experiments. However, our hybrid attack strategy is likely to work for these new attacks as it boosts the optimization attacks by providing better starting points, which we expect is beneficial for most attack algorithms.</p><p>Restricted Black-box Attacks. All the previous attacks assume the adversary can obtain complete prediction scores from the black-box model. Much less information might be revealed at each model query, however, such as just the top few confidence scores or, at worst, just the output label.</p><p>Ilyas et al. <ref type="bibr" target="#b21">[21]</ref>, in addition to their main results of NES attack with full prediction scores, also consider scenarios where prediction scores of the top-k classes or only the model prediction label are revealed. In the case of partial prediction scores, attackers start from an instance in the target class (or class other than the original class) and gradually move towards the original image with the estimated gradient from NES. For the label-only setting, a surrogate loss function is defined to utilize the strategy of partial prediction scores. Brendel et al. <ref type="bibr" target="#b5">[5]</ref> propose a label-only black-box attack, which starts from an example in the target class and performs a random walk from that target example to the seed example. This random walk procedure often requires many queries. Following this work, several researchers have worked to reduce the high query cost of random walk strategies. Cheng et al. formulate a label-only attack as an optimization problem, reducing the query cost significantly compared to the random walk <ref type="bibr" target="#b12">[12]</ref>. Chen et al. also formulate the label-only attack as an optimization problem and show this significantly improves query efficiency <ref type="bibr" target="#b9">[9]</ref>. Brunner et al. <ref type="bibr" target="#b6">[6]</ref> improve upon the random walk strategy by additionally considering domain knowledge of image frequency, region masks and gradients from surrogate models.</p><p>In our experiments, we assume attackers have access to full prediction scores, but we believe our methods are also likely to help in settings where attackers obtain less information from each query. This is because the hybrid attack boosts gradient attacks by providing better starting points and is independent from the specific attack methods or the types of query feedback from the black-box model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hybrid Attacks</head><p>Our hybrid attacks combine the transfer and optimization methods for searching for adversarial examples. Here, we introduce the threat model of our attack, state the hypotheses underlying the attacks, and presents the general hybrid attack algorithm. We evaluate the hypotheses and attacks in Section 4.</p><p>Threat Model. In the black-box attack setting, the adversary does not have direct access to the target model or knowledge of its parameters, but can use API access to the target model to obtain prediction confidence scores for a limited number of submitted queries. We assume the adversary has access to pretrained local models for the same task as the target model. These could be directly available or produced from access to similar training data and knowledge of the model architecture of the target model. The assumption of having access to pretrained local models is a common assumption for research on transfer-based attacks. A few works on substitute training <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b36">36]</ref> have used weaker assumptions such as only having access to a small amount of training data, but have only been effective so far for very small datasets.</p><p>Hypotheses. Our approach stems from three hypotheses about the nature of adversarial examples:</p><p>Hypothesis 1 (H1): Local adversarial examples are better starting points for optimization attacks than original seeds. Liu et al. observe that for the same classification tasks, different models tend to have similar decision boundaries <ref type="bibr" target="#b29">[29]</ref>. Therefore, we hypothesize that, although candidate adversarial examples generated on local models may not fully transfer to the target model, these candidates are still closer to the targeted region than the original seed and hence, make better starting points for optimization attacks.</p><p>Hypothesis 2 (H2): Labels learned from optimization attacks can be used to tune local models. Papernot et al. observe that generating examples crossing decision boundaries of local models can produce useful examples for training local models closer to the target model <ref type="bibr" target="#b36">[36]</ref>. Therefore, we hypothesize that query results generated through the optimization search queries may contain richer information regarding true target decision boundaries. These new labeled inputs that are the byproduct of an optimization attack can then be used to fine-tune the local models to improve their transferability.</p><p>Hypothesis 3 (H3): Local models can help direct gradient search. Since different models tend to have similar decision boundaries for the same classification tasks, we hypothesize that gradient information obtained from local models may also help better calibrate the estimated gradient of gradient based black-box attacks on target model. We are not able to find any evidence to support the third hypothesis (H3), which is consistent with Liu et al.'s results <ref type="bibr" target="#b29">[29]</ref>. They observed that, for ImageNet models, the gradients of local and target models are almost orthogonal to each other. We also tested this for MNIST and CIFAR10, conducting white-box attacks on local models and storing the intermediate images and the corresponding gradients. We found that the local and target models have almost orthogonal gradients (cosine similarity close to zero) and therefore, a naïve combination of gradients of local and target model is not feasible. One possible explanation is the noisy nature of gradients of deep learning models, which causes the gradient to be highly sensitive to small variations <ref type="bibr" target="#b3">[3]</ref>. Although the cosine similar-input :Set of seed images X with labels, local model ensemble F, target black-box model g output :Set of successful adversarial examples 1 R ← X (remaining seeds to attack) 2 A ← ∅ (successful adversarial examples) 3 Q ← X (fine-tuning set for local models) 4 while R is not empty do 5 select and remove the next seed to attack</p><formula xml:id="formula_3">6 x ← selectSeed(R, F) 7</formula><p>use local models to find a candidate adversarial example</p><formula xml:id="formula_4">8 x ← whiteBoxAttack(F, x) 9 x , S ← blackBoxAttack(x, x , g) 10 if x then 11 A.insert(&lt; x, x &gt;) 12 end 13 Q.insert(S) 14</formula><p>use byproduct labels to retrain local models</p><formula xml:id="formula_5">15 tuneModels(F, Q) 16 end 17 return A Algorithm 1: Hybrid Attack.</formula><p>ity is low, two recent works have attempted to combine the local gradients and the estimated gradient of the black-box model by a linear combination <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b13">13]</ref>. However, Brunner et al. observe that straightforward incorporation of local gradients does not improve targeted attack efficiency much <ref type="bibr" target="#b6">[6]</ref>. Cheng et al. successfully incorporated local gradients into untargeted black-box attacks, however, they do not consider the more challenging targeted attack scenario and it is still unclear if local gradients can help in more challenging cases <ref type="bibr" target="#b6">[6]</ref>. Hence, we do not investigate this further in this paper and leave it as an open question if there are more sophisticated ways to exploit local model gradients.</p><p>Attack Method. Our hybrid attacks combine transfer and optimization attacks in two ways based on the first two hypotheses: we use a local ensemble to select better starting points for an optimization attack, and use the labeled inputs obtained in the optimization attack to tune the local models to improve transferability. Algorithm 1 provides a general description of the attack. The attack begins with a set of seed images X, which are natural images that are correctly classified by the target model, and a set of local models, F. The attacker's goal is to find a set of successful adversarial examples (satisfying some attacker goal, such as being classified in a target class with a limited perturbation below starting from a natural image in the source class). The attack proceeds by selecting the next seed to attack (line 6). Section 4 considers the case where the attacker only selects seeds randomly; Section 5 considers ways more sophisticated resource-constrained attackers may improve efficiency by prioritizing seeds. Next, the attack uses the local models to find a candidate adversarial example for that seed. When the local adversarial example is found, we first check its transferability and if the seed directly transfers, we proceed to attack the next seed. If the seed fails to directly transfer, the black-box optimization attack is then executed starting from that candidate. The original seed is also passed into the black-box attack (line 9) since the adversarial search space is defined in terms of the original seed x, not the starting point found using the local models, x . This is because the space of permissible inputs is defined based on distance from the original seed, which is a natural image. Constraining with respect to the space of original seed is important because we need to make sure the perturbations from our method are still visually indistinguishable from the natural image. If the black-box attack succeeds, it returns a successful adversarial example, x , which is added to the returned set. Regardless of success, the black-box attack produces input-label pairs (S ) during the search process which can be used to tune the local models (line 15), as described in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Evaluation</head><p>In this section, we report on experiments to validate our hypothesis, and evaluate the hybrid attack methods. Section 4.1 describes the experimental setup; Section 4.2 describes the attack configuration; Section 4.3 describes the attack goal; Section 4.4 reports on experiments to test the first hypothesis from Section 3 and measure the effectiveness of hybrid attacks; Section 4.5 improves the attack for targeting robust models, and Section 4.6 evaluates the second hypothesis, showing the impact of tuning the local models using the label byproducts. For all of these, we focus on comparing the cost of the attack measured as the average number of queries needed per adversarial example found across a set of seeds. In Section 5, we revisit the overall attack costs in light of batch attacks that can prioritize which seeds to attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Models</head><p>We evaluate our attacks on three popular image classification datasets and a variety of state-of-the-art models.</p><p>MNIST. MNIST <ref type="bibr" target="#b25">[25]</ref> is a dataset of 70,000 28 × 28 greyscale images of handwritten digits (0-9), split into 60,000 training and 10,000 testing samples. For our normal (not adversarially trained) MNIST models, we use the pretrained MNIST models of Bhagoji et al. <ref type="bibr" target="#b4">[4]</ref>, which typically consist of convolutional layers and fully connected layers. We use their MNIST model A as the target model, and models B-D as local ensemble models. To consider the more challenging scenario of attacking a black-box robust model, we use Madry's robust MNIST model, which demonstrates strong robustness even against the best white-box attacks (maintaining over 88% accuracy for L ∞ attacks with = 0.3) <ref type="bibr" target="#b32">[32]</ref>. CIFAR10. CIFAR10 <ref type="bibr" target="#b23">[23]</ref> consists of 60,000 32 × 32 RGB images, with 50,000 training and 10,000 testing samples for object classification (10 classes in total). We train a standard DenseNet model and obtain a test accuracy of 93.1%, which is close to state-of-the-art performance. To test the effectiveness of our attack on robust models, we use Madry's CIFAR10 Robust Model <ref type="bibr" target="#b32">[32]</ref>. Similarly, we also use the normal CIFAR10 target model and the standard DenseNet (StdDenseNet) model interchangeably. For our normal local models, we adopt three simple LeNet structures <ref type="bibr" target="#b26">[26]</ref>, varying the number of hidden layers and hidden units. <ref type="bibr" target="#b2">2</ref> For simplicity, we name the three normal models NA, NB and NC where NA has the fewest parameters and NC has the most parameters. To deal with the lower effectiveness of attacks on robust CIFAR10 model (Section 4.4), we also adversarially train two deep CIFAR10 models (DenseNet, ResNet) similar to the Madry robust model as robust local models. The adversariallytrained DenseNet and ResNet models are named R-DenseNet and R-ResNet.</p><p>ImageNet. ImageNet <ref type="bibr" target="#b14">[14]</ref> is a dataset closer to real-world images with 1000 categories, commonly used for evaluating state-of-the-art deep learning models. We adopt the following pretrained ImageNet models for our experiments: ResNet-50 <ref type="bibr" target="#b19">[19]</ref>, DenseNet <ref type="bibr" target="#b20">[20]</ref>, VGG-16, and VGG-19 <ref type="bibr" target="#b37">[37]</ref> (all from https://keras.io/applications/). We take DenseNet as the target black-box model and the remaining models as the local ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Attack Configuration</head><p>For the hybrid attack, since we have both the target model and local model, we have two main design choices: (1) which white-box attacks to use for the local models , and (2) which optimization attacks to use for the target model. Local Model Configurations. We choose an ensemble of local models in our hybrid attacks. This design choice is motivated by two facts: First, different models tend to have significantly different direct transfer rates to the same target model (see <ref type="figure">Figure 1)</ref>, when evaluated individually. Therefore, taking an ensemble of several models helps avoid ending up with a single local model with a very low direct transfer rate. Second, consistent with the findings of Liu et al. <ref type="bibr" target="#b29">[29]</ref> on attacking an ensemble of local models, for MNIST and CIFAR10, we find that the ensemble of normal local models yields the highest transfer rates when the target model is a normally trained model (note that this does not hold for robust target model, as shown in <ref type="figure">Figure 1</ref> and discussed further in Section 4.5). We validate the importance of normal local ensemble against normal target model by considering different combinations of local models (i.e.,</p><formula xml:id="formula_6">N k , k = 1, ..., N)</formula><p>and checking their corresponding transfer rates and the average query cost. We adopt the same approach as proposed by <ref type="bibr">Liu et al. [29]</ref> to attack multiple models simultaneously, where the attack loss is defined as the sum of the individual model loss. In terms of transfer rate, we observe that a single CIFAR10 or MNIST normal model can achieve up to 53% and 35% targeted transfer rate respectively, while an ensemble of local models can achieve over 63% and 60% transfer rate. In terms of the average query cost against normal target models, compared to a single model, an ensemble of local models on MNIST and CIFAR10 can save on average 53% and 45% of queries, respectively. Since the ensemble of normal local models provides the highest transfer rate against normal target models, to be consistent, we use that configuration in all our experiments attacking normal models. We perform white-box PGD <ref type="bibr" target="#b32">[32]</ref> attacks (100 iterative steps) on the ensemble loss. We choose the PGD attack as it gives a high transfer rate compared to the fast gradient sign method (FGSM) method <ref type="bibr" target="#b17">[17]</ref>.</p><p>Optimization Attacks. We use two state-of-the-art gradient estimation based attacks in our experiments: NES, a natural evolution strategy based attack <ref type="bibr" target="#b21">[21]</ref> and AutoZOOM, an autoencoder-based zeroth-order optimization attack <ref type="bibr" target="#b43">[43]</ref> (see Section 2.2). These two methods are selected as all of them are shown to improve upon <ref type="bibr" target="#b10">[10]</ref> significantly in terms of query efficiency and attack success rate. We also tested with the Bandits TD attack, an improved version of the NES attack that additionally incorporates time and data dependent information <ref type="bibr" target="#b22">[22]</ref>. However, we find that Bandits TD is not competitive with the other two attacks in our attack scenario and therefore we do not include its results here. 3 Both tested attacks follow an attack method that attempts queries for a given seed until either a successful adversarial example is found or the set maximum query limit is reached, in which case they terminate with a failure. For MNIST and CIFAR10, we set the query limit to be 4000 queries for each seed. AutoZOOM sets the default maximum query limit for each as 2000, however as we consider a harder attacker scenario (selecting least likely class as the target class), we decide to double the maximum query limit. NES does not contain evaluation setups for MNIST and CIFAR10 and therefore, we choose to enforce the same maximum query limit as AutoZOOM. <ref type="bibr" target="#b4">4</ref> For ImageNet, we set the maximum query limit as 10,000 following the default setting used in the NES paper <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Attacker Goal</head><p>For MNIST and CIFAR10, we randomly select 100 images from each of the 10 classes for 1000 total images, against  <ref type="table">Table 3</ref>: Impact of starting from local adversarial examples (Hypothesis 1). Baseline attacks that start from the original seeds are Base; the hybrid attacks that start from local adversarial examples are Ours. The attacks against the normal models are targeted (T), and against the robust models are untargeted (U). The Transfer Rate is the direct transfer rate for local adversarial examples. The Success rate is the fraction of seeds for which an adversarial example is found. The Queries/Seed is the average number of queries per seed, regardless of success. The Queries/AE is the average number of queries per successful adversarial example found, which is our primary metric. The Queries/Search is the average number of queries per successful AE found using the gradient attack, excluding those found by direct transfer. Transfer attacks are independent from the subsequent gradient attacks and hence, transfer rates are separated from the specific gradient attacks. All results are averaged over 5 runs.</p><p>which we perform all black-box attacks. For ImageNet, we randomly sample 100 total images across all 1000 classes.</p><p>Target Class. We evaluate targeted attacks on the normal MNIST, CIFAR10, and ImageNet models. Targeted attacks are more challenging and are generally of more practical interest. For the MNIST and CIFAR10 datasets, all of the selected instances belong to one particular original class and we select as the target class the least likely class of the original class given a prediction model, which should be the most challenging class to target. We define the least likely class of a class as the class which is most frequently the class with the lowest predicted probability across all instances of the class. For ImageNet, we choose the least likely class of each image as the target class. For the robust models for MNIST and CIFAR10, we evaluate untargeted attacks as these models are designed to resist untargeted attacks <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. Untargeted attacks against these models are significantly more difficult than targeted attacks against the normal models.</p><p>Attack Distance Metric and Magnitude. We measure the perturbation distance using L ∞ , which is the most widely used attacker strength metric in black-box adversarial examples research. Since the AutoZOOM attack is designed for L 2 attacks, we transform it into an L ∞ attack by clipping the attacked image into the -ball (L ∞ space) of the original seed in each optimization iteration. Note that the original Auto-ZOOM loss function is defined as f (x) + c · δ(x), where f (x) is for misclassification (targeted or untargeted) and δ(x) is for perturbation magnitude minimization. In our transformation to L ∞ -norm, we only optimize f (x) and clip the to L ∞ -ball of the original seed. NES is naturally an L ∞ attack. For MNIST, we choose = 0.3 following the setting in Bhagoji et al. <ref type="bibr" target="#b4">[4]</ref>. For CIFAR10, we set = 0.05, following the same setting in early version of NES paper <ref type="bibr" target="#b21">[21]</ref>. For ImageNet, we set = 0.05, as used by Ilyas et al. <ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Local Candidates Results</head><p>We test the hypothesis that local models produce useful candidates for black-box attacks by measuring the mean cost to find an adversarial example starting from both the original seed and from a candidate found using the local ensemble. All experiments are averaged over 5 runs to obtain more stable results. <ref type="table">Table 3</ref> summarizes our results. In nearly all cases, the cost is reduced by starting from the candidates instead of the original seeds, where candidates are generated by attacking local ensemble models. We measure the cost by the mean number of queries to the target model per adversarial example found. This is computed by dividing the total number of model queries used over the full attack on 1,000 (MNIST, CIFAR10) or 100 (ImageNet) seeds by the number of successful adversarial examples found. The overall cost is reduced by as much as 81% (AutoZOOM attack on the normal MNIST model), and for both the AutoZOOM and for NES attack methods we see the cost drops by at least one third for all of the attacks on normal models (the improvements for robust models are not significant, which we return to in Section 4.5). The cost drops for two reasons: some candidates transfer directly (which makes the query cost for that seed 1); others do not transfer directly but are useful starting points for the gradient attacks. To further distinguish the two factors, we include the mean query cost for adversarial examples  found from non-transfering seeds as the last two columns in <ref type="table">Table 3</ref>. This reduction is significant for all the attacks across the normal models, up to 76% (AutoZOOM attack on normal MNIST models).</p><p>The hybrid attack also offers success rates higher than the gradient attacks (and much higher success rates that transferonly attacks), but with query cost reduced because of the directly transferable examples and boosting effect on gradient attacks from non-transferable examples. For the AutoZOOM and NES attacks on normally-trained MNIST models, the attack failure rates drop dramatically (from 8.7% to 1.1% for AutoZOOM, and from 22.5% to 10.8% for NES), as does the mean query cost (from 1,610 to 282 for AutoZOOM, and from 3,284 to 1,000 for NES). Even excluding the direct transfers, the saving in queries is significant (from 3,248 to 770 for AutoZOOM, and from 8,254 to 3,376 for NES). The candidate starting points are nearly always better than the original seed. For the two attacks on MNIST, there were only at most 28 seeds out of 1,000 where the original seed was a better starting point than the candidate; the worst result is for the AutoZOOM attack against the robust CIFAR10 model where 269 out of 1,000 of the local candidates are worse starting points than the corresponding original seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Attacking Robust Models</head><p>The results in <ref type="table">Table 3</ref> show substantial improvements from hybrid attacks on normal models, but fail to provide improvements against the robust models. The improvements against robust models are less than 4% for both attacks on both targets, except for NES against MNIST where there is ∼17% improvement. We speculate that this is due to differences in the vulnerability space between normal and robust models, which means that the candidate adversarial examples found against the normal models in the local ensemble do not provide useful starting points for attacks against a robust model. This is consistent with Tsipras et al.'s finding that robust models for image classification tasks capture key features of images while normal models capture relatively noisy features <ref type="bibr" target="#b42">[42]</ref>. Because of the differences in extracted features, adversarial examples against robust models require perturbing key features (of the target domain) while adversarial examples can be found against normal models by perturbing irrelevant features. This would explain why we did not see improvements from the hybrid attack when targeting robust models. To validate our hypothesis on the different attack surfaces, we repeat the experiments on attacking the CIFAR10 robust model but replace the normal local models with robust local models, which are adversarially trained DenseNet and ResNet models mentioned in Section 4.1. 5 <ref type="table" target="#tab_2">Table 4</ref> compares the direct transfer rates for adversarial example candidates found using ensembles of normal and robust models against both types of target models. We see that using robust models in the local ensemble increases the direct transfer rate against the robust model from 10.1% to 40.7% (while reducing the transfer rate against the normal target model). We also find that the candidate adversarial examples found using robust local models also provide better starting points for gradient black-box attacks. For example, with the AutoZOOM attack, the mean cost reduction with respect to the baseline mean query (2,632) is significantly improved (from 3.8% to 20.5%). We also observe a significant increase of fraction better (percentages of seeds that starting from the local adversarial example is better than starting from the original seed) from 73.1% to 95.5%, and a slight increase in the overall success rate of the hybrid attack (from 65.3% to 68.7%). When an ensemble of robust local models is used to attack normal target models, however, the attack efficiency degrades significantly, supporting our hypothesis that robust and normal models have different attack surfaces.</p><p>Universal Local Ensemble. The results above validate our hypothesis that the different attack surfaces of robust and normal models cause the ineffectiveness against the robust CIFAR10 model in <ref type="table">Table 3</ref>. Therefore, to achieve better performance, depending on the target model type, the attacker should selectively choose the local models. However, in practice, attackers may not know if the target model is robustly trained, so cannot predetermine the best local models. We <ref type="figure">Figure 1</ref>: Transfer rates of different local ensembles. The Normal-3 ensemble is composed of the three normal models, NA, NB, and NC; the Robust-2 ensemble is composed of R-DenseNet and R-ResNet. The All-5 is composed of all of the 5 local models. Transfer rate is measured on independently sampled test images and is averaged over 5 runs.</p><p>next explore if a universal local model ensemble exists that works well for both normal and robust target models.</p><p>To look for the best local ensemble, we tried all 31 different combinations of the 5 local models (3 normal and 2 robust) and measured their corresponding direct transfer rates against both normal and robust target models. <ref type="figure">Figure 1</ref> reports the transfer rates for each local ensemble against both normal and robust target models. For clarity in presentation, we only include results for the five individual models and four representative ensembles in the figure: an ensemble of NA-NB (selected to represent the mediocre case); Robust-2, an ensemble of the two robust models (R-DenseNet and R-ResNet); Normal-3, an ensemble of three normal models (NA, NB, and NC); and All-5, an ensemble of all five local models. These include the ensembles that have the highest or lowest transfer rates to the target models and transfer rates of all other ensembles fit between the reported highest and lowest values.</p><p>None of the ensembles we tested had high direct transfer rates against both normal and robust target models. Ensembles with good performance against robust targets have poor performance against normal targets (e.g., Robust-2 has 37.8% transfer rate to robust target, but 18.1% to normal target), and ensembles that have good performance against normal targets are bad against robust targets (e.g., Normal-3 has 65.6% transfer rate to the normal target, but only 9.4% to the robust target). Some ensembles are mediocre against both (e.g., NA-NB).</p><p>One possible reason for the failure of ensembles to apply to both types of target, is that when white-box attacks are applied on the mixed ensembles, the attacks still "focus" on the normal models as normal models are easier to attack (i.e., to significantly decrease the loss function). Biasing towards normal models makes the candidate adversarial example less likely to transfer to a robust target model. This conjecture is supported by the observation that although the mixtures of normal and robust models mostly fail against robust target models, they still have reasonable transfer rates to normal target models (e.g., ensemble of 5 local models has 63.5% transfer rate to normal CIFAR10 target model while only 9.5% transfer rate to the robust target model). It might be interesting to explore if one can explicitly enforce the attack to focus more on the robust model when attacking the mixture of normal and robust models.</p><p>In practice, attackers can dynamically adapt their local ensemble based on observed results, trying different local ensembles against a particular target for the first set of attempts and measuring their transfer rate, and then selecting the one that worked best for future attacks. This simulation process adds overhead and complexity to the attack, but may still be worthwhile when the transfer success rates vary so much for different local ensembles.</p><p>For our subsequent experiments on CIFAR10 models, we use the <ref type="bibr">Normal-3</ref> and Robust-2 ensembles as these give the highest transfer rates to normal and robust target models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Local Model Tuning</head><p>To test the hypothesis that the labels learned from optimization attacks can be used to tune local models, we measure the impact of tuning on the local models' transfer rate.</p><p>During black-box gradient attacks, there are two different types of input-label pairs generated. One type is produced by adding small magnitudes of random noise to the current image to estimate target model gradients. The other type is generated by perturbing the current image in the direction of estimated gradients. We only use the latter input-label pairs as they contain richer information about the target model boundary since the perturbed image moves towards the decision boundary. These by-products of the black-box attack search can be used to retrain the local models (line 15 in Algorithm 1  added to the original training set to form the new training set, and the local models are fine-tuned on the new training set. As more images are attacked, the training set size can quickly explode. To avoid this, when the size of new training set exceeds a certain threshold c, we randomly sample c of the training data and conduct fine-tuning using the sampled training set. For MNIST and CIFAR10, we set the threshold c as the standard training data size (60,000 for MNIST and 50,000 for CIFAR10). At the beginning of hybrid attack, the training set consists of the original seeds available to the attacker with their ground-truth labels (i.e., 1,000 seeds for MNIST and CIFAR10 shown in Section 4.2). Algorithm 1 shows the local model being updated after every seed, but considering the computational cost required for tuning, we only update the model periodically. For MNIST, we update the model after every 50 seeds; for CIFAR10, we update after 100 seeds (we were not able to conduct the tuning experiments for the ImageNet models because of the high cost of each attack and of retraining). To check the transferability of the tuned local models, we independently sample 100 unseen images from each of the 10 classes, use the local model ensemble to find candidate adversarial examples, and test the candidate adversarial examples on the black-box target model to measure the transfer rate.</p><p>We first test whether the local model can be fine-tuned by the label by-products of baseline gradient attacks (Baseline attack + H2) by checking the transfer rate of local models before and after the fine-tuning process. We then test whether attack efficiency of hybrid attack can be boosted by fine-tuning local models during the attack process (Baseline attack + H1 + H2) by reporting their average query cost and attack success rate. The first experiment helps us to check applicability of H2 without worrying about possible interactions between H2 with other hypotheses. The second experiment evaluates how much attackers can benefit from fine-tuning the local models in combination with hybrid attacks.</p><p>We report the results of the first experiment in <ref type="table" target="#tab_4">Table 5</ref>. For the MNIST model, we observe increases in the transfer rate of local models by fine-tuning using the byproducts of both attack methods-the transfer rate increases from 60.6% to 77.9% for NES, and from 60.6% to 64.4% for AutoZOOM. Even against the robust MNIST models, the transfer rate improves from the initial value of 3.4% to 4.3% (AutoZOOM) and 4.5% (NES). However, for CIFAR10 dataset, we observe a significant decrease in transfer rate. For the normal CIFAR10 target model, the original transfer rate is as high as 65.6%, but with fine-tuning, the transfer rate decrease significantly (decreased to 8.6% and 33.4% for AutoZOOM and NES respectively). A similar trend is also observed for the robust CIFAR10 target model. These results suggest that the examples used in the attacks are less useful as training examples for the CIFAR10 model than the original training set.</p><p>Our second experiment, reported in <ref type="table" target="#tab_6">Table 6</ref>, combines the model tuning with the hybrid attack. Through our experiments, we observe that for MNIST models, the transfer rate also increases significantly by fine-tuning the local models. For the MNIST normal models, the (targeted) transfer rate increases from the original 60.6% to 74.7% and 76.9% for AutoZOOM and NES, respectively. The improved transfer rate is also higher than the results reported in first experiment. For the AutoZOOM attack, in the first experiment, the transfer rate can only be improved from 60.6% to 64.4% while in the second experiment, it is improved from 60.6% to 76.9%. Therefore, there might be some boosting effects by taking local AEs as starting points for gradient attacks. For the Madry robust model on MNIST, the low (untargeted) transfer rate improves by a relatively large amount, from the original 3.4% to 5.1% for AutoZOOM and 4.8% for NES (still a low transfer rate, but a 41% relative improvement over the original local model). The local models become more robust during the fine-tuning process. For example, with the NES attack, the local model attack success rate (attack success is defined as compromising all the local models) decreases significantly from the original 96.6% to 25.2%, which indicates the tuned local models are more resistant to the PGD attack. The improvements in transferability, obtained as a free by-product of the gradient attack, also lead to substantial cost reductions for the attack on MNIST, as seen in <ref type="table" target="#tab_6">Table 6</ref>. For example, for the AutoZOOM attack on the MNIST normal model, the mean query cost is reduced by 31%, from 282 to 194 and the attack success rate is also increased slightly, from 98.9% for static local models to 99.5% for tuned local models. We observe similar patterns for robust MNIST model and demonstrate that Hypothesis 2 also holds on the MNIST dataset.</p><p>However, for CIFAR10, we still find no benefits from the tuning. Indeed, the transfer rate decreases, reducing both the attack success rate and increasing its mean query cost (Table 6). We do not have a clear understanding of the reasons the CIFAR10 tuning fails, but speculate it is related to the difficulty of training CIFAR10 models.  from gradient-based attacks are highly similar to a particular seed and may not be diverse enough to train effective local models. This is consistent with Carlini et al.'s findings that MNIST models tend to learn well from outliers (e.g., unnatural images) whereas more realistic datasets like CIFAR10 tend to learn well from more prototypical (e.g., natural) examples <ref type="bibr" target="#b7">[7]</ref>. Therefore, fine-tuning CIFAR10 models using label by-products, which are more likely to be outliers, may diminish learning effectiveness. Potential solutions to this problem include tuning the local model with mixture of normal seeds and attack by-products. One may also consider keeping some fraction of model ensembles fixed during the fine-tuning process such that when by-products mislead the tuning process, these fixed models can mitigate the problem. We leave further exploration of this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Batch Attacks</head><p>Section 4 evaluates attacks assuming an attacker wants to attack every seed from some fixed set of initial seeds. In more realistic attack scenarios, each query to the model has some cost or risk to the attacker, and the attacker's goal is to find as many adversarial examples as possible using a limited total number of queries. <ref type="bibr">Carlini et al.</ref> show that, defenders can identify purposeful queries for adversarial examples based on past queries and therefore, detection risk will increase significantly when many queries are made <ref type="bibr" target="#b11">[11]</ref>. We call these attack scenarios batch attacks. To be efficient in these resource-limited settings, attackers should prioritize "easy-to-attack" seeds. A seed prioritization strategy can easily be incorporated into the hybrid attack algorithm by defining the selectSeed function used in step 6 in Algorithm 1 to return the most promising seed:</p><formula xml:id="formula_7">arg min x∈X EstimatedAttackCost(x, F).</formula><p>To clearly present the hybrid attack strategy in the batch setting, we present a two-phase strategy: in the first phase, local model information is utilized to find likely-to-transfer seeds; in the second phase, target model information is used to select candidates for optimization attacks. This split reduces the generality of the attack, but simplifies our presentation and analysis. Since direct transfers have such low cost (that is, one query when they succeed) compared to the optimization attacks, constraining the attack to try all the transfer candidates first does not compromise efficiency. More advanced attacks might attempt multiple transfer attempts per seed, in which case the decision may be less clear when to switch to an optimization attack. We do not consider such attacks here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">First Phase: Transfer Attack</head><p>Since the first phase seeks to find direct transfers, it needs to execute without any information from the target model. The goal is to order the seeds by likelihood of finding a direct transfer before any query is done to the model. As before, we do assume the attacker has access to pretrained local models, so can use those models both to find candidates for transfer attacks and to prioritize the seeds.</p><p>Within the transfer attack phase, we use a prioritization strategy based on the number of PGD-Steps of the local models to predict the transfer likelihood of each image. We explored using other metrics based on local model information such as local model attack loss and local prediction score gap (difference in the prediction confidence of highest and second highest class), but did not find significant differences in the prioritization performance compared to PGD-Step. Hence, we only present results using PGD-Steps here.</p><p>Prioritizing based on PGD Steps. We surmised that the easier it is to find an adversarial example against the local models for a seed, the more likely that seed has a large vulnerability region in the target model. One way to measure this difficult is the number of PGD steps used to find a successful local adversarial example and we prioritize seeds that require less number of PGD steps. To be more specific, we first group images by their number of successfully attacked local models (e.g., k out of K local models), and then prioritize images in each group based on their number of PGD steps used to find the adversarial examples that compromises the k local models. We prioritize adversarial examples that succeed against more of the local models (i.e., larger value of k) with the assumption that adversarial examples succeed on more local models   <ref type="table">Table 7</ref>: Impact of prioritization for first phase (robust CIFAR10 Model, average over 5 runs).</p><p>tend to have higher chance to transfer to the "unknown" target model. Above prioritization strategy is the combination of the metrics of number of successfully compromised local models and PGD steps. We also independently tested the impact of each of the two metrics, and found that the PGD-step based metrics perform better than the number of successfully attacked models, and our current metric of combining the number of PGD steps and the number of successfully attacked models is more stable compared to just using the PGD steps.</p><p>Results. Our prioritization strategy in the first phase sorts images and each seed is queried once to obtain direct transfers.</p><p>We compare with the baseline of random selection of seeds where the attacker queries each seed once in random order to show the advantage of prioritizing seeds based on PGD-Steps. <ref type="figure" target="#fig_1">Figure 2</ref> shows the results of untargeted attack on the Madry robust CIFAR10 model for both normal and robust local model ensembles. Note that first phase attack only checks transferability of the candidate adversarial examples and is independent from the black-box optimization attacks. All results are averaged over five runs. In all cases, we observe that, checking transferability with prioritized order in the first phase is significantly better than checking the transferability in random order. More quantitative information is given in <ref type="table">Table 7</ref>. For the untargeted attack on robust CIFAR10 model with the three normal local models (NA, NB, NC), when attacker is interested in obtaining 1% of the total 1,000 seeds, checking transferability with prioritized order reduces the cost substantially-with prioritization, it takes 20.4 queries on average, compared to 100.8 with random order. We observed similar patterns for other datasets and models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Second Phase: Optimization Attacks</head><p>The transfer attack used in the first phase is query efficient, but has low success rate. Hence, when it does not find enough adversarial examples, the attack continues by attempting the optimization attacks on the remaining images. In this section, we show that the cost of the optimization attacks on these images varies substantially, and then evaluate prioritization strategies to identify low-cost seeds.</p><p>Query Cost Variance of Non-transfers. <ref type="figure">Figure 3</ref> shows the query distributions of non-transferable images for MNIST, CIFAR10 and ImageNet using the NES attack starting from local adversarial examples (similar patterns are observed for the AutoZOOM attack). For ImageNet, when images are sorted by query cost, the top 10% of 97 images (excluding 3 direct transfers and 0 failed adversarial examples from the original 100 images) only takes on average 1,522 queries while the mean query cost of all 100 images is 14,828. So, an attacker interested in obtaining only 10% of the total 100 seeds us- Figure 3: Query cost of NES attack on MNIST, CIFAR10 and ImageNet models. We exclude direct transfers (successfully attacked during first phase) and seeds for which no adversarial example was found within the query limit (4000 for MNIST and CIFAR; 10,000 for ImageNet). All the target models are normal models with NES targeted attacks.</p><p>ing this prioritization reduces their cost by 90% compared to targeting seeds randomly. The impact is even higher for CIFAR10 -the mean query cost for obtaining adversarial examples for 10% of the seeds remaining after the transfer phase is reduced by nearly 95% (from 933 to 51) over the random ordering.</p><p>Prioritization Strategies. These results show the potential cost savings from prioritizing seeds in batch attacks, but to be able to exploit the variance we need a way to identify lowcost seeds in advance. We consider two different strategies for estimating the attack cost to implement the estimator for the EstimatedAttackCost function. The first uses same local information as adopted in the first phase: low-cost seeds tend to have lower PGD steps in the local attacks. The drawback of prioritizing all seeds only based on local model information is that local models may not produce useful estimates of the cost of attacking the target model. Hence, our second prioritization strategy uses information obtained from the single query to the target model that is made for each seed in the first phase. This query results in obtaining a target model prediction score for each seed, which we use to prioritize the remaining seeds in the second phase. Specifically, we find that low-cost seeds tend to have lower loss function values, defined with respect to the target model. The assumption that an input with a lower loss function value is closer to the attacker's goal is the same assumption that forms the basis of the optimization attacks.</p><p>Taking a targeted attack as an example, we compute the loss similarly to the loss function used in AutoZOOM <ref type="bibr" target="#b43">[43]</ref>. For a given input x and target class t, the loss is calculated as</p><formula xml:id="formula_8">l(x, t) = (max it log f (x) i − log f (x) t ) +</formula><p>where f (x) denotes the prediction score distribution of a seed. <ref type="bibr">So, f (x)</ref> i is the model's prediction of the probability that x is in class i. Similarly, for an untargeted attack with original label y, the loss is defined as l(x, y) = max(log f (x) y − max iy log f (x) i ) + . Here, the input x is the candidate starting point for an optimization attack. Thus, for hybrid attacks that start from a local candidate adversarial example, z , of the original seed z, attack loss is computed with respect to z instead of z. For the baseline attack that starts from the original seed z, the loss is computed with respect to z.</p><p>Results. We evaluate the prioritization for the second phase using the same experimental setup as in Section 5.1. We compare the two prioritization strategies (based on local PGD steps and the target model loss) to random ordering of seeds to evaluate their effectiveness in identifying low-cost seeds. The baseline attacks (AutoZOOM and NES, starting from the original seeds) do not have a first phase transfer stage, so we defer the comparison results to next subsection, which shows performance of the combined two-phase attack. <ref type="figure" target="#fig_3">Figure 4</ref> shows the results for untargeted AutoZOOM attacks on the robust CIFAR10 model for local ensembles of both normal and robust models (results for the NES attack are not shown, but exhibit similar patterns). Using the target loss information estimates the attack cost better than the local PGD step ordering, while both prioritization strategies achieve much better performance than the random ordering. <ref type="table" target="#tab_8">Table 8</ref> summarizes the results. For example, for the untargeted AutoZOOM attack on robust CIFAR10 model with the Normal-3 local ensemble, an attacker who wants to obtain ten new adversarial examples (in addition to the 101 direct transfers found in the first phase) can find them using on average 1,248 queries using target model loss in the second phase, compared to 3,465 queries when using only local ensemble information, and 26,336 without any prioritization. . The x-axis denotes the query budget and the y-axis denotes the number of successful adversarial examples found with the given query budget. The maximum query budget is the sum of the query cost for attacking all seed images (i.e., the total number of queries used to attack all 1000 seeds for CIFAR10 models) -1,656,818 for the attack with normal local models, and 1,444,980 for the attack with robust local models. The second phase starts at 1000 queries and the number of direct transfers found because it begins after checking the direct transfers in the first phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Ensemble</head><p>Metric Additional 1% Additional 2% Additional 5% Additional 10%</p><p>Normal-3   <ref type="bibr" target="#b10">(10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr">50</ref>, and 100 out of 1000 total seeds), using the remaining seeds after the first phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Overall Attack Comparison</head><p>To further validate effectiveness of the seed prioritized twophase strategy, we evaluate the full attack combining both phases. Based on our analysis in the previous subsections, we use the best prioritization strategies for each phase: PGD-Step in the first phase and target loss value in the second phase. For the baseline attack, we simply adopt the target loss value to prioritize seeds. We evaluate the effectiveness in comparison with two degenerate strategies:</p><p>• retroactive optimal -this strategy is not realizable, but provides an upper bound for the seed prioritization. It assumes the attackers have prior knowledge of the true rank of each seed. That is, we assume a selectSeed function that always returns the best remaining seed.</p><p>• random -the attacker selects candidate seeds in a random order and conducts optimization attacks exhaustively (until either success or the query limit is reached) on each seed before trying the next one. This represents traditional black-box attacks that just attack every seed.</p><p>Here, we only present results of AutoZOOM attack on robust CIFAR10 model with normal local models and Auto-ZOOM attack on ImageNet models. The attack on robust CIFAR10 is the setting where the performance gain for the hybrid attack is least significant compared to other models (see <ref type="table">Table 3</ref>), so this represents the most challenging scenario for our attack. In the ImageNet setting, the performance of the target loss based prioritization is not a significant improvement over random scheduling, so this represents the worst case for target loss prioritization for the baseline attack. Re-   <ref type="table">Table 9</ref>: Comparison of the target loss value based search to retroactive optimal and random search (AutoZOOM baseline untargeted attack on robust CIFAR10 model, targeted attack on standard ImageNet model, averaged over 5 runs). The "Top x%" columns give the total number of queries needed to find adversarial examples for x% of the total seeds. sults of the two black-box attacks on all the other datasets and different combinations of target models and local models (only for the CIFAR10 dataset) show similar patterns.</p><p>The results of seed prioritization on baseline attacks are shown in <ref type="figure" target="#fig_4">Figure 5</ref> and <ref type="table">Table 9</ref>. For attacks on the robust CIFAR10 model, performance of the target loss strategy is much better than the random scheduling strategy. For example, in order to obtain 1% of the total 1,000 seeds, the target loss prioritization strategy costs 1,070 queries on average, while the random strategy consumes on average 25,005 queries, which is a 96% query savings. The retroactive optimal strategy is very effective in this case and significantly outperforms other strategies by only taking 34 queries. Against the ImageNet model, however, the target loss based strategy offers little improvement over random scheduling <ref type="figure" target="#fig_4">(Figure 5b</ref>). In contrast, performance of the two-phase strategy is still significantly better than random ordering.</p><p>We speculate that the difference in the performance of target loss strategy (for baseline attack) and two-phase strategy (for hybrid attack) on ImageNet is because the baseline attack starts from the original seeds, which are natural images and ImageNet models tend to overfit to these natural images. Therefore, the target loss value computed with respect to these images is less helpful in predicting their actual attack cost, which leads to poor prioritization performance. In contrast, the hybrid attack starts from local adversarial examples, which deviate from the natural distribution so ImageNet models are less likely to overfit to these images. Thus, the target loss is better correlated with the true attack cost and the prioritization performance is also improved. <ref type="figure" target="#fig_5">Figure 6</ref> shows the results for the full two-phase strategy. The seed prioritized two-phase strategy approaches the performance of the (unrealizable) retroactive optimal strategy and substantially outperforms random scheduling. <ref type="table">Table 10</ref> shows the number of queries needed using each prioritization method to successfully attack 1%, 2%, 5% and 10% of the total candidate seeds (1000 images for CIFAR10 and 100 images for ImageNet). For the robust CIFAR10 target model, obtaining 10 new adversarial examples (1%), costs 20.4 queries on average using our two-phase strategy (not far off the 10 required by the unrealizable retroactive optimal, which takes only a single query for each since it can always find the direct transfer), while random ordering takes 24,054 queries. For ImageNet, the cost of obtaining the first new adversarial example (1%) using our two-phase strategy is 28 queries compared to over 15,000 with random prioritization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Our results improve our understanding of black-box attacks against machine learning classifiers and show how efficiently an attacker may be able to successfully attack even robust target models. We propose a hybrid attack strategy, which combines recent transfer-based and optimization-based attacks. Across multiple datasets, our hybrid attack strategy dramatically improves state-of-the-art results in terms of the average query cost, and hence provides more accurate estimation of cost of black-box adversaries. We further consider a more practical attack setting, where the attacker has limited resources and aims to find many adversarial examples with a fixed number of queries. We show that a simple seed prioritization strategy can dramatically improve the overall efficiency of hybrid attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability</head><p>Implementations and data for reproducing our results are available at https://github.com/suyeecav/Hybrid-Attack.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>a) Local Normal-3 Ensemble: NA, NB, NC (b) Local Robust-2 Ensemble: R-DenseNet, R-ResNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: First phase (transfer only) attack prioritization (untargeted attack on robust CIFAR10 model, average over 5 runs). Solid line denotes the mean value and shaded area denotes the 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact of seed prioritization strategies in the second phase (AutoZOOM untargeted attack on robust CIFAR10 model, average over 5 runs). The x-axis denotes the query budget and the y-axis denotes the number of successful adversarial examples found with the given query budget. The maximum query budget is the sum of the query cost for attacking all seed images (i.e., the total number of queries used to attack all 1000 seeds for CIFAR10 models) -1,656,818 for the attack with normal local models, and 1,444,980 for the attack with robust local models. The second phase starts at 1000 queries and the number of direct transfers found because it begins after checking the direct transfers in the first phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of the target loss value based seed prioritization strategy to retroactive optimal and random search strategies (AutoZOOM baseline untargeted attack on robust CIFAR10 model and targeted attack on standard ImageNet model, averaged over 5 runs). Solid line denotes mean value and shaded area denotes the 95% confidence interval. Maximum query budget is 1,699,998 for robust CIFAR10 model, 4,393,314 for ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison of the two-phase seed prioritization strategy to retroactive optimal and random search strategies (AutoZOOM-based hybrid attack on robust CIFAR10 model and standard ImageNet model, average over 5 runs). Solid line denotes mean value and shaded area denotes the 95% confidence interval. Maximum query budget of attack against robust CIFAR10 model is 1,656,818 and attack against ImageNet models is 3,029,844.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Attack performance of all normal and all robust local ensembles on CIFAR10 target models. The Normal-3 ensemble 
is composed of the three normal models, NA, NB, and NC; the Robust-2 ensemble is composed of R-DenseNet and R-ResNet. 
Results are averaged over 5 runs. Local model transfer rates are independent from the black-box attacks, so we separate transfer 
rate results from the black-box attack results. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>) . The newly generated image and label pairs are</head><label>.</label><figDesc></figDesc><table>Model 

Gradient Attack 
Transfer Rate (%) 
Static 
Tuned 

MNIST (N, t) 
AutoZOOM 
60.6 
64.4 
NES 
60.6 
77.9 

MNIST (R, u) 
AutoZOOM 
3.4 
4.3 
NES 
3.4 
4.5 

CIFAR10 (N, t) 
AutoZOOM 
65.6 
8.6 
NES 
65.6 
33.4 

CIFAR10 (R, u) 
AutoZOOM 
9.4 
8.8 
NES 
9.4 
9.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Impact of tuning local models on transfer rates (Base-
line + Hypothesis 2): gradient attacks start from original 
seed. Transfer rate is measured on independently sampled 
test images and is averaged over 5 runs. The results for (N, t) 
are targeted attacks on normal models; (R, u) are untargeted 
attacks on robust models. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Impact of tuning local models (averaged 5 times). Transfer rate is measured on independently sampled test images. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>Impact of different prioritization strategies for optimization attacks (AutoZOOM untargeted attack on robust CIFAR10 
model, average over 5 runs). For different models, their number of direct transfers varies-for Normal-3 there are in average 
101.2, for Robust-2 there are in average 407.4. We report the number of queries needed to find an additional x% </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head></head><label></label><figDesc>). Solid line denotes mean value and shaded area denotes the 95% confidence interval. Maximum query budget is 1,699,998 for robust CIFAR10 model, 4,393,314 for ImageNet.</figDesc><table>Target Model Prioritization Method 
Top 1% 
Top 2% 
Top 5% 
Top 10% 
Robust 
CIFAR10 
(1,000 Seeds) 

Retroactive Optimal 
34.0 ± 2.0 
119.2 ± 4.8 
580.8 ± 35.0 
2, 002 ± 69 
Target Loss 
1, 070 ± 13 
1, 170 ± 16 
1, 765 ± 12 
3, 502 ± 85 
Random 
25, 005 ± 108 
51, 325 ± 221 
130, 284 ± 561 
261, 883 ± 1, 128 
Standard 
ImageNet 
(100 Seeds) 

Retroactive Optimal 
7, 492 ± 1, 078 16, 590 ± 1, 755 
49, 255 ± 4, 945 114, 832 ± 7, 430 
Target Loss 
32, 490 ± 5, 857 58, 665 ± 8, 268 
89, 541 ± 8, 459 257, 594 ± 13, 738 
Random 
22, 178 ± 705 
66, 532 ± 2, 114 199, 595 ± 6, 341 
421, 365 ± 13, 387 

</table></figure>

			<note place="foot" n="2"> We also tested with deep CNN models as our local ensembles. However, they provide only slightly better performance compared to simple CIFAR10 models, while the fine-tuning cost is much higher.</note>

			<note place="foot" n="3"> For example, for the targeted attack on ImageNet, the baseline Bandits TD attack only has 88% success rate and average query cost of 51,745, which are much worse than the NES and AutoZOOM attacks. 4 By running the original AutoZOOM attack with a 4000 query limit compared to their default setting of 2000, we found 17.2% and 25.4% more adversarial samples out of 1000 seeds for CIFAR10 and MNIST respectively.</note>

			<note place="foot" n="5"> We did not repeat the experiments with robust MNIST local models because, without worrying about separately training robust local models, we can simply improve the attack performance significantly by tuning the local models during the hybrid attack process (see Table 6 in Section 4.6). The tuning process transforms the normal local models into more robust ones (details in Section 4.6).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by grants from the National Science Foundation (#1619098, #1804603, and #1850479) and research awards from Baidu and Intel, and cloud computing grants from Amazon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Robust CIFAR10 Model, Local Ensemble: Normal-3 (b) Target: Standard ImageNet Model References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">There are no bit parts for sign bits in black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Dujaili</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Una-May O&amp;apos;</forename><surname>Reilly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06894</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GenAttack: Practical black-box attacks with gradient-free optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustafa</forename><surname>Alzantot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Supriyo</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mani</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Genetic and Evolutionary Computation Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The shattered gradients problem: If resnets are the answer, then what is the question?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennox</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt Wan-Duo</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring the space of black-box attacks on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arjun Nitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Guessing smart: Biased sampling for efficient black-box adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Truong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09803</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<title level="m">Prototypical examples in deep learning: Metrics, characteristics, and utility</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Boundary attack++: Query-efficient decision-based adversarial attack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02144</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ZOO: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM Workshop on Artificial Intelligence and Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Stateful detection of black-box adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05587</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Query-efficient hardlabel black-box attack: An optimization-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06919</idno>
		<title level="m">Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boosting adversarial attacks with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evading defenses to transferable adversarial examples by translation-invariant attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simple black-box adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Prior convictions: Black-box adversarial attacks with bandits and priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Queryefficient black-box attack by active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nattack: Learning the distributions of adversarial examples for an improved black-box attack on deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">CIFAR10 adversarial examples challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://github.com/MadryLab/cifar10_challenge" />
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">MNIST adversarial examples challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://github.com/MadryLab/mnist_challenge" />
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parsimonious black-box adversarial attacks via efficient combinatorial optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungyong</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Simple black-box adversarial perturbations for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Narodytska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shiva Prasad Kasiviswanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<title level="m">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Asia Conference on Computer and Communications Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Query-limited black-box attacks to classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fnu</forename><surname>Suya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop in Machine Learning and Computer Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Targeted adversarial examples for black box audio systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amog</forename><surname>Kamsetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenton</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Vemuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07820</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Chen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paishun</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsieh Cho-Jui</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Natural evolution strategies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Congress on Evolutionary Computation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improving transferability of adversarial examples with input diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
