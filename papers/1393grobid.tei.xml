<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting the Storage Stack in Virtualized NAS Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Hildebrand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Almaden Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Povzner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Almaden Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renu</forename><surname>Tewari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Almaden Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Tarasov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Almaden Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting the Storage Stack in Virtualized NAS Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cloud architectures are moving away from a traditional data center design with SAN and NAS attached storage to a more flexible solution based on virtual machines with NAS attached storage. While VM storage based on NAS is ideal to meet the high scale, low cost, and manageability requirements of the cloud, it significantly alters the I/O profile for which NAS storage is designed. In this paper, we explore the storage stack in a virtualized NAS environment and highlight corresponding performance implications.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There is a shift in cloud architectures from the traditional data center design based on physical machines with SAN or NAS attached storage to a more flexible and lower cost solution using virtual machines and NAS storage. In these architectures, the virtual machine's associated disks exist as files in a NAS datastore and are accessed using a file access protocol such as NFS. Storing the disks of a virtual machine as files (instead of LUNs) makes VMs easier to manage, migrate, clone, and access. The traditional NAS storage stack, however, is altered as the guest block layer now resides above a file access protocol. The block layer, which expects a consistent and lowlatency I/O layer beneath it, must now utilize a file access protocol with a higher latency and looser closeto-open consistency semantics. Moreover, what the NAS storage system views as a file and optimizes for file operations is actually a disk.</p><p>While some work exists that investigates the impact of virtualized environments with traditional block-based storage <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, there is no formal investigation on the impact of virtual machines with NAS storage. In this paper, we investigate how a virtualized stack affects the behavior and workload characteristics of the NAS storage system. Some key observations we make in the paper are:  The NAS workload consists of only I/O requests and no metadata requests, i.e., no create, delete, change attribute operations. Using I/O to perform metadata operations can increase performance in certain instances.  All NAS write requests require a commit to the stable storage instead of the weaker semantics of a commit-on-close or on an fsync.  Sequential I/O at the guest level is highly likely to be transformed to random I/O at the NAS level.</p><p> NAS workload changes from many smaller files to a small number of considerably larger files.  Small writes generate a read-modify-write update from the guest and all writes may generate readmodify-write update on the server side, significantly degrading performance (~20-69%).  Small reads in the guest can double the amount of data read at the NAS level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">VM and NAS: An Overview</head><p>Virtual machines encapsulate the entire guest file system in a virtual disk. A VM residing over NAS increases the number of layers in the virtual software stack as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Virtual disks are typically stored as files, e.g., VMWare .vmdk, on the NAS store. The NAS store in turn consists of an NFS or CIFS server with a back-end file system such as ZFS, WAFL, or GPFS <ref type="bibr" target="#b2">[3]</ref>. Hypervisors provide VMs with access to these disk images using a NAS protocol. <ref type="figure" target="#fig_0">Figure 1</ref> shows the VM with NAS storage stack. Application accesses to the guest file system are sent to the storage controller emulator in the hypervisor by the guest block layer. The storage controller emulator then re-issues requests from the guest block layer as filelevel I/O requests to the disk image via the NAS client. The NAS client in turn performs I/O requests to the disk image file stored in the server file system. The movement of the I/O request through the various layers of the virtualized stack has two adverse side effects. First, the increased number of layers in the software stack increases the amount of processing each request must experience (and hence increase its I/O request latency).</p><p>Second, as <ref type="figure" target="#fig_1">Figure 2</ref> demonstrates, files in the guest file system have their data block addresses stored relative to the guest block layer addresses. These data blocks are actually offsets and extents in the disk image file stored on the server file system. By storing these blocks in a file, the block layer's need to cache entire blocks causes read-modifywrite operations over the NAS protocol, which degrades performance. The server file system then rearranges the original guest data blocks once again as it is stored in the server block layer. Guest data blocks can be split across multiple server file system blocks depending on the start offset of the file system within the disk image file, causing even more read-modifywrite operations on the server side.</p><p>By storing the VM's disk as a file, the virtualized stack changes the workload profile of the server file system from millions of small files to a much smaller number of very large disk images. In addition, our preliminary results indicate that access to these disk images will most likely consist of small and random I/O requests with very few metadata requests, such as create, delete, and change attributes. Supporting small and random I/O requests to large files runs contrary to the current focus on optimizing the backend NAS store for create operations per second and supporting billions of files in a single directory <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Block over File: A Good Idea?</head><p>This section investigates how the I/O workload changes, and the resultant performance impact, when virtual machines are introduced into the NAS environment. We ran several benchmarks in standard NAS (NFS) and virtualized NAS (VM-NFS) environments and compared the generated I/O workloads and subsequent performance. Both environments used GPFS as a server file system running on an IBM System x3550 equipped with a five disk RAID-5. In the standard NAS environment, benchmarks ran directly on an NFS client, and in the virtualized NAS environment, benchmarks ran in a virtual machine. The virtual machine ran on VMware ESX 4.1 with a Fedora 14 guest operating system stored locally on the ESX server. The experiments were executed on an Ext2 file system, whose disk image was stored on the NFS data store. Both VM-NFS and standard NFS client were configured with 512MB of memory and connected to the NAS server with a GigE network. NFS data transfer buffer sizes were set to 64KB ("rsize") and 512KB ("wsize").</p><p>Filebench was used to generate all workloads. We used a multi-layer, correlated tracing framework to collect traces at four trace points in the VM with NAS stack (as shown in <ref type="figure" target="#fig_0">Figure 1</ref>). Through these traces, we could decipher how the guest block layer interacts with the NAS client file layer and changes the generated I/O workload at the server file system.</p><p>In this paper, we characterize and compare workloads seen by the NAS storage server in both standard NAS and virtualized NAS environments by observing types and number of operations, amount of I/O, and I/O patterns. We first investigate metadata transformations in Section 3.1 using create and stat experiments and then investigate data transformations in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metadata Workloads</head><p>In this section, we show the impact of guest file system metadata requests as they are transformed into read and write accesses to a disk image file. The optimizations made by many file systems for metadata operations such as creates become irrelevant in virtualized NAS environments since metadata is always transformed to data I/O. It is important to note that the exact type of transformation of metadata to data and its subsequent performance impact depends on the guest file system and server file system. We used a bare-bones guest file system ("Ext2") in this paper, but plan to use more advanced file systems such as Ext4 or btrfs in the future. We chose GPFS for the server file system due to both our familiarity as well as its prevalence in real-world data centers.</p><p>File creation. To evaluate file creates, we created a directory and filled it with 100,000 zerolength files. Each create file operation was followed by a close file operation. We record the number of ops/sec, the number of NFS create, getattr and mkdir operations, and the amount of data read and written by the server file system. As shown in <ref type="table">Table 1</ref>, create, getattr and mkdir metadata operations observed by the NAS storage in the standard setup (NFS) become read and write operations in the virtualized NAS (VM-NFS). 100,000 create/close calls from the guest became 21.5MB worth of reads and 21MB of writes. <ref type="figure" target="#fig_2">Figure 3</ref> shows that all reads are 4KB, while write sizes are larger, with 49% of writes being 256KB. In addition, 98% of reads and 52% of writes are sequential.</p><p>High sequentiality is attributed to the Ext2 file system attempting to co-locate created inodes.</p><p>In this experiment, the transformation of metadata to data degraded VM-NFS performance to half of standard NFS performance <ref type="table">(Table 1)</ref>. However, <ref type="figure" target="#fig_3">Figure  4</ref> shows that VM-NFS performance greatly depends on the directory tree structure. Each experiment in <ref type="figure" target="#fig_3">Figure  4</ref> creates files in a different directory structure defined by a mean number of entries in each directory, mean dirwidth, starting from 20 (5105 directories, tree depth 4) to 1,000,000 (all files in one directory).</p><p>The larger and deeper directory tree results in the better VM-NFS performance. While the total amount of data transferred remains similar <ref type="table">(Table 1)</ref>, creating files in a deeper directory tree results in larger write sizes (not shown), which translates into higher write throughput. Standard NFS performance remained independent of the directory structure and was worse in most cases than VM-NFS. This can be attributed to the directory locking and lock contentions in the server file system that does not exist when VM-NFS performs I/O instead of file creates.</p><p>File stat. To confirm our observations, we investigate "get file status" transformations. This experiment performs stat() calls for 2 minutes on random files in a single directory of 100,000 files. <ref type="table" target="#tab_1">Table 2</ref> shows that the lookup and getattr operations observed by the server file system with standard NFS became 4K read operations with VM-NFS. About a million stat() calls are transformed into 6743 4K read requests (total 26.3 MB).</p><p>Virtual stat performance also depends on the directory tree structure: stat() files from one directory resulted in 8622 stat/sec, but using 5105 directories/tree depth 4 (same total number of files) resulted in 12167 stat/sec. <ref type="figure" target="#fig_4">Figure 5</ref> shows that using larger directory trees causes stat() operations to transform into read requests with smaller seek distance between reads, thus improving the stat performance.</p><p>Virtual stat performance was better than standard NFS in our experiments <ref type="table" target="#tab_1">(Table 2)</ref>. This was a surprising result, and can be attributed to several factors: NFS attribute caching timeouts; possible server file system lock contentions; and most significantly the ability for Ext2 to co-locate inodes and therefore have each read request prefetch several inodes at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Read and Write Workloads</head><p>Read and write intensive workloads are also transformed when introducing VMs into the NAS environment due to the several layers, including a block layer, which data must pass through before arriving at the NAS client in the hypervisor. This section shows that workloads with smaller than 4KB reads and writes suffer the most from these additional layers due to full page read issues and read-modifywrite at the guest. In addition, guest data blocks that are misaligned with respect to the server block layer decrease performance by causing additional server disk Read and write sizes at server file system layer in the VM-NFS environment when 100,000 files were created in a single directory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single Directory</head><p>Mean  accesses. Writes are further impacted because data flushed from the guest page cache must be immediately committed to disk and cannot be buffered in server file system cache before being written to disk.</p><p>Write workloads. To generate a sequential write workload, Filebench used a single thread to write a 4GB file, varying the write sizes from 2KB to 4MB. <ref type="figure" target="#fig_5">Figure 6</ref> shows sequential write throughput with virtualized NAS (VM-NFS) is approximately half that of standard NFS performance, with an even larger difference with 2KB writes. In both setups, the performance increased as write size increased since disk-based systems can generally handle larger requests more efficiently. <ref type="figure" target="#fig_6">Figure 7</ref> shows that 2KB writes in VM-NFS cause read-modify-write in the guest ("file-read")-in order to write 4GB of data, the guest must also read 4GB of data. Making matters worse, reads are performed as synchronous 4KB requests without any read-ahead from the guest (not shown). In contrast, NFS does not perform file reads when writing in 2KB chunks. The "block-read" and "block-write" parts of <ref type="figure" target="#fig_6">Figure 7</ref> also show that 4GB of reads from the guest results in even larger amount of data read at the server file system block layer due to block misalignment.</p><p>VM-NFS performance also degrades with larger write requests. As shown in <ref type="figure" target="#fig_6">Figure 7</ref> for 128KB write, VM-NFS demonstrates more server file system block layer reads and writes. This is a result of read-modifywrite operations on the server side, as guest data blocks can be split across multiple server blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Read workloads. Read throughput is mostly</head><p>affected in the case of the small request sizes. In this experiment, we compare performing random 2KB and 128KB reads from a 4GB file using 20 read threads, where total amount of data read in each case is 2GB. <ref type="table">Table 3</ref> shows that 2KB reads in VM-NFS setup suffer 4 times lower performance than NFS, while 128KB read performance is very similar to NFS.</p><p>The main reason for VM-NFS 2KB performance degradation is the full page read in the guest OS. <ref type="table">Table 3</ref> shows that in order to read 2GB, the guest must read 4GB from the server file system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Eliminating Client-Side Block on File</head><p>As this paper states, the layering of the guest block layer on top of the NFS file layer can reduce performance, stress client networks, and reduce the server file system's ability to perform standard optimizations. Applications that were once running successfully in a SAN or NAS environment may now see unacceptable storage performance with the same storage hardware.</p><p>One possible solution is for applications to use the NAS client in the guest OS (instead of a disk image), but this technique no longer virtualizes the I/O and places data centers at the mercy of features and bugs in the guest OS.</p><p>Techniques are required to allow virtual machines to continue virtualizing I/O and using disk images while giving the server file system more information of the original guest operations. One possibility is to use   <ref type="table">Table 3</ref>: Random 2KB reads. Performance and amount of data transferred at server file system and block layer for random read (20 threads) from 4GB file. a para-virtualized NAS client driver in the guest OS. In this model, applications running in the guest would not use a local file system such as Ext2 but rather a NFS device that passes its operations to the hypervisor and then to a disk image in the server file system.</p><p>To simulate such an environment, we mounted the disk image in the server file system (using the loopback driver), exported it via NFS, and then mounted it from the guest using the Linux NFS client. As shown in <ref type="table">Table 4</ref>, Guest-NFS more than doubles write throughput as compared with the standard method of translating requests from a SCSI emulator to the NFS (VM-NFS). Guest-NFS sends I/O requests directly to the server, avoiding the need for readmodify-write from the hypervisor. Guest-NFS does not match the performance of standard NFS (Linux-NFS) since the loopback mounted disk image on the server reduces all write requests to 4KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future Work</head><p>We plan to extend this work in several ways. First, more information is needed regarding the impact of virtualizing NAS applications on I/O workloads and the server file system. For instance, how does the elimination of large number of file creates and the existence of many large and sparse disk images change NAS protocols and the server file system design. In addition, we would like to work with specialized benchmarks such as VMMark <ref type="bibr" target="#b12">[13]</ref> as well as find and replay traces from real virtual data centers.</p><p>Second, virtual disk image fragmentation can have a great impact on the randomness of read and write requests sent to the server file system. We plan to use a tool like Impressions <ref type="bibr" target="#b13">[14]</ref> to generate fragmented disk images and study their effect on I/O workloads.</p><p>Finally, as discussed, when layering the guest block layer over the NAS file layer, the original file system operations are lost, as all requests are transformed into read and write requests to the disk image file. Hence, the server file system cannot cache or prefetch data based on whether it was a metadata or data request. This is very common, for example, to ensure large data reads do not evict all directory information from the cache. We are investigating methods of restoring the original file system operations, including client side hints and possible server side reverse engineering of the I/O requests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Several NAS storage systems are available today that target virtual data centers <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. While each vendor provides a different solution, we expect that all have an architecture similar to <ref type="figure" target="#fig_0">Figure 1</ref> and are equally subject to the issues raised in this paper.</p><p>Several papers have shown that I/O performance degradation can be incurred by the use of legacy device drivers and even para-virtualized I/O drivers in the guest <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. Solutions for these device issues include direct access (pass-through) and self-virtualizing devices <ref type="bibr" target="#b10">[11]</ref>.</p><p>Since the NAS client is in the hypervisor, improvements in these I/O virtualization techniques are unlikely to significantly alter the I/O workload generated in VM with NAS architectures.</p><p>Workload characterization using traces is a popular research topic <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. A few virtual machine I/O workload studies exist that show the resultant I/O workload in the hypervisor when applications are executed in the guest <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12]</ref>. Corresponding traces and other publically available traces are usually from only a single layer of the I/O stack <ref type="bibr" target="#b19">[20]</ref>. As far as we are aware, there are no public traces that encapsulate multiple layers of a virtualized I/O stack. We introduce a novel approach of not only focusing on virtualized NAS environments, but also showing the I/O workload at the several different levels in the stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>While the combined use of virtual machines and NAS has the potential to reduce costs and simplify management, VMs may notice a substantial drop in I/O performance due to the introduction of several new software layers, including the introduction of a guest block layer over the NAS file layer. In this paper, we identified and demonstrated several application workloads in a virtual NAS environment that both change the I/O workload and impact performance. In addition, we demonstrate how it is possible to avoid the block on file architecture and send the original application file requests to the NAS server.   <ref type="table">Table 4</ref>: Small sequential write performance. Guest-NFS improves performance by avoiding client side readmodify-write as seen by ESX-NFS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 : VM with NAS -</head><label>1</label><figDesc>Figure 1: VM with NAS -Numerous software layers are punctuated by a block layer in the guest operating system driving the requests to the NAS file layer. Our system traces the stack at 4 points simultaneously to capture the I/O behavior at each level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Data through the layers -File data in the guest file system are organized as blocks in the guest block layer. Block boundaries are lost when represented in the disk image file. Guest data blocks are re-arranged in the server block layer, possibly straddling server block boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : Virtual file create I/O workload.</head><label>3</label><figDesc>Figure 3: Virtual file create I/O workload. Read and write sizes at server file system layer in the VM-NFS environment when 100,000 files were created in a single directory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 : File create performance.</head><label>4</label><figDesc>Figure 4: File create performance. Performance of file create experiment as mean dirwidth increases (decreasing the total number of directories from 5105 to 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 : stat() randomness.</head><label>5</label><figDesc>Figure 5: stat() randomness. Seek distance between reads at server file system with stat() on files in a single directory versus a directory tree with avg. 20 entries in each directory. stat/sec lookup/getattr data read VM-NFS 8622 0 26.3MB NFS 2656 610721 0MB Table 2: File stat() operations. Operations at server file system when performing stat() on random files. VM-NFS executed 6743 4KB read requests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 : NFS and VM-NFS sequential write performance.</head><label>6</label><figDesc>Figure 6: NFS and VM-NFS sequential write performance. Throughput when writing a 4GB file with write sizes ranging from 2KB to 4MB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 : Sequential write data transfer.</head><label>7</label><figDesc>Figure 7: Sequential write data transfer. Amount of data transferred at server file system and server block layer during sequential write of 4GB file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>: File create operations. Operations at server file system layer file creation in a single directory and in a directory tree with average 20 items in each directory.</figDesc><table>Dir. Width 20 
VM-NFS 
NFS 
VM-NFS 
NFS 
ops/sec 
1408 
2270 
7406 
2125 
creates 
0 
100000 0 
100000 
getattrs 0 
500216 0 
500640 
mkdirs 
0 
1 
0 
5105 
reads 
21.5MB 0MB 
25.8MB 0MB 
writes 
21MB 
0MB 
15.5MB 0MB 
Table 1</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>File stat() operations. Operations at server file 
system when performing stat() on random files. VM-NFS 
executed 6743 4KB read requests. 

2KB reads 
128KB reads 
VM-NFS 
NFS 
VM-NFS 
NFS 
MB/s 
0.6MB/s 2.5MB/s 46.5MB/s 50MB/s 
app reads 
2048M 
2048M 
2048M 2048M 
file reads 
4710M 
1140M 
2187M 2804M 
block reads 6758M 
5853M 
3280M 3769M 
</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Easy and Efficient Disk I/O Workload Characterization in VMware ESX Server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IISWC07</title>
		<meeting>IISWC07<address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An analysis of disk performance in VMware ESX server virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Holler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kambo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Makhija</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Workshop on Workload Characterization</title>
		<meeting>the IEEE International Workshop on Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GPFS: A SharedDisk File System for Large Computing Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schmuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Conference on File and Storage Technologies</title>
		<meeting>the USENIX Conference on File and Storage Technologies<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Specsfs</surname></persName>
		</author>
		<ptr target="www.spec.org/sfs93" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">NetApp Storage Solutions for Server Virtualization</title>
		<ptr target="www.netapp.com/us/solutions/infrastructure/virtualization/server" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">IBM Scale Out Network Attached Storage</title>
		<ptr target="www-03.ibm.com/systems/storage/network/sonas" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gluster</title>
		<ptr target="www.gluster.com/2011/02/08/gluster-introduces-first-scale-out-nas-virtual-appliances-for-vmware-and-amazon-web-services/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">EMC</title>
		<ptr target="www.emc.com/solutions/business-need/virtualizing-information-infrastructure/file-virtualizations.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bridging the gap between software and hardware techniques for i/o virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Janakiraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Virtualizing I/O Devices on VMware Workstation&apos;s Hosted Virtual Machine Monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sugerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkitachalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">High performance and scalable I/O virtualization via self-virtualized devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international symposium on High performance distributed computing</title>
		<meeting>the 16th international symposium on High performance distributed computing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Storage Workload Characterization and Consolidation in Virtualized Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Virtualization Performance: Analysis, Characterization, and Tools</title>
		<meeting>the Workshop on Virtualization Performance: Analysis, Characterization, and Tools</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">VMMark 2.0</title>
		<ptr target="www.vmware.com/products/vmmark/overview.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating Realistic Impressions for File-System Benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Conference on File and Storage Technologies</title>
		<meeting>the 7th Conference on File and Storage Technologies<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Characteristics of File System Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roselli</surname></persName>
		</author>
		<idno>UCB/CSD-98- 1029</idno>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
		<respStmt>
			<orgName>University of California at Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis of Personal Computer Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems</title>
		<meeting>the International Symposium on Modeling, Analysis and Simulation of Computer and Telecommunication Systems</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Comparison of File System Workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Technical Conference</title>
		<meeting>USENIX Technical Conference</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Characterization of Storage Workload Traces from Production Windows Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kavalanekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Symposium on Workload Characterization</title>
		<meeting>IEEE International Symposium on Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovery of Application Workloads from Network File Traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yadwadkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gopinath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX Conference on File and Storage Technologies</title>
		<meeting>USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Tools and Analysis Technical Working Group Trace Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snia I/O</forename><surname>Traces</surname></persName>
		</author>
		<idno>iotta.snia.org</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
