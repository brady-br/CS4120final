<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gábor</forename><surname>Berend</surname></persName>
							<email>berendg@inf.u-szeged.hu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">University of Szeged</orgName>
								<address>
									<addrLine>2 ´ Arpád tér</addrLine>
									<postCode>6720</postCode>
									<settlement>Szeged</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Coding of Neural Word Embeddings for Multilingual Sequence Labeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper we propose and carefully evaluate a sequence labeling framework which solely utilizes sparse indicator features derived from dense distributed word representations. The proposed model obtains (near) state-of-the art performance for both part-of-speech tagging and named entity recognition for a variety of languages. Our model relies only on a few thousand sparse coding-derived features, without applying any modification of the word representations employed for the different tasks. The proposed model has favorable generalization properties as it retains over 89.8% of its average POS tagging accuracy when trained at 1.2% of the total available training data, i.e. 150 sentences per language.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Determining the linguistic structure of natural language texts based on rich hand-crafted features has a long-going history in natural language processing. The focus of traditional approaches has mostly been on building linguistic analyzers for a particular kind of analysis, which often leads to the incorporation of extensive linguistic and/or domain knowledge for defining the feature space. Consequently, traditional models easily become language and/or task specific resulting in improper generalization properties.</p><p>A new research direction has emerged recently, that aims at building more general models that require far less feature engineering or none at all. These advancements in natural language processing, pioneered by <ref type="bibr" target="#b4">Bengio et al. (2003)</ref>, followed by <ref type="bibr">Col- lobert and Weston (2008)</ref>, <ref type="bibr" target="#b10">Collobert et al. (2011)</ref>, <ref type="bibr" target="#b24">Mikolov et al. (2013a)</ref> among others, employ a different philosophy. The objective of these works is to find representations for linguistic phenomena in an unsupervised manner by relying on large amounts of text.</p><p>Natural language phenomena are extremely sparse by their nature, whereas continuous word embeddings employ dense representations of words. In our paper we empirically verify via rigorous experiments that turning these dense representations into a much sparser (yet denser than one-hot encoding) form can keep the most salient parts of word representations that are highly suitable for sequence models.</p><p>Furthermore, our experiments reveal that our proposed model performs substantially better than traditional feature-rich models in the absence of abundant training data. Our proposed model also has the advantage of performing well on multiple sequence labeling tasks without any modification in the applied word representations thanks to the sparse features derived from continuous word representations.</p><p>Our work aims at introducing a novel sequence labeling model solely utilizing features derived from the sparse coding of continuous word embeddings. Even though sparse coding had previously been utilized in NLP prior to us <ref type="bibr" target="#b8">Chen et al., 2016)</ref>, to the best of our knowledge, we are the first to propose a sequence labeling framework incorporating it with the following contributions:</p><p>• We show that the proposed sparse representation is general as sequence labeling models trained on them achieve (near) state-of-the-art performances for both POS tagging and NER.</p><p>• We show that the representation is general in the other sense, that it produces reasonable results for more than 40 treebanks for POS tagging,</p><p>• rigorously compare different sparse coding approaches in conjunction with differently trained continuous word embeddings,</p><p>• highlight the favorable generalization properties of our model in settings when access to a very limited training corpus is assumed,</p><p>• release the sparse word representations determined for our experiments at https:// begab.github.io/sparse_embeds to ensure the replicability of our results and to foster further multilingual NLP research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The line of research introduced in this paper relies on distributed word representations (Al-Rfou et al., 2013) and dictionary learning for sparse coding ( <ref type="bibr" target="#b22">Mairal et al., 2010</ref>) and also shows close resemblance to ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Distributed word representations</head><p>Distributed word representations assign some relatively low-dimensional, dense vectors to each word in a corpus such that words with similar context and meaning tend to have similar representations. From an algebraic point of view, the embedding of word i having index idx i in a vocabulary V can be thought of as the result of a matrix-vector multiplication W 1 i , where the i th column of matrix W ∈ R k×|V | contains the k-dimensional (k |V |) embedding for word i and vector 1 i ∈ R |V| is the one-hot representation of word i. The one-hot representation of word i is such a vector, which contains zeros for all of its entries except for index idx i where it stores a one. Depending on how the columns of W (i.e. the word embeddings) get determined, we could distinguish a plethora of approaches ( <ref type="bibr" target="#b4">Bengio et al., 2003;</ref><ref type="bibr" target="#b17">Lebret and Collobert, 2014;</ref><ref type="bibr" target="#b27">Mnih and Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b9">Collobert and Weston, 2008;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013a;</ref><ref type="bibr" target="#b33">Pennington et al., 2014</ref>). Prediction-based distributed word embedding approaches such as word2vec ( <ref type="bibr" target="#b24">Mikolov et al., 2013a</ref>) have been conjectured to have superior performance over count-based word representations ( <ref type="bibr" target="#b3">Baroni et al., 2014</ref>). However, as <ref type="bibr">Lebret and Col- lobert (2015)</ref>, <ref type="bibr" target="#b19">Levy et al. (2015)</ref> and <ref type="bibr" target="#b36">Qu et al. (2015)</ref> point out count-based distributional models can perform on par with prediction-based distributed word embedding models. <ref type="bibr" target="#b19">Levy et al. (2015)</ref> illustrate that the effectiveness of neural word embeddings largely depend on the selection of model hyperparameters and other design choices.</p><p>According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we primarily use the publicly available pre-trained polyglot word embeddings of Al-Rfou et al. (2013) instead, without any task specific modification for our experiments.</p><p>A key thing to note is that polyglot word embeddings are not tailored toward any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by <ref type="bibr" target="#b26">Mikolov et al. (2013c)</ref>. The polyglot project distributes word embeddings for more than 100 languages. Al- <ref type="bibr" target="#b1">Rfou et al. (2013)</ref> also report results on POS tagging, however, word representations they apply for these experiments are different from the task-agnostic representations they made publicly available.</p><p>There has been previous research on training neural networks for learning distributed word representations for various specific language analysis tasks. <ref type="bibr" target="#b10">Collobert et al. (2011)</ref> propose neural network architectures to four natural language processing tasks, i.e. POS tagging, named entity recognition, semantic role labeling and chunking. <ref type="bibr" target="#b10">Collobert et al. (2011)</ref> train word representations on large amounts of unannotated texts from Wikipedia, then update the pretrained word representations for the individual tasks. Our approach is different in that we do not update our word representations for the different tasks and most importantly that we use successfully the features derived from sparse coding in a log-linear model instead of a neural network architecture. A final difference to <ref type="bibr" target="#b10">(Collobert et al., 2011</ref>) is that we experiment with a much wider range of languages while they report results for English only. <ref type="bibr" target="#b36">Qu et al. (2015)</ref> evaluate the impacts of choosing different embedding methods on four sequence labeling tasks, i.e. POS tagging, NER, syntactic chunking and multiword expression identification. The hand-crafted features they employ for POS tagging and NER are the same as in <ref type="bibr" target="#b10">Collobert et al. (2011)</ref> and <ref type="bibr" target="#b44">Turian et al. (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparse coding</head><p>The general goal of sparse coding is to express signals in the form of sparse linear combination of basis vectors and the task of finding an appropriate set of basis vectors is referred to as the dictionary learning problem ( <ref type="bibr" target="#b22">Mairal et al., 2010)</ref>. Generally, given a data matrix X ∈ R k×n with its i th column x i representing the i th k-dimensional signal, the task is to find D ∈ R k×m and α ∈ R m×n , such that X ≈ Dα. This can be formalized into an 1 -regularized linear least-squares minimization problem having the form</p><formula xml:id="formula_0">min D∈C,α 1 2n n i=1 x i − Dα i 2 2 + λα i 1 ,<label>(1)</label></formula><p>with C being the convex set of matrices of column vectors having an 2 norm at most one, matrix D acting as the shared dictionary across the signals, and the columns of the sparse matrix α containing the coefficients for the linear combinations of each of the n observed signals. Performing sparse coding of word embeddings has recently been proposed by , however, the objective function they optimize differs from (1). In Section 4, we compare the effects of employing different sparse coding paradigms including the ones in .</p><p>In their work,  proposed an efficient learning algorithm for determining hierarchically organized sparse word representations using stochastic proximal methods. Most recently, <ref type="bibr" target="#b41">Sun et al. (2016)</ref> have proposed an online learning algorithm using regularized dual averaging to directly obtain 1 regularized continuous bag of words (CBOW) representations <ref type="bibr" target="#b24">(Mikolov et al., 2013a</ref>) without the need to determine dense CBOW representations first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Sequence labeling framework</head><p>This section introduces the sequence labeling framework we use for both POS tagging and NER. Since our goal is to measure the effectiveness of sparse word embeddings alone, we do not apply any features based on gazetters, capitalization patterns or character suffixes.</p><p>As described previously, word embedding methods turn a high-dimensional (i.e., as many dimensions as words in the vocabulary) and extremely sparse (i.e. containing only one non-zero element at the vocabulary index of the word it represents) onehot encoded representation of words into a dense embedding of much lower dimensionality k.</p><p>In our work, instead of using the low dimensional dense word embeddings, we use a dictionary learning approach to obtain sparse codings for the embedded word representations. Formally, given the lookup matrix W ∈ R k×|V | which contains the embedding vectors, we learned D ∈ R k×m being the dictionary matrix shared across all the embedding vectors and α ∈ R m×|V | containing sparse linear combination coefficients for each of the word embeddings so that W −Dα 2 F +λα 1 is minimized. Once the dictionary matrix D is learned, the sparse linear combination coefficients α i can easily be determined for a word embedding vector w i by solving an 1 -regularized linear least-squares minimization problem ( <ref type="bibr" target="#b22">Mairal et al., 2010)</ref>. We define features based on vector α i by taking the signs and indices of its non-zero coefficients, that is</p><formula xml:id="formula_1">f (w i ) = {sign(α i [j])j | α i [j] = 0},<label>(2)</label></formula><p>where α i [j] denotes the j th coefficient in the sparse vector α i . The intuition behind this feature is that words with similar meaning are expected to use an overlapping set of basis vectors from dictionary D.</p><p>Incorporating the signs of coefficients into the feature function can help to distinguish cases when a basis vector takes part in the reconstruction of a word representation "destructively" or "constructively". When assigning features to a target word at some position within a sentence, we determine the same set of feature functions for the target word itself and its neighboring words of window size 1. Experiments with window size 2 were also performed. However, we omit these results for brevity as they do not substantially differ from those obtained with a window size of 1.</p><p>We then use the previously described set of features in a linear chain CRF ( <ref type="bibr" target="#b16">Lafferty et al., 2001)</ref> using CRFsuite <ref type="bibr" target="#b31">(Okazaki, 2007)</ref> with its default settings for hyperparameters, i.e., the coefficients of 1.0 and 0.001 for 1 and 2 regularization, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We rely on the SPArse Modeling Software 1 (SPAMS) ( <ref type="bibr" target="#b22">Mairal et al., 2010</ref>) for performing sparse coding of distributed word representations. For dictionary learning as formulated in Equation 1, one should choose m and λ, controlling the number of the basis vectors and the regularization coefficient affecting the sparsity of α, respectively. Starting with m = 256 and doubling it at each iteration, our preliminary investigations showed a steady growth in the usefulness of sparse word representations as a function of m, plateauing at m = 1024. We set m to that value for further experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline methods</head><p>Brown clustering Various studies have identified Brown clustering ( <ref type="bibr" target="#b6">Brown et al., 1992</ref>) as a useful source of feature generation for sequence labeling tasks <ref type="bibr" target="#b37">(Ratinov and Roth, 2009;</ref><ref type="bibr" target="#b44">Turian et al., 2010;</ref><ref type="bibr" target="#b32">Owoputi et al., 2013;</ref><ref type="bibr" target="#b40">Stratos and Collins, 2015;</ref><ref type="bibr">Der- czynski et al., 2015</ref>). We should note that sparse coding can also be viewed as a kind of clustering that -unlike Brown clustering -has the capability of assigning word forms to multiple clusters at a time (corresponding to the non-zero coefficients in α).</p><p>We thus define a linear chain CRF relying on features from the Brown cluster identifier of words as one of our baseline approach. Since Brown clustering defines a hierarchical clustering over words, cluster supersets can easily function as features. We generate features from length-p (p ∈ {4, 6, 10, 20}) prefixes of Brown cluster identifiers similar to Ratinov and Roth (2009) and <ref type="bibr" target="#b44">Turian et al. (2010)</ref>.</p><p>In our experiments we use the implementation by <ref type="bibr" target="#b20">Liang (2005)</ref> for performing Brown clustering 2 . We provide the very same Wikipedia articles as input text for determining Brown clusters that are used for training the polyglot 3 word embeddings. We <ref type="table">Table 1</ref>: Features and feature templates applied by our feature-rich baseline for target word w t . ⊕ is a binary operator forming a feature from words and their relative positions by combining them together.</p><formula xml:id="formula_2"># Level Feature name 1 char isNumber(w t ) 2 char isTitleCase(w t ) 3 char isNonAlnum(w t ) 4 char prefix(w t , i) 1 ≤ i ≤ 4 5 char suffix(w t , i) 1 ≤ i ≤ 4 6 word w t+j −2 ≤ j ≤ 2 7 word w t ⊕ w t+i 1 ≤ i ≤ 9 8 word w t ⊕ w t−i 1 ≤ i ≤ 9 9 word ⊕ t+j+1 i=t+j w i −2 ≤ j ≤ 1 10 word ⊕ t+j+2 i=t+j w i −2 ≤ j ≤ 0 11 word ⊕ t+j+2 i=t+j−1 w i −1 ≤ j ≤ 0 12 word ⊕ t+2 i=t−2 w i</formula><p>also set the number of Brown clusters to be identified to 1024, which is the number of basis vectors applied during sparse coding (cf. D ∈ R 64×1024 ).</p><p>Feature-rich representation We report results relying on linear chain CRFs that assign standard state-of-the-art feature-rich representation to sequences. We apply the very same features and feature templates included in the POS tagging model of CRFSuite <ref type="bibr">4</ref> . We summarize these features in <ref type="table">Table 1</ref>, where ⊕ denotes the binary operator which defines features as a combination of word forms at different (not necessarily contiguous) positions of a sentence.</p><p>We use the same pool of features described in <ref type="table">Ta- ble 1</ref> for both POS tagging and NER. The reason why we do not adjust the feature-rich representation employed as our baseline for the different tasks is that we do not alter our representation in any way when using our sparse coding-based model either.</p><p>Note that features #1 through #5 in <ref type="table">Table 1</ref> operate at character-level, whereas our proposed framework solely uses features derived from the sparse coding of word forms. We thus distinguish two feature-rich baselines, i.e. FR w+c including both word and character-level features and FR w treating word forms as atomic units to derive features from.</p><p>Using dense word representations As our ultimate goal is to demonstrate the usefulness of sparse features derived from dense word representations, it is important to address the question of whether sparse word representations are more beneficial for sequence labeling tasks compared to their dense counterparts. To this end, we developed a similar model to the one proposed in Section 3, except for using the original dense word representations for inducing features.</p><p>According to this modification, we made the following change in our feature function: instead of calculating Equation <ref type="formula" target="#formula_1">(2)</ref> for some word i, the modified feature function we use for this baseline is</p><formula xml:id="formula_3">f (w i ) = {j : w i [j] | ∀j ∈ {1, . . . , k}}.</formula><p>That is, instead of relying on the nonzero values in α i , each word is characterized by its k real-valued coordinates in the embedding space. In order to notationally distinguish sparse and dense representations, we add subscript SC when we refer to a sparse coded version of some word embedding (e.g. SG SC ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">POS tagging experiments</head><p>Even though it is reasonable to assume that languages share a common coarse set of linguistic categories, linguistic resources had their own notations for part-of-speech tags. The first notable attempt to canonize the multiple tag sets was the Google universal part-of-speech tags introduced by <ref type="bibr" target="#b34">Petrov et al. (2012)</ref> in which the POS tags of various tagging schemes were mapped to 12 language-independent part-of-speech tags.</p><p>The recent initiative of universal dependencies (UD) <ref type="bibr" target="#b30">(Nivre, 2015)</ref> aims to provide a unified notation for multiple linguistic phenomena, including part-of-speech tags as well. The POS tag set proposed for UD has 17 categories which partially overlap with those defined by <ref type="bibr" target="#b34">Petrov et al. (2012)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experiments using CoNLL 2006/07 data</head><p>We use 12 treebanks in the CoNLL-X format from the CoNLL-2006/07 ( <ref type="bibr" target="#b7">Buchholz and Marsi, 2006;</ref><ref type="bibr" target="#b29">Nivre et al., 2007</ref>) shared tasks. The complete list of the treebanks included in our experiments is presented in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>We rely on the official scripts released by Petrov et al. <ref type="formula" target="#formula_0">(2012)</ref>    POS tags to the Google universal POS tags in order to obtain results comparable across languages.</p><p>For our experiments we used the original CoNLL-X train/test splits of the treebanks.</p><p>A key factor for the efficiency of our proposed model resides in the coverage of word embeddings, i.e. the proportion of tokens/word forms for which distributed representation is determined. <ref type="figure" target="#fig_0">Figure 1</ref> depicts these coverage scores calculated over the merged training and test sets for the different languages. <ref type="figure" target="#fig_0">Figure 1</ref> reveals that a substantial amount of tokens has distributed representation defined for (around 90% for the majority of languages, except for Turkish where it is 5 point less). Token coverages of the word embeddings are most likely affected by the morphological richness of the languages and the elaborateness of the corresponding Wikipedia articles used for training word embeddings. Comparing word embeddings Our motivation for choosing polyglot word embeddings as input to sparse coding is that they are publicly available for a variety of languages. However, distributed word representations trained in any other reasonable manner can serve as input to our approach. In order to investigate if some of the popular word embedding techniques seem favorable for our algorithm, we conduct experiments using alternatively trained embeddings, i.e. skip-gram (SG), continuous bagof-words (CBOW) and Glove. In order that the utility of different word embeddings not to be conflated with other factors, we train them on the same Wikipedia dumps used for training the polyglot word vectors. We choose further hyperparameters identically to polyglot, i.e. we train 64 dimensional dense word representations using a symmetric context window of size 2 for both SG/CBOW 6 and Glove <ref type="bibr">7</ref> .  <ref type="figure" target="#fig_1">Figure 2</ref> demonstrates that POS tagging performance is quite insensitive to the choice of λ unless it yields some extreme sparsity level (&gt;99.5%). <ref type="figure" target="#fig_1">Figure 2</ref>   The average tagging performance over the 12 languages when relying on features based on polyglot SC is only 1.3 points below that of F R w+c (i.e. 94.4 versus 95.7). Recall that F R w+c uses a feature-rich representation, whereas our proposed model uses only O(m) features, i.e. it is tied to the number of the basis vectors employed for sparse coding. Furthermore, our model does not employ word identity features, nor does it rely on character-level features of words.</p><p>Analyzing the effects of window size Hyperparameters for training word representations can greatly impact their quality as also concluded by <ref type="bibr" target="#b19">Levy et al. (2015)</ref>. We thus investigate if providing a larger context window size during the training of CBOW, SG and Glove embeddings can improve their performance in our model. According to <ref type="figure" target="#fig_3">Figure 3</ref> applying context window sizes of 2 for training the word embeddings tend to Comparing dense and sparse representations Unless stated otherwise, we use λ = 0.1 for the experiments below in accordance to <ref type="figure" target="#fig_1">Figure 2</ref>. Table 3 demonstrates that performances obtained by models using dense word representations as features are consistently inferior to those models relying on sparse word representations.</p><p>In <ref type="table" target="#tab_3">Table 3b</ref>, we can see that polyglot embeddings perform the best for dense representations as well. When using dense features, the CBOW representation-based model tends to produce results better than by a 1.4 points margin on average compared to SG embeddings. This performance gap between the two word2vec variants vanishes, however, when dense representations are replaced by their sparse counterparts.  Comparing the effects of training corpus size We also investigate the generalization characteristics of the proposed representation by training models that have access to substantially different amounts of training data per language. We distinguish three scenarios, i.e. when using only the first 150, the first 1,500 and all the available training sentences from each corpus. <ref type="figure">Figure 4</ref> illustrates the average POS tagging accuracy over the 12 CoNLL-X datasets for different amounts of training data and models. </p><formula xml:id="formula_4">n i=1 x i − Dα i 2 2 + λα i 1 + τ D 2 2 . (3)</formula><p>The main difference in Eq. 1 and 3 is that the latter does not explicitly constrain D to be a member of the convex set of matrices comprising of column vectors having a pre-defined upper bound on their norm. In order to implicitly control for the norms of the basis vectors </p><formula xml:id="formula_5">min D∈R k×m ≥0 α∈R k×|V | ≥0 1 2n n i=1 x i − Dα i 2 2 + λα i 1 + τ D 2 2 , (4)</formula><p>for which a non-negativity constraint on the elements of α (but no constraint on D) is imposed. When using the objective functions introduced by , we use the default τ = 10 −5 value. Notationally, we distinguish the sparse coding approaches based on the equation they use as their objective function, i.e. SC-i, i ∈ {1, 3, 4}.</p><p>We applied λ = 0.05 for SC-1 and λ = 0.5 for SC-3 and SC-4 in order to obtain word representations of comparable average sparsity levels across the 12 languages, i.e. 95.3%, 94.5% and 95.2%, respectively (cf. the left of <ref type="figure" target="#fig_5">Figure 5</ref>). The right of <ref type="figure" target="#fig_5">Fig- ure 5</ref> further illustrates the spread of POS tagging accuracies over the 12 CoNLL-X treebanks when using models that rely on different sparse coding strategies with comparable sparsity levels.</p><p>Although <ref type="bibr">Murphy et al. (2012)</ref> mentions nonnegativity as a desired property of word representations for cognitive plausibility, <ref type="figure" target="#fig_5">Figure 5</ref> reveals that our sequence labeling model cannot benefit from it as the average POS tagging accuracy for SC-4 is 0.7 points below that of SC-3 approach. The average performances when applying SC-1 and SC-3 are nearly identical with a 0.18 point difference between the two. It is instructive to analyze the patterns different sparse coding approaches exhibit. Even though the objective functions used by the different approaches are similar, decompositions obtained by them convey rather different sparsity structures. <ref type="figure">Figure 6a</ref> illustrates that there exist substantial variation in the length of the basis vectors obtained by SC-3 and SC-4 both within and across languages. However, SC-1 produces practically no variation in the length of the basis vectors comprising D due to the constraint present in the objective function it employs. <ref type="figure">Figure 6b</ref> shows similar differences about the relative frequency of basis vectors taking part in the reconstruction of word embeddings. <ref type="figure" target="#fig_6">Figure 7</ref> shows a strong correlation between the 2 norm of basis vectors and the relative number of times a non-zero coefficient is assigned to them in α for SC-3 and SC-4 but not for SC-1.</p><p>It can be further noted from <ref type="figure" target="#fig_6">Figure 7</ref>  of the basis vectors determined by SC-3 and SC-4 are often orders of magnitude larger than those determined by SC-1. This effect, however, can be naturally mitigated by increasing τ . Overall, the different approaches convey comparable POS tagging accuracies but different decompositions due to the differences in the objective functions they employ. Experiments described below are conducted using the objective function in Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experiments using UD treebanks</head><p>For POS tagging we also experiment with UD v1. <ref type="bibr">2 (Nivre et al., 2015)</ref> treebanks. We used the default train-test splits of the treebanks not utilizing the development sets for fine tuning performance on any of the languages during our experiments. We omitted the Japanese treebank as words in it are stripped off due to licensing issues. Also there is no polyglot vector released for Old Church Slavonic and Gothic. Even though polyglot word representations are released for Arabic, it was of no practical use as it contained unvocalized surface forms of tokens in contrast to the vocalized forms in UD v.1.2. For this reason, we discarded the Arabic treebank as less than 30% of its tokens could be associated with a representation. By omitting these 4 languages from our experiments we are finally left with 33 treebanks for 29 languages. We note that for Ancient Greek treebanks (grc*) we use word embeddings trained on Modern Greek.</p><p>We should add that there are 4 languages (related to 6 treebanks) for which polyglot word vectors are accessible, however, the Wikipedia dumps used for training them are not distributed. For this reason, Brown clustering-based baselines are missing for the affected treebanks.</p><p>We report our results on UD v1.2 in <ref type="table" target="#tab_0">Table 5</ref>. Recall that the default behavior of our sparse codingbased models (SC in <ref type="table" target="#tab_0">Table 5</ref>) is that they do not handle word identity as an explicit feature. We now investigate how much contribution word identity features convey on their own and also when used in conjunction with sparse coding-derived features. For this end we introduce a simple linear chain CRF model generating features solely on the identity of the current word and the ones surrounding it (WI in <ref type="table" target="#tab_0">Table 5</ref>). Likewise, we define a model that relies on WI and SC features simultaneously (WI+SC). Table 5 reveals that SC outperforms WI by a large margin and that combining the two feature sets together yields some further improvements over SC scores.</p><p>We also present in <ref type="table" target="#tab_0">Table 5</ref> the state-of-the-art results of the bidirectional LSTM models by <ref type="bibr" target="#b35">Plank et al. (2016)</ref> for comparative purposes. Note that the authors reported results only on a subset of UD v1.2 (i.e. treebanks with at least 60k tokens), for which reason we can include their results on 21 treebanks. Out of these 21 UD v1.2 treebanks there are 15 and 20 cases, respectively, for which SC and WI+SC produces better results than bi-LSTM w . Only FR w+c and bi-LSTM w+c , models which enjoy the additional benefit of employing character-level features besides word-level ones, are capable of outperforming SC and WI+SC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Named entity recognition experiments</head><p>Besides the POS tagging experiments, we investigated if the very same features as the ones applied for POS tagging can be utilized in a different sequence labeling task, namely named entity recognition. In order to evaluate our approach, we obtained the English, Spanish and Dutch datasets from the 2002 and 2003 CoNLL shared tasks on multilingual Named Entity Recognition <ref type="bibr" target="#b43">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b42">Tjong Kim Sang and De Meulder, 2003)</ref>.</p><p>We use the train-test splits provided by the or-   <ref type="figure" target="#fig_7">Figure 8</ref> includes our NER results obtained using different word embedding representations as input for sparse coding and different levels of sparsity. Similar to our POS tagging experiments, using polyglot SC vectors tend to perform best for NER as well. However, a substantial difference compared to the POS tagging results is that NER performances   do not degrade even for extreme levels of sparsity. Also, the sparse coding-based models perform much better when compared to the FR w+c baseline.</p><p>In <ref type="table" target="#tab_10">Table 6</ref>, we compare the effectiveness of models relying on sparse and dense word representations for NER. In order not to fine-tune hyperparameters for a particular experiment, similarly to our previous choices m and λ are set to 1024 and 0.1, respectively. Results in <ref type="table" target="#tab_10">Table 6</ref> are in line with those reported in <ref type="table" target="#tab_3">Table 3</ref> for POS tagging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we show that it is possible to train sequence models that perform nearly as well as best existing models on a variety of languages for both POS tagging and NER. Our approach does not require word identity features to perform reliably, furthermore, it is capable of achieving comparable results to traditional feature-rich models. We also illustrate the advantageous generalization property of our model as it retained 89.8% of its original average POS tagging accuracy when trained on only 1.2% of the total accessible training sentences.</p><p>As <ref type="bibr" target="#b25">Mikolov et al. (2013b)</ref> pointed out the similarities of continuous word embeddings across languages, we think that our proposed model could be employed not in just multi-lingual, but also in crosslingual language analysis settings. In fact, we investigate its feasibility in our future work. Finally, we have made the sparse coded word embedding vectors publicly available in order to facilitate the reproducibility of our results and to foster multilingual and cross-lingual research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Token and word form-level coverages of the word vectors against the combined train/test sets of the CoNLL-2006/07 POS tagging datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: POS tagging results on the CoNLL 2006/07 treebanks evaluating against universal POS tags. Ticks are placed for λ = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5. The x-axis shows the sparsity of the representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 includes</head><label>2</label><figDesc>POS tagging accuracies over the 12 treebanks from the CoNLL 2006/07 shared tasks evaluated against Google Universal POS tags. Instead of reporting results as a function of λ, we rather present accuracies as a function of the dif- ferent sparsity levels induced by different λ val- ues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of POS tagging accuracies over the 12 CoNLL-X datasets when relying on sparse coded versions of alternative word embeddings trained with context window size of 2 and 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Faruqui et al. (2015) apply an additional regularization term affected by an extra parameter τ in their objective function. Faruqui et al. (2015) also formulated a con- strained objective function of the form</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of the POS tagging accuracies of different sparse coding techniques with comparable average sparseness levels over the 12 CoNLL-X languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Relative frequency of basis vectors receiving nonzero coefficients in α as a function of their 2 norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: NER results relying on sparse coding of different word representations. The x-axis shows the sparsity of the representations with ticks at λ = 0.05, 0.1, 0.2, 0.3, 0.4, 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>5 for mapping the treebank specific</head><label>5</label><figDesc></figDesc><table>Language Source 
bg 
BTB/CoNLL06 (2005) 
da 
DDT/CoNLL06 (2004) 
de 
Tiger/CoNLL06 (2002) 
en 
Penn Treebank (1993) 
es 
Cast3LB/CoNLL06 (2008) 
hu 
Szeged Treebank/CoNLL07 (2005) 
it 
ISST/CoNLL07 (2003) 
nl 
Alpino/CoNLL06 (2002) 
pt 
Floresta Sint(c)tica/CoNLL06 (2002) 
sl 
SDT/CoNLL06 (2006) 
sv 
Talbanken05/CoNLL06 (2006) 
tr 
METU-Sabanci/CoNLL07 (2003) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Treebanks used for POS tagging experiments 
from the CoNLL 2006/07 shared task. 

bg da de en es hu it nl pt sl sv tr Avg. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Performances of sparse and dense word representations for POS tagging over the 12 CoNLL-X datasets. polyglot SC word representations tend to produce superior results over all alternative representations we experiment with. Furthermore, models using polyglot SC consistently outperform the FR w and Brown clustering-based baselines. Models relying on SG SC and CBOW SC represen- tations have an average tagging accuracy of 93.74 and 93.63, respectively, and they typically perform better than the baseline using Brown clustering with an average tagging performance of 93.27. Al- though utilizing Glove embeddings produce the low- est scores (91.92 on average), its scores still surpass those of the FR w baseline for all languages except for Turkish.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 also reveals that) Results obtained with different models when all the training corpora was used.</head><label>3</label><figDesc></figDesc><table>253 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Comparison of models based on different amount of training data. Bold numbers indicate the best results for a 
given training regime (i.e. either training on 150/1,500/all training sentences). polyglot SC uses m = 1024, λ = 0.1. 

sparse word representations improve average POS 
tagging accuracy by 3.3, 5.4, 6.7 and 10.4 points for 
polylgot, CBOW, SG and Glove word represen-
tations, respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 further</head><label>4</label><figDesc>reveals that the average perfor- mance of polyglot SC is 14.55 and 3.76 points better compared to the FR w and FR w+c baselines when using only 1.2% of all the available training data, i.e. 150 sentences per language. By discard- ing 98.8% of the training data polyglot SC obtains 89.8% of its average performance compared to the scenario when it has access to all the training sen- tences. However, under the same scenario the FR w+c and FR w models only manage to preserve 85% and 77% of their original performance, respectively. Our model performs on par with FR w+c and has a</figDesc><table>150 
1500 
all 

Traininggsentences 

0.60 

0.65 

0.70 

0.75 

0.80 

0.85 

0.90 

0.95 

1.00 

Accuracy 

FR w 
FR w + c 
polyglot SC 

Figure 4: Average tagging accuracies over the 12 
CoNLL-X languages using varying amount of training 
sentences. 

6.85 points advantage over FR w with a training cor-
pus of 1,500 sentences. FR w+c has an average of 1.3 
points advantage over polyglot SC when we pro-
vide access to all training data during training, nev-
ertheless FR w still underperforms polyglot SC in 
that setting by 3.67 points. 

Comparing sparse coding techniques Next, we 
compare different sparse coding approaches on the </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Per token POS tagging accuracies for 33 UD treebanks. For sparse coding SPAMS is used on polyglot 
vectors with λ = 0.1 and m = 1024. Results in bold are better than any of bi-LSTM w , FR w and Brown models 
(i.e. the baselines using features based on words only). Average is calculated over the 20 highlighted treebanks for 
which there are results in every column. The bi-LSTM results are from Plank et al. (2016). 

ganizers and report our NER results using the F1 
scores based on the official evaluation script of the 
CoNLL shared task. Similar to Collobert et al. 
(2011) we also apply the 17-tag IOBES tagging 
scheme during training and inference. The best 
F1 scores reported for English by Collobert et al. 
(2011) without employing additional unlabeled texts 
to enhance their language model is 81.47. When 
pre-training their neural language model on large 

amounts of Wikipedia texts they report an F1 score 
of 87.58. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Comparison of the performance of sparse and 
dense word representations for NER. 

</table></figure>

			<note place="foot" n="1"> http://spams-devel.gforge.inria.fr/ 2 https://github.com/percyliang/ brown-cluster 3 https://sites.google.com/site/rmyeid/ projects/polyglot</note>

			<note place="foot" n="4"> http://github.com/chokkan/crfsuite/ blob/master/example/pos.py</note>

			<note place="foot" n="5"> https://github.com/slavpetrov/ universal-pos-tags</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The author would like to thank the TACL editors and the anonymous reviewers for their valuable feedbacks and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Floresta sintá(c)tica&quot;: a treebank for Portuguese</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susana</forename><surname>Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eckhard</forename><surname>Bick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC)</title>
		<meeting>the 3rd International Conference on Language Resources and Evaluation (LREC)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1698" to="1703" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polyglot: Distributed word representations for multilingual NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The annotation process in the Turkish treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kemal</forename><surname>Atalay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilge</forename><surname>Oflazer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Say</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Linguistically Interpreted Corpora (LINC)</title>
		<meeting>the 4th International Workshop on Linguistically Interpreted Corpora (LINC)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Janvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The TIGER treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Dipper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lezius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Treebanks and Linguistic Theories</title>
		<meeting>the Workshop on Treebanks and Linguistic Theories</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="24" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classbased n-gram models of natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">J</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenifer</forename><forename type="middle">C</forename><surname>Della Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CoNLL-X shared task on multilingual dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Buchholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Marsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06</title>
		<meeting>the Tenth Conference on Computational Natural Language Learning, CoNLL-X &apos;06</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compressing neural language models by sparse word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="226" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning, ICML &apos;08</title>
		<meeting>the 25th International Conference on Machine Learning, ICML &apos;08</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Szeged Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dóra</forename><surname>Csendes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Csirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibor</forename><surname>Gyimóthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">András</forename><surname>Kocsor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech and Dialogue, 8th International Conference, TSD 2005 Proceedings</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="123" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards a Slovene dependency treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Chester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Bøgh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Language Resources and Evaluation Conference, LREC 2006</title>
		<meeting>the Fifth International Language Resources and Evaluation Conference, LREC 2006<address><addrLine>Nina Ledinek, Petr Pajas, ZdeněkZdeněkˇZdeněkŽabokrtsk´ZdeněkŽabokrtsk´y, and AndrejaŽeleAndrejaˇAndrejaŽele</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1388" to="1391" />
		</imprint>
	</monogr>
	<note>Proceedings of the International Conference Recent Advances in Natural Language Processing. European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building the Italian syntactic-semantic treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonetta</forename><surname>Montemagni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Building and using Parsed Corpora, Language and Speech series</title>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="189" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sparse overcomplete word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1491" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><forename type="middle">T</forename><surname>Kromann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Line</forename><surname>Mikkelsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stine Kern Lynge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Danish dependency treebank</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning, ICML &apos;01</title>
		<meeting>the Eighteenth International Conference on Machine Learning, ICML &apos;01</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word embeddings through Hellinger PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rehabilitation of count-based models for word vector representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Text Processing and Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="417" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised learning for natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised POS induction with word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Cheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1311" to="1316" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machinea Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of English: The Penn Treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>abs/1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno>abs/1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems, NIPS&apos;13</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning effective and interpretable semantic models using non-negative sparse embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>. Brian Murphy, Partha Talukdar, and Tom Mitchell</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1933" to="1950" />
		</imprint>
	</monogr>
	<note>The COLING 2012 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Talbanken05: A Swedish treebank with phrase structure and dependency annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC 2006)</title>
		<meeting>the Fifth International Conference on Language Resources and Evaluation (LREC 2006)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1392" to="1395" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The CoNLL 2007 shared task on dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Kübler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Yuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</title>
		<meeting>the CoNLL Shared Task Session of EMNLP-CoNLL</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="915" to="932" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards a Universal Grammar for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<ptr target="http://hdl.handle.net/11234/1-1548.LIN-DAT/CLARIN" />
	</analytic>
	<monogr>
		<title level="m">digital library at Institute of Formal and Applied Linguistics</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3" to="16" />
		</imprint>
		<respStmt>
			<orgName>Charles University in Prague. Joakim Nivre</orgName>
		</respStmt>
	</monogr>
	<note>Universal dependencies 1.2</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CRFsuite: a fast implementation of Conditional Random Fields (CRFs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Okazaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Improved part-of-speech tagging for online conversational text with word clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olutobi</forename><surname>Owoputi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="380" to="390" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A universal part-of-speech tagset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC-2012)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2089" to="2096" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="412" to="418" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Big data small data, in domain out-of domain, known word unknown word: The impact of word representations on sequence labelling tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Nineteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="83" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Design challenges and misconceptions in named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Computational Natural Language Learning, CoNLL &apos;09</title>
		<meeting>the Thirteenth Conference on Computational Natural Language Learning, CoNLL &apos;09</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">AnCora: Multilevel annotated corpora for Catalan and Spanish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariona</forename><surname>Taulé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antònia Martí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Recasens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC-08)</title>
		<meeting>the Sixth International Conference on Language Resources and Evaluation (LREC-08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
	<note>European Language Resources Association (ELRA</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extending the annotation of BulTreeBank: Phase 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiril</forename><surname>Simov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petya</forename><surname>Osenova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Fourth Workshop on Treebanks and Linguistic Theories (TLT 2005)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simple semisupervised POS tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="79" to="87" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sparse word embeddings using 1 regularized online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the TwentyFifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press / International Joint Conferences on Artificial Intelligence</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien De</forename><surname>Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
	<note>CONLL &apos;03</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL-2002</title>
		<meeting>CoNLL-2002</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="155" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Word representations: A simple and general method for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10</title>
		<editor>Daciuk, Tanja Gaustad, Robert Malouf, Gertjan van Noord, Robbert Prins, and Begoa Villada</editor>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL &apos;10<address><addrLine>Beek, Gosse Bouma</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="384" to="394" />
		</imprint>
	</monogr>
	<note>Algorithms for Linguistic Processing NWO PIONIER Progress Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning word representations with hierarchical sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
