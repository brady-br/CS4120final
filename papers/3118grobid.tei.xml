<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T03:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Heisenberg Measuring Uncertainty in Lightweight Virtualization Testbeds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Jia</surname></persName>
							<email>qjia@gmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
								<address>
									<settlement>Fairfax</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Wang</surname></persName>
							<email>zwange@gmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
								<address>
									<settlement>Fairfax</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Stavrou</surname></persName>
							<email>astavrou@gmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">George Mason University</orgName>
								<address>
									<settlement>Fairfax</settlement>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Heisenberg Measuring Uncertainty in Lightweight Virtualization Testbeds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The need for large-scale experimentation testbeds involving several hundred, or even thousands, of nodes is undeniable. Testbeds including Emulab [10], and Deter [5] are heavily used for both research and application testing. To scale even further and shed some of the limitations that the relative small number of physical nodes impose , researchers have turned to full virtualization [8] and lightweight, container-based virtualization [16, 10]. Vir-tualization allows running multiple virtual execution environments (VEEs) per physical host. In this paper, we evaluate the use of hundreds of lightweight containers as a testbed to measure the performance of simple applications. We show that, although economically and technically compelling, virtu-alization has some limitations due the sharing of host resources (CPU, network, memory and disk) among same-host VEEs. Determining the number of VEEs that can be deployed in a physical machine without interfering with the fidelity of the experiment is not a trivial task and it cannot be estimated or computed ahead of time using aggregate utilization of individual resource. Furthermore, monitoring the health of an experiment by measuring the individual resource utilization can affect the behavior of the service under test. Therefore, we observed what we call a &quot;weak&quot; form of the Heisenberg uncertainty principle for host resource measurements: increasing the precision and fidelity of the resource measurements can interfere with the behavior of the experiment. We believe that this observation holds in general but it becomes more pronounced when we instantiate hundreds of VEEs due to the necessary context switching.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, the overwhelming majority of applications and services are being performed by large-scale distributed software systems. Being able to emulate the scalability, quality of service, fault tolerance, security properties, and steady state behavior of such planetary size systems ahead of their deployment is highly desirable. Testbeds including Emulab <ref type="bibr" target="#b9">[10]</ref>, and Deter <ref type="bibr" target="#b4">[5]</ref> offer researchers and practitioners the only viable platforms to test their ideas and application beyond custom built corporate clusters.</p><p>Although a step forward, current testbed platforms fall short when it comes to scaling to tens or even hundreds thousands of application or services instances. To address this shortcoming and to allow existing testbed infrastructure to scale even further, researchers have turned to virtualization technologies <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>. Indeed, Virtual Execution Environments (VEEs) are an economical way of scaling beyond the limitations imposed by the relative small number of physical nodes. VEEs can dramatically increase the perceived number of application or service instances by one or two orders of magnitude depending on the available host resources and the type of virtualization.</p><p>In this paper, we study the advantages in terms of scalability but also the limitation from the use of hundreds of lightweight containers as a performance evaluation testbed. We do so by performing a range of simple experiments on a single but relatively powerful host. Although superior in terms of scalability, lightweight virtualization has itself limitations: determining the optimal number of VEEs that can be accommodated in the single host without interfering with the fidelity of the experiment is not a trivial task. Moreover, we show that it is infeasible to compute the VEE capacity of the underlying hardware based on static or even aggregate measurements of host resources (CPU, network, memory, and disk I/O).</p><p>Initially, we attributed this result to the lack of precise, per-VEE measurements that would enable us to effectively monitor the behavior of the application in each of the containers. However, in our effort to increase the fidelity of our measurement collection infrastructure to track the health of each individual VEE, we run into another limitation: increasing the frequency of measurement can decrease the number of concurrent containers we can utilize without interfering with the performance of the experiment itself. Therefore, we observed what we call a "weak" form of the Heisenberg uncertainty principle for host resource measurements: increasing the precision and fidelity of the resource measurements can interfere with the behavior of the experiment, reducing the number of "usable" VEEs. Although uncertainty principle was originated from physics, a system that has to share its resources among many different tasks can exhibit the same behavior. Therefore, we believe that our results holds for any process and any system but it becomes more pronounced for a testbed with hundreds of VEEs due to the necessary context switching among the different CPU tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Testbed System Architecture</head><p>Our initial goal was to setup a single machine testbed where we use Virtual Execution Environments (VEEs) to perform experiments. We were interested in generating experimental scenarios that would be able to scale to potentially thousands of VEE instances. We would like to do so without exceeding the available host resources. With host resources, we refer to CPU, network, memory and disk for our Dell PowerEdge 1950 server equipped with two Quad-Core Intel Xeon E5430 (2.66GHz) processors, 8GB RAM and Gigabit Ethernet NIC. The overall system architecture is illustrated in <ref type="figure" target="#fig_0">figure 1</ref>. For the purpose of our experiments, we used OpenVZ <ref type="bibr" target="#b15">[15]</ref> (kernel patch) version: ovz009.1 on a vanilla Linux kernel version 2.6.24 to instantiate the VEEs. However, any other lightweight container-based systems such as VServers <ref type="bibr" target="#b16">[16]</ref> would suffice and the produced results are not dependent on the specific container solution.</p><p>To lower the disk and memory requirements, Unionfs <ref type="bibr" target="#b23">[23]</ref> plays a critical part in our system. It is a stackable filesystem service which "merges several directories into a single unified view" from each containers' point of view. There are three major advantages of using Unionfs in our system. First, instead of creating a separate copy of filesystem for each container, we only create one template filesystem and share it among all containers. This results in tremendous disk space savings which enables to scale to thousands of simultaneous containers instances in a single machine. Second, Unionfs facilitates memory sharing. When the same user program is running in multiple containers with Unionfs mounted filesystem, its binary image would be loaded only once in memory. Thus, we have automatic savings both in terms of memory access time and space allocation. Third, any changes in system configuration and software is quickly propagated to the containers through modifications on the base template. The base template is mounted to each container as read-only root "/" while a write-enabled slice is mounted on top of the root allowing each container to store its state in a separate directory on the host.</p><p>Except the use of Unionfs, our testbed was composed   <ref type="table">Table 2</ref>: centos-4-i386-minimal container template process memory consumption in kB from out-of-the-box software components. Our which, at first glance, would have enabled the quick estimation of the required resources through both static and dynamic (on-line) analysis. We present this analysis in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Estimating Resource Utilization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pre-experiment Resource Computation</head><p>In order to roughly determine the number of VEEs that can be held in our testbed, we want to estimate the system resource utilization before carrying out any experiment. In our testbed, the size of the Linux operating system is 371MB. Therefore, if we need to instantiate N containers without using UnionFS, they would require a total of 371*N MB. However, with UnionFS, we only require one copy dramatically decreasing the amount of required disk space. This is a significant difference especially for high density testbed systems since such system usually run exactly the same copy of the programs and configurations. Due to the copy-on-write slices, the disk usage for each container also remains small.</p><p>Lightweight virtualization has a small memory footprint which allows to save the state of each container faster and inexpensive in terms of memory overhead. After bootstrapping the containers, there is a small number system processes running. Of course, the number of initial processes depends on the system configuration. In tables 1 and 2 depict the number of processes and their memory utilization.</p><p>These tables allow us to measure how much resident physical memory pages are actually used by a single container. For example, for ubuntu 8.04 guest OS template to populate the containers, 3 basic process are launched, and they are allocated with 2264kB shared memory pages and 912kB non-shared pages. Using a physical memory analysis tool <ref type="bibr" target="#b13">[13]</ref>, we notice that the shared pages are not only shared within container (the .so lib files within the same container), but also across the containers. The code analysis shows that the inode numbers of the shared files remain identical from the kernel's perspective: when kernel performs the ELF loading, kernel will map the same binary file with identical inode number to the same memory region.</p><p>Using this, we can derive the simple formula to compute the memory consumption per container:</p><p>The estimate memory requirements for C containers is: M = 912 · C + 2264 (in kB) Thus, the Memory consumption per container is:</p><p>912 + 2264 C (in kB) where C is the number of containers launched. The second part of the above formula indicates that the shared memory page consumption is not actually linear: the more containers launched, the less it pays for shared memory. The container based virtualization only needs 3.1MB memory for a basic running container.</p><p>Unfortunately, the pre-computation of resources for each VEE can only be applied for memory and disk space, to calculate the CPU and network requirements, we have to rely on the run-time analysis of the running application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Runtime Evaluation of Concurrent VEEs</head><p>To quantify the number of VEEs that a single host testbed can support, we had to perform a set of controlled experiments using a variable number of VEEs and measure our capability to estimate the number of VEEs that the underlying hardware can support. To alleviate measurement errors due to heterogeneous load and resource allocation requirements, we chose to run the the same application inside all VEEs. In addition, we used a light monitoring process running outside the containers (i.e.on the host) to sample the real-time values of resources including network throughput, CPU and, memory utilization for each of the VEEs. Our initial goal was to identify the number of containers we can execute in parallel on the same host without depleting the host resources or otherwise contaminate the obtained experimental and performance results. To that end, we implemented a monitoring process that depends on the Linux proc file system <ref type="bibr" target="#b1">[2]</ref>. This monitoring process was performing periodic reading of the resources status using the available "/proc" entries. To avoid erroneous measurements, we used a real-time kernel scheduler setting to optimize the performace of our measuring process. We caught network data from file "/proc/dev/net" within the directory of each container under "/vz/root". Container CPU usage was obtained from "/proc/vz/vestat" on the host while memory utilization was calculated from "/proc/user beancounters".</p><p>Initially, we set our measuring sampling interval to one measurement per three seconds and we started GNU wget in each container. Wget is a simple console application designed to retrieve files using HTTP protocol among others. To prevent initial resource contention, we initiated wget in randomized intervals ranging from ten (10) to twenty (20) seconds. Each instance downloaded several web pages approximately 400 KB in size from an Apache web server hosted on another physical machine in local network. The capacity of the web server was configured to 4000 requests, which was never attained in our experiments, meaning that the bottleneck was on the client side.</p><p>We selected wget as our application for our performance evaluation because it is not CPU or memory intensive but rather exercises the network and disk I/O. In addition, the network behavior of wget is very sensitive to scarcity of resources and thus provides an easy aggregate measurement for the overall system performance. We performed the same experiment on groups of 100, 200, 400, 600, 800, 1000, 1200 and 1400 VEEs, and collected the real-time system network throughput, CPU and memory usage. The completion time for each container was recorded in memory and the averages are calculated after the end of each experiment. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates our experimental results using the aggregate resource measurement. For each of the graphs, we measured the resource values using our real-time measuring process every three-second intervals and for the entire duration of the experiment. Our aim was to identify a resource that would show signs of depletion or otherwise indicate that the number of VEEs is not sustainable. As a ground-truth for the actual concurrent VEE capacity of the host, we measured the completion time for each experiment. If the VEEs do not interfere with each other in any way, the experiment would finish at approximately that same time.</p><p>Our initial expectation was that we would be able to pinpoint the number of VEEs that we can scale to without losing precision by just looking at the aggregate resource utilization for wget and for a varying number of containers. Indeed, although there is a noticeable change in the experiment completion time for approximately 400 containers, the rest of the resources appear to be nominal and certainly within their installed capacity. The completion time for each container should be approximately 150 seconds. It is not until we reach 600 concurrent VEEs that we notice that the network throughput reaches a maximal point and the completion time is well above 200 seconds, almost a 25% increase over the value for 400 VEEs. Therefore, we are unable to estimate the number of concurrent VEEs that the host can support by just observing the aggregate measurements of the utilization of the primary host resources. Also, while for wget the increase in completion time might appear satisfactory or even desirable since there is an increase in network throughput, for latency-sensitive applications it might produce erroneous results.</p><p>Interpreting the VEE scalability results as a deficiency of our measuring methodology, we decided to observe the experiment more closely and decide if there is a resource scarcity not based on aggregates but rather peak values of individual resources in any of the VEEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Heisenmeasure Uncertainty</head><p>Convinced that the context switching between containers and short term peaks in CPU and network utilization was the cause of the increase of the overall experimental time, we decided to increase the frequency of resource sampling and to keep individual and not aggregate measurements for each containers. To that end, our monitoring process would open the individual proc structures for each of the containers and traverse them at regular intervals every 0.1, 0.01, 0.005 and 0.001 seconds. Since applications in our experiment can be affected by a lot of different resources, we measured all host resources for completeness. We stored the individual measurements in memory and we performed statistical analysis after the completion of the experiment. As a side-note, it is worth mentioning that it is imperative to assign a real-time scheduling policy to the monitoring process. Otherwise, it would fail to run on time producing erroneous results.</p><p>We performed the same experiments as before but with different number of containers, from 100 to 700. We also reduced the initial calling interval of wget to one (1) to ten (10) seconds. For every number of containers, the same experiment is done with different sampling intervals (0.1, 0.01, 0.005 and 0.001 seconds). We were aiming in identifying utilization peaks within a small window of time and estimate when the experimental results become corrupted due to lack of readily available resources. Therefore, by carefully monitoring the health of each VEE we should be able to detect "anomalies" in resource consumption and potentially react in a dynamic fashion. However, our results show frequent, or measurement collected on a small window of time, can adversely affect the available resources for the application under test. In essence, we observed what we call a "weak" form of the Heisenberg uncertainty principle for host resource measurements: increasing the precision and fidelity of the resource measurements can interfere with the behavior of the experiment.</p><p>The completion times for different values of measurement frequencies (0.1-0.001 seconds) is shown in <ref type="figure" target="#fig_2">Fig- ure 3</ref>. Each of the graph lines corresponds to a different sampling intervals. Frequently polling the resource values for each VEE can interfere with the performance of the experiment and increase its completion time. The more frequent the measurements, the smaller the number of containers we can instantiate without affecting the overall experiment. Observing the measurement of the CPU in <ref type="figure" target="#fig_3">Figure 4</ref> is not helpful in determining the "breaking" point for each polling interval. Indeed, we are unable to determine accurately when the experiment is affected just by looking at <ref type="figure" target="#fig_3">Figure 4</ref>: there is a linear CPU increase even for 500 containers and for 0.01 second interval whereas there is a significant discrepancy in the experiment completion time. Unlike CPU, measuring the network utilization is a good measure of when the experiment is affected by our measurement. In <ref type="figure" target="#fig_4">Figure 5</ref>, we can see that the number of containers that can be supported for each of the polling intervals is pronounced: 0.1 seconds can support up to 500 containers, 0.01 can sustain up to 300 containers, and 0.001 cannot go beyond 100 containers. Although our observation holds in principle for any application running in a host, it becomes more pronounced for a system running hundreds of VEEs concurrently. We believe that this is due to the necessary context switching among the different CPU tasks and it imposes a fundamental limitation on the number of VEEs that we can instantiate and use for testing without interfering with the fidelity of the application. The use of an event-driven VEE scheduling for each of the resources including CPU, I/O, and networking would alleviate this allowing us to scale without losing fidelity both in terms of resources and in terms of measuring the health of the experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Computer virtualization was originally introduced as a product by IBM in 1975 <ref type="bibr" target="#b2">[3]</ref>. In recent years it has been revitalized mainly due to the technology introduced by com- . Merely examining this graph we are unable to determine accurately when the experiment is affected: there is a linear CPU increase even for 500 containers and for 0.01 second interval whereas the completion time shows a significant difference. panies and the open source community including VMware <ref type="bibr" target="#b17">[17]</ref>, Xen <ref type="bibr" target="#b3">[4]</ref> , User Mode Linux <ref type="bibr" target="#b6">[7]</ref>, KVM <ref type="bibr" target="#b0">[1]</ref> and VirtualBox <ref type="bibr" target="#b21">[21]</ref>. A virtualization approach that provides more efficiency is the OS-level virtualization. This approach has been implemented in several operating systems such as BSD <ref type="bibr" target="#b11">[11]</ref>, Solaris <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b12">12]</ref> and Linux <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16]</ref>. Virtualization has been employed in the past for building network emulation system <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> and provide live migration capabilities <ref type="bibr" target="#b20">[20]</ref>. Traditional approach to testing large-scale and multitiered network topologies and applications is currently done through simulation models that cannot capture all aspects of system behavior, especially scalability properties and failures.Therefore, simulation cannot substitute real system experimentation and fails to capture software bugs and configuration errors. In addition, the transient state of applications and unexpected component interactions remain unexplored. On the other hand, emulation techniques including Emulab <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b9">10]</ref>, and ModelNet <ref type="bibr" target="#b19">[19]</ref>) offer contained execution environments that use unmodified applications and operating systems. Unfortunately, the scalability of the current testbeds is limited by the number of physical nodes. We cannot expect to see a dramatic increase of the aforementioned testbed to tens or even hundreds of thousands of nodes. For example, emulating 10000 instances of a botnet or a virus propagation using 250 physical machines is currently beyond the capabilities of the most advanced testbeds.</p><p>There is a wealth of previous research on dynamic provision of services <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">18]</ref> or to efficient load balancing <ref type="bibr" target="#b5">[6]</ref>. However, these efforts typically target already running services and they affect the fidelity of the experiment. This is due to the use of full virtualization which does not scale as efficiently as process containers and does not expose the entire stack of driver code to the application. Moreover, they do not offer a solution to resource utilization peaks but rather to initial provision of services <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we analyze the experimental scalability and fidelity limitations when employing lightweight virtualization as testbed environment to measure the performance of simple, large-scale applications. To that end, we show that it is not a trivial task to determine the maximum number of VEEs that can be run concurrently in a physical machine without perturbing the experimental outcome.</p><p>Furthermore, our efforts to monitor the health of each VEE have uncovered a "weak" form of Heisenberg uncertainty principle in measuring lightweight virtualization. The desirable accuracy of measurement is largely dependent on the high sampling frequency, which potentially deprives the containers of available resources and adversely interferes with the experiment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Lightweight Virtual Execution Environment (VEE) single-host testbed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Aggregate resource utilization for wget and for a varying number of containers. Notice that although there is a noticeable change in the experiment completion time for approximately 400 containers, the rest of the resources appear to be nominal. It is not until running 600 concurrent VEEs that the network throughput reaches a maximal point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Completion times when we vary and the number of containers. Each line corresponds to different sampling intervals (0.1-0.001 seconds). Measuring the resource utilization for the VEEs can affect the experimental completion time. The more frequent the measurements, the less containers we can instantiate without interfering with the experiment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: CPU utilization for each container and for different measuring windows (0.1-0.001 seconds). Merely examining this graph we are unable to determine accurately when the experiment is affected: there is a linear CPU increase even for 500 containers and for 0.01 second interval whereas the completion time shows a significant difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Network utilization clearly indicates the number of VEEs that can be supported when sampling frequency can affect the health of the overall experiment.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Kvm: Kernel-based virtual machine system for linux</title>
		<ptr target="http://kvm.sourceforge.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">Linux kernel internals</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Specialized Systems Consultants, Inc</publisher>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sharing data and services in a virtual machine system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Bagley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Floto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP 1975</title>
		<meeting>SOSP 1975<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1975" />
			<biblScope unit="page" from="82" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOSP 2003</title>
		<meeting>SOSP 2003<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experience with deter: A testbed for security research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Benzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Braden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sklower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TRIDENTCOM</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quorum: flexible quality of service for internet services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Blanquer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batchelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wolski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NSDI 2005</title>
		<meeting>NSDI 2005<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="159" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A user-mode port of the linux kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual Linux Showcase and Conference</title>
		<meeting>the 5th Annual Linux Showcase and Conference<address><addrLine>Oakland, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-based resource provisioning in a web service utility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Asad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Usenix USITS 2003</title>
		<meeting>Usenix USITS 2003<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="5" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diecast: testing distributed systems with an accurate scale model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Usenix NSDI 2008</title>
		<meeting>Usenix NSDI 2008<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="407" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large-scale virtualization in the emulab network testbed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guruprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lepreau</surname></persName>
		</author>
		<idno>ATC&apos;08: USENIX 2008</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<title level="m">Annual Technical Conference on Annual Technical Conference</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="113" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Using Jails in FreeBSD for fun and profit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hope</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Solaris Containers What They Are and How to Use Them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lageman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Solutions</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="819" to="2679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linux physical memory analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Movall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ATC Usenix ATC 2005</title>
		<meeting>ATC Usenix ATC 2005<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="39" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Solaris zones: Operating system support for consolidating commercial workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LISA 2004</title>
		<meeting>LISA 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Virtuoso: A system for virtual machine marketplaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoykhet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoykhet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dinda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dinda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Container-based operating system virtualization: a scalable, high-performance alternative to hypervisors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soltesz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pötzl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Fiuczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;07</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="275" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Virtualizing i/o devices on vmware workstation&apos;s hosted virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sugerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkitachalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX ATC 2005</title>
		<meeting>USENIX ATC 2005<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Resource overbooking and application profiling in shared hosting platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI 2002</title>
		<meeting>OSDI 2002<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="239" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalability and accuracy in a largescale network emulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yocum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kosti´ckosti´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI 2002</title>
		<meeting>OSDI 2002<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="271" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virtual routers on the move: live router migration as a network-management primitive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biskeborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Merwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIG-COMM &apos;08</title>
		<meeting>SIG-COMM &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="231" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Virtualbox: bits and bytes masquerading as machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Linux J</title>
		<imprint>
			<biblScope unit="volume">2008</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An integrated experimental environment for distributed systems and networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lepreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guruprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joglekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of OSDI &apos;02</title>
		<meeting>OSDI &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel korner: unionfs: bringing filesystems together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Linux J</title>
		<imprint>
			<biblScope unit="volume">2004</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
