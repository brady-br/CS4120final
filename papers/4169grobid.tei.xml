<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 15-17. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Yuan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Usa</forename><forename type="middle">;</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heqing</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">Xiaofeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">University</forename><surname>Bloomington</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejing</forename><surname>Yuan</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Chen</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Florida Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Long</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Liu</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengzhi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Florida Institute of Technology</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff8">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Metropolitan College</orgName>
								<orgName type="institution" key="instit2">Boston University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heqing</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff9">
								<orgName type="department">School of Informatics and Computing</orgName>
								<orgName type="institution">Indiana University</orgName>
								<address>
									<settlement>Bloomington</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
							<affiliation key="aff7">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Information Engineering</orgName>
								<orgName type="department" key="dep2">School of Cyber Security</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Yuxuan Chen</orgName>
								<orgName type="institution" key="instit5">Florida Institute of Technology</orgName>
								<address>
									<addrLine>Yue Zhao</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Institute of Information Engineering</orgName>
								<orgName type="department" key="dep2">School of Cyber Security</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Yunhui Long</orgName>
								<orgName type="institution" key="instit5">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<addrLine>Xiaokang Liu and Kai Chen</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Information Engineering</orgName>
								<orgName type="department" key="dep2">School of Cyber Security</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">SKLOIS</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit3">University of Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit4">Shengzhi Zhang</orgName>
								<orgName type="institution" key="instit5">Florida Institute of Technology</orgName>
								<orgName type="institution" key="instit6">Metropolitan College</orgName>
								<orgName type="institution" key="instit7">Boston University</orgName>
								<address>
									<settlement>Indiana</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 27th USENIX Security Symposium</title>
						<meeting>the 27th USENIX Security Symposium <address><addrLine>Baltimore, MD, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 15-17. 2018</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX. This paper is included in the https://www.usenix.org/conference/usenixsecurity18/presentation/yuan-xuejing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The popularity of automatic speech recognition (ASR) systems, like Google Assistant, Cortana, brings in security concerns, as demonstrated by recent attacks. The impacts of such threats, however, are less clear, since they are either less stealthy (producing noise-like voice commands) or requiring the physical presence of an attack device (using ultrasound speakers or transducers). In this paper, we demonstrate that not only are more practical and surreptitious attacks feasible but they can even be automatically constructed. Specifically, we find that the voice commands can be stealthily embedded into songs, which, when played, can effectively control the target system through ASR without being noticed. For this purpose, we developed novel techniques that address a key technical challenge: integrating the commands into a song in a way that can be effectively recognized by ASR through the air, in the presence of background noise, while not being detected by a human listener. Our research shows that this can be done automatically against real world ASR applications 1. We also demonstrate that such CommanderSongs can be spread through Internet (e.g., YouTube) and radio, potentially affecting millions of ASR users. Finally we present mitigation techniques that defend existing ASR systems against such threat.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Intelligent voice control (IVC) has been widely used in human-computer interaction, such as Amazon Alexa <ref type="bibr" target="#b0">[1]</ref>, Google Assistant <ref type="bibr" target="#b5">[6]</ref>, Apple Siri <ref type="bibr" target="#b2">[3]</ref>, Microsoft Cortana <ref type="bibr" target="#b9">[14]</ref> and iFLYTEK <ref type="bibr">[11]</ref>. Running the state-ofthe-art ASR techniques, these systems can effectively interpret natural voice commands and execute the corresponding operations such as unlocking the doors of * Corresponding author: chenkai@iie.ac.cn <ref type="bibr" target="#b0">1</ref> Demos of attacks are uploaded on the website (https://sites.google.com/view/commandersong/) home or cars, making online purchase, sending messages, and etc. This has been made possible by recent progress in machine learning, deep learning <ref type="bibr" target="#b26">[31]</ref> in particular, which vastly improves the accuracy of speech recognition. In the meantime, these deep learning techniques are known to be vulnerable to adversarial perturbations <ref type="bibr" target="#b32">[37,</ref><ref type="bibr" target="#b16">21,</ref><ref type="bibr" target="#b22">27,</ref><ref type="bibr" target="#b20">25,</ref><ref type="bibr" target="#b15">20,</ref><ref type="bibr" target="#b44">49,</ref><ref type="bibr" target="#b23">28,</ref><ref type="bibr" target="#b39">44]</ref>. Hence, it becomes imperative to understand the security implications of the ASR systems in the presence of such attacks.</p><p>Threats to ASR Prior research shows that carefullycrafted perturbations, even a small amount, could cause a machine learning classifier to misbehave in an unexpected way. Although such adversarial learning has been extensively studied in image recognition, little has been done in speech recognition, potentially due to the new challenge in this domain: unlike adversarial images, which include the perturbations of less noticeable background pixels, changes to voice commands often introduce noise that a modern ASR system is designed to filter out and therefore cannot be easily misled.</p><p>Indeed, a recent attack on ASR utilizes noise-like hidden voice command <ref type="bibr" target="#b17">[22]</ref>, but the white box attack is based on a traditional speech recognition system that uses a Gaussian Mixture Model (GMM), not the DNN behind today's ASR systems. Another attack transmits inaudible commands through ultrasonic sound <ref type="bibr" target="#b48">[53]</ref>, but it exploits microphone hardware vulnerabilities instead of the weaknesses of the DNN. Moreover, an attack device, e.g., an ultrasonic transducer or speaker, needs to be placed close to the target ASR system. So far little success has been reported in generating "adversarial sound" that practically fools deep learning technique but remains inconspicuous to human ears, and meanwhile allows it to be played from the remote (e.g., through YouTube) to attack a large number of ASR systems.</p><p>To find practical adversarial sound, a few technical challenges need to be addressed: (C1) the adversarial audio sample is expected to be effective in a complicated, real-world audible environment, in the presence of elec-tronic noise from speaker and other noises; (C2) it should be stealthy, unnoticeable to ordinary users; (C3) impactful adversarial sound should be remotely deliverable and can be played by popular devices from online sources, which can affect a large number of IVC devices. All these challenges have been found in our research to be completely addressable, indicating that the threat of audio adversarial learning is indeed realistic.</p><p>CommanderSong. More specifically, in this paper, we report a practical and systematic adversarial attack on real world speech recognition systems. Our attack can automatically embed a set of commands into a (randomly selected) song, to spread to a large amount of audience (addressing C3). This revised song, which we call CommanderSong, can sound completely normal to ordinary users, but will be interpreted as commands by ASR, leading to the attacks on real-world IVC devices. To build such an attack, we leverage an open source ASR system Kaldi <ref type="bibr" target="#b8">[13]</ref>, which includes acoustic model and language model. By carefully synthesizing the outputs of the acoustic model from both the song and the given voice command, we are able to generate the adversarial audio with minimum perturbations through gradient descent, so that the CommanderSong can be less noticeable to human users (addressing C2, named WTA attack). To make such adversarial samples practical, our approach has been designed to capture the electronic noise produced by different speakers, and integrate a generic noise model into the algorithm for seeking adversarial samples (addressing C1, called WAA attack).</p><p>In our experiment, we generated over 200 CommanderSongs that contain different commands, and attacked Kaldi with an 100% success rate in a WTA attack and a 96% success rate in a WAA attack. Our evaluation further demonstrates that such a CommanderSong can be used to perform a black box attack on a mainstream ASR system iFLYTEK 2 <ref type="bibr">[11]</ref> (neither source code nor model is available). iFLYTEK has been used as the voice input method by many popular commercial apps, including WeChat (a social app with 963 million users), Sina Weibo (another social app with 530 million users), JD (an online shopping app with 270 million users), etc. To demonstrate the impact of our attack, we show that CommanderSong can be spread through YouTube, which might impact millions of users. To understand the human perception of the attack, we conducted a user study 3 on Amazon Mechanical Turk <ref type="bibr" target="#b1">[2]</ref>. Among over 200 participants, none of them identified the commands inside our CommanderSongs. We further developed the defense solutions against this attack and demonstrated their effectiveness. <ref type="bibr" target="#b1">2</ref> We have reported this to iFLYTEK, and are waiting for their responses. <ref type="bibr" target="#b2">3</ref> The study is approved by the IRB.</p><p>Contributions. The contributions of this paper are summarized as follows:</p><p>• Practical adversarial attack against ASR systems. We designed and implemented the first practical adversarial attacks against ASR systems. Our attack is demonstrated to be robust, working across air in the presence of environmental interferences, transferable, effective on a black box commercial ASR system (i.e., iFLYTEK) and remotely deliverable, potentially impacting millions of users.</p><p>• Defense against CommanderSong. We design two approaches (audio turbulence and audio squeezing) to defend against the attack, which proves to be effective by our preliminary experiments.</p><p>Roadmap. The rest of the paper is organized as follows: Section 2 gives the background information of our study. Section 3 provides motivation and overviews our approach. In Section 4, we elaborate the design and implementation of CommanderSong. In Section 5, we present the experimental results, with emphasis on the difference between machine and human comprehension. Section 6 investigates deeper understanding on CommanderSongs. Section 7 shows the defense of the CommanderSong attack. Section 8 compares our work with prior studies and Section 9 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we overview existing speech recognition system, and discuss the recent advance on the attacks against both image and speech recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Speech Recognition</head><p>Automatic speech recognition is a technique that allows machines to recognize/understand the semantics of human voice. Besides the commercial products like Amazon Alexa, Google Assistant, Apple Siri, iFLYTEK, etc., there are also open-source platforms such as Kaldi toolkit <ref type="bibr" target="#b8">[13]</ref>, Carnegie Mellon University's Sphinx toolkit <ref type="bibr" target="#b4">[5]</ref>, HTK toolkit <ref type="bibr">[9]</ref>, etc. <ref type="figure" target="#fig_0">Figure 1</ref> presents an overview of a typical speech recognition system, with two major components: feature extraction and decoding based on pre-trained models (e.g., acoustic models and language models).</p><p>After the raw audio is amplified and filtered, acoustic features need to be extracted from the preprocessed audio signal. The features contained in the signal change significantly over time, so short-time analysis is used to evaluate them periodically. Common acoustic feature extraction algorithms include Mel-Frequency Cepstral Coefficients (MFCC) <ref type="bibr" target="#b35">[40]</ref>, Linear Predictive Coefficient (LPC) <ref type="bibr" target="#b29">[34]</ref>, Perceptual Linear Predictive (PLP) <ref type="bibr" target="#b25">[30]</ref>, etc. Among them, MFCC is the most frequently used one in both open source toolkit and commercial products <ref type="bibr" target="#b37">[42]</ref>.</p><p>GMM can be used to analyze the property of the acoustic features. The extracted acoustic features are matched against pre-trained acoustic models to obtain the likelihood probability of phonemes. Hidden Markov Models (HMM) are commonly used for statistical speech recognition. As GMM is limited to describe a non-linear manifold of the data, Deep Neural Network-Hidden Markov Model (DNN-HMM) has been widely used for speech recognition in academic and industry community since 2012 <ref type="bibr" target="#b27">[32]</ref>.</p><p>Recently, end-to-end deep learning becomes used in speech recognition systems. It applies a large scale dataset and uses CTC (Connectionist Temporal Classification) loss function to directly obtain the characters rather than phoneme sequence. CTC locates the alignment of text transcripts with input speech using an allneural, sequence-to-sequence neural network. Traditional speech recognition systems involve many engineered processing stages, while CTC can supersede these processing stages via deep learning <ref type="bibr" target="#b12">[17]</ref>. The architecture of end-toend ASR systems always includes an encoder network corresponding to the acoustic model and a decoder network corresponding to the language model <ref type="bibr" target="#b42">[47]</ref>. DeepSpeech <ref type="bibr" target="#b12">[17]</ref> and Wav2Letter <ref type="bibr" target="#b19">[24]</ref> are popular open source end-to-end speech recognition systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Existing Attacks against Image and Speech Recognition Systems</head><p>Nowadays people are enjoying the convenience of integrating image and speech as new input methods into mobile devices. Hence, the accuracy and dependability of image and speech recognition pose critical impact on the security of such devices. Intuitively, the adversaries can compromise the integrity of the training data if they have either physical or remote access to it. By either revising existing data or inserting extra data in the training dataset, the adversaries can certainly tamper the dependability of the trained models <ref type="bibr" target="#b33">[38]</ref>. When adversaries do not have access to the training data, attacks are still possible. Recent research has been done to deceive image recognition systems into making wrong decision by slightly revising the input data. The fundamental idea is to revise an image slightly to make it "look" different from the views of human being and machines. Depending on whether the adversary knows the algorithms and parameters used in the recognition systems, there exist white box and black box attacks. Note that the adversary always needs to be able to interact with the target system to observe corresponding output for any input, in both white and black box attacks. Early researches <ref type="bibr" target="#b45">[50,</ref><ref type="bibr" target="#b43">48,</ref><ref type="bibr" target="#b14">19]</ref> focus on the revision and generation of the digital image file, which is directly fed into the image recognition systems. The state-of-the-art researches <ref type="bibr" target="#b32">[37,</ref><ref type="bibr" target="#b16">21,</ref><ref type="bibr" target="#b22">27]</ref> advance in terms of practicality by printing the adversarial image and presenting it to a device with image recognition functionality.</p><p>However, the success of the attack against image recognition systems has not been ported to the speech recognition systems until very recently, due to the complexity of the latter. The speech, a time-domain continuous signal, contains much more features compared to the static images. Hidden voice command <ref type="bibr" target="#b17">[22]</ref> launched both black box (i.e., inverse MFCC) and white box (i.e., gradient decent) attacks against speech recognition systems, and generated obfuscated commands to ASR systems. Though seminal in attacking speech recognition systems, it is also limited to make practical attacks. For instance, a large amount of human effort is involved as feedback for the black box approach, and the white box approach is based on GMM-based acoustic models, which have been replaced by DNN-based ones in most modern speech recognition systems. The recent work DolphinAttack <ref type="bibr" target="#b48">[53]</ref> proposed a completely inaudible voice attack by modulating commands on ultrasound carriers and leveraging microphone vulnerabilities (i.e., the nonlinearity of the microphones). As noted by the authors, such attack can be eliminated by an enhanced microphone that can suppress acoustic signals on ultrasound carrier, like iPhone 6 Plus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overview</head><p>In this section, we present the motivation of our work, and overview the proposed approach to generate the practical adversarial attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Recently, adversarial attacks on image classification have been extensively studied <ref type="bibr" target="#b16">[21,</ref><ref type="bibr" target="#b22">27]</ref>. Results show that even the state-of-the-art DNN-based classifier can be fooled by small perturbations added to the original image <ref type="bibr" target="#b32">[37]</ref>, producing erroneous classification results. However, the impact of adversarial attacks on the most advanced speech recognition systems, such as those integrating DNN models, has never been systematically studied. Hence, in this paper, we investigated DNN-based speech recognition systems, and explored adversarial attacks against them. Researches show that commands can be transmitted to IVC devices through inaudible ultrasonic sound <ref type="bibr" target="#b48">[53]</ref> and noises <ref type="bibr" target="#b17">[22]</ref>. Even though the existing works against ASR systems are seminal, they are limited in some aspects. Specifically, ultrasonic sound can be defeated by using a low-pass filter (LPF) or analyzing the signal frequency range, and noises are easy to be noticed by users.</p><p>Therefore, the research in this paper is motivated by the following questions: (Q1) Is it possible to build the practical adversarial attack against ASR systems, given the facts that the most ASR systems are becoming more intelligent (e.g., by integrating DNN models) and that the generated adversarial samples should work in the very complicated physical environment, e.g., electronic noise from speaker, background noise, etc.? (Q2) Is it feasible to generate the adversarial samples (including the target commands) that are difficult, or even impossible, to be noticed by ordinary users, so the control over the ASR systems can happen in a "hidden" fashion? (Q3) If such adversarial audio samples can be produced, is it possible to impact a large amount of victims in an automated way, rather than solely relying on attackers to play the adversarial audio and affecting victims nearby? Below, we will detail how our attack is designed to address the above questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Philosophy of Designing Our Attack</head><p>To address Q3, our idea is to choose songs as the "carrier" of the voice commands recognizable by ASR systems. The reason of choosing such "carrier" is at least two-fold. On one hand, enjoying songs is always a preferred way for people to relax, e.g., listening to the music station, streaming music from online libraries, or just browsing YouTube for favorite programs. Moreover, such entertainment is not restricted by using radio, CD player, or desktop computer any more. A mobile device, e.g., Android phone or Apple iPhone, allows people to enjoy songs everywhere. Hence, choosing the song as the "carrier" of the voice command automatically helps impact millions of people. On the other hand, "hiding" the desired command in the song also makes the command much more difficult to be noticed by victims, as long as Q2 can be reasonably addressed. Note that we do not rely on the lyrics in the song to help integrate the desired command. Instead, we intend to avoid the songs with the lyrics similar to our desired command. For instance, if the desired command is "open the door", choosing a song with the lyrics of "open the door" will easily catch the victims' attention. Hence, we decide to use random songs as the "carrier" regardless of the desired commands. Actually choosing the songs as the "carrier" of desired commands makes Q2 even more challenging. Our basic idea is when generating the adversarial samples, we revise the original song leveraging the pure voice audio of the desired command as a reference. In particular, we find the revision of the original song to generate the adversarial samples is always a trade off between preserving the fidelity of the original song and recognizing the desired commands from the generated sample by ASR systems. To better obfuscate the desired commands in the song, in this paper we emphasize the former than the latter. In other words, we designed our revision algorithm to maximally preserve the fidelity of the original song, at the expense of losing a bit success rate of recognition of the desired commands. However, such expense can be compensated by integrating the same desired command multiple times into one song (the command of "open the door" may only last for 2 seconds.), and the successful recognition of one suffices to impact the victims.</p><p>Technically, in order to address Q2, we need to investigate the details of an ASR system. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, an ASR system is usually composed of two pre-trained models: an acoustic model describing the relationship between audio signals and phonetic units, and a language model representing statistical distributions over sequences of words. In particular, given a piece of pure voice audio of the desired command and a "carrier" song, we can feed them into an ASR system separately, and intercept the intermediate results. By investigating the output from the acoustic model when processing the audio of the desired command, and the details of the language model, we can conclude the "information" in the output that is necessary for the language model to produce the correct text of the desired command. When we design our approach, we want to ensure such "information" is only a small subset (hopefully the minimum subset) of the output from the acoustic model. Then, we carefully craft the output from the acoustic model when processing the original song, to make it "include" such "information" as well. Finally, we inverse the acoustic model and the feature extraction together, to directly produce the adversarial sample based on the crafted output (with the "information" necessary for the language model to produce the correct text of the desired command).</p><p>Theoretically, the adversarial samples generated above can be recognized by the ASR systems as the desired command if directly fed as input to such systems. Since such input usually is in the form of a wave file (in "WAV" format) and the ASR systems need to expose APIs to accept the input, we define such attack as the WAV-To-API (WTA) attack. However, to implement a practical attack as in Q1, the adversarial sample should be played by a speaker to interact with IVC devices over the air. In this paper, we define such practical attack as WAV-Air-API (WAA) attack. The challenge of the WAA attack is when playing the adversarial samples by a speaker, the electronic noise produced by the loudspeakers and the background noise in the open air have significant impact on the recognition of the desired commands from the adversarial samples. To address this challenge, we improve our approach by integrating a generic noise model to the above algorithm with the details in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Attack Approach</head><p>We implement our attack by addressing two technical challenges: (1) minimizing the perturbations to the song, so the distortion between the original song and the generated adversarial sample can be as unnoticeable as possible, and (2) making the attack practical, which means CommanderSong should be played over the air to compromise IVC devices. To address the first challenge, we proposed pdf-id sequence matching to incur minimum revision at the output of the acoustic model, and use gradient descent to generate the corresponding adversarial samples as in Section 4.2. The second challenge is addressed by introducing a generic noise model to simulate both the electronic noise and background noise as in Section 4.3. Below we elaborate the details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Kaldi Platform</head><p>We choose the open source speech recognition toolkit Kaldi <ref type="bibr" target="#b8">[13]</ref>, due to its popularity in research community. Its source code on github obtains 3,748 stars and 1,822 forks <ref type="bibr" target="#b3">[4]</ref>. Furthermore, the corpus trained by Kaldi on "Fisher" is also used by IBM <ref type="bibr" target="#b13">[18]</ref> and Microsoft <ref type="bibr" target="#b47">[52]</ref>.</p><p>In order to use Kaldi to decode audio, we need a trained model to begin with. There are some models on Kaldi website that can be used for research. We took advan- Phoneme is the smallest unit composing a word. There are three states (each is denoted as an HMM state) of sound production for each phoneme, and a series of transitions among those states can identify a phoneme. A transition identifier (transition-id) is used to uniquely identify the HMM state transition. Therefore, a sequence of transition-ids can identify a phoneme, so we name such a sequence as phoneme identifier in this paper. Note that the transition-id is also mapped to pdf-id. Actually, during the procedure of Kaldi decoding, the phoneme identifiers can be obtained. By referring to the pre-obtained mapping between transition-id and pdf-id, any phoneme identifier can also be expressed as a specific sequence of pdf-ids. Such a specific sequence of pdf-ids actually is a segment from the posterior probability matrix computed from DNN. This implies that to make Kaldi decode any specific phoneme, we need to have DNN compute a posterior probability matrix containing the corresponding sequence of pdf-ids.</p><p>To illustrate the above findings, we use Kaldi to process a piece of audio with several known words, and obtain the intermediate results, including the posterior probability matrix computed by DNN, the transition-ids sequence, the phonemes, and the decoded words. <ref type="figure" target="#fig_1">Figure 2</ref> demonstrates the decoded result of Echo, which contains three phonemes. The red boxes highlight the id representing the corresponding phoneme, and each phoneme is identified by a sequence of transition-ids, or the phoneme identifiers. <ref type="table" target="#tab_0">Table 1</ref> is a segment from the the relationship among the phoneme, pdf-id, transition-id, etc. By referring to Table 1, we can obtain the pdf-ids sequence corresponding to the decoded transition-ids sequence <ref type="bibr" target="#b4">5</ref> . Hence, for any posterior probability matrix demonstrating such a pdf-ids sequence should be decoded by Kaldi as eh B . <ref type="figure" target="#fig_2">Figure 3</ref> demonstrates the details of our attack approach. Given the original song x(t) and the pure voice audio of the desired command y(t), we use Kaldi to decode them separately. By analyzing the decoding procedures, we can get the output of DNN matrix A of the original song (Step 1 񮽙 in <ref type="figure" target="#fig_2">Figure 3</ref>) and the phoneme identifiers of the desired command audio (Step 4 񮽙 in <ref type="figure" target="#fig_2">Figure 3</ref>). The DNN's output A is a matrix containing the probability of each pdf-id at each frame. Suppose there are n frames and k pdf-ids, let a i, j (1 ≤ i ≤ n, 1 ≤ j ≤ k) be the element at the ith row and jth column in A. Then a i, j represents the probability of the jth pdf-id at frame i. For each frame, we calculate the most likely pdf-id as the one with the highest probability in that frame. That is,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Gradient Descent to Craft Audio</head><formula xml:id="formula_0">m i = arg max j a i, j .</formula><p>Let m = (m 1 , m 2 ,..., m n ). m represents a sequence of most likely pdf-ids of the original song audio x(t). For simplification, we use g to represent the function that takes the original audio as input and outputs a sequence of most likely pdf-ids based on DNN's predictions. That is,</p><p>g(x(t)) = m.</p><p>As shown in Step 5 񮽙 in <ref type="figure" target="#fig_2">Figure 3</ref>, we can extract a sequence of pdf-id of the command</p><formula xml:id="formula_1">b = (b 1 , b 2 ,..., b n ),</formula><p>where b i (1 ≤ i ≤ n) represents the highest probability pdfid of the command at frame i. To have the original song decoded as the desired command, we need to identify the minimum modification δ (t) on x(t) so that m is same or close to b. Specifically, we minimize the L1 distance between m and b. As m and b are related with the pdfid sequence, we define this method as pdf-id sequence matching algorithm. <ref type="bibr" target="#b4">5</ref> For instance, the pdf-ids sequence for eh B should be 6383, 5760, 5760, 5760, 5760, 5760, 5760, 5760, 5760, 5760.</p><p>Based on these observations we construct the following objective function: arg min</p><formula xml:id="formula_2">δ (t) 񮽙g (x(t) + δ (t)) − b񮽙 1 .<label>(1)</label></formula><p>To ensure that the modified audio does not deviate too much from the original one, we optimize the objective function Eq (1) under the constraint of |δ (t)| ≤ l. Finally, we use gradient descent <ref type="bibr" target="#b38">[43]</ref>, an iterative optimization algorithm to find the local minimum of a function, to solve the objective function. Given an initial point, gradient descent follows the direction which reduces the value of the function most quickly. By repeating this process until the value starts to remain stable, the algorithm is able to find a local minimum value. In particular, based on our objective function, we revise the song x(t) into x 񮽙 (t) = x(t) + δ (t) with the aim of making most likely pdf-ids g (x 񮽙 (t)) equal or close to b. Therefore, the crafted audio x 񮽙 (t) can be decoded as the desired command.</p><p>To further preserve the fidelity of the original song, one method is to minimize the time duration of the revision. Typically, once the pure command voice audio is generated by a text-to-speech engine, all the phonemes are determined, so as to the phoneme identifiers and b. However, the speed of the speech also determines the number of frames and the number of transition-ids in a phoneme identifier. Intuitively, slow speech always produces repeated frames or transition-ids in a phoneme. Typically people need six or more frames to realize a phoneme, but most speech recognition systems only need three to four frames to interpret a phoneme. Hence, to introduce the minimal revision to the original song, we can analyze b, reduce the number of repeated frames in each phoneme, and obtain a shorter b 񮽙 = (b 1 , b 2 ,..., b q ), where q &lt; n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Practical Attack over the Air</head><p>By feeding the generated adversarial sample directly into Kaldi, the desired command can be decoded correctly. However, playing the sample through a speaker to physically attack an IVC device typically cannot work. This is mainly due to the noises introduced by the speaker and environment, as well as the distortion caused by the receiver of the IVC device. In this paper, we do not consider the invariance of background noise in different environments, e.g., grocery, restaurant, office, etc., due to the following reasons: (1) In a quite noisy environment like restaurant or grocery, even the original voice command y(t) may not be correctly recognized by IVC devices; (2) Modeling any slightly variant background noise itself is still an open research problem; (3) Based on our observation, in a normal environment like home, office, lobby, the major impacts on the physical attack are the electronic noise from the speaker and the distortion from the receiver of the IVC devices, rather than the background noise. Hence, our idea is to build a noise model, considering the speaker noise, the receiver distortion, as well as the generic background noise, and integrate it in the approach in Section 4.2. Specifically, we carefully picked up several songs and played them through our speaker in a very quiet room. By comparing the recorded audio (captured by our receiver) with the original one, we can capture the noises. Note that playing "silent" audio does not work since the electronic noise from speakers may depend on the sound at different frequencies. Therefore, we intend to choose the songs that cover more frequencies. Regarding the comparison between two pieces of audio, we have to first manually align them and then compute the difference. We redesign the objective function as shown in Eq (2). arg min</p><formula xml:id="formula_3">μ(t) 񮽙g (x(t) + μ(t) + n(t)) − b񮽙 1 ,<label>(2)</label></formula><p>where μ(t) is the perturbation that we add to the original song, and n(t) is the noise samples that we captured. In this way, we can get the adversarial audio x 񮽙 (t) = x(t) + μ(t) that can be used to launch the practical attack over the air. Such noise model above is quite device-dependent. Since different speakers and receivers may introduce different noises/distortion when playing or receiving specific audio, x 񮽙 (t) may only work with the devices that we use to capture the noise. To enhance the robustness of x 񮽙 (t), we introduce random noise, which is shown in Eq (3). Here, the function rand() returns an vector of random numbers in the interval (-N,N), which is saved as a "WAV" format file to represent n(t). Our evaluation results show that this approach can make the adversarial audio x 񮽙 (t) robust enough for different speakers and receivers.</p><formula xml:id="formula_4">n(t) = rand(t), |n(t)| &lt;= N.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we present the experimental results of CommanderSong. We evaluated both the WTA and WAA attacks against machine recognition. To evaluate the human comprehension, we conducted a survey examining the effects of "hiding" the desired command in the song. Then, we tested the transferability of the adversarial sample on other ASR platforms, and checked whether CommanderSong can spread through Internet and radio. Finally, we measured the efficiency in terms of the time to generate the CommanderSong. Demos of attacks are uploaded on the website (https://sites.google.com/view/commandersong/).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>The pure voice audio of the desired commands can be generated by any Text-To-Speech (TTS) engine (e.g., Google text-to-speech <ref type="bibr" target="#b6">[7]</ref>, etc.) or recording human voice, as long as it can be correctly recognized by Kaldi platform. We also randomly downloaded 26 songs from the Internet. To understand the impact of using different types of songs as the carrier, we intended to choose songs from different categories, i.e., popular, rock, rap, and soft music. Regarding the commands to inject, we chose 12 commonly used ones such as "turn on GPS", "ask Capital One to make a credit card payment", etc., as shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Regarding the computing environment, one GPU server (1075MHz GPU with 12GB memory, and 512GB hard drive) was used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effectiveness</head><p>WTA Attack. In this WTA attack, we directly feed the generated adversarial songs to Kaldi using its exposed APIs, which accept raw audio file as input. Particularly, we injected each command into each of the downloaded 26 songs using the approach proposed in Section 4.2. Totally we got more than 200 adversarial songs in the "WAV" format and sent them to Kaldi directly for recognition. If Kaldi successfully identified the command injected inside, we denote the attack as successful. <ref type="table" target="#tab_1">Table 2</ref> shows the WTA attack results. Each command can be recognized by Kaldi correctly. The success rate 100% means Kaldi can decode every word in the desired command correctly. The success rate is calculated as the ratio of the number of words successfully decoded and the number of words in the desired command. Note in the case that the decoded word is only one character different than that in the desired command, we consider the word is not correctly recognized.</p><p>For each adversarial song, we further calculated the average signal-noise ratio (SNR) against the original song as shown in <ref type="table" target="#tab_1">Table 2</ref>. SNR is a parameter widely used to quantify the level of a signal power to noise, so we use it here to measure the distortion of the adversarial sample over the original song. We then use the following equation SNR(dB) = 10log 10 (P x(t) /P δ (t) ) to obtain SNR, where the original song x(t) is the signal while the perturbation δ (t) is the noise. Larger SNR value indicates a smaller perturbation. Based on the results in <ref type="table" target="#tab_1">Table 2</ref>, the SNR ranges from 14∼18.6 dB, indicating that the perturbation in the original song is less than 4%. Therefore, the perturbation should be too slight to be noticed. WAA Attack. To practically attack Kaldi over the air, the ideal case is to find a commercial IVC device implemented based on Kaldi and play our adversarial samples against the device. However, we are not aware of any such IVC device, so we simulate a pseudo IVC device based on Kaldi. In particular, the adversarial samples are played by speakers over the air. We use the recording functionality of iPhone 6S to record the audio, which is sent to Kaldi API to decode. Overall, such a pseudo IVC device is built using the microphone in iPhone 6S as the audio recorder, and Kaldi system to decode the audio.</p><p>We conducted the practical WAA attack in a meeting room (16 meter long, 8 meter wide, and 4 meter tall). The songs were played using three different speakers including a JBL clip2 portable speaker, an ASUS laptop and a SENMATE broadcast equipment <ref type="bibr" target="#b11">[16]</ref>, to examine the effectiveness of the injected random noise. All of the speakers are easy to obtain and carry. The distance between the speaker and the pseudo IVC device (i.e., the microphone of the iPhone 6S) was set at 1.5 meters. We chose two commands as in <ref type="table" target="#tab_2">Table 3</ref>, and generated adversarial samples. Then we played them over the air using those three different speakers and used the iPhone 6S to record the audios, which were sent to Kaldi to decode. <ref type="table" target="#tab_2">Table 3</ref> shows the WAA attack results. For both of the two commands, JBL speaker overwhelms the other two with the success rate up to 96%, which might indicate its sound quality is better than the other two. All the SNRs are below 2 dB, which indicates slightly bigger perturbation to the original songs due to the random noise from the signal's point of view. Below we will evaluate if such "bigger" perturbation is human-noticeable by conducting a survey.</p><p>Human comprehension from the survey. To evaluate the effectiveness of hiding the desired command in the song, we conducted a survey on Amazon Mechanical Turk  <ref type="bibr" target="#b1">[2]</ref>, an online marketplace for crowdsourcing intelligence. We recruited 204 individuals to participate in our survey <ref type="bibr" target="#b5">6</ref> . Each participant was asked to listen to 26 adversarial samples, each lasting for about 20 seconds (only about four or five seconds in the middle is crafted to contain the desired command.). A series of questions regarding each audio need to be answered, e.g., (1) whether they have heard the original song before; (2) whether they heard anything abnormal than a regular song (The four options are no, not sure, noisy, and words different than lyrics); (3) if choosing noisy option in <ref type="formula" target="#formula_3">(2)</ref>, where they believe the noise comes from, while if choosing words different than lyrics option in (2), they are asked to write down those words, and how many times they listened to the song before they can recognize the words. The entire survey lasts for about five to six minutes. Each participant is compensated $0.3 for successfully completing the study, provided they pass the attention check question to motivate the participants concentrate on the study. Based on our study, 63.7% of the participants are in the age of 20∼40 and 33.3% are 40∼60 years old, and 70.6% of them use IVC devices (e.g., Amazon Echo, Google home, Smartphone, etc.) everyday. <ref type="table" target="#tab_3">Table 4</ref> shows the results of the human comprehension of our WTA samples. We show the average results for songs belonging to the same category. The detailed results for each individual song can be referred to in <ref type="table" target="#tab_6">Table 7</ref> in Appendix. Generally, the songs in soft music category are the best candidates for the carrier of the desired command, with as low as 15% of participants noticed the abnormality. None of the participants could recognize any word of the desired command injected in the adversarial samples of any category. <ref type="table" target="#tab_4">Table 5</ref> demonstrates the results of the human comprehension of our WAA samples. On average, 40% of the participants believed the noise was generated by the speaker or like radio, while only 2.2% of them thought the noise from the samples themselves. In addition, less than 1% believed that there were other words except the original lyrics. However, none of them successfully identified any word even repeating the songs several times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Towards the Transferability</head><p>Finally, we assess whether the proposed CommanderSong can be transfered to other ASR platforms. Transfer from Kaldi to iFLYTEK. We choose iFLY-TEK ASR system as the target of our transfer, due to its popularity. As one of the top five ASR systems in the world, it possesses 70% of the market in China. Some applications supported by iFLYTEK and their downloads on Google Play as well as the number of worldwide users are listed in <ref type="table" target="#tab_7">Table 8</ref> in Appendix. In particular, iFLY-TEK Input is a popular mobile voice input method, which supports mandarin, English and personalized input <ref type="bibr">[12]</ref>. iFLYREC is an online service offered by iFLYTEK to convert audio to text <ref type="bibr">[10]</ref>. We use them to test the transferability of our WAA attack samples, and the success rates of different commands are shown in <ref type="table" target="#tab_5">Table 6</ref>. Note  that WAA audio samples are directly fed to iFLYREC to decode. Meanwhile, they are played using Bose Companion 2 speaker towards iFLYTEK Input running on smartphone LG V20, or using JBL speaker towards iFLY-TEK Input running on smartphone Huawei honor 8/MI note3/iPhone 6S. Those adversarial samples containing commands like open the door or good night can achieve great transferability on both platforms. However, the command airplane mode on only gets 66% success rate on iFLYREC, and 0 on iFLYTEK Input.</p><p>Transferability from Kaldi to DeepSpeech. We also try to transfer CommanderSong from Kaldi to DeepSpeech, which is an open source end-to-end ASR system. We directly fed several adversarial WTA and WAA attack samples to DeepSpeech, but none of them can be decoded correctly. As Carlini et al. have successfully modified any audio into a command recognizable by DeepSpeech <ref type="bibr" target="#b18">[23]</ref>, we intend to leverage their open source algorithm to examine if it is possible to generate one adversarial sample against both two platforms. In this experiment, we started by 10 adversarial samples generated by CommanderSong, either WTA or WAA attack, integrating commands like Okay google call one one zero one one nine one two zero, Echo open the front door, and Echo turn off the light. We applied their algorithm to modify the samples until DeepSpeech can decode the target commands correctly. Then we tested such newly generated samples against Kaldi as WTA attack, and Kaldi can still successfully recognize them. We did not perform WAA attack since their algorithm targeting DeepSpeech cannot achieve attacks over the air. The preliminary evaluations on transferability give us the opportunities to understand CommanderSongs and for designing systematic approach to transfer in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Automated Spreading</head><p>Since our WAA attack samples can be used to launch the practical adversarial attack against ASR systems, we want to explore the potential channels that can be leveraged to impact a large amount of victims automatically. Online sharing. We consider the online sharing platforms like YouTube to spread CommanderSong. We picked up one five-second adversarial sample embedded with the command "open the door" and applied Windows Movie Maker software to make a video, since YouTube only supports video uploading. The sample was repeated four times to make the full video around 20 seconds. We then connected our desktop audio output to Bose Companion 2 speaker and installed iFLYTEK Input on LG V20 smartphone. In this experiment, the distance between the speaker and the phone can be up to 0.5 meter, and iFLY-TEK Input can still decode the command successfully.</p><p>Radio broadcasting. In this experiment, we used HackRF One <ref type="bibr" target="#b7">[8]</ref>, a hardware that supports Software Defined Radio (SDR) to broadcast our CommanderSong at the frequency of FM 103.4 MHz, simulating a radio station. We setup a radio at the corresponding frequency, so it can receive and play the CommanderSong. We ran the WeChat 7 application and enabled the iFLYTEK Input on different smartphones including iPhone 6S, Huawei Honor 8 and XiaoMi MI Note3. iFLYTEK Input can always successfully recognize the command "open the door" from the audio played by the radio and display it on the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Efficiency</head><p>We also evaluate the cost of generating CommanderSong in the aspect of the required time. For each command, we record the time to inject it into different songs and compute the average. Since the time required to craft also depends on the length of the desired command, we define the efficiency as the ratio of the number of frames of the desired command and the required time. <ref type="table" target="#tab_1">Table 2 and Ta- ble 3</ref> show the efficiency of generating WTA and WAA samples for different commands. Most of those adversarial samples can be generated in less than two hours, and some simple commands like "Echo open the front door" can be done within half an hour. However, we do notice that some special words (such as GPS and airplane) in the command make the generation time longer. Probably those words are not commonly used in the training process of the "ASpIRE model" of Kaldi, so generating enough phonemes to represent the words is time-consuming. Furthermore, we find that, for some songs in the rock category such as "Bang bang" and "Roaked", it usually takes longer to generate the adversarial samples for the same command compared with the songs in other categories, probably due to the unstable rhythm of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Understanding the Attacks</head><p>We try to deeply understand the attacks, which could potentially help to derive defense approaches. We raise some In what ways does the song help the attack? We use songs as the carrier of commands to attack ASR systems. Obviously, one benefit of using a song is to prevent listeners from being aware of the attack. Also CommanderSong can be easily spread through Youtube, radio, TV, etc. Does the song itself help generate the adversarial audio samples? To answer this question, we use a piece of silent audio as the "carrier" to generate CommanderSong A cs (WAA attack), and test the effectiveness of it. The results show that A cs can work, which is aligned with our findings -a random song can serve as the "carrier" because a piece of silent audio can be viewed as a special song.</p><p>However, after listening to A cs , we find that A cs sounds quite similar to the injected command, which means any user can easily notice it, so A cs is not the adversarial samples we desire. Note that, in our human subject study, none of the participants recognized any command from the generated CommanderSongs. We assume that some phonemes or even smaller units in the original song work together with the injected small perturbations to form the target command. To verify this assumption, we prepare a song A s and use it to generate the CommanderSong A cs . Then we calculate the difference Δ(A s , A cs ) between them, and try to attack ASR systems using Δ(A s , A cs ). However, after several times of testing, we find that Δ(A s , A cs ) does not work, which indicates the pure perturbations we injected cannot be recognized as the target commands.</p><p>Recall that in <ref type="table" target="#tab_4">Table 5</ref>, the songs in the soft music category are proven to be the best carrier, with lowest abnormality identified by participants. Based on the findings above, it appears that such songs can better aligned with the phonemes or smaller "units" in the target command to help the attack. This is also the reason why Δ(A s , A cs ) cannot directly attack successfully: the "units" What is the impact of noise in generating adversarial samples? As mentioned early, we build a generic random noise model to perform the WAA attack over the air. In order to understand the impact of the noise in generating adversarial samples, we crafted CommanderSong using noises with different amplitude values. Then we observed the differences between the CommanderSong and the original song, the differences between the CommanderSong and the pure command audio, and the success rates of the CommanderSong to attack. To characterize the difference, we leverage Spearman's rank correlation coefficient <ref type="bibr" target="#b41">[46]</ref> (Spearman's rho for short) to represent the similarity between two pieces of audio. Spearman's rho is widely used to represent the correlation between two variables, and can be calculated as follows: The results are shown in <ref type="figure" target="#fig_3">Figure 4</ref>. The x-axis in the figure shows the SNR (in dB) of the noise, and the y-axis gives the correlation. From the figure, we find that the correlation between the CommanderSong and the original song (red line) decreases with SNR. It means that the CommanderSong sounds less like the original song when the amplitude value of the noise becomes larger. This is mainly because the original song has to be modified more to find a CommanderSong robust enough against the introduced noise. On the contrary, the CommanderSong becomes more similar with the target command audio when the amplitude values of the noise increases (i.e., decrease of SNR in the figure, blue line), which means that the CommanderSong sounds more like the target command. The success rate (black dotted line) also increases with the decrease of SNR. We also note that, when <ref type="figure">Figure 6</ref>: Audio turbulence defense. SNR = 4 dB, the success rate could be as high as 88%. Also the correlation between CommanderSong and the original song is 90%, which indicates a high similarity. <ref type="figure" target="#fig_4">Figure 5</ref> shows the results from another perspective. Suppose the dark blue circle is the set of audios that can be recognized as commands by ASR systems, while the light blue circle and the red one represent the sets of audio recognized as commands and songs by human respectively. At first, the original song is in the red circle, which means that neither ASR systems nor human being recognize any command inside. WTA attack slightly modifies the song so that the open source system Kaldi can recognize the command while human cannot. After noises are introduced to generate CommanderSong for WAA attacks, CommanderSong will fall into the light blue area step by step, and in the end be recognized by human. Therefore, attackers can choose the amplitude values of noise to balance between robustness to noise and identifiability by human users.</p><formula xml:id="formula_5">r(X,Y ) = Cov(X,Y )/ 񮽙 Var[X]Var[Y ],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Defense</head><p>We propose two approaches to defend against CommanderSong: Audio turbulence and Audio squeezing. The first defense is effective against WTA, but not WAA; while the second defense works against both attacks. Audio turbulence. From the evaluation, we observe that noise (e.g., from speaker or background) decreases the success rate of CommanderSong while impacts little on the recognition of audio command. So our basic idea is to add noise (referred to as turbulence noise A n ) to the input audio A I before it is received by the ASR system, and check whether the resultant audio A I + 񮽙A n can be interpreted as other words. Particularly, as shown in <ref type="figure">Figure 6</ref>, A I is decoded as text 1 by the ASR system. Then we add A n to A I and let the ASR system extract the text text 2 from A I + 񮽙A n . If text 1 񮽙 =text 2 , we say that the CommanderSong is detected.</p><p>We did experiments using this approach to test the effectiveness of such defense. The target command "open the door" was used to generate a CommanderSong. <ref type="figure" target="#fig_6">Fig- ure 7</ref> shows the result. The x-axis shows the SNR (A I to A n ), and the y-axis shows the success rate. We found that the success rate of WTA dramatically drops when SNR decreases. When SNR = 15 dB, WTA almost always fails and A I can still be successfully recognized, which means this approach works for WTA. However, the success rate of WAA is still very high. This is mainly because CommanderSongs for WAA is generated using random noises, which is robust against turbulence noise.</p><p>Audio squeezing. The second defense is to reduce the sampling rate of the input audio A I (just like squeezing the audio). Instead of adding A n in the defense of audio turbulence, we downsample A I (referred to as D(A I )). Still, ASR systems decode A I and D(A I ), and get text 1 and text 2 respectively. If text 1 񮽙 =text 2 , the CommanderSong is detected. Similar to the previous experiment, we evaluate the effectiveness of this approach. The results are shown in <ref type="figure" target="#fig_7">Figure 8</ref>. The x-axis shows the ratio (1/M) of downsampling (M is the downsampling factor or decimation factor, which means that the original sampling rate is M times of the downsampled rate). When 1/M = 0.7 (if the sample rate is 8000 samples/second, the downsampled rate is 5600 samples/second), the success rates of WTA and WAA are 0% and 8% respectively. A I can still be successful recognized at the rate of 91%. This means that Audio squeezing is effective to defend against both WTA and WAA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Attack on ASR system. Prior to our work, many researchers have devoted to security issues about speech controllable systems <ref type="bibr" target="#b31">[36,</ref><ref type="bibr" target="#b30">35,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b36">41,</ref><ref type="bibr" target="#b46">51,</ref><ref type="bibr" target="#b17">22,</ref><ref type="bibr" target="#b48">53,</ref><ref type="bibr" target="#b18">23]</ref>. Denis et al. found the vulnerability of analog sensor and injected bogus voice signal to attack the microphone <ref type="bibr" target="#b31">[36]</ref>. <ref type="bibr">Kasmi et al. stated</ref> that, by leveraging intentional electromagnetic interference on headset cables, voice command could be injected and carried by FM signals which is further received and interpreted by smart phones <ref type="bibr" target="#b30">[35]</ref>. Diao et al. demonstrated that, through permission bypassing attack in Android smart phones, voice commands could be played using apps with zero permissions <ref type="bibr" target="#b21">[26]</ref>. Mukhopadhyay et al. considered voice impersonation attacks to contaminate a voice-based user authentication system <ref type="bibr" target="#b36">[41]</ref>. They reconstructed the victims voice model from the victims voice data, and launched attacks that can bypass voice authentication systems. Different from these attacks, we are attacking the machine learning models of ASR systems.</p><p>Hidden voice command <ref type="bibr" target="#b17">[22]</ref> launched both black box (i.e., inverse MFCC) and white box (i.e., gradient decent) attacks against ASR systems with GMM-based acoustic models. Different from this work, our target is a DNN-based ASR system. Recently, the authors posted the achievement that can construct targeted audio adversarial examples on DeepSpeech, an end-to-end open source ASR platform <ref type="bibr" target="#b18">[23]</ref>. To perform the attack, the adversary needs to directly upload the adversarial WAV file to the speech recognition system. Our attacks on Kaldi are concurrent to their work, and our attack approaches are independent to theirs. Moreover, our attacks succeed under a more practical setting that let the adversarial audio be played over the air. The recent work DolphinAttack <ref type="bibr" target="#b48">[53]</ref> proposed a completely inaudible voice attack by modulating commands on ultrasound carriers and leveraging microphone vulnerabilities to attack. As noted by the authors, such attack can be eliminated by filtering out ultrasound carrier (e.g., iPhone 6 Plus). Differently, our attack uses songs instead of ultrasound as the carriers, making the attack harder to defend.</p><p>Adversarial research on machine learning. Besides attacking speech recognition systems, there has been substantial work on adversarial machine learning examples towards physical world. <ref type="bibr">Kurakin et al. [37]</ref> proved it is doable that Inception v3 image classification neural network could be compromised by adversarial images.</p><p>Brown et al. <ref type="bibr" target="#b16">[21]</ref> showed by adding an universal patch to an image they could fool the image classifiers successfully. Evtimov et al. <ref type="bibr" target="#b22">[27]</ref> proposed a general algorithm which can produce robust adversarial perturbations into images to overcome physical condition in real world. They successfully fooled road sign classifiers to mis-classify real Stop Sign. Different from them, our study targets speech recognition system.</p><p>Defense of Adversarial on machine learning. Defending against adversarial attacks is known to be a challenging problem. Existing defenses include adversarial training and defensive distillation. Adversarial training <ref type="bibr" target="#b34">[39]</ref> adds the adversarial examples into the model's training set to increase its robustness against these examples. Defensive distillation <ref type="bibr" target="#b28">[33]</ref> trains the model with probabilities of different class labels supported by an early model trained on the same task. Both defenses perform a kind of gradient masking <ref type="bibr" target="#b40">[45]</ref> which increases the difficulties for the adversary to compute the gradient direction. In <ref type="bibr" target="#b24">[29]</ref>, Dawn Song attempted to combine multiple defenses including feature squeezing and the specialist to construct a larger strong defense. They stated that defenses should be evaluated by strong attacks and adaptive adversarial examples. Most of these defenses are effective for white box attacks but not for black box ones. Binary classification is another simple and effective defense for white box attacks without any modifications of the underlying systems. A binary classifier is built to separate adversarial examples apart from the clean data. Similar as adversarial training and defensive distillation, this defense suffers from generalization limitation. In this paper, we propose two novel defenses against CommanderSong attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we perform practical adversarial attacks on ASR systems by injecting "voice" commands into songs (CommanderSong). To the best of our knowledge, this is the first systematical approach to generate such practical attacks against DNN-based ASR system. Such CommanderSong could let ASR systems execute the command while being played over the air without notice by users. Our evaluation shows that CommanderSong can be transferred to iFLYTEK, impacting popular apps such as WeChat, Sina Weibo, and JD with billions of users. We also demonstrated that CommanderSong can be spread through YouTube and radio. Two approaches (audio turbulence and audio squeezing) are proposed to defend against CommanderSong.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of Automatic Speech Recognition System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Result of decoding "Echo".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Steps of attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: SNR impacts on correlation of the audios and the success rate of adversarial audios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Explaination of Kaldi and human recognition of the audios.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where X and Y are the MFCC features of the two pieces of audio. Cov(X,Y ) represents the covariance of X and Y. Var[X] and Var[Y ] are the variances of X and Y respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The results of audio turbulence defense.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Audio squeezing defense result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Relationship between transition-id and pdf-id.</head><label>1</label><figDesc></figDesc><table>Phoneme 
HMM-
state 

Pdf-
id 

Transition-
id 
Transition 

eh B 
0 
6383 
15985 
0→1 
15986 
0→2 

eh B 
1 
5760 
16189 
self-loop 
16190 
1→2 

k I 
0 
6673 
31223 
0→1 
31224 
0→2 

k I 
1 
3787 
31379 
self-loop 
31380 
1→2 

ow E 
0 
5316 
39643 
0→1 
9644 
0→2 

ow E 
1 
8335 
39897 
self-loop 
39898 
1→2 

tage of the "ASpIRE Chain Model" (referred as "ASpIRE 
model" in short), which was one of the latest released 
decoding models when we began our study 4 . After man-
ually analyzing the source code of Kaldi (about 301,636 
lines of shell scripts and 238,107 C++ SLOC), we com-
pletely explored how Kaldi processes audio and decodes 
it to texts. Firstly, it extracts acoustic features like MFCC 
or PLP from the raw audio. Then based on the trained 
probability density function (p.d.f.) of the acoustic model, 
those features are taken as input to DNN to compute the 
posterior probability matrix. The p.d.f. is indexed by the 
pdf identifier (pdf-id), which exactly indicates the column 
of the output matrix of DNN. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : WTA attack results.</head><label>2</label><figDesc></figDesc><table>Command 
Success rate (%) SNR (dB) Efficiency (frames/hours) 
Okay google restart phone now. 
100 
18.6 
229/1.3 
Okay google flashlight on. 
100 
14.7 
219/1.3 
Okay google read mail. 
100 
15.5 
217/1.5 
Okay google clear notification. 
100 
14 
260/1.2 
Okay google good night. 
100 
15.6 
193/1.3 
Okay google airplane mode on. 
100 
16.9 
219/1.1 
Okay google turn on wireless hot spot. 
100 
14.7 
280/1.6 
Okay google read last sms from boss. 
100 
15.1 
323/1.4 
Echo open the front door. 
100 
17.2 
193/1.0 
Echo turn off the light. 
100 
17.3 
347/1.5 
Okay google call one one zero one one 
nine one two zero. 
100 
14.8 
387/1.7 

Echo ask capital one to make a credit 
card payment. 
100 
15.8 
379/1.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : WAA attack results.</head><label>3</label><figDesc></figDesc><table>Command 
Speaker 
Success rate (%) SNR (dB) Efficiency (frames/hours) 
Echo ask capital one 
JBL speaker 
90 
1.7 
to make a credit card 
ASUS Laptop 
82 
1.7 
379/2.0 
card payment. 
SENMATE Broadcast 
72 
1.7 
Okay google call one 
JBL speaker 
96 
1.3 
one zero one one nine 
ASUS Laptop 
60 
1.3 
400/1.8 
one two zero. 
SENMATE Broadcast 
70 
1.3 

(MTurk) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 : Human comprehension of the WTA samples.</head><label>4</label><figDesc></figDesc><table>Music 
Classification 

Listened 
(%) 

Abnormal 
(%) 

Recognize 
Command (%) 
Soft Music 
13 
15 
0 
Rock 
33 
28 
0 
Popular 
32 
26 
0 
Rap 
41 
23 
0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 : Human comprehension of the WAA samples.</head><label>5</label><figDesc></figDesc><table>Song Name 
Listened 
(%) 

Abnormal 
(%) 

Noise-
speaker 
(%) 

Noise-
song 
(%) 
Did You Need 
It 
15 
67 
42 
1 

Outlaw of 
Love 
11 
63 
36 
2 

The Saltwater 
Room 
27 
67 
39 
3 

Sleepwalker 
13 
67 
41 
0 
Underneath 
13 
68 
45 
3 
Feeling Good 
38 
59 
36 
4 
Average 
19.5 
65.2 
40 
2.2 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 : Transferability from Kaldi to iFLYTEK.</head><label>6</label><figDesc></figDesc><table>Command 
iFLYREC 
(%) 

iFLYTEK 
Input (%) 
Airplane mode on. 
66 
0 
Open the door. 
100 
100 
Good night. 
100 
100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 7 : The detailed results of individual song in human comprehension survey for WTA samples. When we were checking the survey results from MTurk, we found the average familiarity of MTurk workers towards our songs is not as good as we expected. So streaming counts from Spotify are also listed in the table, as we want to show the popularity of our sample songs. The song Selling Brick in Street is not in Spotify database so we can not provide the count for it.</head><label>7</label><figDesc></figDesc><table>Music Clas-
sification 
Song Name 
Spotify 
Streaming Count 

Listened 
(%) 

Abnormal 
(%) 

Recognize 
Command (%) 
Heart and Soul 
13,749,471 
15% 
8% 
0 
Castle in the Sky 
2,332,348 
9% 
6% 
0 
Soft Music 
A Comme Amour 
1,878,899 
14% 
18% 
0 
Mariage D'amour 
337,486 
17% 
33% 
0 
Lotus 
49,443,256 
11% 
12% 
0 
Average 
13,548,292 
13% 
15% 
0 
Bang Bang 
532,057,658 
52% 
24% 
0 
Soaked 
29,734 
13% 
32% 
0 
Rock 
Gold 
11,614,629 
14% 
41% 
0 
We are never Getting back together 
113,806,946 
66% 
38% 
0 
When can I See You again 
26,463,993 
20% 
9% 
0 
Average 
136,794,562 
33% 
28% 
0 
Love Story 
109,952,344 
49% 
24% 
0 
Hello Seattle 
9,850,328 
29% 
16% 
0 
Popular 
Good Time 
125,125,693 
48% 
32% 
0 
To the Sky 
4,860,627 
27% 
30% 
0 
A Loaded Smile 
658,814 
8% 
26% 
0 
Average 
50,089,561 
32% 
26% 
0 
Rap God 
349,754,768 
43% 
32% 
0 
Let Me Hold You 
311,569,726 
31% 
15% 
0 
Rap 
Lose Yourself 
483,937,007 
75% 
14% 
0 
Remember the Name 
193,564,886 
48% 
32% 
0 
Selling Brick in Street 
N/A 
6% 
24% 
0 
Average 
334,706,597 
41% 
23% 
0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 8 :</head><label>8</label><figDesc></figDesc><table>The detailed information of some sample applications which utilize iFLYTEK as voice input, including 
number of downloads from Google Play and total user amount. Since Google Services are not accessible in China 
and information of Apple App Store is not collected, the number of users may not be associated with the number of 
downloads in Google Play. As shown in the table, each of these applications has over 0.2 billion users in the world. 

Application 
Usage 
Downloads from 
Google Play 

Total Users Worldwide 
(Billion) 
Sina Weibo 
Social platform 
11,000,000 
0.53 
JD 
Online shopping 
1,000,000 
0.27 
CMbrowser 
Searching engine 
50,000,000 
0.64 
Ctrip 
Travel advice website 
1,000,000 
0.30 
Migu Digital 
Voice assistant 
5,000 
0.46 
WeChat 
Chatting, Social 
100,000,000 
0.96 
iFLYTEK Input 
Typing, Voice Input 
500,000 
0.5 </table></figure>

			<note place="foot" n="4"> There are three decoding models on Kaldi platform currently. ASpIRE Chain Model we used in this paper was released on October 15th, 2016, while SRE16 Xvector Model was released on October 4th, 2017, which was not available when we began our study. The CVTE Mandarin Model, released on June 21st 2017 was trained in Chinese [13].</note>

			<note place="foot" n="6"> The survey will not cause any potential risks to the participants (physical, psychological, social, legal, etc.). The questions in our survey do not involve any confidential information about the participants. We obtained the IRB Exempt certificates from our institutes.</note>

			<note place="foot" n="7"> WeChat is the most popular instant messaging application in China, with approximately 963,000,000 users all over the world by June 2017 [15].</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexa</forename><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://developer.amazon.com/alexa" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon Mechanical</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turk</surname></persName>
		</author>
		<ptr target="https://www.mturk.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apple</forename><surname>Siri</surname></persName>
		</author>
		<ptr target="https://www.apple.com/ios/siri" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspire</surname></persName>
		</author>
		<ptr target="https://github.com/kaldi-asr/kaldi/tree/master/egs/aspire" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmusphinx</surname></persName>
		</author>
		<ptr target="https://cmusphinx.github.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Assistant</surname></persName>
		</author>
		<ptr target="https://assistant.google.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Google Text-to-speech</title>
		<ptr target="https://play.google.com/store/apps" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hackrf</forename><surname>One</surname></persName>
		</author>
		<ptr target="https://greatscottgadgets.com/hackrf/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaldi</surname></persName>
		</author>
		<ptr target="http://kaldi-asr.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Cortana</surname></persName>
		</author>
		<ptr target="https://www.microsoft.com/en-us/cortana" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Number of monthly active WeChat users from 2nd quarter 2010 to 2nd quarter</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>in millions</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senmate</surname></persName>
		</author>
		<ptr target="http://www.114pifa.com/p106/34376.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Building competitive direct acoustics-to-word models for english conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A liveness detection method for face recognition based on optical flow field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IASP 2009. International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
	<note>Image Analysis and Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igino</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Nedimšrndi´cnedimˇnedimšrndi´nedimšrndi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
		<title level="m">Adversarial patch. 31st Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hidden voice commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavish</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Sherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clay</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="513" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Audio adversarial examples: Targeted attacks on speech-to-text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Security Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnet-based speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Your voice assistant is mine: How to abuse speakers to steal information and control your phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Workshop on Security and Privacy in Smartphones &amp; Mobile Devices</title>
		<meeting>the 4th ACM Workshop on Security and Privacy in Smartphones &amp; Mobile Devices</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Amir Rahmati, and Dawn Song. Robust physical-world attacks on deep learning models. Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples. International Conference on Learning Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adversarial example defenses: Ensembles of weak defenses are not strong</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on Offensive Technologies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Perceptual linear predictive (plp) analysis of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hynek</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">the Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1738" to="1752" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Line spectrum representation of linear predictor coefficients of speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumitada</forename><surname>Itakura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">S1</biblScope>
			<biblScope unit="page" from="35" to="35" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iemi threats for information security: Remote command injection on modern smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaouki</forename><surname>Kasmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Lopes Esteves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Electromagnetic Compatibility</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1752" to="1755" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ghost talk: Mitigating emi signal injection attacks against analog sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denis Foo Kune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdae</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2013 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="145" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">BEBP: an poisoning method against machine learning based idss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03965</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voice recognition algorithms using mel frequency cepstral coefficient (mfcc) and dynamic time warping (dtw) techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindasalwa</forename><surname>Muda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mumtaj</forename><surname>Begam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irraivan</forename><surname>Elamvazuthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">All your voices are belong to us: Stealing voices to fool humans and machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dibya</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maliheh</forename><surname>Shirvanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitesh</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Research in Computer Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="599" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic speech recognition: History, methods and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Oshaughnessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2965" to="2979" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Matyasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Lin</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sheatsley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">an adversarial machine learning library</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Spearman rank correlation coefficient. Encyclopedia of statistical sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pirie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Has¸imhas¸im</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spoofing and anti-spoofing measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Stephanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schuckers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Security technical report</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="56" to="62" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fusion of multiple clues for photo-attack detection in face recognition systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Tronci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Muntoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Fadda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maurizio</forename><surname>Pili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Sirena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Murgia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Ristori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sardegna</forename><surname>Ricerche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biometrics (IJCB), 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cocaine noodles: exploiting the gap between human and machine speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tavish</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Sherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clay</forename><surname>Shields</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WOOT</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="10" to="11" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The microsoft 2016 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5255" to="5259" />
		</imprint>
	</monogr>
	<note>2017 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dolphinattack: Inaudible voice commands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="103" to="117" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
