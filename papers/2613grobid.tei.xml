<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Opening Up Black Box Networks with CloudTalk</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costin</forename><surname>Raiciu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mihail Ionescu University Politehnica of Bucharest Dragos</orgName>
								<address>
									<settlement>Niculescu</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Opening Up Black Box Networks with CloudTalk</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Optimizing distributed applications in the cloud requires network topology information, yet this information is kept confidential by the cloud providers. Today, applications can infer network properties and optimize accordingly but this is costly to get right. The cloud can optimize the network via load balancing, but the scope is limited to moving traffic between the available paths. In this paper we challenge this status quo. We show that network topology information is not confidential in the first place by conducting a study of Amazon&apos;s EC2 topology, and we show how such information can have a big impact in optimizing a web search application. We propose that applications and the network should break the silence and communicate to allow better optimizations that will benefit both parties. To this end we design CloudTalk, a very simple language that allows apps to express traffic patterns that are then ranked by the network. The ranking helps application pick the right way to implement its tasks. We provide a proof-of-concept implementation of CloudTalk showing that it is expressive enough to capture many traffic patterns and that it is feasible to use in practice.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many organizations are running large-scale distributed applications in the cloud, benefiting from the abundance of resources and the attractive pay-per-use billing model. Cloud users want their apps to finish as quickly as possible, and they should be able to use the many optimizations that have been proposed to this end <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10]</ref>. Optimizations often require detailed knowledge of the cloud topology and its resources. In particular, network topology information is crucial to getting good performance as the network is often the bottleneck. Meanwhile, the datacenter operators keep topology information confidential.</p><p>This status quo is suboptimal: application optimizations either ignore the network or profile it continuously to get accurate information. The former results in disgruntled customers and low network utilization and the latter wastes traffic. Network-only optimizations are limited to load-balancing traffic between available paths. We are seemingly stuck: optimizing distributed applications requires in-depth topology knowledge that providers are understandably reluctant to give away.</p><p>In this paper we challenge this status quo. First, we seek to understand just how private network topology information is in practice by conducting a measurement study on Amazon's EC2 Infrastructure-as-aService cloud. We find that hiding the network topology is rather difficult: our measurements give a clear blueprint of the EC2 network topology which resembles VL2 <ref type="bibr" target="#b6">[7]</ref> (see Section 2). To understand whether topology information does help optimizing applications, we deploy a distributed web-search application on 100 servers. We run several configurations of this application, one of which has been optimized using the inferred topology. The results show that substantial benefits can be had when topology information is used (Section 3).</p><p>Our practical experiments show that the topology information cannot be hidden, and that optimizing applications requires such information. How should networks and applications communicate to help optimizations? One way transfer of information from the network to the applications is difficult to get right: either the information accurately describes the topology but violates the operator's confidentiality requirements, or the information is somehow anonymized and certain optimizations are not possible anymore.</p><p>In Section 5 we propose an alternative where both applications and the network provide information. We design a simple language called CloudTalk that allows applications to succinctly specify network tasks-each containing related network transfers-and submit them to the network. The network responds with task finish time estimates. These are used for optimizations: applications will choose between different equivalent ways of doing the same computation.</p><p>We have implemented a proof-of-concept implementation of CloudTalk and applied it to optimize web search and MapReduce apps. Initial results show that CloudTalk is expressive enough to optimize interesting applications and feasible to implement in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Inferring Cloud Topology Information</head><p>Cloud providers such as Amazon EC2 or Microsoft Azure do not disclose any information regarding network topology. Nevertheless, a great deal of information can still be inferred by running measurements on cloud instances. We analyzed the topology of Amazon EC2's us-east-1d datacenter by acquiring many "instances" and using well known network diagnosis tools such as ping, traceroute and iperf. We only describe our main findings here due to space constraints.</p><p>Amazon EC2 instances are virtual machines running on top of Xen <ref type="bibr" target="#b2">[3]</ref> hypervisor. On each physical machine there is a special VM called Dom0 and many tenant VMs. Dom0 controls the tenant VMs's access to the network and acts as an IP hop and the default router for the guest machines. Because of this we can only infer L3 topology information. We ran ping and traceroute between all our instances and identified several situations:</p><p>1. The instances (or Virtual Machines, VMs) are on the same physical machine. In this case, there is only one hop between the two VMs, the packets go from V M 1 -Dom0 -V M 2 . The IP addresses of the VMs are usually very close to each other, in the same /24 address space. Using iperf we find that the available bandwidth between such VMs is pretty high at around 4Gb/s.</p><p>2. The VMs are in the same rack. We assume the existence of a top of the rack switch (ToR), but this only functions at L2. There are two hops between the VMs: V M 1 -Dom0 1 -Dom0 2 -V M 2 . The IP addresses of the VMs are in the same /24 address space. The available bandwidth is 1Gb/s.</p><p>3. The VMs are in the same subnet. We found out that a subnet contains machines with addresses of the form 10.n.x.x and 10.n+1.x.x; for example <ref type="bibr">10.194.x.x and 10.195.</ref>x.x will be in the same subnet. In this case, there are three hops:</p><formula xml:id="formula_0">V M 1 -Dom0 1 -EdgeRouter -Dom0 2 -V M 2 .</formula><p>We run the experiments 100 times between each pair of machines and we found out that there are actually two edge routers for each subnet, and it seems that Dom0 (or, less likely, the ToR switch) is load balancing traffic to the routers dynamically. The available bandwidth is 1Gb/s.</p><p>4. The VMs are in different subnets. In this case, there are five intermediate hops:</p><formula xml:id="formula_1">V M 1 -Dom0 1 -EdgeR- outer -CoreRouter -EdgeRouter -Dom0 2 -V M 2 .</formula><p>Running the experiments multiple times, we found that there are two groups of core routers, each of them connecting all edge routers from the different subnets, and providing full redundancy. The available bandwidth is still 1Gb/s, confirming anecdotal evidence that EC2 has full bisection bandwidth.</p><p>An interesting situation surfaced during these tests. The data from traceroute and ping -R was not quite in sync, showing different IP addresses for the same routes. We hypothesized that the actual routers were the same machines, but the addresses reported were coming from different router interfaces. We proved this true by employing a well-known technique for interface disambiguation <ref type="bibr" target="#b1">[2]</ref>. The main idea is that a router, when transmitting an ICMP message back, will use the source address of the outbound interface on which the packet is sent. Therefore, one can transmit a UDP packet to an unused port at each of the interfaces I 1 and I 2 (suspected to be on the same physical machine) from the same source. If the returned ICMP packet comes from the same I 3 , it means that I 1 and I 2 belong to the same router.</p><p>We present in <ref type="figure" target="#fig_0">Figure 1</ref> the topology for EC2's us-east1d datacenter as resulting from our experiments. While we cannot be absolutely sure that this is 100% accurate, it correctly explains all behavior we noticed during our tests, as well as anecdotal information gathered from various informal meetings with people from Amazon. The topology is very similar to the VL2 topology <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimizing Web Search</head><p>Web search is a classical distributed application which uses a "scatter-gather" workflow. Nodes are organized in a hierarchical structure. The query is sent by the frontend towards the leaves, while the results go in the opposite direction. The architecture we used for our experiments has two levels of aggregators and is shown in <ref type="figure">Figure 2</ref>. Each aggregator has around 50 nodes underneath.</p><p>For our implementation, we used Apache Solr on top of the Apache Tomcat web server. This combination is considered by many as the state of the art in open source search engines. Solr naturally supports aggregation on multiple levels using the concept of sharding, where an instance can be configured to return aggregated results from its own data together with results from other servers. Each machine is a High CPU medium instance, with 1.7GB of RAM and 5 EC2 computing units hosting the content of roughly 5 Million indexed URLs for a total of around 4GB of data (the web pages are from a 2.5TB snapshot of the .uk domain taken in 2009).</p><p>We measured the performance of the system in multiple configurations and present the results in <ref type="figure" target="#fig_1">Figure 3</ref>. First, we measure the raw performance of one machine searching its part of the index -this the baseline for the distributed measurements. Second, we measured with only one aggregator contacting all 100 servers. When traffic was low, the system was behaving correctly but with obviously higher delays compared to the singlemachine baseline.</p><p>Once the traffic exceeds 35 queries per second (qps), the aggregator software started crashing. We believe that the effect is due to TCP incast <ref type="bibr" target="#b12">[13]</ref>: each server sends bursts of packets containing query replies to the corresponding aggregator via a single TCP connection. The limited network buffer available on the aggregator's port in the ToR switch is overrun, losing many packets. Each TCP connection has few packets in flight so fast retransmit is not triggered, resulting in a costly timeout. Meanwhile, unserviced requests pile-up at the frontend until the server runs out of memory and crashes.</p><p>The last two tests were done in a configuration similar with the one presented in <ref type="figure">Figure 2</ref>. The only difference among the two tests was the location of the aggregators. We kept the frontend and the nodes constant and we varied the location of the aggregators, using the inferred topology information presented in the previous section.</p><p>For the first case (called "worst"), we placed the aggregators such that each aggregator and its corresponding nodes were in different subnets. For the second case (called the best), each aggregator was placed in such a way to to maximize the number of servers close to the aggregator. For the actual machines we had, this meant that one aggregator had 14 machines in the same rack, 15 in the same subnet and 18 in different subnets, while the other had 20 machines in the same rack, 10 machines in the same subnet and 18 in different subnets.</p><p>The results show that the effects of incast are smaller compared to the single aggregator scenario. Further, when traffic increases, the performance for the "best" scenario gets substantially better than the "worst" case, even by a factor of 3 to 1. We conjecture this is because the RTT distribution is more varied in the best case; we confirm this effect via simulation in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Need for Cloud Talk</head><p>We have shown that "confidential" topology information can be inferred and is key to getting good performance in datacenters. While applications and networks could go on like this, the status quo is suboptimal for all parties. Take the example shown in <ref type="figure">Figure 4</ref> of a scheduler that must schedule two reduce jobs and it can choose between machines A, B and C that have just finished running map jobs. If asked, the network would advice it is best run the reduce jobs on A and B because C's downlinks are congested. In practice, the application will start the reduce jobs on two random machines, and perhaps reschedule if one lands on C and becomes a straggler; however this takes time and wastes resources. Once the scheduler assigns the reduce job, the network can only load balance flows onto its core links for small improvements.</p><p>It therefore seems inevitable that the network and the applications should collaborate to reap mutual benefits. Although there are many ways to design the information flow between the network and its applications, solutions must provide a number of desirable properties including: expressiveness to capture complex application data patterns, efficiency to allow constant use, dynamicity to allow optimizations on short timescales and finally reduced information leakage.</p><p>The most obvious solution is to have the datacenter operator provide each requesting application the real network topology together with buffer sizes. This violates the confidentiality requirement, and does not capture any dynamic information the network may have, for instance the list of congested links or current buffer usage.</p><p>Another solution is to adopt ideas from IETF ALTO (application-layer traffic optimization) Working Group <ref type="bibr" target="#b11">[12]</ref>. ALTO aims to help network operators and endpoints to communicate to solve the following tussle: BitTorrent clients draw most traffic from the best peers, but in the process they create a lot of costly upstream traffic for their ISPs. ALTO servers run by the operator provide requesting applications with a network map and a cost map. The network map is essentially a clustering of IP addresses into groups called PIDs, and the clustering is performed by the operator according to its own routing policy. The cost map provides routing costs between PIDs. The applications use this information to select the most desirable peers from the point of view of the network and the application.</p><p>The ALTO model offers better confidentiality than the solution above, but it is not as expressive as we need it to be in a datacenter: it does not capture costs in manyto-one (e.g. example above) or many-to-many cases, and does not capture dynamic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CloudTalk Language</head><p>We propose a simple solution where applications explicitly ask the network for advice by providing a request which includes a number of equivalent network tasks. The network ranks these alternative tasks using its detailed knowledge of the topology and possibly the current traffic load. The application will then use the most efficient task as told by the network. This type of interaction relieves the network from having to provide potentially verbose, confidential data. The application provides little confidential information beyond what the network could already find by looking at the traffic.</p><p>A task is a set of interrelated network transfers specified by the application; the task finishes when the last transfer of that task finishes. For each individual transfer, the application specifies a task-unique identifier, the source, the destination, the size and the start time of the transfer. All times are given as wall-clock seconds and are relative to the start of the task. Additionally, start times can refer to the finish times of other transfers.</p><p>Let's consider an application wishing to run a scattergather task. The data can be queried from hosts A or B, and the data resides on hosts C and D. Which of A and B should be used? The application will create two tasks, one running scatter-gather from host A and one from host B. Task B's description is similar. The application submits both task descriptions to the network, and the network ranks them according to their performance.</p><p>This description model is very simple yet it can capture most of the traffic patterns we care about in practice. One-to-one communication is trivial to express, and many-to-one or many-to-many interactions can be easily described with tasks similar to the one above. The strength of the model stems from the fact that it includes time explicitly: apps can tell the network which flows are parallel and which depend on other flows. For the latter part, the f inish(ID) operator is used.</p><p>The start time of a task is an arithmetic expression using scalars and other f inish operators, as well as max/min functions. This allows us to express synchronization barriers, such as those seen at the end of the shuffle phase of MapReduce computations. To allow more concise representations, traffic patterns that include multiple sources or destinations can be specified as long as all transfers send the same amount of bytes. In such cases identifiers are assigned sequentially.</p><p>The model also contains the bandwidth operator that allows the applications specify more complex communication patterns. One example is daisy-chaining that is used by distributed file systems to store data simultaneously on a chain of hosts, with all intermediary hosts sending data out as soon as it is received. A model of daisy chaining can leverage the bandwidth operator to limit the throughput on link i + 1 in the chain to be always smaller than or equal to the throughput of the predecessor link i. To decide replica placement, a distributed filesystem can use the following task description: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Optimizing Clouds with CloudTalk</head><p>Implementing CloudTalk requires changes to both applications and the network. Changing applications is notoriously difficult (because of their sheer numbers) but we believe that this is feasible in datacenters where a few key platform applications-such as distributed computation (e.g. Hadoop), distributed filesystems (e.g. HDFS) and search-account for the majority of uses. All these applications feature one or more scheduler machines that decide how to run certain tasks; to support CloudTalk we need to change the schedulers to query for and use network-provided optimization hints. As there is only a handful of widely-used schedulers, we believe modifying these to support CloudTalk is straightforward. We intend to implement this in our future work.</p><p>The network can implement CloudTalk by providing one or a few servers that know the network topology (and maybe the instantaneous traffic load) and answer application queries. To answer queries, the network servers can use packet and flow-level simulation and even linear programming. Given an application request, the network (servers) can simulate servicing the request and then use the results to rank the alternatives provided by the application. We implemented a proof-of-concept implementation of CloudTalk as an extension to the htsim simulator. htsim is a packet-level simulator that can scale to moderately sized datacenters containing hundreds to a few thousand nodes. The advantages of packet-level simulation include the ability to provide accurate results for both long-lived transfers as well as incast situations. On the downside, packet-level simulation time scales linearly at best with the size of the transfers-this may prove costly for analyzing MapReduce tasks or large filesystem reads and writes.</p><p>To understand CloudTalk's feasibility we used htsim to simulate a modified VL2 topology containing 1200 servers. The simulated topology models the topology we inferred in practice. Host-to-switch links are gigabit, while switch-to-switch links are 10 gigabit.</p><p>Our first experiment emulates the search application described in section 3. We used the inferred topology information to place the 100 servers in appropriate racks and subnets in the simulated topology. We then generate three CloudTalk task descriptions describing the network traffic patterns of the query application with one aggregator, two aggregators (local) and non-local.</p><p>When the simulator receives a task description it parses it, instantiates the desired flows and then simulates the transfers in an otherwise idle network. Each task is simulated ten times to capture non-determinism induced by ECMP routing in the network. The results given in the table below give estimated average delays in milliseconds. We provide results for three configurations of the network corresponding to different switch port buffer sizes for the host links. Simulating one task 10 times takes around 1.5s of wall-clock time. The task model assumes a single query packet is sent out and ten packets are returned by each server immediately after the query is received (we make the simplifying assumption that all servers process a request instantly). To mimic the Linux stack we used, htsim's TCP's retransmission timer is lower bounded at 200ms. The simulation results qualitatively agree with our experimental findings -the best setup is the one where two local aggregators are used. Further, the simulations confirm that incast is the root cause of the increased delay observed. It is however unclear why the best setup behaves significantly better than the case where aggregators are farther away from the servers-the reason may be the fact that the local setup has a wider range of RTTs from the servers to the aggregator: a third of hops will be two links away from the server, another third will be 4 links away, and the rest will be 6 links away. In the worst case all servers are 6 hops away from their aggregator and this will increase the contention for the aggregators' buffer.</p><p>Our second experiment analyzes the map phase of MapReduce computation in the same cluster when there are non-local map tasks to run. Here the network is asked to rank two file transfers of 64MB from their source to the possible mappers. htsim answers this task in 1.2s, which is due to the sheer number of packets it needs to simulate (simulation time scales linearly with the number of packets simulated).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Assumptions CloudTalk assumes applications will know the amount of bytes they want to transfer and the task dependencies. Many applications already have this information: a GFS client will know the chunk metadata and the servers it wants to transfer from <ref type="bibr" target="#b5">[6]</ref>. A MapReduce scheduler has data size and placement information for map inputs and outputs as well as reduce outputs.</p><p>Further, CloudTalk assumes that the network can give accurate answers to queries. Exact answers may be challenging to give because of dynamic traffic outside the cloud provider's control, as generated by other tenants and by software not using CloudTalk. To alleviate this problem the network provider could query its switches to figure out the large flows utilizing its links, as shown by Hedera <ref type="bibr" target="#b0">[1]</ref> or DevoFlow <ref type="bibr" target="#b4">[5]</ref>. This information would then be used when answering queries.</p><p>Limitations By default, the CloudTalk model assumes all traffic is TCP and that it is not application bound. Bandwidth constraints can be used to specify known application bounds as well as constant-bit rate traffic; other congestion control algorithms are not yet supported.</p><p>The language does not capture traffic burstiness within the same connection: video traffic follows this pattern, where periods of high throughput that fill the client's play-out buffer alternate with idle periods. Finally, CloudTalk does not express computational bottlenecks which can easily dominate a job's finish time.</p><p>CloudTalk optimizations involve at least a round-trip time where the application queries the network and re-ceives an answer. Even assuming the network replies instantaneously, waiting an additional RTT for every query is unacceptable for web-search. In such cases, the application can still use CloudTalk to optimize the aggregation tree but only at deployment time. As a consequence, such optimizations can only rely on static topology information.</p><p>Scalability For CloudTalk to be practical, the network needs to respond quickly to application requests. Our prototype shows that an existing packet-level simulator answers certain queries reasonably quickly without any additional optimizations. However, packet-level simulation scales poorly with the number of bytes transferred, and this is a problem for large scale data processing jobs.</p><p>To reduce this overhead implementations could make simplifying assumptions such as scaling down the sizes of the simulated transfers by one or a few orders of magnitude. This, however, will impact the accuracy. Choosing an appropriate tradeoff between response time and response accuracy requires further study.</p><p>An alternative to packet-level simulation are flowlevel simulations that scale with the number of flows simulated and are invariant of transfer sizes. The problem with such analytical solutions is that they only work well when the number of flows is constant. Adapting analytical schemes to deal with dynamic flow arrivals, and analyzing their feasibility is subject of our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Many recent works use topology information to optimize distributed applications in the datacenter. Purlieus <ref type="bibr" target="#b9">[10]</ref> uses this knowledge to instantiate virtual machines at appropriate places in the cluster such as to reduce network bottlenecks for different types of load: map intensive, shuffle intensive, or reduce intensive. Orchestra <ref type="bibr" target="#b3">[4]</ref> proposes the use of transfer controllers and global coordination to achieve scheduling across jobs, and across the entire datacenter. They show that scheduling schemes based on maxmin sharing between flows lead to 1.5x reduction in the shuffle component of the MapReduce.</p><p>Mesos <ref type="bibr" target="#b7">[8]</ref> proposes a two-level scheduler hierarchy for optimizing resource usage among different frameworks. Mesos shares our goal of making the datacenter and the applications communicate: the datacenter makes resource offers and applications choose among available ones. Mesos focuses on CPU and memory resources and assumes applications know which resources they want. In contrast, CloudTalk targets the network and assumes applications do not know what the topology is.</p><p>Optimizations such as Hedera <ref type="bibr" target="#b0">[1]</ref> or MPTCP <ref type="bibr" target="#b10">[11]</ref> have been proposed to alleviate the effects of collisions arising from random hashing of flows to paths with ECMP <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>Dialogue between the network and the applications has been acknowledged as the way to solve conflicting goals of P2P users and network providers. We argue that a similar dialogue is necessary in the datacenter. Providers want to keep their network topology a trade secret, while the users need it to optimize their applications. Infrastructure cannot be completely hidden from the users, and we show that even with limited knowledge of the topology it is possible to gain significantly: a web search application on EC2 can be as much as 3 times faster with proper placement of the aggregators.</p><p>We propose CloudTalk, a language designed to allow users to describe their network tasks such that the network can accurately estimate their completion time, and this information then enables application optimizations. Our simulations show that CloudTalk is both expressive and feasible in practice: tasks describing web search are ranked correctly in under 1.5s for a 1200 node network.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: EC2 US-EAST Availability Zone D Topology as inferred by measurement in December 2011</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 2: Web Search architecture with two levels of aggregation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>The description of Task A is:</head><label></label><figDesc></figDesc><table>ID From To Size Start 
1 
A 
C 1000 
0 
2 
A 
D 1000 
0 
3 
C 
A 10000 finish(1) 
4 
D 
A 10000 finish(2) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the anonymous reviewers for their valuable feedback. C. Raiciu has been partially supported by the CHANGE research project, financed by the European Comission through its FP7 programme. M. Ionescu and D. Niculescu have been partially supported by the project POSDRU/89/1.5/S/62557.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic flow scheduling for data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad Al-Fares Et Al</forename><surname>Hedera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Usenix NSDI</title>
		<meeting>Usenix NSDI</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the marginal utility of network topology measurements. IMW &apos;01, ACM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="5" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;03, ACM</title>
		<imprint>
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Managing data transfers in computer clusters with orchestra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf Chowdhury Et</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM &apos;11</title>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Devoflow: scaling flow management for high-performance networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2011 conference</title>
		<meeting>the ACM SIGCOMM 2011 conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="254" to="265" />
		</imprint>
	</monogr>
	<note>SIGCOMM &apos;11, ACM</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay Ghemawat Et</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the nineteenth ACM symposium on Operating systems principles</title>
		<meeting>the nineteenth ACM symposium on Operating systems principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
	<note>SOSP &apos;03, ACM</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">VL2: a scalable and flexible data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert Greenberg El</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Sigcomm</title>
		<meeting>ACM Sigcomm</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mesos: a platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<imprint>
			<publisher>USENIX Association</publisher>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Analysis of an equal-cost multi-path algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Hopps</forename><surname>Rfc</surname></persName>
		</author>
		<ptr target="http://tools.ietf.org/html/rfc2992" />
		<imprint>
			<date type="published" when="2000-12" />
			<biblScope unit="volume">2992</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Purlieus: locality-aware resource allocation for mapreduce in a cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji Palanisamy Et</forename><surname>Al</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>SC &apos;11, ACM</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving datacenter performance and robustness with Multipath TCP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Costin Raiciu Et</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Applicationlayer traffic optimization (ALTO) problem statement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seedorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rfc</surname></persName>
		</author>
		<ptr target="http://tools.ietf.org/html/rfc5693" />
		<imprint>
			<date type="published" when="2009-10" />
			<biblScope unit="volume">5693</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ictcp</surname></persName>
		</author>
		<title level="m">Incast congestion control for TCP in data center networks. Co-NEXT &apos;10, ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
