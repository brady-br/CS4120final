<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trash Day: Coordinating Garbage Collection in Distributed Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Maas</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Oracle Labs</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harris</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Oracle Labs</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asanovi´c</forename><surname>Krste</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Asanovi´c</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kubiatowicz</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Trash Day: Coordinating Garbage Collection in Distributed Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cloud systems such as Hadoop, Spark and Zookeeper are frequently written in Java or other garbage-collected languages. However, GC-induced pauses can have a significant impact on these workloads. Specifically, GC pauses can reduce throughput for batch workloads, and cause high tail-latencies for interactive applications. In this paper, we show that distributed applications suffer from each node&apos;s language runtime system making GC-related decisions independently. We first demonstrate this problem on two widely-used systems (Apache Spark and Apache Cassandra). We then propose solving this problem using a Holistic Runtime System, a distributed language runtime that collectively manages run-time services across multiple nodes. We present initial results to demonstrate that this Holistic GC approach is effective both in reducing the impact of GC pauses on a batch workload, and in improving GC-related tail-latencies in an interactive setting.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Garbage-collected languages are the dominant choice for distributed applications in cloud data centers. Languages such as C#, Go, Java, JavaScript/node.js, PHP/Hack, Python, Ruby and Scala account for a large portion of code in this environment. Popular frameworks such as Hadoop, Spark and Zookeeper are written in these languages, cloud services such as Google AppEngine and Microsoft Azure target these languages directly, and companies such as Twitter <ref type="bibr" target="#b7">[7]</ref> or Facebook <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref> build a significant portion of their software around them.</p><p>This reflects the general trend towards higher-level languages. Their productivity and safety properties of-ten outweigh performance disadvantages, particularly for companies that cannot afford the engineering workforce to maintain large native code bases.</p><p>Garbage collection (GC) underpins many of the advantages of high-level languages. GC helps productivity because it reduces the engineering effort of explicitly managing pointer ownership. It also eliminates a large class of bugs, increases safety and avoids many sources of memory leaks (the latter is very important in cloud settings where applications often run for a long time).</p><p>GC performs well for many single-machine workloads. However, as we show in Section 2, it is a doubleedged sword in the cloud setting. In latency-critical applications such as web servers or databases, GC pauses can cause requests to take unacceptably long times (this is even true for minor GC pauses at the order of milliseconds, as sub-millisecond latency requirements are increasingly common). This is exacerbated in applications that are composed of hundreds of services, where the overall latency depends on the slowest component (as common in data center workloads <ref type="bibr" target="#b10">[10]</ref>). GC also poses a problem for applications that distribute live data across nodes, since pauses can make a node's data unavailable.</p><p>In our work, we investigate the sources of GC-related problems in current data center applications. We first show how to alleviate these problems by coordinating GC pauses between different nodes, such that they occur at times that are convenient for the application. We then show how a Holistic Runtime System <ref type="bibr" target="#b15">[15]</ref> can be used to achieve this in a general way using an approach we call Holistic Garbage Collection (Section 3). We finally present a work-in-progress Holistic Runtime System currently under development at UC Berkeley (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GC in Distributed Applications</head><p>Data center applications written in high-level languages are typically deployed by running each process within its own, independent language runtime system (such as a Java Virtual Machine or Common Language Runtime). Frameworks such as Hadoop or Spark hence run over multiple runtime systems on different nodes, communicating through libraries such as Apache Thrift <ref type="bibr" target="#b1">[2]</ref>.</p><p>A consequence of this approach is that each runtime system makes decisions independently, including over when to perform GC. In practice, this means that GC pauses occur on different nodes at different times based on when memory fills up and needs to be collected. Depending on the collector and workload, these pauses can range from milliseconds to multiple seconds.</p><p>In this section, we show how GC pauses cause problems in two representative real-world systems, Apache Spark <ref type="bibr" target="#b20">[20]</ref> and Apache Cassandra <ref type="bibr" target="#b14">[14]</ref>. We next demonstrate how even simple strategies can alleviate these problems. In Section 3, we then show how these strategies can be generalized to fit a wider range of systems.</p><p>We use the commodity OpenJDK Hotspot JVM (using the GC settings provided by each application). There are specialized systems -such as those used in real-time scenarios -that limit or even eliminate GC pauses by running GC concurrently with the application <ref type="bibr" target="#b18">[18]</ref>. However, this usually comes at the cost of reduced overall performance or increased resource utilization (e.g., from barrier or trap handling). Furthermore, these specialized runtime systems still incur pauses if memory fills up faster than it can be collected. To our knowledge, none of these systems are widely used in cloud settings.</p><p>We perform all experiments on a cluster of 2-socket Xeon E5-2660 machines with 256GB RAM, connected through Infiniband. All our workloads run on dedicated nodes, but the network is shared with other jobs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Case Study I: Apache Spark</head><p>Apache Spark is a distributed computation framework representative for a class of applications often called batch workloads. Spark jobs perform large-scale, longrunning computations on distributed data sets and performance is measured in overall job execution time.</p><p>Problem. When running a job on a cluster, Spark spawns a worker process on each node. Each worker then performs a series of tasks on its local data. Occasionally, workers communicate data between nodes (for example, during shuffle operations). In those cases, no node can continue until data has been exchanged with every other node (equivalent to a cluster-wide barrier).</p><p>This synchronization causes problems in the presence of GC pauses: if even a single node is stuck in GC during such an operation, no other node can make progress, and therefore all nodes stall for a significant amount of time. Worse, once the stalled node finishes its GC, execution will continue and may quickly trigger a GC pause  <ref type="formula">4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29</ref>  (b) Coordinating GC (stop-the-world everywhere) <ref type="figure">Figure 1</ref>: Impact of GC on the superstep durations of Spark PageRank (darker = more nodes performing GC during a superstep; white = no GC). This does not count minor collections, which occur much more frequently but have negligible impact. on a different node, as nodes are likely to not allocate any memory while stalling idly, and therefore only trigger GC while actually performing productive work.</p><p>To illustrate this problem, <ref type="figure">Figure 1a</ref> shows a PageRank computation using Spark on an 8-node cluster (the same workload as in the original Spark paper <ref type="bibr" target="#b20">[20]</ref>). We show the time that each PageRank superstep takes, as well as how many nodes incur a GC pause during that superstep. We clearly see that while superstep times are homogeneous in the absence of GC, they increase significantly as soon as at least one node is performing a collection. <ref type="figure" target="#fig_0">Figure 2a</ref> shows the reason for this -memory occupancy increases independently on the different nodes, and once it reaches a threshold, a collection is performed, independently from the state of the other nodes.</p><p>Strategy. Instead of nodes collecting independently once their memory fills up, we want all nodes to perform GC at the same time -this way, none of the nodes waste time waiting for another node to finish collecting. This is reminsiscent of the use of gang-scheduling in multithreaded synchronization-heady workloads. <ref type="figure">Figure 1b</ref> and <ref type="figure" target="#fig_0">Figure 2b</ref> show the effect of this strategy: we instrumented all the JVMs in our Spark cluster to track their occupancy, and as soon as any one node reaches an occupancy threshold (80% in this case), we triggered a GC on all nodes in the Spark cluster. Even on our small cluster, the PageRank computation completed 15% faster overall, without tuning or modifications to the JVM. This effect will become significantly more pronounced on larger cluster sizes, since this increases the likelihood of some node incurring a GC during each superstep. Typical Spark deployments can contain 100 nodes or more <ref type="bibr" target="#b20">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Case Study II: Apache Cassandra</head><p>Apache Cassandra is a distributed key-value store. It uses consistent hashing to replicate key-value pairs across multiple nodes. Requests can be sent to any node, which then acts as a coordinator and contacts all replicas of a particular key-value pair to assemble a read or write quorum. Cassandra is an example of an interactive workload: The most important metric is the latency of each query. Systems usually require that queries take a somewhat predictable time. Requests that take longer than, say, the 99.9 percentile of requests are called "stragglers" and cause problems with other services relying on data from the Cassandra cluster, e.g., to serve web requests.</p><p>Cassandra uses a different collector than the Spark example from Section 2.1: pauses are only incurred for collecting the young generation that stores freshly allocated objects. A concurrent collector is used for the old generation (running alongside the application workload on other threads). While Spark incurs minor collections as well, they have little impact there -however, they do affect Cassandra due to its low-latency requirements.</p><p>Problem. GC is a key contributor to stragglers in many interactive systems <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>. <ref type="figure" target="#fig_1">Figure 3a</ref> shows Cassandra update request latencies for a YCSB <ref type="bibr" target="#b9">[9]</ref> workload averaged over 10ms intervals. While 99.9% of queries take less than 3.3ms, the remainder takes up to 25× longer. While GC is not the only cause, most of these long query latencies coincide with GC on at least one of the nodes.</p><p>The impact of GC is two-fold. While many stragglers stem from a GC pause in the coordinator responsible for a request, others result from GC pauses in nodes holding accessed replicas, making it impossible for the coordintor to assemble a quorum. Other (non-GC) sources of stalls include periodic tasks such as anti-entropy <ref type="bibr" target="#b0">[1]</ref>. Strategy. Stragglers due to stalled coordinators can be avoided by anticipating GCs and redirecting requests to nodes that are not about to incur a GC pause. Even though minor GC pauses may only be a few ms in duration, modern data center networks enable such coordination between machines over much finer timescales.</p><p>We ran an experiment where we expose memory occupancy of all nodes in the cluster to the YCSB load generator and steer requests away from nodes that are about to collect, indicated by 80% occupancy of the young generation <ref type="figure" target="#fig_2">(Figure 4</ref>). This strategy is effective in eliminating many of the stragglers <ref type="figure" target="#fig_1">(Figure 3b</ref>). The 99.9 %ile update latency is improved from 3.3 ms to 1.6 ms, the worst case from 83 ms to 19 ms. A more general strategy could extend steering to periodic tasks, as well as ensuring that only one node within each quorum is collecting at a time (by staggering GCs, as we describe in Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A General Solution: Holistic GC</head><p>After identifying several strategies to coordinate GC in distributed systems (Section 2), our goal is to generalize these ideas to a wider range of applications. Today, applications implement ad-hoc solutions to this problem, such as reducing GC frequency using non-idiomatic Java (e.g., large byte arrays to store data structures), writing critical parts of Java applications in C (with explicit memory management), or treating old-generation GC as a failure mode and restarting the application. We believe that these strategies can be implemented in a better and more general way via a Holistic Runtime System <ref type="bibr" target="#b15">[15]</ref>.</p><p>A Holistic Runtime System is a language runtime system in which resource management policies span multiple nodes. Instead of making decisions about GC, JIT, etc. independently, the per-node runtime systems that underly a distributed application form a distributed system that allows the runtimes to make globally coordinated consensus decisions. We call the GC decisions made by such a system Holistic GC. Note that the heaps on the different nodes remain separate, and software still uses their same communication libraries for sharing data.</p><p>We briefly highlight the key components for enabling Holistic GC within a Holistic Runtime System: (I) GC Policy. Single-node runtime systems provide different garbage collectors to match the requirements of different applications (e.g., throughput vs. pause times). The same can be true for Holistic GC. In the examples from Section 2, we saw two very different GC strategies: For Spark workloads, a good strategy is Stop-theworld everywhere (STWE), where all nodes stop for GC at the same time, while Cassandra requires the opposite, a Staggered GC (SGC) strategy that avoids bringing down too many replicas of an entry at the same time.</p><p>(II) Communication. It is often beneficial for the Holistic GC framework to communicate with the application-level framework. One example is exposing information about which nodes are about to perform GC, so that applications can select nodes that will remain available to fulfill a request (as with the Cassandra requests above). Similarly, we may want to handle maintenance tasks such as anti-entropy in addition to GC. The Holistic Runtime could call into the application-level framework to execute such tasks at suitable times.</p><p>Communication in the other direction is important as well. For example, an application can communicate how long it expects to be idle, so that the Holistic Runtime can make decisions to run an incremental GC pass <ref type="bibr" target="#b2">[3]</ref>.</p><p>(III) Reconfiguration. The correct strategy is not just dependent on the application, it can also depend on the application's program phase. For example, STWE may be correct for a shuffle phase in Spark, but not while Spark is accessing the file system (in which case it may be better to collect only on data nodes that are not currently accessed). Furthermore, the set of nodes that need to be coordinated may change depending on the operation (e.g., a distributed coordination operation may only involve a subset of the nodes). The system must be able to reconfigure itself to respond to these changes.</p><p>The above features should be implemented in a general and extensible way -there will be many applicationspecific special cases, and it must be possible to express them. At the same time, configuration in the general case should be easy -ideally, the developer should be able to choose from a set of prepared, configurable strategies, similar to how garbage collectors are configured today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Towards a Prototype Holistic Runtime</head><p>We will now describe our vision for implementing these ideas in a Holistic Runtime System currently under development at UC Berkeley.</p><p>Our system is based on the OpenJDK Hotspot JVM as the per-node runtime. It is a drop-in replacement for Java; all that is required is to change the PATH, and Java programs will run under the Holistic Runtime System, transparently to the application. This approach lets us run a large number of unmodified workloads (including the Apache Hadoop ecosystem). We augment each pernode runtime with a management process that is logically part of the runtime system and connects to Hotspot through its management interface. This process can interact with Hotspot by (for example) reading out memory diagnostics or triggering GC. We will extend Hotspot as necessary to expose additional control features not supported through the current management interface.</p><p>The per-node runtimes automatically connect to each other and implement a consensus protocol that executes the GC Policy (I). The policy is written in a DSL and is a function that considers the current state of the entire system (such as the memory occupancy on each node) and produces a plan describing what language events each node should perform next ( <ref type="figure" target="#fig_3">Figure 5)</ref>.</p><p>We now divide execution into epochs of varying lengths, during which the runtimes exchange their state and execute the policy to generate a plan that defines 1) the length of the next epoch, 2) on which nodes to perform language operations such as GC during the next epoch, 3) any necessary reconfiguration (III) and 4) updates to a set of key-value pairs stored on each node. The latter are a general mechanism for communication be- tween Holistic Runtime and application (II), through an API to access the key-value pairs from user-level applications and frameworks.</p><p>Once the plan has been created, it is shared between all nodes and executed throughout the next epoch. The function describing the policy is user-defined (making it very flexible), but a library of standard strategies can be supplied for ease-of-use, and as a basis to compose them into more complex strategies. The system will allow users to write policies in a domain-specific language that can query state (such as memory occupancy or the key-value pairs), construct a plan (i.e., define language events to be triggered at a particular time on a specific set of nodes), and compose policies.</p><p>To make this possible, the framework abstracts away decentralized policy execution and time synchronization (we are planning to achieve this by using a failuretolerant consensus protocol such as Raft <ref type="bibr" target="#b16">[16]</ref>). We are also planning to integrate the Holistic Runtime with cluster managers such as Mesos <ref type="bibr" target="#b13">[13]</ref>, to feed information to the cluster manager that may help it to make scheduling decisions, and vice-versa. From a user perspective, all that needs to be implemented is a policy function that takes the states of all nodes and produces a plan.</p><p>Example Strategies. We anticipate that applications will require different strategies to coordinate GC between nodes. While these strategies will probably be based on a small set of primitives (e.g., STWE), there will be application-specific aspects that require knowledge available to the programmer (e.g., about strategies that work best in particular phases of the application).</p><p>We therefore expect that applications will be shipped with a GC policy included. For example, the strategies identified for the two case studies from Section 2 could be described as follows (ignoring reconfiguration):</p><p>Algorithm 1 Spark (Section 2.1) GC Policy (I) STWE (in: nodes, out: plan) if nodes.filter(x: x.oldgen ≥ 0.8) = {} then plan += MajorGC(nodes)</p><p>We assume a constant epoch for simplicity (e.g., 100ms), but both the epoch and the GC threshold (0.8) could be dynamically adapted to match the allocation rate.</p><p>Algorithm 2 Cassandra (Section 2.2) GC Policy (I) SGC (in: nodes, out: plan) static gc group = 0 for n ∈ nodes do state[n steeraway] = (n.newgen &gt; 0.8); if (n%Q == gc group and n.newgen &gt; 0.8) then gc nodes += n if len(gc nodes) = 0 then plan += MinorGC(gc nodes) gc group = (gc group+1)%Q Communication (II) KEY-VALUE PAIRS bool n steeraway: true tells application to avoid node</p><p>Here, the gc_group variable ensures that only one node in a quorum (where Q is the quorum size) can ever perform GC: Nodes in Cassandra are arranged in a ring, with replicas in successive nodes, so by ensuring that nodes that are Q apart from each other perform GC at the same time, we stagger GC in such a way that no two nodes in a quorum perform GC in the same time. Note that this assumes that memory does not fill up before a GC is triggered -this is ensured by steering requests away from nodes close to GC, reducing their allocation rates. The application-level code that steers the load to the different nodes can access the "n steeraway" key-value pairs to decide which nodes to send requests to.</p><p>Related Work. Several concurrent projects are also investigating the interaction between GC and distributed workloads. Broom <ref type="bibr" target="#b12">[12]</ref> confirms the problems we showed in Spark for iterative Naiad workloads and proposes to eliminate GC altogether through region-based memory management. Other concurrent work confirms the impact of GC on interactive workloads <ref type="bibr" target="#b17">[17]</ref>, and proposes strategies <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b19">19]</ref> similar to our steering approach. In contrast, we focus on supporting all these strategies in a general Holistic Runtime System that is simple to deploy and minimally invasive to existing applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We are currently working on a complete Holistic Runtime System, supporting a wide range of applications and allowing developers to easily implement whichever GC policies they like for their applications. In doing so, we want to provide fault tolerance and integrate into existing ecosystems such as Apache (e.g., supporting YARN). Our plan is to make this system openly available, for developers to implement and experiment with GC policies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Memory (old generation) occupancy and GC pauses of Spark PageRank. Each of the colors represents a node, and vertical lines indicate the start of a GC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Latencies of Cassandra update queries over time. Colors represent the nodes that act as the coordinator for each query, and the faded vertical lines in the background indicate GC pauses on those nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Avoiding GC-induced stragglers in Cassandra by steering requests away from nodes during GC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of the Holistic GC approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30</head><label>1</label><figDesc></figDesc><table>Superstep 

0 
5 
10 
15 
20 
25 
30 
35 

Execution time (s) 

(a) Baseline System (no coordination) 

1 2 3 </table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">AntiEntropy (Apache Wiki Entry)</title>
		<ptr target="https://wiki.apache.org/cassandra/AntiEntropy" />
		<imprint/>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Apache Thrift</title>
		<ptr target="http://thrift.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">G1: One Garbage Collector To Rule Them All</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Available</surname></persName>
		</author>
		<ptr target="http://www.infoq.com/articles/G1-One-Garbage-Collector-To-Rule-Them-All" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hack: A New Programming Language for HHVM</title>
		<ptr target="https://code.facebook.com/posts/264544830379293/hack-a-new-programming-language-for-hhvm/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On garbage collection</title>
		<ptr target="http://hhvm.com/blog/431/on-garbage-collection" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cinnober on GC pause-free Java applications through orchestrated memory management</title>
		<ptr target="http://www.cinnober.com/sites/cinnober.com/files/news/Cinnober%20on%20GC%20pause%20free%20Java%20applications.pdf" />
		<imprint/>
	</monogr>
	<note>Predictable Low Latency</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Twitter Shifting More Code to JVM, Citing Performance and Encapsulation As Primary Drivers</title>
		<ptr target="http" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Measuring SOLR Query Performance</title>
		<ptr target="https://29min.wordpress.com/2013/07/31/measuring-solr-query-performance/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1st ACM Symposium on Cloud Computing</title>
		<meeting>1st ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 21st ACM SIGOPS Symposium on Operating Systems Principles</title>
		<meeting>21st ACM SIGOPS Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the Causes of Consistency Anomalies in Apache Cassandra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mckenzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Golab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Broom: Sweeping out Garbage Collection from Big Data systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Giceva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Viswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramalingan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX/ACM Workshop on Hot Topics in Operating Systems</title>
		<meeting>the 15th USENIX/ACM Workshop on Hot Topics in Operating Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mesos: A Platform for Fine-grained Resource Sharing in the Data Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation</title>
		<meeting>the 8th USENIX Conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cassandra: A Decentralized Structured Storage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Case for the Holistic Language Runtime System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First International Workshop on Rackscale Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
		<title level="m">Proceedings of the 2014 USENIX Annual Technical Conference</title>
		<meeting>the 2014 USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Search of an Understandable Consensus Algorithm</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making Sense of Performance in Data Analytics Frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ratnasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Networked Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">C4: The Continuously Concurrent Compacting Collector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Memory Management</title>
		<meeting>the International Symposium on Memory Management</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Blade: A Data Center Garbage Collector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Terei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation</title>
		<meeting>the 9th USENIX Conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
