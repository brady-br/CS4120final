<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">New</forename><surname>York</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
							<email>mjcollins@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Polynomial-Time Dynamic Programming Algorithm for Phrase-Based Decoding with a Fixed Distortion Limit</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Decoding of phrase-based translation models in the general case is known to be NP-complete, by a reduction from the traveling salesman problem (Knight, 1999). In practice, phrase-based systems often impose a hard distortion limit that limits the movement of phrases during translation. However, the impact on complexity after imposing such a constraint is not well studied. In this paper, we describe a dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The runtime of the algorithm is O(nd!lh d+1) where n is the sentence length, d is the distortion limit, l is a bound on the number of phrases starting at any position in the sentence, and h is related to the maximum number of target language translations for any source word. The algorithm makes use of a novel representation that gives a new perspective on decoding of phrase-based models.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phrase-based translation models ( <ref type="bibr" target="#b9">Koehn et al., 2003;</ref><ref type="bibr" target="#b16">Och and Ney, 2004</ref>) are widely used in statistical machine translation. The decoding problem for phrase-based translation models is known to be difficult: the results from <ref type="bibr" target="#b8">Knight (1999)</ref> imply that in the general case decoding of phrase-based translation models is NP-complete.</p><p>The complexity of phrase-based decoding comes from reordering of phrases. In practice, however, various constraints on reordering are often imposed in phrase-based translation systems. A common constraint is a "distortion limit", which places a hard constraint on how far phrases can move. The complexity of decoding with such a distortion limit is an open question: the NP-hardness result from Knight * On leave from Columbia University.</p><p>(1999) applies to a phrase-based model with no distortion limit.</p><p>This paper describes an algorithm for phrasebased decoding with a fixed distortion limit whose runtime is linear in the length of the sentence, and for a fixed distortion limit is polynomial in other factors. More specifically, for a hard distortion limit d, and sentence length n, the runtime is O(nd!lh d+1 ), where l is a bound on the number of phrases starting at any point in the sentence, and h is related to the maximum number of translations for any word in the source language sentence.</p><p>The algorithm builds on the insight that decoding with a hard distortion limit is related to the bandwidth-limited traveling salesman problem (BTSP) ( <ref type="bibr" target="#b13">Lawler et al., 1985)</ref>. The algorithm is easily amenable to beam search. It is quite different from previous methods for decoding of phrase-based models, potentially opening up a very different way of thinking about decoding algorithms for phrasebased models, or more generally for models in statistical NLP that involve reordering. <ref type="bibr" target="#b8">Knight (1999)</ref> proves that decoding of word-to-word translation models is NP-complete, assuming that there is no hard limit on distortion, through a reduction from the traveling salesman problem. Phrasebased models are more general than word-to-word models, hence this result implies that phrase-based decoding with unlimited distortion is NP-complete.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Phrase-based systems can make use of both reordering constraints, which give a hard "distortion limit" on how far phrases can move, and reordering models, which give scores for reordering steps, often penalizing phrases that move long distances. Moses ( <ref type="bibr" target="#b11">Koehn et al., 2007b</ref>) makes use of a distortion limit, and a decoding algorithm that makes use of bit-strings representing which words have been translated. We show in Section 5.2 of this paper that this can lead to at least 2 n/4 bit-strings for an input sentence of length n, hence an exhaustive version of this algorithm has worst-case runtime that is exponential in the sentence length. The current paper is concerned with decoding phrase-based models with a hard distortion limit.</p><p>Various other reordering constraints have been considered. <ref type="bibr" target="#b21">Zens and Ney (2003)</ref> and <ref type="bibr" target="#b22">Zens et al. (2004)</ref> consider two types of hard constraints: the IBM constraints, and the ITG (inversion transduction grammar) constraints from the model of <ref type="bibr" target="#b19">Wu (1997)</ref>. They give polynomial time dynamic programming algorithms for both of these cases. It is important to note that the IBM and ITG constraints are different from the distortion limit constraint considered in the current paper. Decoding algorithms with ITG constraints are further studied by <ref type="bibr" target="#b5">Feng et al. (2010)</ref> and <ref type="bibr" target="#b2">Cherry et al. (2012)</ref>. <ref type="bibr" target="#b12">Kumar and Byrne (2005)</ref> describe a class of reordering constraints and models that can be encoded in finite state transducers. <ref type="bibr" target="#b14">Lopez (2009)</ref> shows that several translation models can be represented as weighted deduction problems and analyzes their complexities. 1 <ref type="bibr" target="#b9">Koehn et al. (2003)</ref> describe a beamsearch algorithm for phrase-based decoding that is in widespread use; see Section 5 for discussion.</p><p>A number of reordering models have been proposed, see for example <ref type="bibr" target="#b18">Tillmann (2004)</ref>, <ref type="bibr" target="#b10">Koehn et al. (2007a)</ref> and <ref type="bibr" target="#b6">Galley and Manning (2008)</ref>. <ref type="bibr" target="#b3">DeNero and Klein (2008)</ref> consider the phrase alignment problem, that is, the problem of finding an optimal phrase-based alignment for a sourcelanguage/target-language sentence pair. They show that in the general case, the phrase alignment problem is NP-hard. It may be possible to extend the techniques in the current paper to the phrasealignment problem with a hard distortion limit.</p><p>Various methods for exact decoding of phrasebased translation models have been proposed. <ref type="bibr">Za- slavskiy et al. (2009)</ref> describe the use of travel-1 An earlier version of this paper states the complexity of decoding with a distortion limit as O(I 3 2 d ) where d is the distortion limit and I is the number of words in the sentence; however (personal communication from Adam Lopez) this runtime is an error, and should be O(2 I ) i.e., exponential time in the length of the sentence. A corrected version of the paper corrects this. ing salesman algorithms for phrase-based decoding. <ref type="bibr" target="#b1">Chang and Collins (2011)</ref> describe an exact method based on Lagrangian relaxation. <ref type="bibr" target="#b0">Aziz et al. (2014)</ref> describe a coarse-to-fine approach. These algorithms all have exponential time runtime (in the length of the sentence) in the worst case.</p><p>Galley and Manning (2010) describe a decoding algorithm for phrase-based systems where phrases can have discontinuities in both the source and target languages. The algorithm has some similarities to the algorithm we propose: in particular, it makes use of a state representation that contains a list of disconnected phrases. However, the algorithms differ in several important ways: <ref type="bibr" target="#b7">Galley and Manning (2010)</ref> make use of bit string coverage vectors, giving an exponential number of possible states; in contrast to our approach, the translations are not formed in strictly left-to-right ordering on the source side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background: The Traveling Salesman Problem on Bandwidth-Limited Graphs</head><p>This section first defines the bandwidth-limited traveling salesman problem, then describes a polynomial time dynamic programming algorithm for the traveling salesman path problem on bandwidth limited graphs. This algorithm is the algorithm proposed by <ref type="bibr" target="#b13">Lawler et al. (1985)</ref> 2 with small modifications to make the goal a path instead of a cycle, and to consider directed rather than undirected graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bandwidth-Limited TSPPs</head><p>The input to the problem is a directed graph G = (V, E), where V is a set of vertices and E is a set of directed edges. We assume that V = {1, 2, . . . , n}.</p><p>A directed edge is a pair (i, j) where i, j ∈ V , and i = j. Each edge (i, j) ∈ E has an associated weight w i,j . Given an integer k ≥ 1, a graph is bandwidth-limited with bandwidth k if</p><formula xml:id="formula_0">∀(i, j) ∈ E, |i − j| ≤ k</formula><p>The traveling salesman path problem (TSPP) on the graph G is defined as follows. We will assume that vertex 1 is the "source" vertex and vertex n is the "sink" vertex. The TSPP is to find the minimum cost directed path from vertex 1 to vertex n, which passes through each vertex exactly once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An Algorithm for Bandwidth-Limited TSPPs</head><p>The key idea of the dynamic-programming algorithm for TSPPs is the definition of equivalence classes corresponding to dynamic programming states, and an argument that the number of equivalence classes depends only on the bandwidth k.</p><p>The input to our algorithm will be a directed graph G = (V, E), with weights w i,j , and with bandwidth k. We define a 1-n path to be any path from the source vertex 1 to the sink vertex n that visits each vertex in the graph exactly once. A 1-n path is a subgraph (V , E ) of G, where V = V and E ⊆ E.</p><p>We will make use of the following definition:</p><p>Definition 1. For any 1-n path H, define H j to be the subgraph that H induces on vertices 1, 2, . . . j, where 1 ≤ j ≤ n.</p><p>That is, H j contains the vertices 1, 2, . . . j and the edges in H between these vertices.</p><p>For a given value for j, we divide the vertices V into three sets A j , B j and C j :</p><formula xml:id="formula_1">• A j = {1, 2, . . . , (j − k)} (A j is the empty set if j ≤ k).</formula><p>• B j = {1 . . . j} \ A j . <ref type="bibr">3</ref> • C j = {j + 1, j + 2, . . . , n} (C j is the empty set if j = n).</p><p>Note that the vertices in subgraph H j are the union of the sets A j and B j . A j is the empty set if j ≤ k, but B j is always non-empty. The following Lemma then applies:</p><p>Lemma 1. For any 1-n path H in a graph with bandwidth k, for any 1 ≤ j ≤ n, the subgraph H j has the following properties:</p><p>1. If vertex 1 is in A j , then vertex 1 has degree one. 2. For any vertex v ∈ A j with v ≥ 2, vertex v has degree two. 3. H j contains no cycles.</p><p>Proof. The first and second properties are true because of the bandwidth limit. Under the constraint of bandwidth k, any edge (u, v) in H such that u ∈ A j , must have v ∈ A j ∪ B j = H j . This follows because if v ∈ C j = {j + 1, j + 2, . . . n} and u ∈ A j = {1, 2, . . . j − k}, then |u − v| &gt; k. Similarly any edge (u, v) ∈ H such that v ∈ A j must have u ∈ A j ∪ B j = H j . It follows that for any vertex u ∈ A j , with u &gt; 1, there are edges (u, v) ∈ H j and (v , u) ∈ H j , hence vertex u has degree 2. For vertex u ∈ A j with u = 1, there is an edge (u, v) ∈ H j , hence vertex u has degree 1. The third property (no cycles) is true because H j is a subgraph of H, which has no cycles.</p><p>It follows that each connected component of H j is a directed path, that the start points of these paths are in the set {1} ∪ B j , and that the end points of these paths are in the set B j .</p><p>We now define an equivalence relation on subgraphs. Two subgraphs H j and H j are in the same equivalence class if the following conditions hold (taken from <ref type="bibr" target="#b13">Lawler et al. (1985)</ref>):</p><p>1. For any vertex v ∈ B j , the degree of v in H j and H j is the same. 2. For each path (connected component) in H j there is a path in H j with the same start and end points, and conversely.</p><p>The significance of this definition is as follows.</p><p>Assume that H * is an optimal 1-n path in the graph, and that it induces the subgraph H j on vertices 1 . . . j. Assume that H j is another subgraph over vertices 1 . . . j, which is in the same equivalence class as H j . For any subgraph H j , define c(H j ) to be the sum of edge weights in H j :</p><formula xml:id="formula_2">c(H j ) = (u,v)∈H j w u,v</formula><p>Then it must be the case that c(H j ) ≥ c(H j ). Otherwise, we could simply replace H j by H j in H * , thereby deriving a new 1-n path with a lower cost, implying that H * is not optimal.</p><p>This observation underlies the dynamic programming approach. Define σ to be a function that maps a subgraph H j to its equivalence class σ(H j ). The equivalence class σ(H j ) is a data structure that stores the degrees of the vertices in B j , together with the start and end points of each connected component in H j .</p><p>Next, define ∆ to be a set of 0, 1 or 2 edges between vertex (j + 1) and the vertices in B j . For any subgraph H j+1 of a 1-n path, there is some ∆, simply found by recording the edges incident to vertex (j + 1). For any H j , define τ (σ(H j ), ∆) to be the equivalence class resulting from adding the edges in ∆ to the data structure σ(H j ). If adding the edges in ∆ to σ(H j ) results in an ill-formed subgraph-for example, a subgraph that has one or more cyclesthen τ (σ(H j ), ∆) is undefined. The following recurrence then defines the dynamic program (see Eq. 20 of <ref type="bibr" target="#b13">Lawler et al. (1985)</ref>):</p><formula xml:id="formula_3">α(j + 1, S) = min ∆,S :τ (S ,∆)=S α(j, S ) + c(∆)</formula><p>Here S is an equivalence class over vertices {1 . . . (j +1)}, and α(S, j +1) is the minimum score for any subgraph in equivalence class S. The min is taken over all equivalence classes S over vertices {1 . . . j}, together with all possible values for ∆.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Dynamic Programming Algorithm for Phrase-Based Decoding</head><p>We now describe the dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. We first give basic definitions for phrasebased decoding, and then describe the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic Definitions</head><p>Consider decoding an input sentence consisting of words x 1 . . . x n for some integer n. We assume that x 1 = &lt;s&gt; and x n = &lt;/s&gt; where &lt;s&gt; and &lt;/s&gt; are the sentence start and end symbols respectively. A phrase-based lexicon specifies a set of possible translations in the form of phrases p = (s, t, e), where s and t are integers such that 1 ≤ s ≤ t ≤ n, and e is a sequence of m ≥ 1 target-language words e 1 . . . e m . This signifies that words x s . . . x t in the source language have a translation as e 1 . . . e m in the target language. We use s(p), t(p) and e(p) to refer to the three components of a phrase p = (s, t, e), and e 1 (p) . . . e m (p) to refer to the words in the targetlanguage string e(p). We assume that (1, 1, &lt;s&gt;) and (n, n, &lt;/s&gt;) are the only translation entries with s(p) ≤ 1 and t(p) ≥ n respectively. A derivation is then defined as follows: Definition 2 (Derivations). A derivation is a sequence of phrases p 1 . . . p L such that</p><formula xml:id="formula_4">• p 1 = (1, 1, &lt;s&gt;) and p L = (n, n, &lt;/s&gt;).</formula><p>• Each source word is translated exactly once.</p><p>• The distortion limit is satisfied for each pair of phrases p i−1 , p i , that is:</p><formula xml:id="formula_5">|t(p i−1 ) + 1 − s(p i )| ≤ d ∀ i = 2 . . . L.</formula><p>where d is an integer specifying the distortion limit in the model.</p><p>Given a derivation p 1 . . . p L , a target-language translation can be obtained by concatenating the target-language strings</p><formula xml:id="formula_6">e(p 1 ) . . . e(p L ).</formula><p>The scoring function is defined as follows:</p><formula xml:id="formula_7">f (p 1 . . . p L ) = λ(e(p 1 ) . . . e(p L )) + L i=1 κ(p i ) + L i=2 η × |t(p i−1 ) + 1 − s(p i )| (1)</formula><p>For each phrase p, κ(p) is the translation score for the phrase. The parameter η is the distortion penalty, which is typically a negative constant. λ(e) is a language model score for the string e. We will assume a bigram language model:</p><formula xml:id="formula_8">λ(e 1 . . . e m ) = m i=2 λ(e i |e i−1 ).</formula><p>The generalization of our algorithm to higher-order n-gram language models is straightforward. The goal of phrase-based decoding is to find y * = arg max y∈Y f (y) where Y is the set of valid derivations for the input sentence.</p><p>Remark (gap constraint): Note that a common restriction used in phrase-based decoding ( <ref type="bibr" target="#b9">Koehn et al., 2003;</ref><ref type="bibr" target="#b1">Chang and Collins, 2011</ref>), is to impose an additional "gap constraint" while decoding. See Chang and Collins (2011) for a description. In this case it is impossible to have a dynamicprogramming state where word x i has not been translated, and where word x i+k has been translated, for k &gt; d. This limits distortions further, and it can be shown in this case that the number of possible bitstrings is O(2 d ) where d is the distortion limit. Without this constraint the algorithm of <ref type="bibr" target="#b9">Koehn et al. (2003)</ref> actually fails to produce translations for many input sentences ( <ref type="bibr" target="#b1">Chang and Collins, 2011</ref>). Figure 1: Sub-derivations H j for j ∈ {1, 3, 4, 6, 7, 8, 9} induced by the full derivation H = (1, 1, &lt;s&gt;)(2, 3, we must)(4, 4, also)(8, 8, take)(5, 6, these criticisms)(7, 7, seriously)(9, 9&lt;/s&gt;) . Note that H j includes the phrases that cover spans ending before or at position j. Sub-derivation H j is extended to another subderivation H j+i by incorporating a phrase of length i.</p><formula xml:id="formula_9">H 1 = π 1 = 1, 1, &lt;s&gt; H 3 = π 1 = 1, 1, &lt;s&gt; 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Algorithm</head><p>We now describe the dynamic programming algorithm. Intuitively the algorithm builds a derivation by processing the source-language sentence in strictly left-to-right order. This is in contrast with the algorithm of <ref type="bibr" target="#b11">Koehn et al. (2007b)</ref>, where the targetlanguage sentence is constructed from left to right.</p><p>Throughout this section we will use π, or π i for some integer i, to refer to a sequence of phrases:</p><formula xml:id="formula_10">π = p 1 . . . p l where each phrase p i = (s(p i ), t(p i ), e(p i ))</formula><p>, as defined in the previous section. We overload the s, t and e operators, so that if</p><formula xml:id="formula_11">π = p 1 . . . p l , we have s(π) = s(p 1 ), t(π) = t(p l ), and e(π) = e(p 1 ) · e(p 2 ) . . . · e(p l )</formula><p>, where x · y is the concatenation of strings x and y.</p><p>A derivation H consists of a single phrase sequence π = p 1 . . . p L :</p><formula xml:id="formula_12">H = π = p 1 . . . p L</formula><p>where the sequence p 1 . . . p L satisfies the constraints in definition 2.</p><p>We now give a definition of sub-derivations and complement sub-derivations:</p><formula xml:id="formula_13">Definition 3 (Sub-derivations and Complement Sub- -derivations). For any H = p 1 . . . p L , for any j ∈ {1 . . . n} such that ∃ i ∈ {1 . . . L} s.t. t(p i ) =</formula><p>j, the sub-derivation H j and the complement subderivation ¯ H j are defined as</p><formula xml:id="formula_14">H j =π 1 . . . π r , ¯ H j = ¯ π 1 . . . ¯ π r</formula><p>where the following properties hold:</p><p>• r is an integer with r ≥ 1.</p><p>• Each π i for i = 1 . . . r is a sequence of one or more phrases, where each phrase p ∈ π i has t(p) ≤ j.</p><p>• Each ¯ π i for i = 1 . . . (r − 1) is a sequence of one or more phrases, where each phrase p ∈ ¯ π i has s(p) &gt; j.</p><p>• ¯ π r is a sequence of zero or more phrases, where each phrase p ∈ ¯ π r has s(p) &gt; j. We have zero phrases in ¯ π r iff j = n where n is the length of the sentence.</p><p>• Finally,</p><formula xml:id="formula_15">π 1 · ¯ π 1 · π 2 · ¯ π 2 . . . π r · ¯ π r = p 1 . .</formula><p>. p L where x · y denotes the concatenation of phrase sequences x and y. Note that for any j ∈ {1 . . . n} such that i ∈ {1 . . . L} such that t(p i ) = j, the sub-derivation H j and the complement sub-derivation ¯ H j is not defined.</p><p>Thus for each integer j such that there is a phrase in H ending at point j, we can divide the phrases in H into two sets: phrases p with t(p) ≤ j, and phrases p with s(p) &gt; j. The sub-derivation H j lists all maximal sub-sequences of phrases with t(p) ≤ j. The complement sub-derivation ¯ H j lists all maximal sub-sequences of phrases with s(p) &gt; j. <ref type="figure">Figure 1</ref> gives all sub-derivations H j for the derivation</p><formula xml:id="formula_16">H = p 1 . . . p 7 = (1, 1, &lt;s&gt;)(2, 3, we must)(4, 4, also)</formula><p>(8, 8, take)(5, 6, these criticisms) (7, 7, seriously)(9, 9, &lt;/s&gt;)</p><p>As one example, the sub-derivation H 7 = π 1 , π 2 induced by H has two phrase sequences:</p><formula xml:id="formula_17">π 1 = (1, 1, &lt;s&gt;)(2, 3, we must)(4, 4, also)</formula><p>π 2 = (5, 6, these criticisms)(7, 7, seriously)</p><p>Note that the phrase sequences π 1 and π 2 give translations for all words x 1 . . . x 7 in the sentence. There 63 are two disjoint phrase sequences because in the full derivation H, the phrase p = (8, 8, take), with t(p) = 8 &gt; 7, is used to form a longer sequence of phrases π 1 p π 2 . For the above example, the complement subderivation ¯ H 7 is as follows:</p><formula xml:id="formula_18">¯ π 1 = (8, 8, take) ¯ π 2 = (9, 9, &lt;/s&gt;)</formula><p>It can be verified that π 1 · ¯ π 1 ·π 2 · ¯ π 2 = H as required by the definition of sub-derivations and complement sub-derivations.</p><p>We now state the following Lemma: Lemma 2. For any derivation H = p 1 . . . p L , for any j such that ∃i such that t(p i ) = j, the subderivation H j = π 1 . . . π r satisfies the following properties:</p><p>1. s(π 1 ) = 1 and e 1 (π 1 ) = &lt;s&gt;. 2. For all positions i ∈ {1 . . . j}, there exists a phrase p ∈ π, for some phrase sequence</p><formula xml:id="formula_19">π ∈ H j , such that s(p) ≤ i ≤ t(p). 3. For all i = 2 . . . r, s(π i ) ∈ {(j − d + 2) . . . j} 4. For all i = 1 . . . r, t(π i ) ∈ {(j − d) . . . j}</formula><p>Here d is again the distortion limit.</p><p>This lemma is a close analogy of Lemma 1. The proof is as follows: Proof of Property 1: For all values of j, the phrase p 1 = (1, 1, &lt;s&gt;) has t(p 1 ) ≤ j, hence we must have π 1 = p 1 . . . p k for some k ∈ {1 . . . L}. It follows that s(π 1 ) = 1 and e 1 (π 1 ) = &lt;s&gt;. Proof of Property 2: For any position i ∈ {1 . . . j}, define the phrase (s, t, e) in the derivation H to be the phrase that covers word i; i.e., the phrase such that s ≤ i ≤ t. We must have s ∈ {1 . . . j}, because s ≤ i and i ≤ j. We must also have t ∈ {1 . . . j}, because otherwise we have s ≤ j &lt; t, which contradicts the assumption that there is some i ∈ {1 . . . L} such that t(p i ) = j. It follows that the phrase (s, t, e) has t ≤ j, and from the definition of sub-derivations it follows that the phrase is in one of the phrase sequences π 1 . . . π r . Proof of Property 3: This follows from the distortion limit. Consider the complement sub-derivation ¯ H j = ¯ π 1 . . . ¯ π r . For the distortion limit to be satisfied, for all i ∈ {2 . . . r}, we must have</p><formula xml:id="formula_20">|t(¯ π i−1 ) + 1 − s(π i )| ≤ d</formula><p>We must also have t(¯ π i−1 ) &gt; j, and s(π i ) ≤ j, by the definition of sub-derivations. It follows that s(π i ) ∈ {(j − d + 2) . . . j}. Proof of Property 4: This follows from the distortion limit. First consider the case where ¯ π r is non-empty. For the distortion limit to be satisfied, for all i ∈ {1 . . . r}, we must have</p><formula xml:id="formula_21">|t(π i ) + 1 − s(¯ π i )| ≤ d</formula><p>We must also have t(π i ) ≤ j, and s(¯ π i ) &gt; j, by the definition of sub-derivations. It follows that t(π i ) ∈ {(j − d) . . . j}.</p><p>Next consider the case where ¯ π r is empty. In this case we must have j = n. For the distortion limit to be satisfied, for all i ∈ {1 . . . (r − 1)}, we must have</p><formula xml:id="formula_22">|t(π i ) + 1 − s(¯ π i )| ≤ d</formula><p>We must also have t(π i ) ≤ j, and s(¯ π i ) &gt; j, by the definition of sub-derivations. It follows that t(π i ) ∈ {(j − d) . . . j} for i ∈ {1 . . . (r − 1)}. For i = r, we must have t(π i ) = n, from which it again follows that t(π r ) = n ∈ {(j − d) . . . j}.</p><p>We now define an equivalence relation between sub-derivations, which will be central to the dynamic programming algorithm. We define a function σ that maps a phrase sequence π to its signature. The signature is a four-tuple:</p><formula xml:id="formula_23">σ(π) = (s, w s , t, w t ).</formula><p>where s is the start position, w s is the start word, t is the end position and w t is the end word of the phrase sequence. We will use s(σ), w s (σ), t(σ), and w t (σ) to refer to each component of a signature σ.</p><p>For example, given a phrase sequence</p><formula xml:id="formula_24">π = (1, 1, &lt;s&gt;) (2, 2, we) (4, 4, also) ,</formula><p>its signature is σ(π) = (1, &lt;s&gt;, 4, also).</p><p>The signature of a sub-derivation H j = π 1 . . . π r is defined to be σ(H j ) = σ(π 1 ) . . . σ(π r ).</p><p>For example, with H 7 as defined above, we have σ(H 7 ) = 1, &lt;s&gt;, 4, also , 5, these, 7, seriously</p><p>Two partial derivations H j and H j are in the same equivalence class iff σ(H j ) = σ(H j ). We can now state the following Lemma: Lemma 3. Define H * to be the optimal derivation for some input sentence, and H * j to be a subderivation of H * . Suppose H j is another subderivation with j words, such that σ(H j ) = σ(H * j ). Then it must be the case that f (H * j ) ≥ f (H j ), where f is the function defined in Section 4.1.</p><p>Proof. Define the sub-derivation and complement sub-derivation of H * as</p><formula xml:id="formula_25">H * j = π 1 . . . π r ¯ H * j = ¯ π 1 . . . ¯ π r</formula><p>We then have</p><formula xml:id="formula_26">f (H * ) = f (H * j ) + f ( ¯ H * j ) + γ<label>(2)</label></formula><p>where f (. . .) is as defined in Eq. 1, and γ takes into account the bigram language modeling scores and the distortion scores for the transitions</p><formula xml:id="formula_27">π 1 → ¯ π 1 , ¯ π 1 → π 2 , π 2 → ¯ π 2 , etc.</formula><p>The proof is by contradiction. Define</p><formula xml:id="formula_28">H j = π 1 . . . π r and assume that f (H * j ) &lt; f (H j ). Now consider H = π 1 ¯ π 1 π 2 ¯ π 2 . . . π r ¯ π r</formula><p>This is a valid derivation because the transitions</p><formula xml:id="formula_29">π 1 → ¯ π 1 , ¯ π 1 → π 2 , π 2 → ¯ π 2</formula><p>have the same distortion distances as π 1 → ¯ π 1 , ¯ π 1 → π 2 , π 2 → ¯ π 2 , hence they must satisfy the distortion limit.</p><p>We have</p><formula xml:id="formula_30">f (H ) = f (H j ) + f ( ¯ H * j ) + γ<label>(3)</label></formula><p>where γ has the same value as in Eq. 2. This follows because the scores for the transitions</p><formula xml:id="formula_31">π 1 → ¯ π 1 , ¯ π 1 → π 2 , π 2 → ¯ π 2 are identical to the scores for the transitions π 1 → ¯ π 1 , ¯ π 1 → π 2 , π 2 → ¯ π 2 , because σ(H * j ) = σ(H j ). It follows from Eq. 2 and Eq. 3 that if f (H j ) &gt; f (H * j ), then f (H ) &gt; f (H * ). But this contradicts the assumption that H * is optimal. It follows that we must have f (H j ) ≤ f (H * j )</formula><p>. This lemma leads to a dynamic programming algorithm. Each dynamic programming state consists of an integer j ∈ {1 . . . n} and a set of r signatures: <ref type="figure" target="#fig_0">Figure 2</ref> shows the dynamic programming algorithm. It relies on the following functions:</p><formula xml:id="formula_32">T = (j, {σ 1 . . . σ r })</formula><p>Inputs:</p><p>• An integer n specifying the length of the input sequence.</p><p>• A function δ(T ) returning the set of valid transitions from state T .</p><p>• A function τ (T, ∆) returning the state reached from state T by transition ∆ ∈ δ(T ).</p><p>• A function valid(T ) returning TRUE if state T is valid, otherwise FALSE.</p><p>• A function score(∆) that returns the score for any transition ∆.</p><p>Initialization:</p><formula xml:id="formula_33">T 1 = (1, {(1, &lt;s&gt;, 1, &lt;s&gt;)}) α(T 1 ) = 0 T 1 = {T 1 }, ∀j ∈ {2 . . . n}, T j = ∅ for j = 1, . . . , n − 1 for each state T ∈ T j for each ∆ ∈ δ(T ) T = τ (T, ∆) if valid(T ) = FALSE: continue score = α(T ) + score(∆)</formula><p>Define t to be the integer such that</p><formula xml:id="formula_34">T = (t, {σ 1 . . . σr}) if T / ∈ Tt Tt = Tt ∪ {T } α(T ) = score bp(T ) = (∆) else if score &gt; α(T ) α(T ) = score bp(T ) = (∆)</formula><p>Return: the score of the state (n, {(1, &lt;s&gt;, n, &lt;/s&gt;)}) in Tn, and backpointers bp defining the transitions leading to this state. • For any state T , δ(T ) is the set of outgoing transitions from state T .</p><p>• For any state T , for any transition ∆ ∈ δ(T ), τ (T, ∆) is the state reached by transition ∆ from state T .</p><p>• For any state T , valid(T ) checks if a resulting state is valid.</p><p>• For any transition ∆, score(∆) is the score for the transition. We next give full definitions of these functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Definitions of δ(T ) and τ (T, ∆)</head><p>Recall that for any state T , δ(T ) returns the set of possible transitions from state T . In addition τ (T, ∆) returns the state reached when taking transition ∆ ∈ δ(T ).</p><p>Given the state T = (j, {σ 1 . . . σ r }), each transition is of the form ψ 1 p ψ 2 where ψ 1 , p and ψ 2 are defined as follows:</p><p>• p is a phrase such that s(p) = j + 1.</p><p>• ψ 1 ∈ {σ 1 . . . σ r } ∪ {φ}. If ψ 1 = φ, it must be the case that |t(ψ 1 ) + 1 − s(p)| ≤ d and t(ψ 1 ) = n.</p><p>• ψ 2 ∈ {σ 1 . . . σ r } ∪ {φ}. If ψ 2 = φ, it must be the case that |t(p) + 1 − s(ψ 2 )| ≤ d and s(ψ 2 ) = 1.</p><p>• If ψ 1 = φ and ψ 2 = φ, then ψ 1 = ψ 2 . Thus there are four possible types of transition from a state T = (j, {σ 1 . . . σ r }):</p><p>Case 1: ∆ = φ p φ. In this case the phrase p is incorporated as a stand-alone phrase. The new state T is equal to (j , {σ 1 . . . σ r+1 }) where j = t(p), where σ i = σ i for i = 1 . . . r, and σ r+1 = (s(p), e 1 (p), t(p), e m (p)). , where j = t(p), where σ i is replaced by (s(p), e 1 (p), t(σ i ), w t (σ i )), and where σ i = σ i for all i = i.</p><p>Case 4: ∆ = σ i p σ i for some σ i , σ i ∈ {σ 1 . . . σ r }, with i = i. In this case phrase p is appended to signature σ i , and prepended to signature σ i , effectively joining the two signatures together. In this case the new state T = τ (T, ∆) is of the form (j , σ 1 . . . σ r−1 ), where signatures σ i and σ i are replaced by a new signature (s(σ i ), w s (σ i ), t(σ i ), w t (σ i )), and all other signatures are copied across from T to T . <ref type="figure" target="#fig_2">Figure 3</ref> gives the dynamic programming states and transitions for the derivation H in <ref type="figure">Figure 1</ref>. For example, the sub-derivation</p><formula xml:id="formula_35">H 7 =</formula><p>(1, 1, &lt;s&gt;)(2, 3, we must)(4, 4, also) , (5, 6, these criticisms) <ref type="bibr">(7, 7, seriously)</ref> will be mapped to a state T = 7, σ(H 7 ) = 7, (1, &lt;s&gt;, 4, also), (5, these, 7, seriously) 1,</p><formula xml:id="formula_36">σ 1 = 1, &lt;s&gt;, 1, &lt;s&gt; 3, σ 1 = 1, &lt;s&gt;, 3, must 4, σ 1 = 1, &lt;s&gt;, 4, also 6, σ 1 = 1, &lt;s&gt;, 4, also , σ 2 =</formula><p>5, these, 6, criticisms 7, σ 1 = 1, &lt;s&gt;, 4, also , σ 2 = 5, these, 7, seriously 8, σ 1 = 1, &lt;s&gt;, 7, seriously 9, </p><formula xml:id="formula_37">σ 1 = 1, &lt;s&gt;, 9, &lt;/s&gt;</formula><formula xml:id="formula_38">σ i = σ(π i ) for all π i ∈ H j .</formula><p>The transition σ 1 (8, 8, take) σ 2 from this state leads to a new state,</p><formula xml:id="formula_39">T = 8, σ 1 = (1, &lt;s&gt;, 7, seriously)</formula><p>4.3 Definition of score(∆) <ref type="figure">Figure 4</ref> gives the definition of score(∆), which incorporates the language model, phrase scores, and distortion penalty implied by the transition ∆. <ref type="figure" target="#fig_3">Figure 5</ref> gives the definition of valid(T ). This function checks that the start and end points of each signature are in the set of allowed start and end points given in Lemma 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Definition of valid(T )</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">A Bound on the Runtime of the Algorithm</head><p>We now give a bound on the algorithm's run time. This will be the product of terms N and M , where N is an upper bound on the number of states in the dynamic program, and M is an upper bound on the number of outgoing transitions from any state. For any j ∈ {1 . . . n}, define first(j) to be the set of target-language words that can begin at position j and last(j) to be the set of target-language <ref type="figure">Figure 4</ref>: Four operations that can extend a state T = (j, {σ 1 . . . σ r }) by a phrase p = (s, t, e 1 . . . e m ), and the scores incurred. We definê</p><formula xml:id="formula_40">∆ Resulting phrase sequence score(∆) φ p φ (s, e 1 , t, em) ˆ w(p) σ i p φ (s(σ i ), ws(σ i ), t, em) ˆ w(p) + λ(e 1 |wt(σ i )) + η × |t(σ i ) + 1 − s| φ p σ i (s, e 1 , t(σ i ), wt(σ i )) ˆ w(p) + λ(ws(σ i )|em) + η × |t + 1 − s(σ i )| σ i p σ i (s(σ i ), ws(σ i ), t(σ i ), wt(σ i )) ˆ w(p) + λ(e 1 |wt(σ i )) + η × |t(σ i ) + 1 − s| +λ(ws(σ i )|em) + η × |t + 1 − s(σ i )|</formula><formula xml:id="formula_41">w(p) = κ(p) + m i=2 λ(e i (p)|e i−1 (p)).</formula><p>The functionˆwfunctionˆ functionˆw(p) includes the phrase translation model κ and the language model scores that can be computed using p alone. The weight η is the distortion penalty. In addition, define singles(j) to be the set of phrases that translate the single word at position j: singles(j) = {p : s(p) = j and t(p) = j} Next, define h to be the smallest integer such that for all j, |first(j)| ≤ h, |last(j)| ≤ h, and |singles(j)| ≤ h. Thus h is a measure of the maximal ambiguity of any word x j in the input.</p><formula xml:id="formula_42">Function valid(T ) Input: State T = j, {σ 1 . . . σr} for i = 1 . . . r if s(σ i ) &lt; j − d + 2 and s(σ i ) = 1 return FALSE if t(σ i ) &lt; j − d return FALSE return TRUE</formula><p>Finally, for any position j, define start(j) to be the set of phrases starting at position j:</p><formula xml:id="formula_43">start(j) = {p : s(p) = j}</formula><p>and define l to be the smallest integer such that for all j, |start(j)| ≤ l. Given these definitions we can state the following result: Theorem 1. The time complexity of the algorithm is O(nd!lh d+1 ).</p><p>To prove this we need the following definition: Definition 4 (p-structures). For any finite set A of integers with |A| = k, a p-structure is a set of r ordered pairs {(s i , t i )} r i=1 that satisfies the following properties: 1) 0 ≤ r ≤ k; 2) for each i ∈ {1 . . . r}, s i ∈ A and t i ∈ A (both s i = t i and s i = t i are allowed); 3) for each j ∈ A, there is at most one index i ∈ {1 . . . r} such that (s i = j) or (t i = j) or (s i = j and t i = j).</p><p>We use g(k) to denote the number of unique pstructures for a set A with |A| = k.</p><p>We then have the following Lemmas:</p><p>Lemma 4. The function g(k) satisfies g(0) = 0, g(1) = 2, and the following recurrence for k ≥ 2:</p><formula xml:id="formula_44">g(k) = 2g(k − 1) + 2(n − 1)g(k − 2)</formula><p>Proof. The proof is in Appendix A.</p><formula xml:id="formula_45">Lemma 5. Consider the function h(k) = k 2 × g(k). h(k) is in O((k − 2)!).</formula><p>Proof. The proof is in Appendix B.</p><p>We can now prove the theorem: Proof of Theorem 1: First consider the number of states in the dynamic program. Each state is of the form (j, {σ 1 . . . σ r }) where the set {(s(σ i ), t(σ i ))} r i=1 is a p-structure over the set {1}∪ {(j − d) . . . d}. The number of possible values for {(s(σ i ), e(σ i ))} r i=1 is at most g(d + 2). For a fixed choice of {(s(σ i ), t(σ i ))} r i=1 we will argue that there are at most h d+1 possible values for</p><formula xml:id="formula_46">{(w s (σ i ), w t (σ i ))} r i=1</formula><p>. This follows because for each k ∈ {(j − d) . . . j} there are at most h possible choices: if there is some i such that s(σ i ) = k, and t(σ i ) = k, then the associated word w s (σ i ) is in the set first(k); alternatively if there is some i such that t(σ i ) = k, and s(σ i ) = k, then the associated word w t (σ i ) is in the set last(k); alternatively if there is some i such that s(σ i ) = t(σ i ) = k then the associated words w s (σ i ), w t (σ i ) must be the first/last word of some phrase in singles(k); alternatively there is no i such that s(σ i ) = k or t(σ i ) = k, in which case there is no choice associated with position k in the sentence. Hence there are at most h choices associated with each position k ∈ {(j − d) . . . j}, giving h d+1 choices in total. Combining these results, and noting that there are n choices of the variable j, implies that there are at most ng(d + 2)h d+1 states in the dynamic program. Now consider the number of transitions from any state. A transition is of the form ψ 1 pψ 2 as defined in Section 4.2.1. For a given state there are at most (d + 2) choices for ψ 1 and ψ 2 , and l choices for p, giving at most (d + 2) 2 l choices in total.</p><p>Multiplying the upper bounds on the number of states and number of transitions for each state gives an upper bound on the runtime of the algorithm as</p><formula xml:id="formula_47">O(ng(d + 2)h d+1 (d + 2) 2 l).</formula><p>Hence by Lemma 5 the runtime is O(nd!lh d+1 ) time.</p><p>The bound g(d + 2) over the number of possible values for {(s(σ i ), e(σ i ))} r i=1 is somewhat loose, as the set of p-structures over</p><formula xml:id="formula_48">{1} ∪ {(j − d) . . . d} in- cludes impossible values {(s i , t i )} r i=1</formula><p>where for example there is no i such that s(σ i ) = 1. However the bound is tight enough to give the O(d!) runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>We conclude the paper with discussion of some issues. First we describe how the dynamic programming structures we have described can be used in conjunction with beam search. Second, we give more analysis of the complexity of the widely-used decoding algorithm of <ref type="bibr" target="#b9">Koehn et al. (2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Beam Search</head><p>Beam search is widely used in phrase-based decoding; it can also be applied to our dynamic programming construction. We can replace the line for each state T ∈ T j in the algorithm in <ref type="figure" target="#fig_0">Figure 2</ref> with</p><formula xml:id="formula_49">for each state T ∈ beam(T j )</formula><p>where beam is a function that returns a subset of T j , most often the highest scoring elements of T j under some scoring criterion. A key question concerns the choice of scoring function γ(T ) used to rank states. One proposal is to define γ(T ) = α(T ) + β(T ) where α(T ) is the score used in the dynamic program, and β(T ) = i:ws(σ i ) =&lt;s&gt; λ u (w s (σ i )). Here λ u (w) is the score of word w under a unigram language model. The β(T ) scores allow different states in T j , which have different words w s (σ i ) at the start of signatures, to be comparable: for example it compensates for the case where w s (σ i ) is a rare word, which will incur a low probability when the bigram w w s (σ i ) for some word w is constructed during search.</p><p>The β(T ) values play a similar role to "future scores" in the algorithm of <ref type="bibr" target="#b9">Koehn et al. (2003)</ref>. However in the <ref type="bibr" target="#b9">Koehn et al. (2003)</ref> algorithm, different items in the same beam can translate different subsets of the input sentence, making futurescore estimation more involved. In our case all items in T j translate all words x 1 . . . x j inclusive, which may make comparison of different hypotheses more straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Complexity of Decoding with Bit-string Representations</head><p>A common method for decoding phrase-based models, as described in <ref type="bibr" target="#b9">Koehn et al. (2003)</ref>, is to use beam search in conjunction with a search algorithm that 1) creates the target language string in strictly left-to-right order; 2) uses a bit string with bits b i ∈ {0, 1} for i = 1 . . . n representing at each point whether word i in the input has been translated. A natural question is whether the number of possible bit strings for a model with a fixed distortion limit d can grow exponentially quickly with respect to the length of the input sentence. This section gives an example that shows that this is indeed the case. Assume that our sentence length n is such that (n − 2)/4 is an integer. Assume as before x 1 = &lt;s&gt; and x n = &lt;/s&gt;. For each k ∈ {0 . . . ((n − 2)/4 − 1)}, assume we have the following phrases for the words x 4k+2 . . . x 4k+5 :</p><formula xml:id="formula_50">(4k + 2, 4k + 2, u k ) (4k + 3, 4k + 3, v k ) (4k + 4, 4k + 4, w k ) (4k + 5, 4k + 5, z k ) (4k + 4, 4k + 5, y k )</formula><p>Note that the only source of ambiguity is for each k whether we use y k to translate the entire phrase x 4k+4 x 4k+5 , or whether we use w k and z k to translate x 4k+4 and x 4k+5 separately. With a distortion limit d ≥ 5, the number of possible bit strings in this example is at least 2 (n−2)/4 . This follows because for any setting of the variables b 4k+4 ∈ {0, 1} for k ∈ {0 . . . ((n − 2)/4 − 1)}, there is a valid derivation p 1 . . . p L such that the prefix p 1 . . . p l where l = 1 + (n − 2)/4 gives this bit string. Simply choose p 1 = (1, 1, &lt;s&gt;) and for l ∈ {0 . . . (n − 2)/4 − 1} choose p l +2 = (4l + 4, 4l + 5, y i ) if b 4k+4 = 1, p l +2 = (4l + 5, 4l + 5, z i ) otherwise. It can be verified that p 1 . . . p l is a valid prefix (there is a valid way to give a complete derivation from this prefix). As one example, for n = 10, and b 4 = 1 and b 8 = 0, a valid derivation is (1, 1, &lt;s&gt;)(4, 5, y 1 )(9, 9, z 2 )(7, 7, v 2 )(3, 3, v 1 ) (2, 2, u 1 )(6, 6, u 2 )(8, 8, w 2 )(10, 10, &lt;/s&gt;)</p><p>In this case the prefix (1, 1, &lt;s&gt;)(4, 5, y 1 )(9, 9, z 2 ) gives b 4 = 1 and b 8 = 0. Other values for b <ref type="bibr">4</ref> and b 8 can be given by using (5, 5, z 1 ) in place of (4, 5, y 1 ), and (8, 9, y 2 ) in place of (9, 9, z 2 ), with the following phrases modified appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have given a polynomial-time dynamic programming algorithm for phrase-based decoding with a fixed distortion limit. The algorithm uses a quite different representation of states from previous decoding algorithms, is easily amenable to beam search, and leads to a new perspective on phrase-based decoding. Future work should investigate the effectiveness of the algorithm in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Lemma 4</head><p>Without loss of generality assume A = {1, 2, 3, . . . k}. We have g(1) = 2, because in this case the valid p-structures are {(1, 1)} and ∅. To calculate g(k) we can sum over four possibilities:</p><p>Case 1: There are g(k − 1) p-structures with s i = t i = 1 for some i ∈ {1 . . . r}. This follows because once s i = t i = 1 for some i, there are g(k − 1) possible p-structures for the integers {2, 3, 4 . . . k}.</p><p>Case 2: There are g(k − 1) p-structures such that s i = 1 and t i = 1 for all i ∈ {1 . . . r}. This follows because once s i = 1 and t i = 1 for all i, there are g(k − 1) possible p-structures for the integers {2, 3, 4 . . . k}.</p><p>Case 3: There are (k − 1) × g(k − 2) p-structures such that there is some i ∈ {1 . . . r} with s i = 1 and t i = 1. This follows because for the i such that s i = 1, there are (k − 1) choices for the value for t i , and there are then g(k − 2) possible p-structures for the remaining integers in the set {1 . . . k}/{1, t i }.</p><p>Case 4: There are (k − 1) × g(k − 2) p-structures such that there is some i ∈ {1 . . . r} with t i = 1 and s i = 1. This follows because for the i such that t i = 1, there are (k − 1) choices for the value for s i , and there are then g(k − 2) possible p-structures for the remaining integers in the set {1 . . . k}/{1, s i }.</p><p>Summing over these possibilities gives the following recurrence:</p><formula xml:id="formula_51">g(k) = 2g(k − 1) + 2(k − 1) × g(k − 2)</formula><p>B Proof of Lemma 5</p><p>Recall that h(k) = f (k) × g(k) where f (k) = k 2 . Define k 0 to be the smallest integer such that for all k ≥ k 0 ,</p><formula xml:id="formula_52">2f (k) f (k − 1) + 2f (k) f (k − 2) · k − 1 k − 3 ≤ k − 2 (4)</formula><p>For f (k) = k 2 we have k 0 = 9. Now choose a constant c such that for all k ∈ {1 . . . </p><formula xml:id="formula_53">= 2f (k) f (k − 1) h(k − 1) + 2f (k) f (k − 2) (k − 1)h(k − 2) ≤ 2cf (k) f (k − 1) + 2cf (k) f (k − 2) · k − 1 k − 3 (k − 3)! (6) ≤ c(k − 2)!<label>(7)</label></formula><p>Eq. 5 follows from g(k) = 2g(k−1)+2(k−1)g(k− 2). Eq. 6 follows by the inductive hypothesis that h(k − 1) ≤ c(k − 3)! and h(k − 2) ≤ c(k − 4)!. Eq 7 follows because Eq. 4 holds for all k ≥ k 0 .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The phrase-based decoding algorithm. α(T ) is the score for state T . The bp(T ) variables are backpointers used in recovering the highest scoring sequence of transitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Case 2 :</head><label>2</label><figDesc>∆ = σ i p φ for some σ i ∈ {σ 1 . . . σ r }. In this case the phrase p is appended to the signa- ture σ i . The new state T = τ (T, ∆) is of the form (j , σ 1 . . . σ r ), where j = t(p), where σ i is replaced by (s(σ i ), w s (σ i ), t(p), e m (p)), and where σ i = σ i for all i = i. Case 3: ∆ = φ p σ i for some σ i ∈ {σ 1 . . . σ r }. In this case the phrase p is prepended to the signa- ture σ i . The new state T = τ (T, ∆) is of the form (j , σ 1 . . . σ r )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Dynamic programming states and the transitions from one state to another, using the same example as in Figure 1. Note that σ i = σ(π i ) for all π i ∈ H j .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The valid function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(k 0 − 1)}, h(k) ≤ c × (k − 2)!. We will prove by induction that under these definitions of k 0 and c we have h(k) ≤ c(k − 2)! for all integers k, hence h(k) is in O((k − 2)!). For values k ≥ k 0 , we have h(k) = f (k)g(k) = 2f (k)g(k − 1) + 2f (k)(k − 1)g(k − 2) (5)</figDesc></figure>

			<note place="foot" n="2"> The algorithm is based on the ideas of Monien and Sudborough (1981) and Ratliff and Rosenthal (1983).</note>

			<note place="foot" n="3"> For sets X and Y we use the notation X \ Y to refer to the set difference: i.e., X \ Y = {x|x ∈ X and x / ∈ Y }.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Exact decoding for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exact decoding of phrase-based translation models through Lagrangian relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On hierarchical re-ordering and permutation parsing for phrase-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Workshop on Statistical Machine Translation</title>
		<meeting>the Seventh Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The complexity of phrase alignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th</title>
		<meeting>the 46th</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="25" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An efficient shift-reduce decoding algorithm for phrasedbased machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics: Posters</title>
		<meeting>the 23rd International Conference on Computational Linguistics: Posters</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="285" to="293" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple and effective hierarchical phrase reordering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate non-hierarchical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="966" to="974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoding complexity in wordreplacement translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Statistical phrase-based translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><forename type="middle">Josef</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="48" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Edinburgh system description for the 2005 IWSLT speech translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amittai</forename><surname>Axelrod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation, StatMT &apos;07</title>
		<meeting>the Second Workshop on Statistical Machine Translation, StatMT &apos;07<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moses: Open source toolkit for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bertoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions</title>
		<meeting>the 45th annual meeting of the ACL on interactive poster and demonstration sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Alexandra Constantin, and Evan Herbst</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Local phrase reordering models for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="161" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Traveling Salesman Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">Leighton</forename><surname>Lawler</surname></persName>
		</author>
		<editor>Karel Lenstra, Alexander Hendrik George Rinnooy Kan, and David Bernard Shmoys</editor>
		<imprint>
			<date type="published" when="1985-01" />
			<publisher>John Wiley &amp; Sons Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Translation as weighted deduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 12th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="532" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bandwidth constrained NP-complete problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Monien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><forename type="middle">Hal</forename><surname>Sudborough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth annual ACM symposium on Theory of computing</title>
		<meeting>the thirteenth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1981" />
			<biblScope unit="page" from="207" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The alignment template approach to statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="449" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Orderpicking in a rectangular warehouse: a solvable case of the traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Ratliff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arnon S Rosenthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="507" to="521" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A unigram orientation model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL 2004: Short Papers</title>
		<meeting>HLT-NAACL 2004: Short Papers</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stochastic inversion transduction grammars and bilingual parsing of parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Phrase-based statistical machine translation as a traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Zaslavskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Dymetman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Cancedda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A comparative study on reordering constraints in statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 41st Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reordering constraints for phrase-based statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taro</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on Computational Linguistics</title>
		<meeting>the 20th international conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
