<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:03+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gatekeeper: Supporting Bandwidth Guarantees for Multi-tenant Datacenter Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Rodrigues</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">International Computer Science Institute (ICSI)</orgName>
								<orgName type="laboratory">Hewlett-Packard Laboratories (HP Labs)</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais (UFMG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Renato</forename><surname>Santos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">International Computer Science Institute (ICSI)</orgName>
								<orgName type="laboratory">Hewlett-Packard Laboratories (HP Labs)</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais (UFMG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshio</forename><surname>Turner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">International Computer Science Institute (ICSI)</orgName>
								<orgName type="laboratory">Hewlett-Packard Laboratories (HP Labs)</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais (UFMG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Soares</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">International Computer Science Institute (ICSI)</orgName>
								<orgName type="laboratory">Hewlett-Packard Laboratories (HP Labs)</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais (UFMG)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorgival</forename><surname>Guedes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">International Computer Science Institute (ICSI)</orgName>
								<orgName type="laboratory">Hewlett-Packard Laboratories (HP Labs)</orgName>
								<orgName type="institution">Universidade Federal de Minas Gerais (UFMG)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Gatekeeper: Supporting Bandwidth Guarantees for Multi-tenant Datacenter Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Cloud environments should provide network performance isolation for co-located untrusted tenants in a virtual-ized datacenter. We present key properties that a performance isolation solution should satisfy, and present our progress on Gatekeeper, a system designed to meet these requirements. Experiments on our Xen-based implementation of Gatekeeper in a datacenter cluster demonstrate effective and flexible control of ingress/egress link bandwidth for tenant virtual machines under both TCP and greedy unresponsive UDP traffic.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In cloud computing <ref type="bibr" target="#b1">[2]</ref> environments, mutually non-trusted tenants deploy their services in a shared datacenter infrastructure. Each tenant consists of a collection of one or more virtual machines (VMs) placed on one or more physical machines. Cloud environments have a strong requirement to enforce performance isolation among tenants that share a datacenter, but currently mechanisms are lacking to provide performance isolation for datacenter network I/O resources. Effective management of network bandwidth will be crucial to handle the growing range of service workloads that stress local area network resources in the datacenter. For example, data-intensive applications on scalable frameworks like MapReduce <ref type="bibr" target="#b4">[5]</ref> can be highly network-intensive. Also, future datacenters will merge traditional messaging traffic with network storage traffic onto a single converged datacenter fabric, using new network standards <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref> and distributed storage and file systems <ref type="bibr" target="#b9">[9]</ref>. This paper proposes properties that multi-tenant network performance isolation solutions should provide to meet the practical needs of both cloud users and cloud datacenter providers. We show that existing techniques fall short of meeting all of these requirements, and we report on our significant progress in building an I/O virtualization control system called Gatekeeper that is intended to fulfill these needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REQUIREMENTS</head><p>We argue that solutions to the tenant network performance isolation problem must have the following properties to be practical.</p><p>Scalable. A cloud datacenter supports thousands of physical servers hosting 10s of thousands of tenants and 10s to 100s of thousands of VMs. The VMs and tenants in the datacenter come and go dynamically, with a high rate of churn. Each datacenter network link is thus potentially shared by a large and churning set of VMs. A solution for network performance isolation must work at these large scales. For example, techniques that require per-tenant or per-VM state to be maintained at each switch are impractical if the need to manage a large amount of state at high speed renders the switches prohibitively expensive for cloud computing infrastructure.</p><p>Simple service performance abstraction. The cloud infrastructure provider should clearly describe the performance level that users can expect when they deploy tenants. Typically, the cloud provider presents a menu of choices for the service level of VMs to deploy. For example, Amazon EC2 offers different "instance types" like small or medium instances. While each instance type offers a clear description of the CPU performance and memory and storage capacities, I/O performance is currently only vaguely specified. We argue that users should be offered more meaningful guidelines for expected network I/O performance, allowing users to better gauge the trade-off between service level and the monetary cost the users pay the cloud provider to host their tenants.</p><p>Robust to untrusted/malicious tenants. A key advantage of Infrastructure as a Service (IaaS) cloud computing is that it allows users to run arbitrary code as tenants, giving users great flexibility for innovation. But this flexibility has the downside of allowing tenants to execute malicious code that threatens to subvert the performance of other tenants or the datacenter infrastructure itself. A performance isolation solution should limit the performance impact that malicious tenants can inflict on others without restricting tenant code flexibility by, for example, mandating the use of TCP (versus UDP, etc.) or a particular implementation of the transport protocol.</p><p>Service level flexibility. Customers need deterministic guarantees ensuring predictable performance independent of VM placement and migration, and the traffic and churn of other tenants. However, deterministic guarantees can lead to overly conservative network resource allocation with severe underutilization of the physical resources. To achieve greater resource efficiency, the cloud provider should have the flexibility to offer service levels allowing tenants to exceed their minimum guarantees. The service level should specify both minimum and maximum bandwidth levels to trade-off determinism and resource efficiency. Supporting maximum rates is important for service providers that do not want their customers to get used to high high service levels and get disappointed if their services are later reduced to their minimum guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXISTING MECHANISMS</head><p>TCP: TCP congestion control has been widely used to share network links across multiple flows. While TCP works well to provide best-effort service, it cannot enforce per tenant service guarantees. For example, if two tenants are sharing a single network link, and one tenant generates 99 TCP connections while the other generates only one, TCP will try to partition the bandwidth equally among the flows, giving 99% of the bandwidth to one tenant and only 1% to the other. Basically, TCP is designed to achieve fairness among flows and not among tenants.</p><p>Bandwidth Capping: Hypervisors such as VMware ESX and Xen have bandwidth capping mechanisms that enforce a maximum transmission rate for each virtual network interface (vNIC) associated with a virtual machine (VM). Bandwidth capping can be used to guarantee per vNIC transmission bandwidth using VM admission control to limit the total allocated bandwidth. More recent versions of hypervisors can also enforce receive bandwidth capping per vNIC. For well-behaved TCP connections, dropping packets that exceed the allocated bandwidth at the receiving vNIC causes TCP senders to reduce their rates and adapt to the available vNIC bandwidth. So using bandwidth capping could provide bandwidth guarantees at the server access links to the network in both TX and RX directions. However, this would require trusting that tenants run well-behaved TCP implementations. In essence, bandwidth capping is unable to control the ingress link bandwidth when tenants are not trusted. Another disadvantage of bandwidth capping is potential under-utilization of the link bandwidth. Using a more flexible traffic shaper such as Linux Hierarchical Token Bucket (HTB) can allow available bandwidth to be distributed to VMs with extra traffic and better utilize the link bandwidth. However, such schedulers can only be used in the transmit direction and cannot provide efficient use of receive bandwidth.</p><p>Secondnet: The Secondnet paper <ref type="bibr" target="#b12">[11]</ref> describes a datacenter network allocation mechanism that provides bandwidth guarantees for traffic between each VM pair. We argue that providing end-to-end bandwidth guarantees for each pair of tenant VM is not the ideal model, from the tenant perspective. In general, tenants do not understand their applications' communication patterns well enough to specify their bandwidth requirements between each pair of VMs. Moreover, typical communication patterns are very dynamic, and the amount of data exchanged between any pair of VM will vary significantly over time. Creating bandwidth reservations in every network link in the path for every pair of communicating VMs is likely to be inefficient, since many reservations are expected to be unused at any time. In large datacenters, efficient use of network resources will only be possible in this model with statistical guarantees. However, this would require accurate statistical models of communication patterns in tenant applications which are very difficult to determine.</p><p>Seawall: The Seawall paper <ref type="bibr" target="#b20">[19]</ref> describes a mechanism that allocates bandwidth on every link of a datacenter network by controlling rate limiters in the virtualization layer in each server at the edge of the network. Seawall's goal is to partition the bandwidth in each congested network link according to weights associated with each VMs sending traffic through that link. Congestion controlled tunnels between each pair of source and destination VM are created using sequence numbers added to each packet sent in the tunnel. Sequence numbers are stripped at the destination server and are used to detect packet losses due to network congestion. Upon receiving congestion notification messages from receivers, senders use network topology information to detect bottleneck links and adjust transmission rates at tunnels using that link. Rates are adjusted using weighted additive rate increase and multiplicative rate decrease functions, the goal of which is to partition the bandwidth in the bottleneck links according to the weights associated with VMs sending traffic to that link.</p><p>Seawall has several good design properties that are similar to our Gatekeeper design. First, since rates are enforced at the virtualization layer in the edge of the network, and tenant and rate state is distributed over the servers, the design is scalable to large datacenters. Second, the use of explicit feedback from receivers allows traffic to be throttled at the sources before they use network resources, and prevents a malicious VM to hog bandwidth in the network.</p><p>However, Seawall does not satisfy our predictable service level requirement. While Seawall can provide minimum guarantees if the maximum weight associated with each link is limited to a maximum value, it cannot enforce maximum rates to support deterministic behavior. More importantly, Seawall's bandwidth allocation does not divide the link bandwidth among tenants using the link, but among the total number of VMs sending traffic through that link. This favors tenants with a large number of VMs. For example, if a tenant has a single VM on a server but is receiving traffic from many senders it will use a significantly higher fraction of the server link receive bandwidth than a VM of a different tenant on the same server that has the same weight but is receiving traffic from just one sender. As we describe later, Gatekeeper will allocate the same bandwidth to each of the receiver VMs in this case because our service model jointly satisfies receiver and sender bandwidth guarantees.</p><p>AF-QCN: QCN (IEEE draft standard 802.1Qau) is a switch-based congestion control mechanism for datacenters. AF-QCN <ref type="bibr" target="#b13">[12]</ref> proposes extensions to QCN for multi-tenancy. Like Seawall, AF-QCN divides link bandwidth among sending VMs without respect to receivers.</p><p>Netshare: Netshare <ref type="bibr" target="#b15">[14]</ref> is the only mechanism that divides link bandwidth among tenants instead of sender VMs. However, it relies on a centralized bandwidth allocator which is difficult to scale to large datacenters and to deal with workload changes and the high rate of tenant and VM churn of cloud datacenters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Service Model</head><p>A key design decision is to choose the form of network performance guarantees that should be provided to each tenant. We argue that tenants should be given a simple performance guarantee model that is easy for them to understand and specify. <ref type="figure">Figure 1</ref> shows a simple model. In this model, all VMs of a tenant connect to a single logical non-blocking switch with guaranteed bandwidth on each access link. As it is common in real physical deployments to attach multiple servers directly to the same switch, this model should be familiar and easy to understand for users who deploy tenants in a datacenter. This model is similar to the hose model <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[10]</ref> in which throughputs are constrained only by the guaranteed bandwidths of the access links of the VMs. The use of a single logical switch has also been proposed by others as a means of applying virtualization to the network <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>To obtain a better balance between determinism and efficiency, a tenant may be offered a variation of the above model in which a VM may exceed its guaranteed minimum bandwidth at times, if there is unused bandwidth on the physical links. The amount by which a VM may exceed its minimum guarantee can be limited to a specified maximum rate, and potentially could be assigned based on a dynamic pricing scheme like a spot market.</p><p>The model can be further extended to allow composition of multiple logical switches. That is, a VM can have multiple access links each attached to a different logical switch. For example, as shown in <ref type="figure">Figure 2</ref>, in a 3-tier web service one logical switch could be used to interconnect VMs of the web server and application server tiers, and a second logical switch could connect VMs of the application server and database tiers. Each application server VM would have two access links, one attached to each logical switch with an independently specified rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reserving link bandwidth</head><p>Mapping the simple tenant performance model in <ref type="figure">Figure 1</ref> to link bandwidth reservations of an arbitrary datacenter network topology is a dificult task. Tenant applications can generate many different communication patterns that could satisfy their access link bandwidth guarantees, but generate completely different demands on each network link.</p><p>We argue that it is useful and feasible to solve a subset of this general problem that is of particular importance in practice. In particular, several recent advances in datacenter networking research <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b19">[18]</ref>, commercial products <ref type="bibr" target="#b8">[8]</ref>, and Ethernet standards <ref type="bibr" target="#b21">[20]</ref> promise to make it practical to cost-effectively scale the bisection bandwidth of large datacenter networks using multi-path switching. Even with traditional datacenter networks, network topology-aware placement of service workloads can provide full bisection bandwidth among the tenant VMs <ref type="bibr" target="#b16">[15]</ref>.</p><p>Our key observation is that using emerging scalable networks or placing tenants in bisection network regions shifts the bottleneck from the network fabric to the endpoint links that connect each physical server to the network fabric. This allows translating the problem of managing tenant network bandwidth into the more tractable problem of managing each server's network access links. Thus, tenant bandwidth management can focus on the endpoint server links, which are potentially shared by all VMs hosted on a server, instead of having to reason about network bottlenecks that could arise anywhere in the fabric which are difficult to predict without an accurate traffic pattern model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. GATEKEEPER ARCHITECTURE</head><p>Our Gatekeeper system provides network isolation for multi-tenant datacenters using a distributed mechanism implemented at the virtualization layer of each datacenter server. Gatekeeper achieves scalability using a simple point-to-point protocol and minimal datacenter-wide control state.</p><p>Gatekeeper controls the usage of each server's network access link. It provides per-vNIC link bandwidth guarantees in both directions of the network link at each physical server, i.e., for both ingress and egress traffic. Minimum bandwidth guarantees are achieved using an admission control mechanism that limits the sum of guarantees to the available physical link bandwidth. Each vNIC can exceed its guaranteed allocation when extra bandwidth is available at both transmitting and receiving endpoints. However, to provide deterministic behavior Gatekeeper limits each vNIC bandwidth to a maximum rate. By configuring the maximum rate, the system administrator can tradeoff determinism for efficiency. Complete determinism is provided by setting equal maximum rate and minimum guarantee. Maximum efficiency is provided by having no maximum rate limit. Operation between these extremes is provided by setting the maximum to a factor of the guaranteed rate.</p><p>For scheduling transmission bandwidth, Gatekeeper uses a traditional weighted fair scheduler that provides minimum bandwidth guarantees. For controlling receive bandwidth, Gatekeeper monitors the receive traffic rate at each vNIC and the physical link and determines the receive bandwidth allocation to each vNIC at periodic intervals (10 ms in our current implementation), taking into account the link usage and the minimum and maximum rates for each vNIC. If a vNIC receive bandwidth exceeds its computed allocation, Gatekeeper sends a feedback message to other remote Gatekeeper instances hosting VMs contributing to its traffic. The feedback message includes an explicit rate that is computed by distributing the desired vNIC receive rate to the senders. <ref type="figure">Figure 3</ref> shows an overview of the Gatekeeper architecture. Gatekeeper has a set of rate limiters associated with each vNIC interface. A root rate limiter enforces the maximum transmission rate of each vNIC. Additional rate limiters are dynamically created to reduce the rate of traffic sent to remote congested vNICs. A packet filter classifies outgoing packets based on their MAC address and direct then to the appropriate rate limiter.</p><p>In the absence of congestion notification messages the rate limit is increased in periodic intervals according to a linear function, and if it reaches the maximum rate the limiter is removed after a timeout interval.</p><p>Gatekeeper also keeps a dynamic set of counters associated with each vNIC. These counters measure the rate between every pair of communicating vNICs. The counters are created and deleted dynamically based on the active set of remote vNICs sending traffic to the corresponding local vNIC. Every counter also stores the MAC address of the corresponding remote vNIC which is used to send feedback messages 1 . Counters are created when packets from new sources are received and deleted after timeouts (we extended the Open vSwitch per flow packet counters to measure traffic rates). Periodically (every 10 ms in the current prototype) the measured rates are used to determine new allocated RX rates, and congestion feedback messages are generated if needed. A congestion message is generated if the aggregate rate on the physical link exceeds a given threshold (95% of the link bandwidth in the current prototype) or if a vNIC exceeds its maximum rate. If the aggregate rate exceeds the threshold, congestion feedback is generated for the vNIC that is exceeding its guaranteed receive rate by the largest relative amount. Gatekeeper sets the desired receive vNIC rate to its minimum guarantee and divides this rate among its active senders. A sender is considered active if its measured rate exceeds a threshold. A congestion feedback message is sent to each active sender with this explicit rate. The feedback message also includes the number of senders of the same tenant that are contributing to the receiver congestion. The sender uses this information to calibrate its rate increase function, such that the aggregate rate increase function at the receiving vNIC is independent of the number of senders.</p><p>Our current rate decrease function causes traffic exceeding its guarantee to be reduced to its minimum guarantee. This may be too aggressive and the link can become under-utilized for some time. An open question left for future work is to understand the tradeoff of different response functions that trade fast reaction to congestion versus fast recovery of available bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>We implemented a Gatekeeper prototype on Xen 3.4.2 using the Open vSwitch 1.1.0pre2 (www.openvswitch.org) in Dom0 running Linux 2.6.34.6. Our prototype extends the Open vSwitch flow table to track flow rates. We use Linux hierarchical token bucket (HTB) scheduler to implement our link scheduler and rate limiters. Our current implementation does not yet support dynamic creation and deletion of rate limiters; we use a preconfigured set of limiters that matches our experimental setup.</p><p>We evaluate Gatekeeper for simple scenarios using a configuration with five servers, each with a one Gb/s Ethernet interface connected to a single switch, as shown in <ref type="figure">Figure 4</ref>. The system hosts two tenants. Tenant A has two VMs and tenant B has four VMs. One shared host runs one VM from each tenant, while the others run a single VM each. Each tenant runs a netperf (www.netperf.org) microbenchmark between its VM in the shared host and all its other VMs in the other hosts, i.e. tenant A runs one netperf flow and tenant B runs 3 netperf flows. We examine two scenarios: 1) transmit (TX) bottleneck where traffic is transmitted from each VM in the shared host to the other VMs of the same tenant, and 2) receive (RX) bottleneck where traffic is transmitted from the hosts with a single VM to the shared host. We allocate 70% of each server link bandwidth to tenant A and 30% to tenant B. Tenant A is a well behaved tenant running a single TCP connection. We consider three cases for tenant B traffic type: a) no traffic b) 3 well behaved TCP flows, c) 3 UDP flows representing a malicious tenant that does not use a well behaved TCP stack. <ref type="figure" target="#fig_0">Figures 5 and 6</ref> show the results. We consider four different bandwidth allocation mechanisms: 1) No control, 2) RX and TX bandwidth capping, 3) Gatekeeper with equal maximum rate and minimum guarantee, 4) Gatekeeper without maximum rate. The horizontal dotted lines show the ideal bandwidth shares for tenant A and tenant B given their minimum guarantees.</p><p>The results show that while bandwidth capping works well for well behaved tenants with TCP traffic, it cannot enforce bandwidth allocation for "misbehaving" tenants that generate unresponsive traffic. In addition, bandwidth capping cannot take advantage of unused bandwidth. Gatekeeper on the other hand can enforce the desired bandwidth allocation even for misbehaving tenants with unresponsive traffic for both the transmit and receive scenarios. Furthermore, Gatekeeper can take advantage of unused bandwidth both at the transmit and receive sides up to a maximum rate specified by the system administrator for each vNIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>There is a wide variation of network performance guarantees that can be offered to tenants in Cloud computing environments. We argue that current models are not satisfactory and propose a simple tenant performance model abstraction. We describe the Gatekeeper mechanism that supports this performance model in virtualized data centers. Our preliminary results show that Gatekeeper works well in simple scenarios. As part of future work, we plan to evaluate Gatekeeper behavior for larger configurations and dynamic workloads and explore alternative congestion response functions. In addition, our current implementation implements increase/decrease policies and congestion feedback generation in a user level daemon in Xen Dom0. While this approach facilitates policy experimentation, it adds overhead to the implementation because of kernel-user crossings. Currently, turning on Gatekeeper increases CPU load on Xen Dom0 by around 10% of a CPU core (Intel i7-930 2.8GHz) to manage a 1Gbps link under some traffic scenarios. We plan to migrate policy functions from user space to kernel level to minimize the CPU cycles consumed by Gatekeeper.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VM</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>none TCP UDP none TCP UDP none TCP UDP none TCP UDP Type of traffic from Tenant BFig. 6 .</head><label>6</label><figDesc>Fig. 3. Gatekeeper architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>VM</head><label></label><figDesc></figDesc><table>VM 
VM 

BW2 
BW1 
BW2 
BW3 

BW4 
BW10 

Non-blocking core 

VM 
VM 

BW5 
BW9 
VM 
VM 

BW8 
BW7 
BW6 

VM 
VM 
VM 

Fig. 1. Tenant Model 

VM 

Non-blocking core 

BW1 

Web 

BW2 
BW3 

Web 
Web 

App 
App 

BW4 
BW5 
BW6 

App 

BW7 
BW8 
BW9 

DB 
DB 

BW10 
BW11 
BW12 

DB 

Non-blocking core 

Fig. 2. Tenant Model with Logical Switch Composition 

VM 

VM 
VM 

TX rate 
RX 
rate 
RX 
rate 
RX 

vNIC 
vNIC 
vNIC 

limiters counters 
limiters counters 
limiters counters 

vSWITCH 

TX 
RX rate 
scheduler 
monitor 

NIC NIC 

network link 

</table></figure>

			<note place="foot" n="1"> Our current prototype is based on the Open vSwitch (www.openvswitch.org).</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A scalable, commodity data center network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2008 Conference</title>
		<meeting>the ACM SIGCOMM 2008 Conference<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Above the clouds: A berkeley view of cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Griffith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-02" />
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report UCB/EECS-2009-28</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Virtualizing the network forwarding plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Casado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koponen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Programmable Routers for Extensible Services of Tomorrow, PRESTO &apos;10</title>
		<meeting>the Workshop on Programmable Routers for Extensible Services of Tomorrow, PRESTO &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Remote direct memory access over the converged enhanced ethernet fabric: Evaluating the options</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Talpey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Recio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crupnicoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 17th IEEE Symposium on High Performance Interconnects (HOTI &apos;09)</title>
		<meeting>the 2009 17th IEEE Symposium on High Performance Interconnects (HOTI &apos;09)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Symposium on Operating System Design and Implementation (OSDI &apos;04)</title>
		<meeting>the 6th Symposium on Operating System Design and Implementation (OSDI &apos;04)<address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12" />
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fcoe in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Advanced Infocomm Technology (ICAIT &apos;08)</title>
		<meeting>the 2008 International Conference on Advanced Infocomm Technology (ICAIT &apos;08)<address><addrLine>Shenzhen, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A flexible model for resource management in virtual private networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Duffield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Van Der Merive</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM</title>
		<meeting>the ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conference</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="95" to="108" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">FocalPoint in large-scale Clos switches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fulcrum</forename><surname>Microsystems</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 19th ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>Bolton Landing, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VL2: A scalable and flexible data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM</title>
		<meeting>the ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conference</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-08" />
			<biblScope unit="page" from="51" to="62" />
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Secondnet: a data center network virtualization architecture with bandwidth guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference, Co-NEXT &apos;10</title>
		<meeting>the 6th International Conference, Co-NEXT &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Af-qcn: Approximate fairness with quantized congestion notification for multitenanted data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kabbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Interconnects (HOTI), 2010 IEEE 18th Annual Symposium on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="58" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The &quot;platform as a service&quot; model for networking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rexford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 internet network management conference on Research on enterprise networking, INM/WREN&apos;10</title>
		<meeting>the 2010 internet network management conference on Research on enterprise networking, INM/WREN&apos;10<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="4" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">NetShare: Virtualizing data center networks across services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
		<idno>CS2010-0957</idno>
		<imprint>
			<date type="published" when="2010-05" />
			<pubPlace>San Diego</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of California</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A case for topologyaware resource allocation for data-intensive applications in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<idno>HPL-2009-335</idno>
		<imprint>
			<date type="published" when="2009" />
			<publisher>HP Labs</publisher>
			<pubPlace>Palo Alto, CA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spain: Cots data-center ethernet for multipathing over arbitrary topologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mudigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Fares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 7th Symposium on Networked Systems Design and Implementation (NSDI)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">PortLand: A scalable fault-tolerant layer 2 data center network fabric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">N</forename><surname>Mysore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pamboris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Farrington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Pardis Miri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2009 Conference</title>
		<meeting>the ACM SIGCOMM 2009 Conference<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Killer fabrics for scalable datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlansker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tourrilhes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Communications (ICC)</title>
		<imprint>
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sharing the data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Symposium on Networked Systems Design and Implementation (NSDI). USENIX</title>
		<meeting>the 8th USENIX Symposium on Networked Systems Design and Implementation (NSDI). USENIX</meeting>
		<imprint>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">RFC 5556: Transparent interconnection of lots of links (trill): Problem and applicability statement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Touch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
