<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Italian for Beginners: The Next Steps for SLO-Based Management *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lakshmi</forename><forename type="middle">N</forename><surname>Bairavasundaram</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Soundararajan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Mathur</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaladhar</forename><surname>Voruganti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kleiman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NetApp, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Italian for Beginners: The Next Steps for SLO-Based Management *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Literature is rife with compelling ideas on simplifying storage management using service-level objectives (SLOs). However, very few of these ideas have actually been productized, despite the fact that many of the original ideas came from industry and were developed more than a decade ago. While many good research ideas do not become products, in this case, we believe that there are important reasons why adoption has been slow. This paper investigates the reasons for slow adoption and discusses ideas that can truly simplify storage management using SLOs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Consolidating and virtualizing hardware resources has been the mantra for constructing cloud services, both public and private <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. The benefits of consolidation in large data centers include reduced costs of hardware, power, and cooling. In such a multiplexed environment, applications are in competition for data-center resources, such as, CPU, memory, and I/O bandwidth. At the same time, the cloud service is still required to meet per-application goals for performance, data protection, etc. Recent research <ref type="bibr" target="#b18">[19]</ref> has extended and explored automated management using service-level objectives (SLOs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref> to meet application goals in the cloud while keeping management costs low.</p><p>SLOs are a specification of an application's requirements, primarily in technology-independent terms. The term SLO may refer to application needs at different levels of the software stack; we focus on storage. To satisfy business needs, an application may specify performance (e.g., average I/O latency), capacity, reliability, and security needs for its data. SLOs have also been referred to as "service-level requirements" or "service-level agreements" or "quality of service" (ignoring some differences). With SLOs, administrators can focus more on what they need from storage than on how it is achieved.</p><p>SLO-based management is attractive in principle and a series of research efforts, many of them from industry <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, have developed various techniques that cover the entire spectrum of activities: monitoring workloads and systems, analyzing the interaction of workloads with configurations and with each other, planning remedial actions when SLOs are not met, and executing the actions in an efficient fashion. However, very few of these ideas have been productized, especially in the context of storage systems. Unless we first address productization, we may not be solving the real problems, whether for the cloud or elsewhere.</p><p>In this paper, we analyze why SLO-based management, despite having a compelling vision, suffers from poor adoption in products and what we can do to better enable such adoption. Our analysis shows that (a) SLOs are not as simple to specify as we would like, (b) system models, which proactively assess system behavior, have considerable error relative to manual approaches, and (c) the cost of remedying a modeling error is too high.</p><p>In order to address these issues, we offer the following research directions:</p><p>• We should focus on process, not product; processes that limit the scope of SLO-based management to a pre-defined, well-known set of SLOs ("qualified SLOs") will greatly improve (i) the ability of users to select SLOs, (ii) the accuracy of models, and (iii) the ability of vendors to support systems.</p><p>• We should leverage the similarity of workload instances across the data center or even the entire customer base to gather data on workloads and continually update system models and support workflows.</p><p>• We should develop lightweight dynamic reconfiguration techniques to mitigate the cost of modeling errors.  <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> and products <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32]</ref> to determine the extent to which research has been productized as well as how well they match with an ideal system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Is Adoption Really Slow?</head><p>We perform a survey of research papers and products available in the market to evaluate the extent of SLObased management proposed as research ideas and its adoption in products. For a long time, the primary method for implementing service levels has been creating a silo for each application (i.e., independent allocation of resources, often entire storage systems) and using different "tiers" of storage depending on the application's needs. While the degree of SLO-based management available in products has increased (in order to handle multi-tenant sharing), many of them have not moved past the tiering approach, and the differences are evident in our comparison.</p><p>As shown in <ref type="table">Table 1</ref>, we compare research systems and products against the ideal case along multiple axes: (i) SLO specification, (ii) monitoring and reporting, (iii) dynamic analysis of a system's ability to support a new workload (beyond simple thresholds), and (iv) techniques to handle SLO violations and ability to compare multiple remedial actions.</p><p>We find that while research systems are not quite ideal, products are significantly lacking in SLO-based management features. The crucial differences are seen in the technology-dependent nature of SLO specification for many attributes, the lack of multidimensional SLO specification, the lack of system models for impact analysis, and the lack of an automated planner for selecting remedial actions for SLO violations. Neither research nor products have adequate solutions for end-to-end management (monitoring, reporting, and action coordination) across multiple layers of the software stack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reasons for Non-Adoption</head><p>John Wilkes <ref type="bibr" target="#b35">[36]</ref> believes that the greatest barrier to adoption is the developers' ability to convince administrators that the system can be trusted; he suggests that these systems be simple-to-use, more predictable, and open about the decision-making process. While we agree, we believe that there are additional important issues to be addressed in order to improve adoption, and we detail them in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SLO Specification</head><p>While specifying workload requirements are significantly simpler than specifying the storage-system configuration for a workload, many administrators may not know precise workload characteristics and requirements either. Complicating the problem is that the administrator needs to take into account multiple dimensions -that is, understand the impact of performance, protection, security, cost, etc. in a holistic manner -to provide the final specification to the storage system. Thus, the real goal of simplifying storage management is not addressed simply by introducing SLO-based management. This observation is borne out in I/O prioritization techniques being productized <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">32]</ref>. It is significantly easier for the administrator to say that one workload is more important than the other (or specify a share of the total capacity), than to specify each workload's requirements. However, I/O prioritization may not help in actually satisfying the applications' requirements.</p><p>Some prior work has examined ways to simplify SLO specification <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b29">30]</ref>. The easiest specification is none at all, as exemplified by techniques to iteratively derive these requirements <ref type="bibr" target="#b35">[36]</ref> instead of having administrators specify them. However, such an approach works primarily only for performance attributes and can also make support much harder for vendors; imagine a support scenario where the requirements are determined dynamically and SLO violations still occur. With respect to reliability requirements, Keeton et al. <ref type="bibr" target="#b15">[16]</ref> suggest a simpler abstraction for users based on the dollar cost of data loss or unavailability (as compared to metrics such as recovery time objectives). However, other service-level attributes do not lend themselves easily to such a model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Errors</head><p>Without SLO-based management, users are expected to manually model the impact (performance, etc.) of workload additions or system-configuration changes based on their expertise in examining system utilization levels. With SLO-based management, this burden shifts to vendor's knowledge of internal workings of the system (i.e., white-box models <ref type="bibr" target="#b35">[36]</ref>) or to statistical machine-learning techniques (i.e., black-box performance models <ref type="bibr" target="#b19">[20]</ref>).</p><p>In particular, the modeling errors occur because, for white-box models, the model builders usually trail the product feature release cycle as it takes longer for them to capture the complex interplay between the various features. Similarly, it is not possible to train black-box models for all the permutations and the lack of this training data gives large errors in the extrapolation region. Irrespective of the approach, errors occur.</p><p>Modeling errors can also be expected to increase with increasing system complexity. We believe that given lower complexity -one application per system, modeling primarily RAID latency/bandwidth, only a few infrastructure layers -many users in the past may have been better at modeling than computer counterparts (although Alvarez et al. show otherwise in one specific experiment <ref type="bibr" target="#b1">[2]</ref>). However, current storage systems are highly complex and modeling the interactions between various sub-systems is non-trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reconfiguration Cost</head><p>More important than modeling errors per se, is the impact of modeling errors. The penalty for an incorrect model is often an SLO violation that requires heavy-weight corrective actions, such as migrating the dataset or purchasing a new system. Thus, administrators leave systems underutilized to minimize SLO violations; i.e., keeping storage administration simpler by spending more.</p><p>In order for administrators to embrace systemmanaged complexity, the impact of a mistake has to be lowered. This improvement is essential even when models are highly accurate; when workloads are inherently very dynamic, one can provision storage for either the peak load or the average load, but not both, thus causing poor utilization or periodic SLO violations respectively. Thus, a lack of dynamic low-cost techniques to handle modeling errors and also counter the dynamic nature of workloads, results in poor adoption of automated SLObased management.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why SLOs Now?</head><p>This section looks at the recent technical trends that make automated management essential, showing why we should be concerned about poor adoption of SLOs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Configuration Complexity</head><p>Prior work <ref type="bibr" target="#b35">[36]</ref> uses configuration complexity as motivation, focusing on the number and type of disk drives needed to support a given workload. The complexity today is significantly higher due to a plethora of new features like deduplication <ref type="bibr" target="#b37">[38]</ref>, use of flash memory <ref type="bibr" target="#b21">[22]</ref>, use of multiple levels of caches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, striping of data across nodes <ref type="bibr" target="#b27">[28]</ref>, etc. It is hard to predict properties like performance when different features are combined. To make things worse, system properties change over time forcing administrators to keep up with all of the changes in the products of multiple vendors. This resulting increase in complexity may favor computer-based models; their errors will increase as complexity increases but at a lower rate than manual modeling, thus encouraging adoption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Tenant Sharing</head><p>In a multi-tenant environment, applications with potentially different requirements may share the same storage system. The availability of flash, as well as the pervasive use of caches, permit the same system to support different "tiers" of storage depending on requirements <ref type="bibr" target="#b10">[11]</ref>; e.g., data with high performance requirements can make greater use of flash than data with lower requirements. The availability of a large number of workloads to provision allows service providers to "thin-provision" resources, assuming that not all workloads will require peak performance at the same time.</p><p>While consolidation has great benefits, it makes storage-management difficult. Specifically, with flash and hard disks in one array, administrators need to control the amount of flash provided to each application. As flash technology changes or application needs change, the flash allocation should be changed as well. Thus, these tasks should be automated using SLOs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Scale and Dynamism</head><p>In cloud storage environments, two important problems are much worse as compared to that in prior work <ref type="bibr" target="#b35">[36]</ref>: scale and dynamism. The increase in the amount of data stored as well as the number of storage devices, objects, users, policies, and sites that need to be managed in an integrated manner make storage management particularly challenging. Further, new applications are provisioned at a fast rate and their requirements may change significantly over time. Therefore, administrators should ideally not be involved in processing every provisioning request or in managing the load on individual systems. Again, these tasks should be automated using SLOs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Research Directions</head><p>The lessons of the past give us insights on how to ease the adoption of SLOs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Process, Not Product</head><p>Much of the focus on SLO-based management has been on developing a better storage-management product. Less time has been devoted to research on improving processes, including requirement specification by users, building and updating of system models by the vendor, and post-sales support for products and SLOs.</p><p>We propose that the vendors identify SLOs for popular workloads ahead of time by (a) leveraging the expertise of select customers and partners, (b) developing tools to translate between requirements specified at the application and storage levels, and (c) developing tools to resolve differences in specification derived from various sources. The starting point for this approach would be to obtain information such as application name and its configuration, e.g., Microsoft R Exchange with N mailboxes, as used in best-practices documents <ref type="bibr" target="#b24">[25]</ref>. We term SLOs generated thus as "qualified SLOs." Thus, most users would select a qualified SLO based on the application name or type or even the stack being deployed. The SLO that users select is also an atomic unit (e.g., performance is not independent of security); systems should prevent the specification of inconsistent goals <ref type="bibr" target="#b16">[17]</ref>.</p><p>We believe that the process defined above would make SLO specification (selection) easier for users, reduce the burden on the storage vendor in creating models of the storage systems as well as in supporting both the storage systems and their models; there are fewer, but important, workloads to focus on when creating models; and when a support request is made, the support engineers always have prior workload-based training as well as current context (expected SLO, delivered service, expected workload, actual workload) to troubleshoot with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dynamic Low-Impact Reconfiguration</head><p>For users to trust automated management, the impact of errors needs to be lowered. These systems should be nimble in handling dynamism of workloads when the original provisioning is aggressive (for non-peak load).</p><p>Systems should include low-time and low-cost mechanisms to handle SLO violations, including automatically and non-disruptively reconfiguring resources or migrating small amounts of data based on current application needs and resource usage. The HP AutoRAID system <ref type="bibr" target="#b36">[37]</ref> is an early example. Recent efforts have explored dynamic data layouts that take advantage of flash storage <ref type="bibr" target="#b10">[11]</ref>. Mechanisms that separate namespace and location <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b27">28]</ref> enable object-granular migration.</p><p>Further research could focus on taking advantage of hardware fluidity. Specifically, caching could be managed throughout the storage stack dynamically with techniques for deciding whether to create new caches, grow/shrink existing caches, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Community Wisdom</head><p>While the large scale of operation is typically problematic from a management viewpoint, we believe that it can also be leveraged to reduce the difficulty of management.</p><p>First, many instances of the same application may be provisioned in a single data center (especially in a cloud service). A phenomenal amount of data is now available for training system models and for comparing actual workloads against those the models are trained with. Second, since storage systems have various degrees of reporting back to vendors <ref type="bibr" target="#b4">[5]</ref>, modeling data from the large customer base can be leveraged to improve inhouse models further. Third, scalable data analytics is available through new computing paradigms <ref type="bibr" target="#b7">[8]</ref>. Combinations of the above techniques have been developed in the context of handling misconfigurations <ref type="bibr" target="#b34">[35]</ref> and bugs <ref type="bibr" target="#b17">[18]</ref>. We propose that the techniques be extended to improve system models, construct support workflows, and guide customers towards best practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">End-to-End Management</head><p>This research direction is prompted less by the failings of the past than by the need to avoid emerging pitfalls. Specifically, the number of infrastructure layers has increased significantly: hypervisors, caches, etc. As a result, an application's performance and reliability depend on multiple pieces of infrastructure, managed by different people or tools, making it hard to derive guarantees. SLO-based management should move towards a more end-to-end approach. First, we need protocols that can be used to specify SLOs throughout the stack and to aggregate information from all monitoring stations. Orchestration tools <ref type="bibr" target="#b6">[7]</ref> aid in such monitoring to a limited extent. Second, we need mechanisms to coordinate control. For example, data migration can be performed at different layers -storage-level <ref type="bibr" target="#b25">[26]</ref> and hypervisor-level <ref type="bibr" target="#b11">[12]</ref> -to handle SLO violations. When these tools work independently, they may expend more resources than needed to address SLO needs; worse, they would lead to many more SLO violations, more useless work, and potentially, unavailability. Thus, without cooperation, even bug-free components can cause the system as a whole to be unstable -an example of emergent "mis"behavior <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>"Simplicity, simplicity, simplicity! I say, let your affairs be as two or three, and not a hundred or a thousand;" Henry David Thoreau</p><p>As we research the new directions proposed herein and develop ways to make storage management simpler for users, we should strive to keep it simple enough for ourselves (i.e., the developers). Methods (e.g., building system models) that require constant or hard, manual refinement may break down because development or support for SLOs cannot keep up with products.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank the anonymous reviewers and our shepherd, Pradeep Padala, for their insightful comments that helped improve the paper. We also thank members of the NetApp Advanced Technology Group (ATG) for their valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><forename type="middle">Akorri</forename><surname>Akorri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balancepoint</surname></persName>
		</author>
		<ptr target="http://akorri.com/products-features.htm" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">An Automated Resource Provisioning Tool for Large-Scale Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alvarez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ACM TOCS</publisher>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="http://aws.amazon.com" />
	</analytic>
	<monogr>
		<title level="j">Amazon, Inc. Amazon Web Services</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Above the clouds: A berkeley view of cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Armbrust</surname></persName>
		</author>
		<idno>UCB/EECS-2009-28</idno>
		<ptr target="http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.html" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Analysis of Latent Sector Errors in Disk Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bairavasundaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS&apos;07</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">FlexShare Design and Implementation Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhargava</surname></persName>
		</author>
		<ptr target="http://www.netapp.com/us/library/technical-reports/tr-3459.html" />
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>NetApp</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bmc</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Bmc Atrium</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Orchestrator</surname></persName>
		</author>
		<ptr target="http://documents.bmc.com/products/documents/13/00/101300/101300.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mapreduce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;04</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emc</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename></persName>
		</author>
		<ptr target="http://www.emc.com/collateral/software/data-sheet/c1152-emc-navisphere-quality-service-manager.pdf" />
	</analytic>
	<monogr>
		<title level="j">EMC Navisphere Quality of Service Manager</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Umbrella File System: Storage Management Across Heterogeneous Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reddy</forename><surname>Garrison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cost Effective Storage using Extent Based Dynamic Tiering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guerra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BASIL: Automated I/O Load Balancing Across Storage Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">PARDA: Proportional Allocation of Resources for Storage Access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gulati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename></persName>
		</author>
		<ptr target="http://h18000.www1.hp.com/products/quickspecs/12191_" />
		<title level="m">HP Storage Essentials SRM Software Suite</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibm</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Ibm Tivoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Software</surname></persName>
		</author>
		<ptr target="http://www-01.ibm.com/software/tivoli/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Designing for Disasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keeton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;04</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Vision of Autonomic Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chess</forename><surname>Kephart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cooperative Bug Isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liblit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated Control for Elastic Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC&apos;10</title>
		<meeting><address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling the Relative Fitness of Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mesnier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMET-RICS&apos;07</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emergent (Mis)behavior vs. Complex Software Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys&apos;06</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Netapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netapp Flash Cache</surname></persName>
		</author>
		<ptr target="http://media.netapp.com/documents/ds-2811-flash-cache-pam-II.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Netapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netapp Flexcache</surname></persName>
		</author>
		<ptr target="http://media.netapp.com/documents/tr-3669.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Netapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netapp Provisioning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manager</surname></persName>
		</author>
		<ptr target="http://media.netapp.com/documents/ds_2742_provisioningmgr.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Netapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft Exchange</surname></persName>
		</author>
		<ptr target="http://media.netapp.com/documents/tr-3683.pdf" />
		<title level="m">on VMware Infrastructure 3 and NetApp iSCSI Storage</title>
		<meeting><address><addrLine>NetApp</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">NetApp DataMotion for Volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Netapp</surname></persName>
		</author>
		<ptr target="http://media.netapp.com/documents/ds-3122.pdf" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Managing Quality of Service (QoS) in a Shared Storage Environment</title>
		<ptr target="https://pillardata.box.net/shared/static/qyvnx4vvid.pdf" />
		<imprint>
			<publisher>Pillar Data, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network File System (NFS) Version 4 Minor Version 1 Protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shepler</surname></persName>
		</author>
		<ptr target="http://tools.ietf.org/html/rfc5661" />
	</analytic>
	<monogr>
		<title level="j">RFC</title>
		<imprint>
			<biblScope unit="volume">5661</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic Resource Allocation for Database Servers running on Virtual Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using Utility Functions to Provision Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tracking Activity in a Distributed Storage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thereska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS&apos;06</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Vmware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vsphere Resource Management Guide</surname></persName>
		</author>
		<ptr target="http://www.vmware.com/pdf/vsphere4/r41/vsp_41_resource_mgmt.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Argon: Performance Insulation for Shared Storage Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;07</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Proportional share scheduling for distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merchant</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;07</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic Misconfiguration Troubleshooting with PeerPressure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;04</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Traveling to Rome: A Retrospective on the Journey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R2D2&apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The HP AutoRAID Hierarchical Storage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP&apos;95</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Avoiding the Disk Bottleneck in the Data Domain Deduplication System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;08</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microsoft is a registered trademark of Microsoft Corporation. VMware is a registered trademark and vSphere is a trademark of VMware, Inc. All other brands or products are trademarks or registered trademarks of their respective holders and should be treated as such</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Netapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Go</forename><surname>Netapp Logo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Further</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>FlexShare are trademarks or registered trademarks of NetApp, Inc. in the United States and/or other countries</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
