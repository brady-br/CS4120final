<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interactive Second Language Learning from News Websites</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-07-31">July 31. 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<email>taochen@comp.nus.edu.sg</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naijia</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muthu</forename><forename type="middle">Kumar</forename><surname>Chandrasekaran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NUS Interactive and Digital Media Institute</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interactive Second Language Learning from News Websites</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications</title>
						<meeting>The 2nd Workshop on Natural Language Processing Techniques for Educational Applications <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="34" to="42"/>
							<date type="published" when="2015-07-31">July 31. 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We propose WordNews, a web browser extension that allows readers to learn a second language vocabulary while reading news online. Injected tooltips allow readers to look up selected vocabulary and take simple interactive tests. We discover that two key system components needed improvement, both which stem from the need to model context. These two issues are real-world word sense disambiguation (WSD) to aid translation quality and constructing interactive tests. For the first, we start with Mi-crosoft&apos;s Bing translation API but employ additional dictionary-based heuristics that significantly improve translation in both coverage and accuracy. For the second, we propose techniques for generating appropriate distractors for multiple-choice word mastery tests. Our preliminary user survey confirms the need and viability of such a language learning platform.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning a new language from language learning websites is time consuming. Research shows that regular practice, guessing, memorization <ref type="bibr">(Ru- bin, 1975</ref>) as well as immersion into real scenarios <ref type="bibr" target="#b8">(Naiman, 1978)</ref> hastens the language learning process. To make second language learning attractive and efficient, we interleave language learning with the daily activity of online news reading.</p><p>Most existing language learning software are either instruction-driven or user-driven.</p><p>Duolingo 1 is a popular instruction-driven system that teaches through structured lessons. Instruction driven systems demand dedicated learner time on a daily basis and are limited by learning materials as lesson curation is often labor-intensive.</p><p>In contrast, many people informally use Google Translate 2 or Amazon Kindle's Vocabulary Builder <ref type="bibr">3</ref> to learn vocabulary, making these prominent examples of user-driven systems. These systems, however, lack the rigor of a learning platform as they omit tests to allow learners to demonstrate mastery. In our work, we merge learning and assessment within the single activity of news reading. Our system also adapts to the learner's skill during assessment.</p><p>We propose a system to enable online news readers to efficiently learn a new language's vocabulary. Our prototype targets Chinese language learning while reading English language news. Learners are provided translations of opendomain words for learning from an English news page. In the same environment -for words that the system deems mastered by the learner -learners are assessed by replacing the original English text in the article with their Chinese translations and asked to translate them back given a choice of possible translations. The system, WordNews, deployed as a Chrome web browser extension, is triggered when readers visit a preconfigured list of news websites (e.g., CNN, BBC).</p><p>A key design property of our WordNews web browser extension is that it is only active on certain news websites. This is important as news articles typically are classified with respect to a news category, such as finance, world news, and sports. If we know which category of news the learner is viewing, we can leverage this contextual knowledge to improve the learning experience.</p><p>In the development of the system, we discovered two key components that can be affected by this context modeling. We report on these developments here. In specific, we propose improved algorithms for two components: (i) for translating English words to Chinese from news articles, (ii) for generating distractors for learner assessment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The WordNews Chrome Extension</head><p>Our method to directly enhance the web browser is inspired by earlier work in the computer-aided language learning community that also uses the web browser as the delivery vehicle for language learning. <ref type="bibr">WERTi (Metcalf and Meurers, 2006;</ref><ref type="bibr" target="#b6">Meurers et al., 2010)</ref> was a monolingual, userdriven system that modified web pages in the target language to highlight or remove certain words from specific syntactic patterns to teach difficultto-learn English grammar.</p><p>Our focus is to help build Chinese vocabulary for second language learners fluent in English. We give a running scenario to illustrate the use of WordNews. When a learner browses to an English webpage on a news website, our extension either selectively replaces certain original English words with their Chinese translation or underlines the English words, based on user configuration <ref type="figure" target="#fig_0">(Figure 1, middle)</ref>. While the meaning of the Chinese word is often apparent in context, the learner can choose to learn more about the replaced/underlined word, by mousing over the word to reveal a definition tooltip <ref type="figure" target="#fig_0">(Figure 1</ref>, left) to aid mastery of the Chinese word. Once the learner has encountered the target word a few times, WordNews assesses learner's mastery by generating a multiple choice translation test on the target word <ref type="figure" target="#fig_0">(Figure 1, right)</ref>. Our learning platform thus can be viewed as three logical use cases: translating, learning and testing.</p><p>Translating. We pass the main content of the webpage from the extension client to our server for candidate selection and translation. As certain words are polysemous, the server must select the most appropriate translation among all possible meanings. Our initial selection method replaces any instance of words stored in our dictionary. For translation, we check the word's stored meanings against the machine translation of each sentence obtained from the Microsoft Bing Translation API 4 (hereafter, "Bing"). Matches are deemed as correct translations and are pushed back to the Chrome client for rendering.</p><p>Learning. Hovering the mouse over the replacement Chinese word causes a tooltip to appear, which gives the translation, pronunciation, and simplified written form, and a More link that loads additional contextual example sentences (that were previously translated by the backend) for the learner to study. The More link must be clicked for activation, as we find this two-click architecture helps to minimize latency and the loading of unnecessary data. The server keeps record of the learning tooltip activations, logging the enclosing webpage URL, the target word and the user identity.</p><p>Testing. After the learner encounters the same word a pre-defined number t = 3 times, WordNews generates a multiple choice question (MCQ) test to assess mastery. When the learner hovers over the replaced word, the test is shown for the learner to select the correct answer. When an option is clicked, the server logs the selection and updates the user's test history, and the client reveals the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">News Categories</head><p>As our learning platform is active only on certain news websites, we can model the news category (for individual words and whole webpages) as additional evidence to help with tasks. Of particular importance to WordNews is the association of words to a news category, which is used downstream in both word sense disambiguation (Section 3) and the generation of distractors in the interactive tests (Section 4). Here, our goal is to automatically find highly relevant words to a particular news category -e.g., "what are typical finance words?"</p><p>We first obtain a large sample of categorized English news webpages, by creating custom crawlers for specific news websites (e.g. CNN). We use a seed list of words that are matched  Entertainment "superstar", "明星" 2. World Military, International, Social "attacks", "军事"</p><p>3. Finance Finance "investment", "财富" 4. Sports Sports "score", "比 赛"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Fashion</head><p>Beauty &amp; Health "jewelry", "时髦" 6. Technology Technology "cyber", "互联网" 7. Travel "natural" against a target webpage's URL. If any match, the webpage is deemed to be of that category. For example, a webpage that has the seed word "football" in its URL is deemed of category "Sports". Since the news category is also required for Chinese words for word sense disambiguation, we must perform a similar procedure to crawl Chinese news (e.g., BaiduNews 5 ) However, Chinese news sites follow a different categorization scheme, so we first manually align the categories based on observation (see <ref type="table" target="#tab_0">Table 1</ref>), creating seven bilingual categories: namely, "World", "Technology", "Sports", "Entertainment", "Finance", "Fashion" and "Travel". We tokenize and part-of-speech tag the main body text of the categorized articles, discarding <ref type="bibr">5</ref> http://news.baidu.com punctuation and stopwords. For Chinese, we segment words using the Stanford Chinese word segmenter ( <ref type="bibr" target="#b1">Chang et al., 2008</ref>). The remaining words are classified to a news category based on document frequency. A word w is classified to a category c if it appears more often (a tunable threshold δ 6 ) than its average category document frequency. Note that a word can be categorized to multiple categories under this scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Word Sense Disambiguation (WSD) Component</head><p>Our extension needs to show the most appropriate translation sense based on the context. Such a translation selection task -cross-lingual word sense disambiguation -is a common problem in machine translation. In this section, we describe how we improved WordNews' WSD capabilities through a series of six approaches. The context evidence that we leverage for WSD comes in two forms: the news category of the target word and its enclosing sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Bilingual Dictionary and Baseline</head><p>WordNews's server component includes a bilingual lexicon of English words with possible Chinese senses. The English words in our dictionary is based on the publicly-available College English Test (CET 4) list, which has a breadth of about 4,000 words. We augment the list to include the relative frequency among Chinese senses, with their part-of-speech, per English word.</p><p>Our baseline translation uses the most frequent sense: for an English word to be translated, it chooses the most frequent relative Chinese translation sense c from the possible set of senses C. <ref type="bibr">6</ref> We empricially set δ to 10.</p><p>3  </p><formula xml:id="formula_0">关闭 密切 亲 亲 亲密 密 密 亲 亲 亲密 密 密 亲 亲 亲密 密 密 (2) ... kids can't stop singing ... verb: 停止, 站, 阻止, 停 ... 停 停 停止 止 止 阻止 停 停 停止 止 止 停 停 停止 止 止 停 停 停止 止 止<label>(</label></formula><formula xml:id="formula_1">noun: 旅, 旅程 ... 旅游 ... 旅 旅 旅 旅 旅 旅行 行 行 旅 旅 旅行 行 行 (5) ... winning more points in the match ... noun: 匹 配, 比 赛, 赛, 敌手, 对手, 火柴 ... 匹配 匹配 比 比 比赛 赛 赛 比 比 比赛 赛 赛 比 比 比赛 赛 赛 (6) ... state department spokeswoman Jen Psaki said that the allies ... noun: 态, 国, 州, ... verb: 声明, 陈述, 述, 申 明 ... 发言 ... adj: 国家的 ... 态 态 发言 发言 人 国 国 国家 家 家</formula><p>This method has complete coverage over the CET 4 list (as the word frequency rule always yields a prospective translation), but as it lacks any context model, it is the least accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Approach 1: News Category</head><p>Topic information has been shown to be useful in WSD <ref type="bibr" target="#b0">(Boyd-Graber et al., 2007</ref>). For example, consider the English word interest. In finance related articles, "interest" is more likely to carry the sense of "a share, right, or title in the ownership of property" ("利息" in Chinese), over other senses. Therefore, analysing the topic of the original article and selecting the translation with the same topic label might help disambiguate the word sense. For a target English word e, for each prospective Chinese sense c ∈ C, choose the first (in terms of relative frequency) sense that has the same news category as the containing webpage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Approach 2: Part-of-Speech</head><p>Part-of-Speech (POS) tags are also useful for word sense disambiguation <ref type="bibr">(Wilks and Steven- son, 1998</ref>) and machine translation ( <ref type="bibr" target="#b12">Toutanova et al., 2002;</ref><ref type="bibr" target="#b14">Ueffing and Ney, 2003)</ref>. For example, the English word "book" can function as a verb or a noun, which gives rise to two different dominant senses: "reserve" ("预定" in Chinese) and "printed work" ("书"), respectively. As senses often correspond cross-lingually, knowledge of the English word's POS can assist disambiguation. We employ the Standford log-linear Part-of-Speech tagger ( <ref type="bibr" target="#b13">Toutanova et al., 2003</ref>) to obtain the POS tag for the English word, whereas the POS tag for target Chinese senses are provided in our dictionary. In cases where multiple candidate Chinese translations fit the same sense, we again break ties using relative frequency of the prospective candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Approaches 3-5: Machine Translation</head><p>Neighbouring words provide the necessary context to perform WSD in many contexts. In our work, we consider the sentence in which the target word appears as our context. We then acquire its translation from Microsoft Bing Translator using its API. As we access the translation as a third party, the Chinese translation comes as-is, without the needed explicit word to locate the target English word to translate in the original input sentence. We need to perform alignment of the Chinese and English sentences in order to recover the target word's translation from the sentence translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Approach 3 -Substring Match. As potential Chinese translations are available in our dictionary, a straightforward use of substring matching recovers a Chinese translation; i.e., check whether the candidate Bing translation is a substring of the Chinese translation. If more than one candidate matches, we use the longest string match heuristic and pick the one with the longest match as the final output. If none matches, the system does not output a translation for the word.</p><p>Approach 4 -Relaxed Match. The final rule in the substring match method unfortunately fires often, as the coverage of WordNews's lexicon is limited. As we wish to offer correct translations that are not limited by our lexicon, we relax our substring condition, allowing the Bing translation to be a superset of a candidate translation in our dictionary (see Example 4 in <ref type="table" target="#tab_1">Table 2</ref>, where the Bing translation "旅行" is allowed to be relaxed to match the dictionary "旅"). To this end, we must know the extent of the words in the translation. We first segment the obtained Bing translation with the Stanford Chinese Word Segmenter, and then use string matching to find a Chinese translation c. If more than one candidate matches, we heuristically use the last matched candidate. This technique significantly augments the translation range of our extension beyond the reach of our lexicon.</p><p>Approach 5 -Word Alignment. The relaxed method runs into difficulties when the target English e's Chinese prospective translations which come from our lexicon generate several possible matches. Consider Example 6 in <ref type="table" target="#tab_1">Table 2</ref>. The target English word "state" has corresponding Chinese entries "发言" and "国家的" in our dictionary. For this reason, both "国家" ("country", correct) and "发言人" ("spokeswoman", incorrect) are relaxed matches. As relaxed approach always picks up the last candidate, "发 言人" is the final output, which is incorrect.</p><p>To address this, we use the Bing Word Alignment API <ref type="bibr">7</ref> to provide a possibly different prospective Chinese sense c. In this example, "state" matches "国家" ("country", correct) from word alignment, and the final algorithm chooses "国家" as the output. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation</head><p>To evaluate the effectiveness of our proposed methods, we randomly sampled 707 words and their sentences from recent CNN 8 news articles, manually annotating the ground truth translation for each target English word. We report both the coverage (i.e., the ability of the system to return a translation) and accuracy (i.e., whether the translation is contextually accurate). <ref type="table" target="#tab_3">Table 3</ref> shows the experimental results for the six approaches. As expected, frequency-based baseline achieves 100% coverage, but a low accuracy (57.3%); POS also performs similarly . The category-based approach performs the worst, due to low coverage. This is because news category only provides a high-level context and many of the Chinese word senses do not have a strong topic tendency.</p><p>Of most promise is our use of web based translation related APIs. The three Bing methods iteratively improve the accuracy and have reasonable coverage. Among all the methods, the additional step of word alignment is the best in terms of accuracy (97.4%), significantly bettering the others. This validates previous work that sentence-level context is helpful in WSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distractor Generation Component</head><p>Assesing mastery over vocabulary is the other key functionality of our prototype learning platform. The generation of the multiple choice selection test requires the selection of alternative choices aside from the correct answer of the target word. In this section, we investigate a way to automatically generate such choices (called distractors in the literature) in English, given a target word.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Related Work</head><p>Multiple choice question (MCQ) is widely used in vocabulary learning. Semantic and syntactic properties of the target word need to be considered while generating their distractors. In particular, ( <ref type="bibr" target="#b9">Pho et al., 2014</ref>) did an analysis on real-life MCQ corpus, and validated there are syntactic and semantic homogeneity among distractors. Based on this, automatic distractor generation algorithms have been proposed.</p><p>For instance, ( <ref type="bibr" target="#b3">Lee and Seneff, 2007)</ref> generate distractors for English prepositions based on collocations, and idiosyncratic incorrect usage learned from non-native English corpora. Lärka ( <ref type="bibr" target="#b15">Volodina et al., 2014</ref>) -a Swedish language learning system -generates vocabulary assessment exercises using a corpus. They also have different modes of exercise generation to allow learning and testing via the same interface. ( <ref type="bibr" target="#b11">Susanti et al., 2015)</ref> generate distractors for TOEFL vocabulary test using WordNet and word sense disambiguation given a target word. While these approaches serve in testing mastery, they do not provide the capability for learning new vocabulary in context. The most related prior work is WordGap system ( <ref type="bibr" target="#b2">Knoop and Wilske, 2013)</ref>, a mobile application that generates MCQ tests based on the text selected by users. WordGap customizes the reading context, however, the generation of distractors -based on syntactic and semantic homogeneityis not contextualized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Approach</head><p>WordNews postulates "a set of suitable distractors" as: 1) having the same form as the target word, 2) fitting the sentence's context, and 3) having proper difficulty level according to user's level of mastery. As input to the distractor generation algorithm, we provide the target word, its partof-speech (obtained by tagging the input sentence first) and the enclosing webpage's news category. We restrict the algorithm to produce distractors matching the input POS, and which match the news category of the page.</p><p>We can design the test to be more difficult by choosing distractors that are more similar to the target word. By varying the semantic distance, we can generate tests at varying difficulty levels. We quantify similarity by using the Lin distance <ref type="bibr" target="#b4">(Lin, 1998</ref>) between two input candidate concepts in WordNet <ref type="bibr" target="#b7">(Miller, 1995)</ref>:</p><formula xml:id="formula_2">sim(c1, c2) = 2 * logP (lso(c1, c2)) logP (c1) + logP (c2)<label>(1)</label></formula><p>where P (c) denotes the probability of encountering concept c, and lso(c1, c2) denotes the lowest common subsumer synset, which is the lowest node in the WordNet hierarchy that is a hypernym of both c1 and c2. This returns a score from 0 (completely dissimilar) to 1 (semantically equivalent).</p><p>If we use a target word e as the starting point, we can use WordNet to retrieve related words using WordNet relations (hypernyms/hyponyms, synonyms/antonyms) and determine their similarity using Lin distance.</p><p>We empirically set 0.1 as the similarity threshold -words that are deemed more similar than 0.1 are returned as possible distractors for our algorithm. We note that Lin distance often returns a score of 0 for many pairs and the threshold of 0.1 allows us to have a large set of distractors to choose from, while remaining fairly efficient in run-time distractor generation.</p><p>We discretize a learner's knowledge of the word based on their prior exposure to it. We then adopt a strategy to generate distractors for the input word based learners' knowledge level:</p><p>Easy: The learner has been exposed to the word at least t = 3 times. Two distractors are randomly selected from words that share the same news category as the target word e. The third distractor is generated using our algorithm.</p><p>Hard: The learner has passed the Easy level test x = 6 times. All three distractors are generated from the same news category, using our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>The WordGap system ( <ref type="bibr" target="#b2">Knoop and Wilske, 2013)</ref> represents the most related prior work on automated distractor generation, and forms our baseline. WordGap adopts a knowledge-based approach: selecting the synonyms of synonyms (also computed by WordNet) as distractors. They first 6 select the most frequently used word, w1, from the target word's synonym set, and then select the synonyms of w1, called s1. Finally, WordGap selects the three most frequently-used words from s1 as distractors.</p><p>We conducted a human subject evaluation of distractor generation to assess its fitness for use. The subjects were asked to rank the feasibility of a distractor (inclusive of the actual answer) from a given sentential context. The contexts were sentences retrieved from actual news webpages, identical to WordNews's use case.</p><p>We randomly selected 50 sentences from recent news articles, choosing a noun or adjective from the sentence as the target word. We show the original sentence (leaving the target word as blank) as the context, and display distractors as choices (see <ref type="figure" target="#fig_3">Figure 2</ref>). Subjects were required to read the sentence and rank the distractors by plausibility: 1 (the original answer), 2 (most plausible alternative) to 7 (least plausible alternative). We recruited 15 subjects from within our institution for the survey. All of them are fluent English speakers, and half are native speakers.</p><p>We evaluated two scenarios, for two different purposes. In both evaluations, we generate three distractors using each of the two systems, and add the original target word for validation (7 options in total, conforming to our ranking options of 1-7).</p><p>Since we have news category information, we wanted to check whether that information alone could improve distractor generation. Evaluation 1 tests the WordGap baseline system versus a Random News system that uses random word selection. It just uses the constraint that chosen distractors must conform to the news category (be classified to the news category of the target word).</p><p>In our Evaluation 2, we tested our Hard setup where our algorithm is used to generate all distractors against WordGap. This evaluation aims to assess the efficacy of our algorithm over the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Results and Analysis</head><p>Each question was answered by five different users. We compute the average ranking for each choice. A lower rating means a more plausible (harder) distractor. The rating for all the target words is low (1.1 on average) validating their truth and implying that the subjects answered the survey seriously, assuring the validity of the evaluation.</p><p>For each question, we deem an algorithm to be the winner if its three distractors as a whole (the sum of three average ratings) are assessed to be more plausible than the distractors by its competitor. We calculate the number of wins for each algorithm over the 50 questions in each evaluation.  We display the results of both evaluations in <ref type="table" target="#tab_4">Table 4</ref> and <ref type="table" target="#tab_5">Table 5</ref>. We see that the WordGap baseline outperforms the random selection, constrained solely by news category, by 4 wins and a 0.26 lower average score. This shows that word news category alone is insufficient for generating good distractors. When a target word does not have a strong category tendency, e.g., "venue" and "week", the random news method cannot select highly plausible distractors.</p><p>In the second table, our distractor algorithm significantly betters the baseline in both number of 7  <ref type="table" target="#tab_6">Table 6</ref> we show the distractors generated for the target word "lark" in the example survey question <ref type="figure" target="#fig_3">(Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Platform Viability and Usability Survey</head><p>We have thus far described and evaluated two critical components that can benefit from capturing the learner's news article context. In the larger context, we also need to check the viability of second language learning intertwined with news reading. In a requirements survey prior to the prototype development, two-thirds of the respondents indicated that although they have interest in learning a second language, they only have only used language learning software infrequently (less than once per week) yet frequently read news, giving us motivation for our development. Post-prototype, we conducted a summative survey to assess whether our prototype product satisfied the target niche, in terms of interest, usability and possible interference with normal reading activities. We gathered 16 respondents, 15 of which were between the ages of 18-24. 11 (the majority) also claimed native Chinese language proficiency.</p><p>The respondents felt that the extension platform was a viable language learning platform (3.4 of 5; on a scale of 1 "disagreement" to 5 "agreement") and that they would like to try it when available for their language pair (3 of 5).</p><p>In our original prototype, we replaced the original English word with the Chinese translation. While most felt that replacing the original English with the Chinese translation would not hamper their reading, they still felt a bit uncomfortable (3.7 of 5). This finding prompted us to change the default learning tooltip behavior to underlining to hint at the tooltip presence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We described WordNews, a client extension and server backend that transforms the web browser into a second language learning platform. Leveraging web-based machine translation APIs and a static dictionary, it offers a viable user-driven language learning experience by pairing an improved, context-sensitive tooltip definition service with the generation of context-sensitive multiple choice questions. WordNews is potentially not confined to use in news websites; one respondent noted that they would like to use it on arbitrary websites, but currently we feel usable word sense disambiguation is difficult enough even in the restricted news domain. We also note that respondents are more willing to use a mobile client for news reading, such that our future development work may be geared towards an independent mobile application, rather than a browser extension. We also plan to conduct a longitudinal study with a cohort of second language learners to better evaluate WordNews' real-world effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Merged screenshots of our Chrome extension on the CNN English article Spotify wants to be the soundtrack of your life. Underlined components are clickable to yield tooltips of two different forms: (left) a definition for learning, (right) a multiple-choice interactive test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>8</head><label></label><figDesc>http://edition.cnn.com</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5</head><label>5</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample distractor ranking question.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>News category alignment between En-
glish and Chinese. 

English 
Category 
Chinese 
Category 
Example 
Words 
1. Entertain-
ment 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Example translations from our approaches to WSD. Target words are italicized and correct translations are bolded.</head><label>2</label><figDesc></figDesc><table>English Sentence 
Dictionary 
Baseline POS 
Machine Translation 
Substring Relax Align 
(1) ... a very close 
friend of ... 
verb: 关闭, 合, 关 ... 
adj: 密切, ... 亲密 ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : WSD performance over our test set.</head><label>3</label><figDesc></figDesc><table>Coverage Accuracy 
Baseline 
100% 
57.3% 
News Category 
2.0% 
7.1% 
POS 
94.5% 
55.2% 
Bing -Substring 78.5% 
79.8% 
Bing -Relaxed 
75.7% 
80.9% 
Bing -Align 
76.9% 
97.4% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>WordGap vs. Random News. Lower 
scores are better. 

# of wins Avg. score 
WordGap 
27 
3.84 
Random News 
23 
4.10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>WordGap vs. WordNews Hard. 
Lower scores are better. 

# of wins Avg. score 
WordGap 
21 
4.16 
WordNews Hard 29 
3.49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 : Distractors generated by WordGap and WordNews Hard for example question in Figure 2. The identified news category for the enclosing webpage was Entertainment.</head><label>6</label><figDesc></figDesc><table>System 
Distractor Lin Dist. Avg. Rate 
Target Word lark 
1.33 

WordGap 
frolic 
3.33 
runaround 
5.67 
cavort 
4.17 
WordNews 
art 
0.154 
1.67 
Hard 
film 
0.147 
3.33 
actress 
0.217 
4.83 

wins (8 more) and average score (0.67 lower). 
This further confirms that context and semantic in-
formation are complementary for distractor gener-
ation. As we mentioned before, a good distractor 
should fit the reading context and have a certain 
level of difficulty. Finally, in </table></figure>

			<note place="foot" n="1"> https://www.duolingo.com 2 https://translate.google.com 3 http://www.amazon.com/gp/help/ customer/display.html?nodeId=201733850</note>

			<note place="foot" n="4"> https://www.bing.com/translator 2</note>

			<note place="foot" n="7"> https://msdn.microsoft.com/enus/library/dn198370.aspx</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Topic Model for Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL&apos;07</title>
		<meeting>the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL&apos;07</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1024" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing Chinese Word Segmentation for Machine Translation Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pi-Chuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Statistical Machine Translation, StatMT&apos;08</title>
		<meeting>the Third Workshop on Statistical Machine Translation, StatMT&apos;08</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">WordGapAutomatic Generation of Gap-filling Vocabulary Exercises for Mobile Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Knoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabrina</forename><surname>Wilske</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second Workshop NLP Computer-Assisted Language Learning</title>
		<meeting>Second Workshop NLP Computer-Assisted Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="39" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic generation of cloze items for prepositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Seneff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2173" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating web-based english preposition exercises from real-world texts. Presentation at EUROCALL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Metcalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
		<ptr target="http://purl.org/dm/handouts/eurocall06-metcalf-meurers.pdf" />
		<imprint>
			<date type="published" when="2006-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enhancing authentic web pages for language learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Detmar</forename><surname>Meurers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramon</forename><surname>Ziai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriane</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vanessa</forename><surname>Metcalf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niels</forename><surname>Ott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications</title>
		<meeting>the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Good Language Learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Naiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multilingual Matters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple choice question corpus analysis for distractor characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Pho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne-Laure</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ligozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Illouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>François</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Language Resources and Evaluation (LREC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What the &quot;good language learner&quot; can teach us. TESOL quarterly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="page" from="41" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatic generation of english vocabulary tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuni</forename><surname>Susanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takenobu</forename><surname>Tokunaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Computer Supported Education (CSEDU 2015)</title>
		<meeting>the 7th International Conference on Computer Supported Education (CSEDU 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="77" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extensions to HMM-based Statistical Word Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Tolga Ilhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP EMNLP&apos;02</title>
		<meeting>the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP EMNLP&apos;02</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature-rich Part-ofspeech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL &apos;03</title>
		<meeting>the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, NAACL &apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using POS Information for Statistical Machine Translation into Morphologically Rich Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Ueffing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on European Chapter of the Association for Computational Linguistics, EACL&apos;03</title>
		<meeting>the 10th Conference on European Chapter of the Association for Computational Linguistics, EACL&apos;03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A flexible language learning platform based on language resources and web services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Volodina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildikó</forename><surname>Pilán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Borin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Therese</forename><forename type="middle">Lindström</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Grammar of Sense: Using Part-of-speech Tags As a First Step in Semantic Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yorick</forename><surname>Wilks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="143" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
