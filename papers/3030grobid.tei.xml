<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ultra-Low Latency SSDs&apos; Impact on Overall Energy Efficiency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Harris</surname></persName>
							<email>bryan.harris.1@louisville.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Louisville</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nihat</forename><surname>Altiparmak</surname></persName>
							<email>nihat.altiparmak@louisville.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering Department</orgName>
								<orgName type="institution">University of Louisville</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ultra-Low Latency SSDs&apos; Impact on Overall Energy Efficiency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Recent technological advancements have enabled a generation of Ultra-Low Latency (ULL) SSDs that blurs the performance gap between primary and secondary storage devices. However, their power consumption characteristics are largely unknown. In addition, ULL performance in a block device is expected to put extra pressure on operating system components , significantly affecting energy efficiency of the entire system. In this work, we empirically study overall energy efficiency using a real ULL storage device, Optane SSD, a power meter, and a wide range of IO workload behaviors. We present a comparative analysis by laying out several critical observations related to idle vs. active behavior, read vs. write behavior, energy proportionality, impact on system software, as well as impact on overall energy efficiency. To the best of our knowledge, this is the first published study of a ULL SSD&apos;s impact on the system&apos;s overall power consumption, which can hopefully lead to future energy-efficient designs.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Various innovations have taken place in the storage subsystem within the past few years, including wide adoption of the NVMe interface for new generation storage devices <ref type="bibr">[1]</ref>, the corresponding multi-queue request submission/completion capability (blk-mq) implemented in the Linux block IO layer <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b1">3]</ref>, and new IO scheduling algorithms specifically designed for blk-mq <ref type="bibr" target="#b2">[4]</ref><ref type="bibr" target="#b3">[5]</ref><ref type="bibr" target="#b4">[6]</ref>. We have also seen the emergence of new storage technologies such as Intel's Optane SSDs based on their 3D XPoint technology <ref type="bibr" target="#b5">[7]</ref>, Samsung's Z-SSD with their SLC based 3D NAND technology <ref type="bibr" target="#b6">[8]</ref>, and Toshiba's XLFlash design using a similar technology as Z-SSDs <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10]</ref>. All these innovations enable a new generation of Ultra-Low Latency (ULL) SSDs that are broadly defined as providing sub-10 Âµs data access latency <ref type="bibr" target="#b9">[11]</ref>. <ref type="table" target="#tab_1">Table 1</ref> compares ULL SSD performance with existing storage technologies and shows how ULL SSDs help close the performance gap between primary and secondary storage devices.</p><p>Depending on the NVM technology that they use, ULL SSDs may come with completely different internal char-   <ref type="bibr" target="#b10">[12]</ref>. Recently, Wu et al. released their work towards an unwritten contract for Optane SSDs and further outlined performance characteristics of the 3D XPoint technology, including the lack of garbage collection, equal treatment of reads and writes, and comparable performance of sequential and random requests <ref type="bibr" target="#b11">[13]</ref>. However, the power consumption characteristics of Optane SSDs and their 3D XPoint technology are largely unknown. Furthermore, no previous work has investigated the impact of ULL IO performance on overall energy efficiency, especially considering the pressure it puts on system software. In this paper, first we characterize the power consumption behavior of the Optane SSD, including its idle vs. active power consumption, read vs. write power consumption, and energy proportionality by comparing it to traditional storage technologies such as spinning magnetic disks and flash SSDs. Second, we analyze the impact of ULL disk IO on overall energy efficiency of the entire system. We believe that energy efficiency is a crucial but commonly neglected issue in system design, and hope that our observations can aid both industry and academia in developing future energy-efficient systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Various recent work investigated the performance characteristics of ULL SSDs <ref type="bibr" target="#b9">[11]</ref><ref type="bibr" target="#b10">[12]</ref><ref type="bibr" target="#b11">[13]</ref><ref type="bibr" target="#b12">[14]</ref><ref type="bibr" target="#b13">[15]</ref>; however, except for limited analysis of individual devices <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref>, their power consumption characteristics are largely unknown and their impact on the overall energy efficiency of the entire system has unfortunately not yet been thoroughly studied. Existing energy efficiency research in the area of storage systems has mainly focused on HDDs and traditional flash-based SSDs, such as techniques to reduce HDD R/W head movement <ref type="bibr" target="#b16">[18]</ref> or to power-off/spin-down a subset of HDDs organized in a RAID structure <ref type="bibr" target="#b17">[19]</ref>. Further research focused on improving HDD energy proportionality <ref type="bibr" target="#b18">[20]</ref>, reducing the power consumption of data center storage systems composed of HDDs and flash SSDs <ref type="bibr" target="#b19">[21]</ref>, characterizing the power consumption of HDD based enterprise-scale backup storage systems <ref type="bibr" target="#b20">[22]</ref>, and reducing HDD spin-ups in archival storage systems using data grouping techniques <ref type="bibr" target="#b21">[23]</ref>. More recently, researchers analyzed the design tradeoffs of flash SSDs for improved energy efficiency <ref type="bibr" target="#b22">[24]</ref> and the impact of SSD RAIDs on server energy consumption <ref type="bibr" target="#b23">[25]</ref>.</p><p>New generation ULL SSDs are generally equipped with alternative NVM designs, such as phase-change memory used in 3D XPoint technology <ref type="bibr" target="#b10">[12]</ref>, and a different controller design based on the ULL performance goals and the physical characteristics of the underlying NVM <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b11">13]</ref>. In addition, ULL storage performance is expected to impact the overall energy efficiency of the system due to increased pressure on system software, especially considering the new blk-mq layer, IO schedulers, and NVMe driver, legacy performance optimizations performed with older storage characteristics in mind, and the current interrupt based IO handling mechanism. Existing assumptions do not apply to ULL SSDs and a new, systematical study is necessary to understand their power consumption characteristics and impact on overall energy efficiency considering recent changes in the Linux kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Energy Characterization and Efficiency 3.1 Methodology</head><p>We evaluate energy efficiency using four storage devices in an enterprise level system, which use three different storage technologies as summarized in <ref type="table" target="#tab_3">Table 2</ref>. The first is a spinning magnetic disk, a 3.5" WD Black 7200 rpm HDD on a SATA 3.1 bus. The second and third are NAND flash based traditional SSDs by Samsung. The 850 EVO is connected to a SATA bus and the 960 EVO is an M.2 NVM using a PCIe 3 adapter. The fourth is a representative of a ULL device, an Intel Optane 900P, using their proprietary phase change memory based 3D XPoint technology. These four devices are at a similar price point and include a range of technologies, interfaces, performance, and energy efficiency, all of which represent three generations of storage mechanics and assumptions:</p><p>Spinning magnetic storage, such as an HDD, is well understood with decades of research and software optimizations based on its internal mechanics. HDDs are fundamentally sequential devices; performance suffers from random access, but they can modify data in place. Motors keep spinning the platter while the device is idle, yielding high idle  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Setup</head><p>We attached our test storage devices to a Dell PowerEdge R230, which has a single socket Intel Xeon E3-1230 quadcore CPU (3.4 GHz) and 64 GB RAM. We installed CentOS 8.1 and upgraded the Linux kernel to version 5.5.0 (current stable). The system also has a Dell-certified SSD used as the OS disk, which was present in all experiments but not used for testing. During experiments, only one test storage device was physically installed at a time.</p><p>Power consumption was measured using an Onset HOBO UX120-018 Data Logger <ref type="bibr">[26]</ref>, an in-line power meter connected to the power supply of our test system, which does not include a redundant power supply. This meter records voltage, current, active power, active energy, apparent power, and power factor at a polling rate of once a second and stores it internally until it is read out through a USB interface. These measurements are later synchronized to our workloads using time stamps recorded by our scripts. Both system clocks are synchronized to the same time server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Workloads</head><p>For each storage device, we measured the system's overall power consumption when the device is idle and active under light to heavy IO workloads for a range of request sizes. Workloads were generated using fio, the Flexible IO tester <ref type="bibr" target="#b24">[27]</ref>, version 3.18. In order to capture the maximal range of effects on the system, we wish to push the storage device to its limits. We submit asynchronous direct requests to the storage device, that bypass the page cache, and tested a range of specific IO behaviors similar to the workloads generated by Wu et al. <ref type="bibr" target="#b11">[13]</ref>. The request sizes tested were 1 KB and powers of 2 from 4 KB through 256 KB. Modern storage, like modern computers and applications, are highly parallel. We tested both a single thread and four threads (since we used a quad-core CPU), and with a range of requests outstanding to the application, referred to as IO depth or queue depth. These outstanding requests may be queued in the block layer, device driver, or in the device itself. From the application's perspective they are submitted asynchronously, but not yet completed. We used ext4 as the file system and mq-deadline as the blk-mq scheduler since they are current default choices in Linux. In order to record a sufficient number of power measurements from the in-line meter, we ran each workload three times for a fixed two minutes and averaged results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Idle vs. Active Power Consumption</head><p>Observation 1. Idle power consumption of Optane SSD is higher than that of traditional SSDs but lower than HDD.</p><p>As an initial investigation, we installed a single test storage device at a time and allowed the system to boot and sit idle for 10 minutes while we measured the power consumption of the entire system. Our test system at idle, with no test storage device installed, used an average of 29 W over the 10 minute test period. With a single HDD added, it used 39 W. Using the flash-based SATA and NVMe SSDs had approximately the same idle system power of 31 W. With the Optane SSD installed, the system consumed noticeably more power than traditional SSDs, 34 W; but less than the HDD. These values of power consumption for the entire system at idle are illustrated by the bottoms of the bars in <ref type="figure" target="#fig_0">Figure 1</ref>. HDDs have a notoriously high idle power consumption without performing any work due to their constantly spinning disks, damaging energy efficiency. This impact increases when more HDDs are added to system, such as with a RAID array <ref type="bibr" target="#b23">[25]</ref>. This problem was significantly alleviated by the introduction of fully electronic solid-state flash media. However, we caution that new ULL storage devices may reintroduce this behavior to provide continuously low data access latency. We observed this behavior in the Optane SSD, and believe that its contributors could be a high performance controller design, internal physics of 3D XPoint technology, and the lack of support for Autonomous Power State Transitions (APST).  APST, a feature configurable through the NVMe interface <ref type="bibr">[1]</ref>, allows the controller to autonomously transition the storage device into lower power states when idle. Using the command line tool nvme-cli <ref type="bibr" target="#b25">[28]</ref>, our tested Flash NVMe reports five APST power states as shown in <ref type="table" target="#tab_5">Table 3</ref>, but the Optane SSD has only one. Each state is flagged as "operational" or "non-operational." An operational state can service IO requests without violating its maximum power. The controller will automatically enter an operational state if the IO request doorbell is signaled while in a non-operational state. <ref type="table" target="#tab_5">Table 3</ref> also includes relative read/write performance rankings and the latency required to enter/exit power states.</p><p>APST plays an important role in idle energy consumption. If the storage device supports APST, then the host may actively instruct the controller to change states, or allow the controller to change states autonomously based on a given amount of idle time. For instance, the Linux kernel configures the controller of an APST supported device to enter the next lower-power non-operational state after an idle time of 50Ã the sum of its enter and exit latency. It also forbids power states with an exit latency greater than a user-given QoS latency. Due to its associated performance cost, ULL SSDs might refrain from supporting APST; however, low idle power consumption is crucial for energy efficient systems and we believe that further research is necessary to implement idle power saving features while providing ULL IO performance. Observation 2. Active power consumption increases as device latency decreases.</p><p>In order to analyze active power consumption, we run storage workloads across a range of behaviors from low to high intensity, and record overall power consumption of the system. The top of each bar in <ref type="figure" target="#fig_0">Figure 1</ref> represents the maximum power across all workloads for each device. Notice that with lower device latency, active power consumption increases. The power range of the system from idle to active is, of course, not due solely to the storage device, as there are other system components such as CPU and RAM that are impacted by ULL IO. Newer storage technologies are often introduced with the appeal of greater performance, but it should be noted that there is a cost in the form of energy, especially due to their impact on other system components, as we discuss later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Read vs. Write Power Consumption</head><p>Many performance issues of flash technology stem from its "erase before write" characteristic and consequent write amplification phenomenon, yielding inconsistent performance during garbage collection. Moreover, it takes longer to write to flash memory than to read from it, which creates what we call "RW asymmetry," in this case performance asymmetry. On the other hand, Wu et al. observed that the Optane SSD yields a similar performance for reads and writes <ref type="bibr" target="#b11">[13]</ref>, which makes it performance symmetric. In this section, we explore RW asymmetry of storage generations in terms of energy. Observation 3. From older to newer storage generations, devices move from energy symmetric to energy asymmetric. <ref type="figure" target="#fig_1">Figure 2</ref> shows the difference between power consumption of writes and reads (W â R) for random workloads. The HDD <ref type="figure" target="#fig_1">(Fig. 2a</ref>) drew approximately the same power for reads and writes, and is therefore energy symmetric. This is mainly due to the motor spinning the platter whether the RW head is reading or writing. We observed that flash-based SSDs <ref type="figure" target="#fig_1">(Figs. 2b &amp;  2c</ref>) are roughly energy symmetric, causing approximately the same power consumption for reads and writes; but asymmetries begin to emerge. On the other hand, we realized that the Optane SSD <ref type="figure" target="#fig_1">(Fig. 2d)</ref> is energy asymmetric and it consistently used more power for writes than for reads, except for tiny requests of 1 KB that are suggested to be avoided <ref type="bibr" target="#b11">[13]</ref>. For tiny requests, the bottom row shows greater power consumption for reads than for writes. While the energy asymmetry behavior deserves more research to understand the internal characteristics involved; it can potentially be exploited for energy-efficient hybrid drive designs combining 3D XPoint with flash or magnetic storage media, where requests can be redirected to the more energy-efficient option depending on their RW energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Energy Proportionality</head><p>Barroso and HÃ¶lzle describe energy proportional machines in their seminal work as consuming energy in proportion to the amount of work performed <ref type="bibr" target="#b26">[29]</ref>. A characteristic feature of this proportionality is a wide dynamic power range. The ideal energy proportional machine uses zero power at zero utilization (idle) and its maximum power at its peak utilization. Observation 4. Newer technological advancements in the storage subsystem lead to better energy proportionality.</p><p>With newer innovations from each device, the overall system shows a greater dynamic power range. We define storage performance utilization as throughput (IOPS) normalized to peak throughput for each device. <ref type="figure">Figure 3</ref> illustrates system power usage (power normalized to the peak power observed in our system) as a function of storage performance utilization for 4 KB requests. We show the trend using a quadratic curve. Based on <ref type="figure">Figure 3</ref>, the HDD is clearly not energy proportional. Its power usage is flat as it consumes approximately the same power regardless of the amount of work it performs. The traditional flash-based SATA and NVMe SSDs, on the other hand, are more energy proportional as they show an increase in IO performance with greater power usage. However, the Optane SSD is clearly the most energy proportional as it shows the greatest range of power usage, spanning more than half the peak system power, which makes it ideal for use in energy-efficient data centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Impact on System Software</head><p>Energy consumption is affected not only by the storage device, but also by the system software that runs it. An increase in system software load can cause other components such as CPU and memory to consume more power. Observation 5. As device latency decreases, the pressure on system software increases, resulting in increased overall energy consumption.</p><p>With each newer storage technology, we observed the expected latency improvement in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="figure" target="#fig_3">Figure 4a</ref> shows the latency of 4 KB random reads for the devices tested, where the traditional SSDs (middle) are around two orders of magnitude faster than the HDD, and an order of magnitude slower than the Optane SSD. As we put more requests into the system by increasing the IO depth, this relationship did not change. However, with lower latency, a device can simply serve more requests in the same amount of time, and this increase in throughput puts greater pressure on system software.</p><p>One major indicator of the pressure on system software is the number of storage interrupts (NVMe or AHCI) per second that the OS handles. We measured the number of interrupts during each storage workload by querying the kernel (/proc/interrupts) beforehand and afterwards. The SATA devices use only one AHCI interrupt number; however, in order to allow for greater parallelism, the NVMe driver with multiqueue (blk-mq) uses a separate interrupt number per queue, each with its own processor affinity. For our devices that use the NVMe interface, we sum the interrupts for all queues. <ref type="figure" target="#fig_3">Figures 4b and 4c</ref> show the average number of interrupts per second for 4 KB random reads and writes, with increasing IO depth. As we push the system, the storage interrupt rate can get quite high: nearly 600K/sec, which is a few orders of magnitude greater than even the rate of timer interrupts. This raises questions about the energy efficiency of using interrupt based IO, in addition to its known performance hit due to context switches. An interesting research question here is whether full or hybrid polling techniques can be more energy efficient than the existing interrupt based design for ULL devices. Increased pressure on system software is also indicated by greater kernel CPU utilization; however, we realized that CPU utilization by itself might be misleading to design energyefficient systems; researchers should always consult overall power consumption. <ref type="figure" target="#fig_3">Figures 4d and 4e</ref> show CPU utilization and power consumption, respectively, as a function of storage interrupts per second for 4 KB random reads and writes. In <ref type="figure" target="#fig_3">Figure 4d</ref>, writes for the Optane SSD caused around 300% system CPU usage with the interrupt rate of 400K/sec, while reads causing only 100% system CPU usage had an interrupt rate of 600K/sec. A similar imbalance exists for the Flash NVMe as well. However, their overall power consumption in <ref type="figure" target="#fig_3">Figure 4e</ref> is not directly proportional to their CPU utilization, where peak power consumption for reads is slightly greater than writes. Nevertheless, for the same interrupt rate (performance) in <ref type="figure" target="#fig_3">Figure 4e</ref>, writes are clearly more power hungry than reads, even for a performance symmetric device such as the Optane SSD. One obvious contributor is Optane's energy asymmetric behavior as shown in Section 3.5; however, we believe that write-based performance optimizations designed with flash characteristics in mind, including IO merging, prefetching, log structuring, and buffering, might exacerbate this situation further, and deserves more research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Impact on Overall Energy Efficiency</head><p>For overall energy efficiency, we measure the amount of work done per unit energy. Since we have two measurements of performance, we provide two metrics of energy efficiency: one based on bandwidth, the other on throughput. The first metric, "bytes per joule" is the ratio of bandwidth (bytes per second) to power (watts) since one watt is one joule per second. This metric adjusts for requests of various sizes. The second metric, "IOs per joule" is the ratio of throughput (IOPS) to power (watts), and is used to compare for a fixed request size. Observation 6. For the same IO depth, energy efficiency as bytes per joule increases as request size increases. <ref type="figure" target="#fig_4">Figure 5</ref> shows the overall energy efficiency as bytes per joule for random read workloads, where the Optane SSD is the "greenest" choice by performing the most work per unit energy. We observed the same trend for writes as well. In addition, we notice that for any given IO depth and storage device, larger requests consistently provide better overall energy efficiency measured as bytes per joule. Even though a larger request has more data to be transferred than a smaller request, it seems that the energy cost of transferring additional data in one request is less significant than the cost of managing the request itself and the pressure put on system software. Observation 7. For the same request size, energy efficiency as IOs per joule is coupled to internal device parallelism.  <ref type="figure" target="#fig_5">Figure 6</ref> interprets the overall energy efficiency in IOs per joule, which we use to clearly observe the most energyefficient IO depth given a request size for our tested NVMe devices. For the Optane SSD shown in <ref type="figure" target="#fig_5">Figure 6a</ref>, power consumption increases as IO depth increases for the same request size; however, this increased power consumption does not correspond to increased throughput after its internal parallelism is saturated. Others have observed saturated performance for ULL SSDs beyond an IO depth of 7 in Optane SSD <ref type="bibr" target="#b11">[13]</ref> and 6 in Z-SSD <ref type="bibr" target="#b12">[14]</ref>. With this work, we complement these observations by also showing a negative impact on energy efficiency beyond this parallelism saturation. Therefore, we see that on the right side of <ref type="figure" target="#fig_5">Figure 6a</ref>, overall energy efficiency begins to decrease for the Optane SSD. On the other hand, we do not observe the traditional Flash NVMe showing this behavior in <ref type="figure" target="#fig_5">Figure 6b</ref> due to its richer internal parallelism; in other words, increased power consumption corresponds to continuously increased IO performance for the Flash NVMe. Observations 6 and 7 outline best practices for overall energy efficiency. As described in Section 3.6, the Optane SSD has the greatest energy proportionality since its power consumption scales better than previous storage generations based on its range of throughput. Although Optane's peak power consumption is higher, it still yields a better energy efficiency as measured in bytes per joule and IOs per joule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We characterized the power consumption behavior of the Optane SSD and observed that it has undesirable idle power consumption, it consumes higher power at peak load compared to older storage generations, and it is energy asymmetric where writes cost more energy. Nevertheless, the Optane SSD stands out as the most energy-efficient and energy-proportional storage generation making it ideal for data centers. We further analyzed the energy implications of system software design, storage hardware design, and IO behavior for ULL performance, by detailing open questions in building energy-efficient systems supporting new generation ULL storage devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section, we introduce open questions requiring discussion and further research: I. Rethink system software for energy efficiency: Are there opportunities to make system software more energy efficient? Given the negative impact of low latency block devices on operating systems, are there ways to reduce or simplify system software to save energy? What would be the effect of classic or adaptive/hybrid polling techniques on energy efficiency compared to existing interrupt based design? Can polling be more energy efficient than interrupts for ULL devices? How about richer vs. simpler IO submission/completion mechanisms of blk-mq and the NVMe driver; can they be designed in a more energy efficient way considering ULL device behaviors? What are the implications of file system designs, IO scheduling algorithms, and write-based optimizations such as IO merging, prefetching, log structuring, and buffering on the energy efficiency of ULL storage?</p><p>II. Investigate read/write asymmetry: What is the cause of read/write imbalance in power consumption for the 3D XPoint technology? Why do writes use more system CPU but trigger fewer interrupts? Are there any existing system-level design choices based on previous storage technologies that get in the way of 3D XPoint? Otherwise, previous work indicates an equal treatment of reads and writes in 3D XPoint in terms of performance <ref type="bibr" target="#b11">[13]</ref>.</p><p>III. Investigate the energy efficiency of alternative userand kernel-space IO interfaces: What is the effect of kernel-bypass storage architectures, such as SPDK <ref type="bibr" target="#b27">[30]</ref> on overall energy efficiency? Would bypassing the kernel and implementing the NVMe driver in user space be more energy-efficient, or cause greater energy consumption due to its enforced polling based IO completion mechanism? What is the energy efficiency of the new io_uring kernel IO interface, that allows asynchronous IO in polled mode with a dedicated polling thread <ref type="bibr" target="#b28">[31]</ref><ref type="bibr" target="#b29">[32]</ref><ref type="bibr" target="#b30">[33]</ref>?</p><p>IV. Investigate energy-efficient hybrid drives with ULL SSDs: What are the energy-efficient design strategies for combining ULL SSDs with traditional SSDs or HDDs? Can we use request redirecting techniques exploiting energy asymmetry in 3D XPoint? What about when the ULL SSD is used as a caching layer?</p><p>V. Mind energy efficiency as a whole: What are the implications of recent storage performance optimizations (both at the device and system software levels) on overall energy efficiency? Due to their low power nature compared to other system resources like CPUs and memory, is the effect of the storage subsystem on the overall energy efficiency undermined? Do storage performance optimizations lose sight of overall energy efficiency?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Total system power with a single storage device</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Difference in power (watts) between random write and random read (W â R) fio microbenchmarks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Energy proportionality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact on system software</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Energy efficiency in bytes transferred per joule</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Energy efficiency in IOs per joule</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Performance gap in storage hierarchy 

acteristics than HDDs and traditional SSDs. For example, 
3D XPoint developers state that traditional techniques used in 
HDDs and Flash SSDs, such as IO merging in IO schedulers, 
prefetching, use of log structures instead of in-place updates, 
and using an intermediate buffer cache are not only unneces-
sary but get in the way of Optane SSDs </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Summary of tested storage devices 

power consumption. Power saving states that spin down 
the platter are known to be costly both in latency and wear. 
Traditional solid-state storage, is fully electronic with no 
moving parts based on NAND flash. These devices must 
"erase before write" and an erase unit is typically many 
times larger than a write unit, eliminating in-place updates. 
Floating gate transistors wear out after an excessive number 
of writes, thus the controller must dynamically manage 
allocations, wear leveling, and garbage collection. 
New generation ULL storage, based on either a new NVM 
technology or an optimized flash-based design, is character-
ized by low data access latency (sub-10 Âµs). The represen-
tative we measure here is Intel's Optane 900P SSD, block 
addressable storage based on Intel and Micron's proprietary 
3D XPoint technology, a form of phase change memory. 
Unlike flash-based SSDs, the Optane SSD performs in-
place updates, there is no internal garbage collection, and 
it is a true random access storage device [13]. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Power states of Samsung 960 Flash NVMe SSD 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and Myoungsoo Jung, our shepherd, for their feedback. This research was supported in part by the U.S. National Science Foundation (NSF) under grants CNS-1657296, CNS-1828521, and OIA-1849213.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linux block io: Introducing multi-queue SSD access on multi-core systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>BjÃ¸rling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bonnet</surname></persName>
		</author>
		<idno type="doi">10.1145/2485732.2485740</idno>
		<ptr target="http://doi.acm.org/10.1145/2485732.2485740" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Systems and Storage Conference, SYSTOR &apos;13</title>
		<meeting>the 6th International Systems and Storage Conference, SYSTOR &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mqsim: A framework for enabling realistic studies of modern multi-queue SSD devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Tavakkol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>GÃ³mez-Luna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadrosadati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saugata</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/fast18/presentation/tavakkol" />
	</analytic>
	<monogr>
		<title level="m">16th USENIX Conference on File and Storage Technologies (FAST 18)</title>
		<meeting><address><addrLine>Oakland, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="49" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Budget fair queueing (bfq) storage-I/O scheduler</title>
		<ptr target="http://algo.ing.unimo.it/people/paolo/disk_sched/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Kyber multiqueue I/O scheduler</title>
		<ptr target="https://patchwork.kernel.org/patch/9672023/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mq-deadline multiqueue I/O scheduler</title>
		<ptr target="https://github.com/torvalds/linux/blob/master/block/mq-deadline.c" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="https://www.intel.com/content/dam/www/public/us/en/documents/product-briefs/optane-ssd-900p-brief.pdf" />
		<title level="m">Intel optane ssd 900p series product brief</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sz985 Z-Nand</forename><surname>Samsung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssd</surname></persName>
		</author>
		<ptr target="https://www.samsung.com/us/labs/pdfs/collateral/Samsung_Z-NAND_Technology_Brief_v5.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scaling flash technology to meet application demands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(jeff)</forename><surname>Shigeo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ohshima</surname></persName>
		</author>
		<ptr target="https://flashmemorysummit.com/English/Conference/Keynotes.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="https://www.bloomberg.com/press-releases/2019-09-30/goke-2311-series-drives-toshiba-memory-xl-flash-based-nvme-ssds" />
		<title level="m">Goke 2311 ssd</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Asynchronous I/O stack: A low-latency kernel I/O stack for ultra-low latency SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyusun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokha</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsuk</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae Jun</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinkyu</forename><surname>Jeong</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc19/presentation/lee-gyusun" />
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference (USENIX ATC 19)</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="page" from="603" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Platform storage performance with 3d xpoint technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Hady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Veal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<idno type="doi">10.1109/JPROC.2017.2731776</idno>
		<ptr target="https://doi.org/10.1109/JPROC.2017.2731776" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1822" to="1833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards an unwritten contract of intel optane SSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/hotstorage19/presentation/wu-kan" />
	</analytic>
	<monogr>
		<title level="m">11th USENIX Workshop on Hot Topics in Storage and File Systems</title>
		<meeting><address><addrLine>Renton, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring system challenges of ultra-low latency solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changrim</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miryeong</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myoungsoo</forename><surname>Jung</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/hotstorage18/presentation/koh" />
	</analytic>
	<monogr>
		<title level="m">10th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 18)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flashshare: Punching through server storage stack from kernel to firmware for ultra-low latency SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miryeong</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoon</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlim</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Alian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myoungjun</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Mahmut Taylan Kandemir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myoungsoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jung</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi18/presentation/zhang" />
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<meeting><address><addrLine>Carlsbad, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="477" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">System evaluation of the intel optane byte-addressable nvm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ivy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">W</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Green</surname></persName>
		</author>
		<idno type="doi">10.1145/3357526.3357568</idno>
		<ptr target="https://doi.org/10.1145/3357526.3357568" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Memory Systems, MEMSYS &apos;19</title>
		<meeting>the International Symposium on Memory Systems, MEMSYS &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="304" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster than flash: An in-depth study of system challenges for emerging ultra-low latency ssds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jung</surname></persName>
		</author>
		<idno type="doi">10.1109/IISWC47752.2019.9042009</idno>
		<ptr target="https://doi.org/10.1109/IISWC47752.2019.9042009" />
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="216" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fs2: Dynamic data replication in free disk space for improving disk performance and energy consumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanda</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><forename type="middle">G</forename><surname>Shin</surname></persName>
		</author>
		<idno type="doi">10.1145/1095809.1095836</idno>
		<ptr target="https://doi.org/10.1145/1095809.1095836" />
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="263" to="276" />
			<date type="published" when="2005-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Energy conservation techniques for disk array-based servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
		<idno type="doi">10.1145/1006209.1006220</idno>
		<ptr target="https://doi.org/10.1145/1006209.1006220" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual International Conference on Supercomputing, ICS &apos;04</title>
		<meeting>the 18th Annual International Conference on Supercomputing, ICS &apos;04<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="68" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Srcmap: Energy proportional storage using dynamic consolidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Rangaswami</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/fast-10/srcmap-energy-proportional-storage-using-dynamic-consolidation" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on File and Storage Technologies, FAST&apos;10</title>
		<meeting>the 8th USENIX Conference on File and Storage Technologies, FAST&apos;10<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Power-reduction techniques for data-center storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bostoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sape</forename><surname>Mullender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yolande</forename><surname>Berbers</surname></persName>
		</author>
		<idno type="doi">10.1145/2480741.2480750</idno>
		<ptr target="https://doi.org/10.1145/2480741.2480750" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power consumption in enterprise-scale backup storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">M</forename><surname>Greenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">W</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/legacy/events/fast12/tech/full_papers/Li.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies, FAST&apos;12<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic data placement for power management in archival storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wildani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<idno type="doi">10.1109/PDSW.2010.5668053</idno>
		<ptr target="https://doi.org/10.1109/PDSW.2010.5668053" />
	</analytic>
	<monogr>
		<title level="m">5th Petascale Data Storage Workshop (PDSW &apos;10)</title>
		<imprint>
			<date type="published" when="2010-11" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design tradeoffs of SSDs: From energy consumption&apos;s perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokhei</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjip</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sooyong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyuk</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
		</author>
		<idno type="doi">10.1145/2644818</idno>
		<ptr target="https://doi.org/10.1145/2644818" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A comparative study of HDD and SSD RAIDs&apos; impact on server energy consumption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erica</forename><surname>Tomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nihat</forename><surname>Altiparmak</surname></persName>
		</author>
		<idno type="doi">10.1109/CLUSTER.2017.103</idno>
		<ptr target="https://doi.org/10.1109/CLUSTER.2017.103" />
	</analytic>
	<monogr>
		<title level="m">19th IEEE International Conference on Cluster Computing</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I/O</forename><surname>Flexible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tester</surname></persName>
		</author>
		<ptr target="https://github.com/axboe/fio" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<ptr target="https://github.com/linux-nvme/nvme-cli" />
		<title level="m">NVM-Express user space tooling for linux</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The case for energy-proportional computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">AndrÃ©</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urs</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>HÃ¶lzle</surname></persName>
		</author>
		<idno type="doi">10.1109/MC.2007.443</idno>
		<ptr target="https://doi.org/10.1109/MC.2007.443" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="33" to="37" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spdk: A development kit to build high performance storage applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Verkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Paul</surname></persName>
		</author>
		<idno type="doi">10.1109/CloudCom.2017.14</idno>
		<ptr target="https://doi.org/10.1109/CloudCom.2017.14" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Efficient io through io_uring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<ptr target="https://kernel.dk/io_uring.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Faster io through io_uring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<ptr target="https://kernel-recipes.org/en/2019/talks/faster-io-through-io_uring/.KernelRecipes" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improved storage performance using the new linux kernel i/o interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kariuki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<ptr target="https://www.youtube.com/watch?v=vjudg0lCEhQ" />
		<title level="m">Storage Developer Conference (SDC), SDC&apos;19. SNIA</title>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
