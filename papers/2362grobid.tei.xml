<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:36+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST &apos;15). Open access to the Proceedings of the 13th USENIX Conference on File and Storage Technologies is sponsored by USENIX Efficient Intra-Operating System Protection Against Harmful DMAs Efficient Intra-Operating System Protection Against Harmful DMAs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 16-19,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Malka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Technology</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Amit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Technology</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Technology</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Malka</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Technology</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Amit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Technology</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Technology</orgName>
								<orgName type="department" key="dep2">Institute of Technology</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Israel, Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST &apos;15). Open access to the Proceedings of the 13th USENIX Conference on File and Storage Technologies is sponsored by USENIX Efficient Intra-Operating System Protection Against Harmful DMAs Efficient Intra-Operating System Protection Against Harmful DMAs</title>
					</analytic>
					<monogr>
						<title level="m">USENIX Association 13th USENIX Conference on File and Storage Technologies (FAST &apos;15)</title>
						<imprint>
							<biblScope unit="page">29</biblScope>
							<date type="published">February 16-19,</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Operating systems can defend themselves against misbehaving I/O devices and drivers by employing intra-OS protection. With &quot;strict&quot; intra-OS protection, the OS uses the IOMMU to map each DMA buffer immediately before the DMA occurs and to unmap it immediately after. Strict protection is costly due to IOMMU-related hardware overheads , motivating &quot;deferred&quot; intra-OS protection, which trades off some safety for performance. We investigate the Linux intra-OS protection mapping layer and discover that hardware overheads are not exclusively to blame for its high cost. Rather, the cost is amplified by the I/O virtual address (IOVA) allocator, which regularly induces linear complexity. We find that the nature of IOVA allocation requests is inherently simple and constrained due to the manner by which I/O devices are used, allowing us to deliver constant time complexity with a compact, easy-to-implement optimization. Our optimization improves the throughput of standard benchmarks by up to 5.5x. It delivers strict protection with performance comparable to that of the baseline deferred protection. To generalize our case that OSes drive the IOMMU with suboptimal software, we additionally investigate the FreeBSD mapping layer and obtain similar findings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The role that the I/O memory management unit (IOMMU) plays for I/O devices is similar to the role that the regular memory management unit (MMU) plays for processes. Processes typically access the memory using virtual addresses translated to physical addresses by the MMU. Likewise, I/O devices commonly access the memory via direct memory access operations (DMAs) associated with I/O virtual addresses (IOVAs), which are translated to physical addresses by the IOMMU. Both hardware units are implemented similarly with a page table hierarchy that the operating system (OS) maintains and the hardware walks upon an (IO)TLB miss.</p><p>The IOMMU can provide inter-and intra-OS protection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b57">59]</ref>. Inter protection is applicable in virtual setups. It allows for "direct I/O", where the host assigns a device directly to a guest virtual machine (VM) for its exclusive use, largely removing itself from the guest's I/O path and thus improving its performance <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">42]</ref>. In this mode, the VM directly programs device DMAs using its notion of (guest) "physical" addresses. The host uses the IOMMU to redirect these accesses to where the VM memory truly resides, thus protecting its own memory and the memory of the other VMs. With inter protection, IOVAs are mapped to physical memory locations infrequently, only upon such events as VM creation and migration, and host management operations such as memory swapping, deduplication, and NUMA migration. Such mappings are therefore denoted persistent or static <ref type="bibr" target="#b54">[57]</ref>.</p><p>Intra-OS protection allows the OS to defend against errant/malicious devices and buggy drivers, which account for most OS failures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b46">49]</ref>. Drivers/devices can initiate/perform DMAs to arbitrary memory locations, and IOMMUs allow OSes to protect themselves by restricting these DMAs to specific physical memory locations. Intra-OS protection is applicable in: (1) non-virtual setups where the OS has direct control over the IOMMU, and in (2) virtual setups where IOMMU functionality is exposed to VMs via paravirtualization <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b54">57]</ref>, full emulation <ref type="bibr" target="#b3">[4]</ref>, or, recently, hardware support for nested IOMMU translation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">36]</ref>. In this mode, IOVA (un)mappings are frequent and occur within the I/O critical path. The OS programs DMAs using IOVAs rather than physical addresses, such that each DMA is preceded and followed by the mapping and unmapping of the associated IOVA to the physical address it represents <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b43">46]</ref>. For this reason, such mappings are denoted single-use or dynamic <ref type="bibr" target="#b15">[16]</ref>. The context of this paper is intra-OS protection ( §2).</p><p>To do its job, the intra-OS protection mapping layer must allocate IOVA values: integer ranges that serve as page identifiers. IOVA allocation is similar to regular memory allocation. But it is different enough to merit its own allocator ( §3). One key difference is that regular allocators dedicate much effort to preserving locality and to combating fragmentation, whereas the IOVA allocator disallows locality and enjoys a naturally "unfragmented" workload. This difference makes the IOVA allocator 1-2 orders of magnitude smaller in terms of lines of code.</p><p>Another difference is that, by default, the IOVA subsystem trades off some safety for performance. It delays the completion of IOVA deallocations while letting the OS believe that the deallocations have been processed. Specifically, freeing an IOVA implies purging it from the IOTLB such that the associated physical buffer is no longer acces-marks by up to 5.50x and 1.71x for strict and deferred protection, respectively, and it reduces the CPU consumption by up to 0.53x. Interestingly, EIOVAR delivers strict protection with performance that is similar to that of the baseline system when employing deferred protection.</p><p>Accelerating allocation (of IOVAs in our case) using freelists is a well-known technique commonly utilized by memory allocators <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b52">55]</ref> ( §8). Our additional contributions are: identifying that the performance of the IOMMU mapping layer can be dramatically improved by employing this technique across the OSes we tested and thus refuting the common wisdom that the poor performance is largely due to the hardware slowness; carefully studying the IOMMU mapping layer workload; finding that it is very "well behaved"; which ensures that even our simplistic EIOVAR freelist provides fast, constant-time IOVA allocation while remaining compact in size ( §9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Intra-OS Protection</head><p>DMA refers to the ability of I/O devices to read from or write to the main memory without CPU involvement. It is a heavily used mechanism, as it frees the CPU to continue to do work between the time it programs the DMA until the time the associated data is sent or received. As noted, drivers of devices that stress the IOVA mapping layer initiate DMA operations via a ring buffer, which is a circular array in main memory that constitutes a shared data structure between the driver and its device. Each entry in the ring contains a DMA descriptor, specifying the address(es) and size(s) of the corresponding target buffer(s); the I/O device will write/read the data to/from the latter, at which point it will trigger an interrupt to let the OS know that the DMA has completed. (Interrupts are coalesced if their rate is high.) I/O device are commonly associated with more than one ring, e.g., a receive ring denoted Rx for DMA read operations, and a transmit ring denoted Tx for DMA write operations.</p><p>In the past, I/O devices used physical addresses in order to access main memory, namely, each DMA descriptor contained a physical address of its target buffer. Such unmediated DMA activity directed at the memory makes the system vulnerable to rogue devices performing errant or malicious DMAs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b55">58]</ref>, or to buggy drivers that might program their devices to overwrite any part of the system memory <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">42,</ref><ref type="bibr" target="#b46">49,</ref><ref type="bibr" target="#b53">56]</ref>. Subsequently, all major chip vendors introduced IOMMUs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b34">36]</ref>, alleviating the problem as follows.</p><p>The OS associates each DMA target buffer with some IOVA, used instead of the physical address when filling out the associated ring descriptor. The I/O device is oblivious to the change, processing the DMA as if the IOVA was a physical memory address. The IOMMU then translates the IOVA, routing the operation to the appropriate The functionality of the IOMMU is equivalent to that of the regular MMU. It permits IOVA memory accesses to go through only if the OS previously inserted matching translations. The OS can thus protect itself by allowing a device to access a target buffer just before the corresponding DMA occurs (add mapping), and by revoking access just after (remove mapping), exerting fine-grained control over what portions of memory may be used in I/O transactions at any given time. This state-of-the-art strategy of IOMMU-based protection was termed intra-OS protection by <ref type="bibr">Willmann et al. [57]</ref>.It is recommended by hardware vendors <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">38]</ref>, and it is used by operating systems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b42">45]</ref>. For example, the DMA API of Linux-which we use in this study-notes that "DMA addresses should be mapped only for the time they are actually used and unmapped after the DMA transfer" <ref type="bibr" target="#b43">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IOVA vs. Memory Allocation</head><p>The task of generating IOVAs-namely, the actual integer numbers that the OS assigns to descriptors and the devices then use-is similar to regular memory allocation. But it is sufficiently different to merit its own allocator, because it optimizes for different objectives, and because it is required to make different tradeoffs, as follows.</p><p>Locality Memory allocators spend much effort in trying to (re)allocate memory chunks in a way that maximizes reuse of TLB entries and cached content. The IOVA mapping layer of the OS does the opposite. The numbers it allocates correspond to whole pages, and they are not allowed to stay warm in hardware caches in between allocations. Rather, they must be purged from the IOTLB and from the page table hierarchy immediately after the DMA completes. Moreover, while purging an IOVA, the mapping layer must flush each cache line that it modifies in the hierarchy, as the IOMMU and CPU do not reside in the same coherence domain. 1</p><p>Fragmentation Memory allocators invest much effort in combating fragmentation, attempting to eliminate unused memory "holes" and utilize the memory they have before requesting the system for more. As we further discuss in §5- §6, it is trivial for the IOVA mapping layer to avoid fragmentation due to the simple workload that it services, which exclusively consists of requests whose size is a power of two number of pages. The IOMMU driver rounds up all IOVA range requests to 2 j for two reasons. First, because IOTLB invalidation of 2 j ranges is faster <ref type="bibr" target="#b34">[36,</ref><ref type="bibr" target="#b37">39]</ref>. And second, because the allocated IOVA range does not correspond to 2 j pages of real memory. Rather it merely corresponds to to a pair of integers marking the beginning and end of the range. Namely, the IOMMU driver maps only the physical pages it was given, but it reserves a bigger IOVA range so as to make the subsequent associated IOTLB invalidation speedier. It can thus afford to be "wasteful". (In our experiments, the value of j was overwhelmingly 0. Namely, the allocated IOVA ranges almost always consist of one page only.)</p><p>Complexity Simplicity and compactness matter and are valued within the kernel. Not having to worry about locality and fragmentation while enjoying a simple workload, the mapping layer allocation scheme is significantly simpler than regular memory allocators. In Linux, it is comprised of only a few hundred lines of codes instead of thousands <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b39">41]</ref> or tens of thousands <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">32]</ref>.</p><p>Safety &amp; Performance Assume a thread T 0 frees a memory chunk M, and then another thread T 1 allocates memory. A memory allocator may give M to T 1 , but only after it processes the free of T 0 . Namely, it would never allow T 0 and T 1 to use M together. Conversely, the IOVA mapping layer purposely allows T 0 (the device) and T 1 (the OS) to access M simultaneously for a short period of time. The reason: invalidation of IOTLB entries is costly <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b54">57]</ref>. Therefore, by default, the mapping layer trades off safety for performance by (1) accumulating up to W unprocessed 'free' operations and only then (2) freeing those W IOVAs and (3) invalidating the entire IOTLB en masse. Consequently, target buffers are actively being used by the OS while the device might still access them through stale IOTLB entries. This weakened safety mode is called deferred protection. Users can instead employ strict protection-which processes invalidations immediately-by setting a kernel command line parameter.</p><p>Metadata Memory allocators typically use the memory that their clients (de)allocate to store their metadata. For example, by inlining the size of an allocated area just before the pointer that is returned to the client. Or by using linked lists of free objects whose "next" pointers are kept within the areas the comprise the lists. The IOVA mapping layer cannot do that, because the IOVAs that it invents are pointers to memory that is used by some other entity (the device or the OS). An IOVA is just an additional identifier for a page, which the mapping layer does not own.</p><p>Pointer Values Memory allocators running on 64-bit machines typically use native 64-bit pointers. The IOVA mapping layer prefers to use 32-bit IOVAs, as utilizing 64-bit addresses for DMA would force a slower, dual address cycle on the PCI bus <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Supposed O(1) Complexity of Baseline</head><p>In accordance to §3, the allocation scheme employed by the Linux/x86 IOVA mapping layer is different than, and independent of, the regular kernel memory allocation subsystem. The underlying data structure of the IOVA allocator is the generic Linux kernel red-black tree. The elements of the tree are ranges. A range is a pair of integer numbers <ref type="bibr">[L, H]</ref> that represent a sequence of currently allocated I/O virtual page numbers L, L + 1, ..., H − 1, H, such that L ≤ H stand for "low" and "high", respectively. Ranges are pairwise disjoint, namely, given two ranges</p><formula xml:id="formula_0">[L 1 , H 1 ] 񮽙 = [L 2 , H 2 ], then either H 1 &lt; L 2 or H 2 &lt; L 1 .</formula><p>Newly requested IOVA integers are allocated by scanning the tree right-to-left from the highest possible value downwards towards zero in search for a gap that can accommodate the requested range size. The allocation scheme attempts and-as we will later see-ordinarily succeeds to allocate the new range from within the highest gap available in the tree.</p><p>The allocator begins to scan the tree from a cache node that it maintains, denoted C. The allocator iterates from C through the ranges in a descending manner until a suitable gap is found. C is maintained such that it usually points to a range that is higher than (to the right of) the highest free gap, as follows. When (1) a range R is freed and C currently points to a range lower than R, then C is updated to point to R's successor. And (2) when a new range Q is allocated, then C is updated to point to Q; if Q was the highest free gap prior to its allocation, then C still points higher than the highest free gap after this allocation. <ref type="figure" target="#fig_0">Figure 2</ref> lists the pseudo code of the IOVA allocation  The functions rb_next and rb_prev return the successor and predecessor of the node they receive, respectively. scheme as was just described. Clearly, the algorithm's worst case complexity is linear due to the 'while' loop that scans previously allocated ranges beginning at the cache node C. But when factoring in the actual workload that this algorithm services, the situation is not so bleak: the complexity turns out to actually be constant rather than linear (at least conceptually).</p><p>Recall that the workload is commonly induced by a circular ring buffer, whereby IOVAs of DMA target buffers are allocated and freed in a repeated, cyclic manner. Consider, for example, an Ethernet NIC with a Rx ring of size n, ready to receive packets. Assume the NIC initially allocates n target buffers, each big enough to hold one packet (1500 bytes). The NIC then maps the buffers to n newly allocated, consecutive IOVAs with which it populates the ring descriptors. Assume that the IOVAs are n, n − 1, ..., 2, 1. (The series is descending as IOVAs are allocated from highest to lowest.) The first mapped IOVA is n, so the NIC stores the first received packet in the memory pointed to by n, and it triggers an interrupt to let the OS know that it needs to handle the packet.</p><p>Upon handling the interrupt, the OS first unmaps the corresponding IOVA, purging it from the IOTLB and IOMMU page table to prevent the device from accessing the associated target buffer (assuming strict protection). The unmap frees IOVA=n, thus updating C to point to n's successor in the red-black tree (free_iova in <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>The OS then immediately re-arms the ring descriptor for future packets, allocating a new target buffer and associating it with a newly allocated IOVA. The latter will be n, and it will be allocated in constant time, as C points to n's immediate successor (alloc_iova in <ref type="figure" target="#fig_0">Figure 2</ref>). The same scenario will cyclically repeat itself for n − 1, n − 2, ..., 1 and then again n, ..., 1 and so on as long as the NIC is operational.</p><p>Our soon to be described experiments across multiple devices and workloads indicate that the above description is fairly accurate. IOVA allocations requests are overwhelmingly for one page ranges (H = L), and the freed IOVAs are indeed re-allocated shortly after being freed, enabling, in principle, the allocator in <ref type="figure" target="#fig_0">Figure 2</ref> to operate in constant time as described. But the algorithm succeeds to operate in this ideal manner only for some bounded time. We find that, inevitably, an event occurs and ruins this ideality thereafter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Long-Lasting Ring Interference</head><p>The above O(1) algorithm description assumes there exists only one ring in the I/O virtual address space. In reality, however, there are often two or more, for example, the Rx and Tx receive and transmit rings. Nonetheless, even when servicing multiple rings, the IOVA allocator provides constant time allocation in many cases, so long as each ring's free_iova is immediately followed by a matching alloc_iova for the same ring (the common case). Allocating for one ring and then another indeed causes linear IOVA searches due to how the cache node C is maintained. But large bursts of I/O activity flowing in one direction still enjoy constant allocation time.</p><p>The aforementioned event that forever eliminates the allocator's ability to accommodate large I/O bursts with constant time occurs when a free-allocate pair of one ring is interleaved with that of another. Then, an IOVA from one ring is mapped to another, ruining the contiguity of the ring's I/O virtual address. Henceforth, every cycle of n allocations would involve one linear search prompted whenever the noncontiguous IOVA is freed and reallocated. We call this pathology long-lasting ring interference and note that its harmful effect increases as additional inter-ring free-allocate interleavings occur. <ref type="table">Table 1</ref> illustrates the pathology. Assume that a server mostly receives data and occasionally transmits. Suppose that Rx activity triggers a Rx.free_iova(L) of address L (1). Typically, this action would be followed by <ref type="bibr">Rx</ref>   to iterate through the tree from H (6) to L (7), inducing a linear overhead. Notably, once H is mapped for Rx, the pathology is repeated every time H is (de)allocated. This repetitiveness is experimentally demonstrated in <ref type="figure" target="#fig_2">Figure  3</ref>, showing the per-allocation number of rb_prev invocations. The calls are invoked in the loop in alloc_iova while searching for a free IOVA. We show below that the implications of long-lasting ring interference can be dreadful in terms of performance. How, then, is it possible that such a deficiency is overlooked? We contend that the reason is twofold. The first is that commodity I/O devices were slow enough in the past such that IOVA allocation linearity did not matter. The second reason is the fact that using the IOMMU hardware is slow and incurs a high price, motivating the deferred protection safety/performance tradeoff. Being that slow, the hardware served as a scapegoat, wrongfully held accountable for most of the overhead penalty and masking the fact that software is equally to blame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">The EIOVAR Optimization</head><p>Suffering from frequent linear allocations, the baseline IOVA allocator is ill-suited for high-throughput I/O devices that are capable of performing millions of I/O transactions per second. It is too slow. One could proclaim that this is just another case of a special-purpose allocator proved inferior to a general-purpose allocator and argue that the latter should be favored over the former despite the notable differences between the two as listed in §4. We contend, however, that the simple, repetitive, and inherently ring-induced nature of the workload can be adequately served by the existing simplistic allocatorwith only a small, minor change-such that the modified version is able to consistently support fast (de)allocations.</p><p>We propose the EIOVAR optimization (Efficient IOVA allocatoR), which rests of the following observation. I/O devices that stress the intra-OS protection mapping layer are not like processes, in that the size of their virtual address spaces is relatively small, inherently bounded by the size of their rings. A typical ring size n is a few hundreds or a few thousands of entries. The number of per-device virtual page addresses that the IOVA allocator must simultaneously support is proportional to the ring size, which means it is likewise bounded and relatively small. Moreover, unlike "regular" memory allocators, the IOVA mapping layer does not allocate real memory pages. Rather, it allocates integer identifiers for those pages. Thus, it is reasonable to keep O(n) of these identifiers alive under the hood for quick (de)allocation, without really (de)allocating them (in the traditional, malloc sense of (de)allocation).</p><p>In numerous experiments with multiple devices and workloads, the maximal number of per-device different IOVAs we have observed is 12K. More relevant is that, across all experiments, the maximal number of previouslyallocated-but-now-free IOVAs has never exceeded 668 (and was 155 on average). Additionally, as noted earlier, the allocated IOVA ranges have a power of two size H − L + 1 = 2 j , where j is overwhelmingly 0. EIO-VAR leverages these workload characteristic to efficiently cache freed IOVAs so as to satisfy future allocations quickly, similarly to what regular memory allocators do when allocating real memory <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b52">55]</ref>.</p><p>EIOVAR is a thin layer that masks the red-black tree, resorting to using it only when EIOVAR cannot fulfill IOVA allocation on its own using previously freed elements. When configured to have enough capacity, all tree allocations that EIOVAR is unable to mask are assured to be fast and occur in constant time.</p><p>EIOVAR's main data structure is a one-dimensional array called "the freelist", or f for short. The array consists of M linked lists of IOVA ranges. Lists are empty upon initialization. When an IOVA range <ref type="bibr">[L, H]</ref> whose size is H − L + 1 = 2 j is freed, instead of actually freeing it, EIOVAR adds it to the head of the linked list of the corresponding exponent, namely, to f <ref type="bibr">[ j]</ref>. Because most ranges are comprised of one page (H = L), most ranges end up in the f [0] list after they are freed. The upper bound on the size of the ranges supported by EIOVAR is 2 M+12 bytes (assuming 2 12 = 4KB pages), as EIOVAR allocates page numbers. Thus, M = 28 is enough, allowing for up to a terabyte range.</p><p>EIOVAR allocation performs the reverse operation of freeing. When a range whose exponent is j is being allocated, EIOVAR removes the head of the f [ j] linked list in order to satisfy the allocation request. EIOVAR resorts to utilizing the baseline red-black tree only if a suitable range is not found in the freelist.</p><p>When no limit is imposed on the freelist, after a very short while, all EIOVAR (de)allocation operations are satisfied by f due to the inherently limited size of the ringinduced workload. All freelist (de)allocations are performed in constant time, taking 50-150 cycles per operation. Initial allocations that EIOVAR satisfies by resorting to the baseline tree are likewise done in constant time, because the freelist is limitless and so the tree never observes deallocations, which means its cache node C always points to its smallest, leftmost node <ref type="figure" target="#fig_0">(Figure 2</ref>).</p><p>We would like to make sure that the freelist is compact and is not effectively leaking memory. To bound the size of the freelist, EIOVAR has a parameter k that serves as f 's maximal capacity of freed IOVAs. We use the EIOVAR k notation to express this limit, with k = ∞ indicating no upper bound. We demonstrate that setting k to be a relatively small number is equivalent to setting it to ∞, because the number of previously-allocated-but-now-free IOVAs is constrained by the size of the corresponding ring. Consequently, we can be certain that the freelist of EIOVAR ∞ is a compact. At the same time, k = ∞ guarantees that (de)allocations are always satisfied in constant time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EIOVAR with Strict Protection</head><p>To understand the behavior and effect of EIOVAR, we begin by analyzing five EIOVAR k variants as compared to the baseline under strict protection, where IOVAs are (de)allocated immediately before and after the associated DMAs. We use the standard Netperf stream benchmark that maximizes throughput on one TCP connection. We initially restart the NIC interface for each allocation variant (thus clearing IOVA structures), and then we execute the benchmark iteratively. The exact experimental setup is described in §7. The results are shown in <ref type="figure">Figure 4</ref>. <ref type="figure">Figure 4a</ref> shows that the throughput of all EIOVAR variants is similar and is 20%-60% better than the baseline. The baseline gradually decreases except for the last iteration. <ref type="figure">Figure 4b</ref> highlights why even EIOVAR 1 is sufficient to provide the observed benefit. It plots the rate of IOVA allocations that are satisfied by the freelist, showing that k = 1 is enough to satisfy nearly all allocations. This result indicates that each call to free_iova is followed by alloc_iova, such that the IOVA freed by the former is returned by the latter, coinciding with the ideal scenario outlined in §4.   cause of two additional (de)allocation that are performed when Netperf starts running and that remain in the freelist thereafter. <ref type="figure">Figure 4d</ref> shows the average length of the 'while' loop from <ref type="figure" target="#fig_0">Figure 2</ref>, which searches for the next free IOVA. It depicts a rough mirror image of <ref type="figure">Figure 4a</ref>, indicating throughput is tightly negatively correlated with the traversal length. <ref type="figure">Figure 5</ref> (left) shows the time it takes the baseline to map an IOVA, separating allocation from the other activities. Whereas the latter remains constant, the former exhibits a trend identical to <ref type="figure">Figure 4d</ref>. Conversely, the alloc_iova time of EIOVAR ( <ref type="figure">Figure 5, right)</ref> is negligible across the board. EIOVAR is immune to long-lasting ring interface, as interfering transactions are absorbed by the freelist and reused in constant time. <ref type="figure" target="#fig_5">Figure 6</ref> is similar to <ref type="figure">Figure 5</ref>, but it pertains to the unmap operation rather than to map. It shows that the duration of free_iova remains stable across iterations with both EIO-VAR and the baseline. EIOVAR deallocation is still faster as it is performed in constant time whereas the baseline is logarithmic. But most of the overhead is not due to free_iova. Rather, it is due to the costly invalidation that purges the IOVA from the IOTLB to protect the corresponding target buffer. This is the aforementioned hardware overhead that motivated deferred protection, which amortizes the cost by delaying invalidations until enough IOVAs are accumulated and then processing all of them together. As noted, deferring the invalidations trades off safety for performance, because the relevant memory is accessible by the device even though it is already used by the kernel for other purposes. <ref type="figure" target="#fig_6">Figure 7</ref> compares between the baseline and the EIO-VAR variants under deferred protection. Interestingly, the resulting throughput divides the variants into two, with EIOVAR 512 and EIOVAR ∞ above 6Gbps and all the rest at around 4Gbps <ref type="figure" target="#fig_6">(Figure 7a)</ref>. We again observe a strong negative correlation between the throughput and the length of the search to find the next free IOVA <ref type="figure" target="#fig_6">(Figure 7a</ref> vs. 7d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">EIOVAR with Deferred Protection</head><p>In contrast to the strict setup <ref type="figure">(Figure 4</ref>), here we see that EIOVAR variants with smaller k values roughly perform as bad as the baseline. This finding is somewhat surprising, because, e.g., 25% of the allocations of EIOVAR 64 are satisfied by the freelist <ref type="figure" target="#fig_6">(Figure 7b</ref>), which should presumably improve its performance over the baseline. A finding that helps explain this result is noticing that the average size of the EIOVAR 64 freelist is 32 <ref type="figure" target="#fig_6">(Figure 7c</ref>), even though it is allowed to hold up to k = 64 elements. Notice that EIOVAR ∞ holds around 128 elements on average, so we know there are enough deallocations to fully populate the EIOVAR 64 freelist. One might therefore expect that the latter would be fully utilized, but it is not.</p><p>The average size of the EIOVAR 64 freelist is 50% of its capacity due to the following reason. Deferred invalidations are aggregated until a high-water mark W (kernel parameter) is reached, and then all the W addresses are deallocated in bulk. <ref type="bibr" target="#b1">2</ref> When k &lt; W , the freelist fills up to  hold k elements, which become k − 1 after the subsequent allocation, and then k − 2 after the next allocation, and so on until zero is reached, yielding an average size of 1 k+1 Σ k j=0 j ≈ k/2 as our measurements show. Importantly, when k &lt; W , EIOVAR k is unable to absorb all the W consecutive deallocations. The remaining W − k deallocations are thus freed by the baseline free_iova. Thus, only k of the W subsequent allocation are satisfied by the freelist, and the remaining W −k are serviced by the baseline alloc_iova. The baseline free_iova and alloc_iova are therefore regularly invoked in an uncoordinated way despite the freelist. As described in §5, the interplay between these two routines eventually causes long-lasting ring interference that induces repeated linear searches. In contrast, when k is big enough (≥ W ), the freelist has sufficient capacity to absorb all W deallocations, which are then used to satisfy the subsequent W allocations and thus secure the conditions for preventing the harmful effect. <ref type="figure" target="#fig_7">Figure 8</ref> demonstrates this threshold behavior, depicting the throughput as a function of the maximal freelist size k. Increasingly bigger k slowly improves performance, as more-but not yet all-allocations are served by the freelist. When k reaches W = 250, the freelist is finally big enough, and the throughput suddenly increases by 26%. <ref type="figure" target="#fig_8">Figure 9</ref> provides further insight into this result. It shows the per-allocation length of the loop within alloc_iova that iterates through the red-black tree in search for the next free IOVA (similarly to <ref type="figure" target="#fig_2">Figure 3</ref>). The subgraphs correspond to 3 points from <ref type="figure" target="#fig_7">Figure 8</ref> with k values 64, 240, and 250. We see that the smaller k (left) yields longer searches relative to the bigger k (middle), and that the length of the search becomes zero when k = W (right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Experimental Setup We implement EIOVAR in the Linux kernel, and we experimentally compare its performance against the baseline IOVA allocation. In an effort to attain more general results, we conducted the evaluation using two setups involving two different NICs with two corresponding different device drivers that generate different workloads for the IOVA allocation layer.</p><p>The Mellanox setup consists of two identical Dell PowerEdge R210 II Rack Servers that communicate through Mellanox ConnectX3 40Gbps NICs. The NICs are connected back to back configured to use Ethernet. One machine is the server and the other is a workload generator client. Each machine has 8GB 1333MHz memory and a single-socket 4-core Intel Xeon E3-1220 CPU running at 3.10GHz. The chipset is Intel C202, which supports VT-d, Intel's Virtualization Technology that provides IOMMU functionality. We configure the server to utilize one core only, and we turn off all power optimizations-sleep states (C-states) and dynamic voltage and frequency scaling (DVFS)-to avoid reporting artifacts caused by nondeterministic events. The two machines run Ubuntu 12.04 and utilize the Linux 3.4.64 kernel.</p><p>The Broadcom setup is similar, with the difference that: the two R210 machines communicate through Broadcom NetXtreme II BCM57810 10GbE NICs (connected via a CAT7 10GBASE-T cable for fast Ethernet); have 16GB memory; and run the Linux 3.11.0 kernel.</p><p>The drivers of the Mellanox and Broadcom NICs differ in many respects. Notably, the Mellanox driver uses more ring buffers and allocates more IOVAs (we observed around 12K addresses for <ref type="bibr">Mellanox and 3K for Broad- com)</ref>. In particular, the Mellanox driver uses two buffers per packet and hence two IOVAs, whereas the Broadcom driver allocates only one buffer and thus only one IOVA.</p><p>Benchmarks We use the following benchmarks to drive our experiments. Netperf TCP stream <ref type="bibr" target="#b35">[37]</ref> is a standard tool to measure networking throughput. It attempts to maximize the amount of data sent over one TCP connection, simulating an I/O-intensive workload. This is the benchmark used when studying long-lasting ring interference ( §5) and the impact of k on EIOVAR k ( §6). We use the default 16KB message size unless otherwise stated.</p><p>Netperf UDP RR (request-response) is the second canonical configuration of Netperf. It models a latency sensitive workload by repeatedly sending a single byte and waiting for a matching single byte response. The latency is then calculated as the inverse of the observed number of transactions per second.</p><p>Apache <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> is a HTTP web server. We drive it with ApacheBench <ref type="bibr" target="#b4">[5]</ref> (a.k.a. "ab"), a workload generator distributed with Apache. ApacheBench assess the number of concurrent requests per second that the server is capable of handling by requesting a static page of a given size from within several concurrent threads. We run it on the client machine configured to generate 100 concurrent requests. We use two instances of the benchmark to request a smaller (1KB) and a bigger (1MB) file. Logging is disabled to avoid disk write overheads.</p><p>Memcached <ref type="bibr" target="#b24">[25]</ref> is an in-memory key-value storage server. It is used, e.g., by websites for caching results of slow database queries, thus improving the sites' overall performance. We run Memslap <ref type="bibr" target="#b0">[1]</ref> (part of the libmemcached client library) on the client machine, generating requests and measuring the completion rate. By default, Memslap generates a random workload comprised of 90% get and 10% set operations. Unless otherwise stated, Memslap is set to use 16 concurrent requests.</p><p>Methodology Before running each benchmark, we shut down and bring up the interface of the NIC using the ifconfig utility, such that the IOVA allocation is redone from scratch using a clean tree, clearing the impact of previous harmful long-lasting ring interferences. We then iteratively run the benchmark 150 times, such that individual runs are configured to take about 20 seconds. We present the corresponding results, on average.</p><p>Results <ref type="figure">Figure 10</ref> shows the resulting average performance for the Mellanox (top) and Broadcom (bottom) setups. Higher numbers indicate better throughput in all cases but for Netperf RR, which depicts latency (inverse of throughput). The corresponding normalized valuesspecifying relative improvement-are shown in the first part of <ref type="table" target="#tab_5">Table 2</ref>. Here, for consistency, the normalized throughput is shown for all benchmarks including RR.</p><p>Mellanox Setup We first examine the results of the Mellanox setup (left of <ref type="table" target="#tab_5">Table 2</ref>). In the topmost part, we see that EIOVAR yields throughput 1.07-4.58x better than the baseline, and that improvements are more pronounced under strict protection. The second part of the table shows that the improved performance of EIOVAR is due to reducing the average IOVA allocation time by 1-2 orders of magnitude, from up to 50K cycles to around 100-200. EIOVAR further reduces the average IOVA deallocation time by about 75%-85%, from around 250-550 cycles to around 65-85 (4th part of the table).</p><p>As expected, the duration of the IOVA allocation routine is tightly correlated to the length of the search loop within this routine, such that a longer loop implies a longer duration (3rd part of <ref type="table" target="#tab_5">Table 2</ref>). Notice, however, that there is not necessarily such a direct correspondence between EIOVAR's throughput improvement (1st part of table) and the associated IOVA allocation overhead (2nd part). The reason: latency sensitive applications are less affected by the allocation overhead, because other components in their I/O paths have higher relative weights. For example, under strict protection, the latency sensitive Netperf RR has higher allocation overhead as compared to the throughput sensitive Netperf Stream (10,269 cycles vs. 7,656, respectively), yet the throughput improvement of RR is smaller <ref type="figure" target="#fig_0">(1.27x vs. 2.37x)</ref>. Similarly, the IOVA allocation overhead of Apache/1KB is higher than that of Apache/1MB (49,981 cycles vs. 17,776), yet its throughput improvement is lower <ref type="bibr">(2.35x vs. 3.65x)</ref>.</p><p>While there is not necessarily a direct connection between throughput and allocation overheads when examining strict safety only, the connection becomes apparent when comparing strict to deferred protection. Clearly, the benefit of EIOVAR in terms of throughput is greater under strict protection because the associated baseline allocation overheads are higher than that of deferred protection (7K-50K cycles for strict vs. 2K-3K for deferred).</p><p>Broadcom Setup Let us now examine the results of the Broadcom setup (right of <ref type="table" target="#tab_5">Table 2</ref>). Strict EIOVAR yields throughput that is 1.07-2.35x better than the baseline. Deferred EIOVAR, on the other hand, only improves the throughput by up to 10%, and, in the case of Netperf Stream and Apache/1MB, it offers no improvement. Thus, while still significant, throughput improvements in this setup are less pronounced. The reason for this difference is twofold. First, as noted above, the driver of the Mellanox NIC utilizes more rings and more IOVAs, increasing the load on the IOVA allocation layer relative to the Broad-  <ref type="figure">Figure 10</ref>: The performance of baseline vs. EIOVAR allocation, under strict and deferred protection regimes for the <ref type="bibr">Mellanox (top)</ref> and Broadcom (bottom) setups. Except for in the case of Netperf RR, higher values indicated better performance. Error bars depict the standard deviation (sometimes too small to be seen).</p><p>com driver and generating more opportunities for ring interference. This difference is evident when comparing the duration of alloc_iova in the two setups, which is significantly lower in the Broadcom case. In particular, the average allocation time in the Mellanox setup across all benchmarks and protection regimes is about 15K cycles, whereas it is only about 3K cycles in the Broadcom setup.</p><p>The second reason for the less pronounced improvements in the Broadcom setup is that the Broadcom NIC imposes a 10 Gbps upper bound on the bandwidth, which is reached in some of the benchmarks. Specifically, the aforementioned Netperf Stream and Apache/1MB-which exhibit no throughput improvement under deferred EIOVAR-hit this limit. These benchmarks are already capable of obtaining line rate (maximal throughput) in the baseline/deferred configuration, so the lack of throughput improvement in their case should come as no surprise. Importantly, when evaluating I/O performance in a setting whereby the I/O channel is saturated, the interesting evaluation metric ceases to be throughput and becomes CPU usage. Namely, the question becomes which system is capable of achieving line rate using fewer CPU cycles. The bottom/right part of <ref type="table" target="#tab_5">Table 2</ref> shows that EIOVAR is indeed the more performant alternative, using 21% less CPU cycles in the case of the said Netperf Stream and Apache/1MB under deferred protection. (In the Mellanox setup, it is the CPU which is saturated in all cases but the latency sensitive Netperf RR.) Deferred Baseline vs. Strict EIOVAR We explained above that deferred protection trades off safety to get better performance. We now note that, by <ref type="figure">Figure 10</ref>, the performance attained by EIOVAR when strict protection is employed is similar to the performance of the baseline configuration that uses deferred protection (the default in Linux). Specifically, in the Mellanox setup, on average, strict EIOVAR achieves 5% higher throughput than the deferred baseline, and in the Broadcom setup EIOVAR achieves 3% lower throughput. Namely, if strict EIOVAR is made the default, it will simultaneously deliver similar performance and better protection as compared to the current default configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Different Message Sizes</head><p>The default configuration of Netperf Stream utilizes a 16KB message size, which is big enough to optimize throughput. Our next experiment systematically explores the performance tradeoffs when utilizing smaller message sizes. Such messages can overwhelm the CPU and thus reduce the throughput. Another issue that might negatively affect the throughput of small packets is the maximal number of packets per second (PPS), which NICs commonly impose in conjunction with an upper bound on the throughput. (For example, the specification of our Broadcom NIC lists a maximal rate of 5.7 million PPS <ref type="bibr" target="#b31">[33]</ref>, and a rigorous experimental evaluation of this NIC reports that a single port in it is capable of delivering less than half that much <ref type="bibr" target="#b20">[21]</ref>.) <ref type="figure">Figure 11</ref> shows the throughput (top) and consumed CPU (bottom) as a function of message size for strict (left) and deferred safety (right) using the Netperf Stream benchmark in the Broadcom setup. With a 64B message size, the PPS limit dominates the throughput in all four configurations. Strict/baseline saturates the CPU with a message size as small as 256B; from that point on it achieves the same throughput (4Gbps), because the CPU remains its bottleneck. The other three configurations enjoy a gradually increasing throughput until line rate is reached. However, to achieve the same level of throughput, strict/EIOVAR requires more CPU than deferred/baseline, which in turn requires more CPU than deferred/EIOVAR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concurrency</head><p>We next experiment concurrent I/O streams, as concurrency amplifies the harmful long-lasting ring interference. <ref type="figure" target="#fig_0">Figure 12 depicts</ref>    <ref type="bibr" target="#b9">[10]</ref>. We find that, similarly to Linux, FreeBSD uses a redblack tree for IOVA space management. Although it does not employ the problematic cached node optimization, the relevant source code can call fall back to a linear iteration through the tree nodes upon allocation. The comment preceding the linear iteration acknowledges that "this falls back to linear iteration over the free space in the high region"; however, the comment further notes that the said "high regions are almost unused" <ref type="bibr" target="#b25">[26]</ref>.</p><p>Using DTrace, the dynamic tracing tool <ref type="bibr" target="#b27">[28]</ref>, we profiled the IOVA mapping layer of FreeBSD while running the Netperf TCP stream benchmark. We measured each function along the call stack in a separate run, because multiple probe points affected the perceived results. <ref type="table" target="#tab_7">Ta- ble 3</ref> show the outcome, indicating that the FreeBSD IOMMU mapping layer overheads are larger than those of baseline Linux (compare with left of <ref type="figure" target="#fig_5">Figures 5-6</ref>). Specifically, whereas FreeBSD IOVA allocation is comparable  to that of Linux, IOVA freeing takes an order of magnitude longer, and the (un)mapping is 4-5x slower altogether. Our profiling reveals some of the root causes for these overheads. The aforementioned linear iteration remained inactive, as promised. But IOVA allocation turned out to nevertheless require the traversal of 11 red-black tree nodes on average. And the tree was rebalanced in almost every deallocation, introducing an overhead that is considerably higher than that of baseline Linux.</p><p>In addition to its inefficient IOVA (de)allocation, FreeBSD makes several suboptimal implementation choices that significantly slow down its mapping layer as compared to Linux. For instance, when a page within the IOMMU page table hierarchy is no longer in use, Linux usually does not reclaim it, rightfully assuming that it is likely to get reused soon. Conversely, FreeBSD does reclaim such pages, thereby reducing the memory footprint somewhat at the cost of increased CPU overheads.</p><p>The most wasteful unoptimized FreeBSD code we observed relates to synchronizing the I/O page table hierarchy between the IOMMU and the CPU. Upon every unmapping (ctx_unmap_buf_locked), FreeBSD flushes all the cachelines of the corresponding page table, although merely flushing the affected page table entries (PTEs) would have been enough. We applied the latter optimization to the FreeBSD unmap code and thus shortened it by ∼10K cycles on average, which improved the throughput of Netperf from 530 to 935 Mbps (1.76x higher). <ref type="bibr" target="#b2">3</ref> Consequently, in according to our hypothesis, we find that the FreeBSD mapping layer consists of suboptimal code that allows for easy optimizations that dramatically boost performance, possibly due to the perception that IOMMU hardware overheads are inherently high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Several studies recognized the poor performance associated with using the IOMMU <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b47">50,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b57">59]</ref>. Willmann et al. suggested to alleviate IOMMU overheads somewhat via "shared mappings", creating only one mapping for buffers that happen to reside on the same page instead of associating each of them with a different IOVA <ref type="bibr" target="#b54">[57]</ref>. Amit et al. proposed to use "optimistic teardown", whereby unmappings are delayed for a few milliseconds in the hope they will get immediately reused, creating a riskier policy than deferred protection that is more performant <ref type="bibr" target="#b3">[4]</ref>. They also proposed to transparently offload the (un)mapping activity to computational cores different than the ones that perform the I/O. These approaches leave the original, unoptimized code intact and therefore EIOVAR is complementary to them.</p><p>Tomonori suggested to manage the IOVA space using bitmaps instead of trees, reporting an improvement in performance of 9% <ref type="bibr" target="#b47">[50,</ref><ref type="bibr" target="#b48">51]</ref>. Cascardo showed that performance is greatly improved if the driver of the I/O device can be modified to perform much fewer (un)mappings <ref type="bibr" target="#b17">[18]</ref>. In a followup work, we proposed to redesign the IOMMU hardware to directly support the ring-induced workload and thus provide strict safety within 0.77-1.00x the throughput, 1.00-1.04x the latency, and 1.00-1.22x the CPU consumption of a system without an IOMMU <ref type="bibr" target="#b41">[44]</ref>.</p><p>Using freelists to speed up object allocation-as in EIO-VAR-is a standard technique among memory allocators <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b52">55]</ref>. We discuss the contributes of this paper relative to such allocators in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Discussion, Conclusions, Future Work</head><p>Clements et al. made the case that implementations of OS kernels can be made scalable if they are designed beforehand such that their system calls commute, contending that "this rule aids developers in building more scalable software" <ref type="bibr" target="#b19">[20]</ref>. Conversely, Linus Torvalds proclaimed that "Linux is evolution, not intelligent design" <ref type="bibr" target="#b21">[22]</ref>, likely more accurately reflecting the manner by which OSes are built, typically using the simplest implementation until experience proves that this is the wrong way to go.</p><p>When implementing new kernel functionality, a linear algorithm is often favored as being the simplest. For example, such was the case with the original linear Linux scheduler, which survived a decade <ref type="bibr" target="#b44">[47]</ref>. And such is still the case with vmalloc, which is the internal Linux kernel function that is responsible for allocating virtually contiguous memory <ref type="bibr" target="#b49">[52]</ref> (as opposed to kmalloc, which allocates physically contiguous memory). The pro of favoring linearity is simplicity. The con is that it might hinder performance when assumptions change.</p><p>The Intel/Linux IOVA allocation algorithm admittedly models the vmalloc algorithm <ref type="bibr">[43]</ref>. From examining the source code, we see that both use a red-black tree for storing address ranges; both cache the location of the last freed range; and both use the cache as the starting point for subsequent allocations, traversing the tree elements in search for a large enough hole. We are not aware of workloads that utilize vmalloc whose performance noticeably degrades as a consequence. We demonstrate, however, that I/O intensive workloads suffer greatly form the linearity of IOVA allocation, which is induced by the "long-term ring interference" pathology that we characterize.</p><p>We conjecture that this deficiency exists because the IOMMU has been falsely perceived as the main responsible party for the significant overheads of intra-OS protection, and possibly because I/O devices fast enough to be noticeably affected have become widespread only in the last few years. We support our conjecture with experimental data from both Linux and FreeBSD.</p><p>We employ the compact EIOVAR optimization that proxies IOVA (de)allocations, resorting to the underlying redblack tree only if EIOVAR is unable to satisfy requests with its freelist. EIOVAR makes the baseline allocator orders of magnitude faster, improving the performance of common benchmarks by up to 5.5x.</p><p>Using freed object caches for fast allocation similarly to EIOVAR's freelist is not new. It is a standard technique employed by memory allocators <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b52">55]</ref>. Our contribution lies not in inventing the technique but rather: in (1) noticing it is applicable to, and substantially improves the performance of, the IOMMU mapping layer, which goes against the common wisdom that the slowness of this layer is due to the slowness of the hardware; in (2) carefully characterizing the workload experienced by the mapping layer; and in (3) finding that the workload characteristics allow for even the most basic/minimal freelist mechanism to deliver high performance, since (3.1) allocation requests exclusively consist of rounded up power-of-two areas that accelerate IOTLB invalidations without wasting real memory, and since (3.2) the freelist population size is inherently constrained by the relatively small size of the associated ring, so it can be used without worrying that the population of the previously-allocatedbut-now-free IOVAs would explode.</p><p>EIOVAR eliminates one serious bottleneck of the IOMMU mapping layer. But we suspect that other bottlenecks exist, notably in relation to its locking regime, which affects subsystems different than the IOVA allocator and might hinder scalability. In the future, we therefore intend to study how the mapping layer scales as corecount increases. Another interesting question we intend to study is whether it is possible, and how hard is it, to exploit the window of vulnerability inherent to deferred protection as compared to strict protection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Pseudo code of the baseline IOVA allocation scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>following the baseline allocation algorithm detailed in Figure 2. (Assume that all addresses are initially allocated.)length of search loop [num of rb_prev calls] allocation [serial number]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The length of each alloc_iova search loop in a 40K (sub)sequence of alloc_iova calls performed by one Netperf run. One Rx-Tx interference leads to regular linearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4c supports this observation by depicting the average size of the freelist.</head><label></label><figDesc>The average of EIOVAR 1 is inevitably 0.5, as every allocation and deal- location contributes to the average 1 and 0 respectively. Larger k values are similar, with an average of 2.5 be-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Netperf TCP stream iteratively executed under strict protection. The x axis shows the iteration number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Cycles breakdown of unmap with Netperf/strict.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Netperf TCP stream iteratively executed under deferred protection. The x axis shows the iteration number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Under deferred protection, EIOVAR k eliminates costly linear searches when k exceeds the high-water mark W .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Length of the alloc_iova search loop under the EIOVAR k deferred protection regime for three k values when running Netperf TCP Stream. Bigger capacity implies that the searches become shorter on average. Big enough capacity (k ≥ W = 250) eliminates the searches altogether.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>.alloc_iova, which would then return L (2). But some- times a Tx operation sneaks in between. If this Tx op- eration is Tx.free_iova(H) such that H &gt; L (3), then the allocator would update the cache node C to point to H's successor (4). The next Rx.alloc_iova would be satisfied by H (5), but then the subsequent Rx.alloc_iova would have</head><label></label><figDesc></figDesc><table>operation 
without Tx 
with Tx 
return 
C 
C return 
C 
C 
value before after value before 
after 
Rx.free(L=151) (1) 
152 152 
152 
152 
Tx.free(H=300) (3) 
152 (4) 301 
Rx.alloc 
(2) 151 
152 151 (5) 300 
301 
300 
Rx.free(150) 
151 151 
300 (6) 300 
Rx.alloc 
150 
151 150 (7) 151 
300 
151 

Table 1: Illustrating why Rx-Tx interferences cause linearity, 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>38 13th USENIX Conference on File and Storage Technologies (FAST '15)</head><label>38</label><figDesc></figDesc><table>USENIX Association 

0 

3 

6 

9 
throughput 
[Gbp/s] 

Netperf stream 

0 

10 

20 

30 

40 

latency [µsec] 

Netperf RR 

0.0k 

0.4k 

0.8k 

1.2k 
requests/sec 

Apache 1MB 

0k 

4k 

8k 

12k 
requests/sec 

Apache 1KB 

0k 

40k 

80k 

120k 
transactions/sec 

Memcached 

0 

3 

6 

9 

strict defer 

throughput 
[Gbp/s] 

baseline 
eiovar 

0 

10 

20 

30 

40 

strict defer 
latency [µsec] 

0.0k 

0.4k 

0.8k 

1.2k 

strict defer 

requests/sec 

0k 

4k 

8k 

12k 

strict defer 

requests/sec 

0k 

40k 

80k 

120k 

strict defer 

transactions/sec 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>the results of running</head><label></label><figDesc></figDesc><table>Mellanox 
protect benchmark 

baseline EiovaR 

diff 
throughput 
strict 
Netperf stream 
1.00 
2.37 +137% 
(normalized) 
Netperf RR 
1.00 
1.27 +27% 
Apache 1MB 
1.00 
3.65 +265% 
Apache 1KB 
1.00 
2.35 +135% 
Memcached 
1.00 
4.58 +358% 

defer 
Netperf stream 
1.00 
1.71 +71% 
Netperf RR 
1.00 
1.07 
+7% 
Apache 1MB 
1.00 
1.21 +21% 
Apache 1KB 
1.00 
1.11 +11% 
Memcached 
1.00 
1.25 +25% 

alloc 
strict 
Netperf stream 
7656 
88 
-99% 
(cycles) 
Netperf RR 
10269 
175 
-98% 
Apache 1MB 
17776 
128 
-99% 
Apache 1KB 
49981 
204 -100% 
Memcached 
50606 
151 -100% 

defer 
Netperf stream 
2202 
103 
-95% 
Netperf RR 
2360 
183 
-92% 
Apache 1MB 
2085 
130 
-94% 
Apache 1KB 
2642 
206 
-92% 
Memcached 
3040 
171 
-94% 

search 
strict 
Netperf stream 
153 
0 -100% 
(length) 
Netperf RR 
206 
0 -100% 
Apache 1MB 
381 
0 -100% 
Apache 1KB 
1078 
0 -100% 
Memcached 
893 
0 -100% 

defer 
Netperf stream 
32 
0 -100% 
Netperf RR 
32 
0 -100% 
Apache 1MB 
30 
0 -100% 
Apache 1KB 
33 
0 -100% 
Memcached 
33 
0 -100% 

dealloc / free strict 
Netperf stream 
289 
66 
-77% 
(cycles) 
Netperf RR 
446 
87 
-81% 
Apache 1MB 
360 
70 
-81% 
Apache 1KB 
565 
85 
-85% 
Memcached 
525 
73 
-86% 

defer 
Netperf stream 
273 
65 
-76% 
Netperf RR 
242 
66 
-73% 
Apache 1MB 
278 
65 
-76% 
Apache 1KB 
300 
66 
-78% 
Memcached 
334 
65 
-80% 

cpu 
strict 
Netperf stream 
100 
100 
+0% 
(%) 
Netperf RR 
32 
29 
-8% 
Apache 1MB 
100 
99 
-0% 
Apache 1KB 
99 
98 
-1% 
Memcached 
100 
100 
+0% 

defer 
Netperf stream 
100 
100 
+0% 
Netperf RR 
30 
29 
-5% 
Apache 1MB 
99 
99 
-0% 
Apache 1KB 
98 
98 
-0% 
Memcached 
100 
100 
+0% 

Broadcom 
protect benchmark 

baseline EiovaR 

diff 
throughput 
strict 
Netperf stream 
1.00 
2.35 +135% 
(normalized) 
Netperf RR 
1.00 
1.07 
+7% 
Apache 1MB 
1.00 
1.22 +22% 
Apache 1KB 
1.00 
1.16 +16% 
Memcached 
1.00 
1.40 +40% 

defer 
Netperf stream 
1.00 
1.00 
+0% 
Netperf RR 
1.00 
1.02 
+2% 
Apache 1MB 
1.00 
1.00 
+0% 
Apache 1KB 
1.00 
1.10 +10% 
Memcached 
1.00 
1.05 
+5% 

alloc 
strict 
Netperf stream 
14878 
70 -100% 
(cycles) 
Netperf RR 
3359 
100 
-97% 
Apache 1MB 
1469 
74 
-95% 
Apache 1KB 
2527 
116 
-95% 
Memcached 
5797 
110 
-98% 

defer 
Netperf stream 
1108 
96 
-91% 
Netperf RR 
1029 
118 
-89% 
Apache 1MB 
833 
88 
-89% 
Apache 1KB 
1104 
133 
-88% 
Memcached 
1021 
130 
-87% 

search 
strict 
Netperf stream 
345 
0 -100% 
(length) 
Netperf RR 
68 
0 -100% 
Apache 1MB 
27 
0 -100% 
Apache 1KB 
39 
0 -100% 
Memcached 
128 
0 -100% 

defer 
Netperf stream 
13 
0 -100% 
Netperf RR 
9 
0 -100% 
Apache 1MB 
9 
0 -100% 
Apache 1KB 
9 
0 -100% 
Memcached 
9 
0 -100% 

dealloc / free strict 
Netperf stream 
294 
47 
-84% 
(cycles) 
Netperf RR 
282 
48 
-83% 
Apache 1MB 
250 
50 
-80% 
Apache 1KB 
425 
52 
-88% 
Memcached 
342 
47 
-86% 

defer 
Netperf stream 
268 
47 
-82% 
Netperf RR 
273 
47 
-83% 
Apache 1MB 
234 
47 
-80% 
Apache 1KB 
279 
47 
-83% 
Memcached 
276 
47 
-83% 

cpu 
strict 
Netperf stream 
100 
53 
-49% 
(%) 
Netperf RR 
13 
12 
-12% 
Apache 1MB 
99 
99 
-0% 
Apache 1KB 
98 
98 
-0% 
Memcached 
99 
95 
-4% 

defer 
Netperf stream 
55 
44 
-21% 
Netperf RR 
12 
11 
-7% 
Apache 1MB 
91 
72 
-21% 
Apache 1KB 
98 
98 
-0% 
Memcached 
93 
92 
-2% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Summary of the results obtained with the Mellanox setup (left) and the Broadcom setup (right). 

Memcached in the Mellanox setup with an increasing 
number of clients. The left sub-graph reveals that the 
baseline allocation hampers scalability, whereas EIOVAR 
allows the benchmark to scale such that it is up to 5.5x 
more performant than the baseline (with 32 clients). The 
right sub-graphs highlights why, showing that the baseline 
IOVA allocation becomes costlier proportionally to the 
number of clients, whereas EIOVAR allocation remains 
negligible across the board. 

FreeBSD We hypothesize that, like Linux, other OSes 
drive the IOMMU with suboptimal software, likely due 
to the perception that the IOMMU hardware is slow, pos-
sibly combined with the fact that I/O devices that are fast 
enough to significantly suffer from the consequences have 
become prevalent fairly recently. We test this hypothesis 
by studying the IOMMU mapping layer of FreeBSD. Our 
hypothesis coincides with the announcement of IOMMU 
support being added to FreeBSD, which says that "it </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 : FreeBSD IOMMU mapping layer overheads in cycles.</head><label>3</label><figDesc></figDesc><table>(Compare with Linux's overheads in Figures 5-6.) 

</table></figure>

			<note place="foot" n="1"> Intel IOMMU specification documents a capability bit that indicates whether the IOMMU and CPU coherence could be turned on [36], but we do not own such hardware and believe it is not yet common.</note>

			<note place="foot" n="2"> They cannot be freed before they are purged from the IOTLB, or else they could be re-allocated, which would be a bug since their stale mappings might reside in the IOTLB and point to somewhere else.</note>

			<note place="foot" n="3"> We confirmed this optimization with the relevant FreeBSD maintainer [11] and committed a patch that will be included in the next FreeBSD release [3].</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Aker</surname></persName>
		</author>
		<ptr target="http://docs.libmemcached.org/bin/memslap.html" />
		<title level="m">Memslap -load testing and benchmarking a server</title>
		<imprint/>
	</monogr>
	<note>libmemcached 1.1.0 documentation</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">AMD IOMMU architectural specification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amd</forename><surname>Inc</surname></persName>
		</author>
		<idno>2.00</idno>
		<ptr target="http://support.amd.com/TechDocs/48882.pdf" />
		<imprint>
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">svn commit: r277023 -head/sys/x86/iommu, FreeBSD kerenl commit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Belousov</surname></persName>
		</author>
		<ptr target="https://lists.freebsd.org/pipermail/svn-src-head/2015-January/066777.html" />
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">vIOMMU: efficient IOMMU emulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="73" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Apachebench</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/ApacheBench" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Thunderbolt device driver programming guide: Debugging VT-d I/O MMU virtualization</title>
		<ptr target="https://developer.apple.com/library/mac/documentation/HardwareDrivers/Conceptual/ThunderboltDevGuide/DebuggingThunderboltDrivers/DebuggingThunderboltDrivers.html" />
		<imprint>
			<date type="published" when="2013-05" />
			<publisher>Apple Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ARM system memory management unit architecture specification -SMMU architecture version 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arm</forename><surname>Holdings</surname></persName>
		</author>
		<ptr target="http://infocenter.arm.com/help/topic/com.arm.doc.ihi0062c/IHI0062C_system_mmu_architecture_specification.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thorough static analysis of device drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ella</forename><surname>Bounimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Con</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohus</forename><surname>Ondrusek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ustuner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Eurosys</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="73" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FireWire: all your memory are belong to us</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Becher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximillian</forename><surname>Dornseif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">N</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CanSecWest applied security conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Belousov</surname></persName>
		</author>
		<ptr target="http://lists.freebsd.org/pipermail/freebsd-arch/2013-May/014368.html" />
		<title level="m">FreeBSD x86 IOMMU support (DMAR</title>
		<imprint>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">FreeBSD VT-d IOMMU implementation. Private email communication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Belousov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The price of safety: Evaluating IOMMU performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimi</forename><surname>Xenidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Ostrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Rister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Bruemmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leendert</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ottawa Linux Symposium (OLS)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hoard: A scalable memory allocator for multithreaded applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Emery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">R</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Slab allocator: An object-caching kernel memory allocator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bonwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Summer Annual Technical Conf</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="87" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Magazines and Vmem: Extending the Slab allocator to many CPUs and arbitrary resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bonwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="15" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dynamic DMA mapping using the generic device</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottomley</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/DMA-API.txt.Linuxkerneldocumentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hardware-based memory acquisition procedure for digital investigations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">D</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Grand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Investigation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">DMA API performance and contention on IOMMU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thadeu</forename><surname>Cascardo</surname></persName>
		</author>
		<ptr target="http://events.linuxfoundation.org/images/stories/slides/lfcs2013_cascardo.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical study of operating systems errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chelf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Hallem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawson</forename><surname>Engler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles (SOSP)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="73" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The scalable commutativity rule: Designing scalable software for multicore processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Frans</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nickolai</forename><surname>Zeldovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles (SOSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">QLogic FCoE/iSCSI and IP networking adapter evaluation (previously: Broadcom BCM957810 10Gb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llc</forename><surname>Demartek</surname></persName>
		</author>
		<ptr target="http://www.demartek.com/Reports_Free/Demartek_QLogic_57810S_FCoE_iSCSI_Adapter_Evaluation_2014-05.pdf" />
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perpetual development: A model of the Linux kernel life cycle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feitelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="859" to="875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Apache HTTP server project</title>
		<ptr target="http://httpd.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The Apache HTTP server project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gail</forename><surname>Fielding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="88" to="90" />
			<date type="published" when="1997-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed caching with memcached</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Fitzpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux Journal</title>
		<imprint>
			<biblScope unit="issue">124</biblScope>
			<date type="published" when="2004-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<ptr target="https://github.com/freebsd/freebsd/blob/release/10.1.0/sys/x86/iommu/intel_gas.c#L447" />
		<title level="m">FreeBSD Foundation. x86/iommu/intel_gas.c, source code file of FreeBSD 10.1.0</title>
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ELI: Bare-metal performance for I/O virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Har&amp;apos;el</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="411" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">DTrace Dynamic Tracing in Oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Gregg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Mauro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Prentice Hall</publisher>
			<pubPlace>Solaris, Mac OS X, and FreeBSD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">CustoMalloc: Efficient synthesized memory allocators. Software Practice and Experience (SPE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Zorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-08" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="851" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Failure resilience for device drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jorrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Herder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Gras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Homburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/IFIP Annual International Conference on Dependable Systems and Networks (DSN)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Integrating an EDK custom peripheral with a LocalLink interface into Linux</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Hill</surname></persName>
		</author>
		<idno>XAPP1129 (v1.0</idno>
		<imprint>
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Family data sheet: Broadcom NetXtreme network adapters for HP ProLiant Gen8 servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hp Development Company</surname></persName>
		</author>
		<ptr target="http://www.broadcom.com/docs/features/netxtreme_ethernet_hp_datasheet.pdf" />
	</analytic>
	<monogr>
		<title level="j">Rev</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">PowerLinux servers -64-bit DMA concepts</title>
		<ptr target="http://pic.dhe.ibm.com/infocenter/lnxinfo/v3r0m0/topic/liabm/liabmconcepts.htm." />
	</analytic>
	<monogr>
		<title level="m">IBM Corporation</title>
		<meeting><address><addrLine>Accessed</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">AIX kernel extensions and device support programming concepts</title>
		<ptr target="https://publib.boulder.ibm.com/infocenter/aix/v7r1/topic/com.ibm.aix.kernelext/doc/kernextc/kernextc_pdf.pdf" />
	</analytic>
	<monogr>
		<title level="m">IBM Corporation</title>
		<imprint>
			<date type="published" when="2013-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Intel virtualization technology for directed I/O, architecture specification -architecture specification -rev</title>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/product-specifications/vt-directed-io-spec.pdf" />
		<imprint>
			<date type="published" when="2013-09" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A network performance benchmark (revision 2.0)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><forename type="middle">A</forename><surname>Jones</surname></persName>
		</author>
		<ptr target="http://www.netperf.org/netperf/training/Netperf.html" />
		<imprint>
			<date type="published" when="1995" />
			<pubPlace>Hewlett Packard</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Tolerating hardware device failures in software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Renzelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles (SOSP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keshavamurthy</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2007/8/1/402" />
		<title level="m">optimize sg map/unmap calls. Linux Kernel Mailing List</title>
		<imprint>
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
	<note>patch -mm. Intel-IOMMU</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">A memory allocator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Lea</surname></persName>
		</author>
		<ptr target="http://g.oswego.edu/dl/html/malloc.html" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Lea</surname></persName>
		</author>
		<ptr target="ftp://g.oswego.edu/pub/misc/malloc.c" />
		<imprint>
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unmodified device driver reuse and improved system dependability via virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Levasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkmar</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Stoess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Götz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating System Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">rIOMMU: Efficient IOMMU for I/O devices that employ ring buffers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Malka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Mamtani</surname></persName>
		</author>
		<ptr target="http://download.microsoft.com/download/a/f/d/afdfd50d-6eb9-425e-84e1-b4085a80e34e/sys-t304_wh07.pptx" />
		<title level="m">DMA directions and Windows</title>
		<imprint>
			<date type="published" when="2007-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Dynamic DMA mapping guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Jelinek</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/DMA-API-HOWTO.txt.Linuxkerneldocumentation" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<ptr target="http://lxr.free-electrons.com/source/Documentation/scheduler/sched-design.txt?v=2.6.25" />
		<title level="m">Goals, design and implementation of the new ultra-scalable O(1) scheduler</title>
		<imprint>
			<date type="published" when="2002-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SecVisor: A tiny hypervisor to provide lifetime kernel code integrity for commodity OSes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Perrig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles (SOSP)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="335" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Improving the reliability of commodity operating systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Bershad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="110" />
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">DMA representations sg_table vs. sg_ring IOMMUs and LLD&apos;s restrictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fujita</forename><surname>Tomonori</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/legacy/events/lsf08/tech/IO_tomonori.pdf" />
	</analytic>
	<monogr>
		<title level="m">USENIX Linux Storage and Filesystem Workshop (LSF)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Intel IOMMU (and IOMMU for virtualization) performances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fujita</forename><surname>Tomonori</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2008/6/4/250" />
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Linus Torvalds and others. mm/vmalloc.c, source code file of Linux</title>
		<ptr target="http://lxr.free-electrons.com/source/mm/vmalloc.c?v=3.17" />
		<imprint>
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Zone garbage collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Van Sciver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Mach Workshop</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">I/O virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM (CACM)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2012-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Quick Fit: An efficient algorithm for heap storage allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Weinstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="141" to="148" />
			<date type="published" when="1988-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Device driver safety through a reference validation mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Emin Gün</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><forename type="middle">B</forename><surname>Sirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating System Design and Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="241" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Protection strategies for direct access to virtualized I/O devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Willmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Subverting the Xen hypervisor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Wojtczuk</surname></persName>
		</author>
		<ptr target="http://www.blackhat.com/presentations/bh-usa-08/Wojtczuk/BH_US_08_" />
	</analytic>
	<monogr>
		<title level="m">Black Hat</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">Pdf</forename><surname>Wojtczuk_Subverting_The_Xen_Hypervisor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-05" />
			<pubPlace>Accessed</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the DMA mapping problem in direct device assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Ami</forename><surname>Yassour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orit</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Systems and Storage Conference (SYSTOR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
