<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAVER Machine Translation System for WAT 2015</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoung-Gyu</forename><surname>Lee</surname></persName>
							<email>hg.lee@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NAVER LABS</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
								<address>
									<settlement>Seongnam-si, Gyeonggi-do</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesong</forename><surname>Lee</surname></persName>
							<email>jaesong.lee@navercorp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Seok</forename><surname>Kim</surname></persName>
							<email>jun.seok@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NAVER LABS</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
								<address>
									<settlement>Seongnam-si, Gyeonggi-do</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Ki</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NAVER LABS</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
								<address>
									<settlement>Seongnam-si, Gyeonggi-do</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Kangwon National University</orgName>
								<address>
									<addrLine>Gangwon-do</addrLine>
									<settlement>Chuncheon-si</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NAVER Machine Translation System for WAT 2015</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we describe NAVER machine translation system for English to Japanese and Korean to Japanese tasks at WAT 2015. We combine the traditional SMT and neural MT in both tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>This paper explains the NAVER machine translation system for the 2nd Workshop on Asian Translation (WAT 2015) ( <ref type="bibr">Nakazawa et al., 2015</ref>). We participate in two tasks; English to Japanese (EnJa) and Korean to Japanese (Ko-Ja).</p><p>Our system is a combined system of traditional statistical machine translation (SMT) and neural machine translation (NMT). We adopt the tree-tostring syntax-based model as En-Ja SMT baseline, while we adopt the phrase-based model as Ko-Ja. We propose improved SMT systems for each task and an NMT model based on the architecture using recurrent neural network (RNN) ( <ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr">Sutskever et al., 2014</ref>).</p><p>We give detailed explanations of each SMT system in section 2 and section 3. We describe our NMT model in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">English to Japanese</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training data</head><p>We used 1 million sentence pairs that are contained in train-1.txt of ASPEC-JE corpus for training the translation rule tables and NMT models. We also used 3 million Japanese sentences that are contained in train-1.txt, train-2.txt,train-3.txt of ASPEC-JE corpus for training the 5-gram language model. We also used 1,790 sentence pairs of dev.txt for tuning the weights of each feature of SMT linear model and as validation data of neural network. We filtered out the sentences that have 100 or more tokens from training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Language Analyzer</head><p>We used Moses tokenizer and Berkeley constituency parser 1 for tokenizing and parsing an English sentence. We used our own Japanese tokenizer and part-of-speech tagger for tokenizing and tagging a Japanese sentence. After running the tokenizer and the tagger, we make a token from concatenation of a word and its part-of-speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tree-to-string Syntax-based SMT</head><p>To determining the baseline model, we first performed comparative experiments with the phrasebased, hierarchical phrase-based and syntax-based models. As a result, we chose the tree-to-string syntax-based model.</p><p>The SMT models that consider source syntax such as tree-to-string and forest-to-string brought out better performance than the phrase-based and hierarchical phrase-based models in the WAT 2014 En-Ja task.</p><p>The tree-to-string model was proposed by <ref type="bibr">Huang (2006)</ref> and <ref type="bibr">Liu (2006)</ref>. It utilizes the constituency tree of source language to extract translation rules and decode a target sentence. The translation rules are extracted from a source-parsed and word-aligned corpus in the training step. We use synchronous context free grammar (SCFG) rules.</p><p>In addition, we used a rule augmentation method which is known as syntax-augmented machine translation ( <ref type="bibr">Zollmann and Venugopal, 2006</ref>). Because the tree-to-string SMT makes some constraints on extracting rules by considering syntactic tree structures, it usually extracts fewer rules than hierarchical phrase-based SMT (HPBSMT) <ref type="bibr" target="#b1">(Chiang, 2005</ref>). Thus it is required to augment tree-to-string translation rules. The rule augmentation method allows the training system to extract more rules by modifying parse trees. Given a parse tree, we produce additional nodes by combining any pairs of neighboring nodes, not only children nodes, e.g. NP+VP. We limit the maximum span of each rule to 40 tokens in the rule extraction process. The tree-to-string decoder use a chart parsing algorithm with cube pruning proposed by <ref type="bibr" target="#b1">Chiang (2005)</ref>.</p><p>Our En-Ja SMT system was developed by using the open source SMT engines; Moses and Giza++. Its other specifications are as follows:</p><p>• Grow-diag-final-and word alignment heuristic</p><p>• Good-Turing discounting for smoothing probabilities</p><p>• Minimum Error Rate Training (MERT) for tuning feature weights</p><p>• Cube-pruning-pop-limit = 3000</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Handling Out-of-Vocabulary</head><p>In order to handle out-of-vocabulary (OOV) words, we use two techniques; hyphen word split and spell error correction. The former is to split a word with hyphen (-) to two separate tokens before running the language analyzer. The latter is to automatically detect and correct spell errors in an input sentence. We give a detailed description of spell error correction in section 2.4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">English Spell Correction</head><p>It is not easy to translate a word including errata, because the erroneous word has only a slim chance of occurrence in the training data. We discovered a lot of spell errors among OOV words that appear in English scientific text. We introduce English spell correction for reducing OOV words in input sentences. We developed our spell corrector by using Aspell 2 .</p><p>For detecting a spell error, we skip words that have only capitals, numbers or symbols, because they are likely to be abbreviations or mathematic expressions. Then we regard words detected by Aspell as spell error words.</p><p>For correcting spell error, we use only top-3 suggestion words from Aspell. We find that a large gap between an original word and its suggestion word makes wrong correction. To avoid excessive correction, we introduce a gap thresholding technique, that ignores the suggestion word that has 3 or longer edit distance and selects one that has 3 Korean to Japanese</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training data</head><p>We used 1 million sentence pairs that are contained in JPO corpus for training phrase tables and NMT models. We also used Japanese part of the corpus for training the 5-gram language model. We also used 2,000 sentence pairs of dev.txt for tuning the weights of each feature of SMT linear model and as validation data of neural network. We did not filter out any sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Language Analyzer</head><p>We used MeCab-ko 3 for tokenizing a Korean sentence. We used Juman 4 for tokenizing a Japanese sentence. We did not perform part-of-speech tagging for both languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Phrase-based SMT</head><p>As in the En-Ja task, we first performed comparative experiments with the phrase-based and hierarchical phrase-based models, and then adopt the phrase-based model as our baseline model.</p><p>For the Ko-Ja task, we develop two phrasebased systems; word-level and character-level. We use word-level tokenization for the word-based system. We found that setting the distortion limit to zero yields better translation in aspect of both BLEU and human evaluation. We use the 5-gram language model. We use character-level tokenization for character-based system. We use the 10-gram language model and set the maximum phrase length to 10 in the phrase pair extraction process.</p><p>We found that the character-level system does not suffer from tokenization error and out-ofvocabulary issue. The JPO corpus contains many technical terms and loanwords like chemical compound names, which are more inaccurately tokenized and allow a lot of out-of-vocabulary tokens to be generated. Since Korean and Japanese share similar transliteration rules for loanwords, the character-level system can learn translation of unseen technical words. It generally produces better translations than a table-based transliteration.</p><p>Moreover, we tested jamo-level tokenization <ref type="bibr">5</ref> for Korean text, however, the preliminary test did not produce effective results.</p><p>We also investigated a parentheses imbalance problem. We solved the problem by filtering out parentheses-imbalanced translations from the nbest results. We found that the post-processing step can improve the BLEU score with low order language models, but cannot do with high order language models. We do not use the step for final submission.</p><p>To boosting the performance, we combine the word-level phrase-based model (Word PB) and the character-level phrase-based model (Char PB). If there are one or more OOV words in an input sentence, our translator choose the Char PB model, otherwise, the Word PB model.</p><p>Our Ko-Ja SMT system was developed by using the open source SMT engines; Moses and Giza++. Its other specifications are as follows:</p><p>• Grow-diag-final-and word alignment heuristic</p><p>• Good-Turing discounting for smoothing probabilities</p><p>• Minimum Error Rate Training (MERT) for tuning feature weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Machine Translation</head><p>Neural machine translation (NMT) is a new approach to machine translation that has shown promising results compared to the existing approaches such as phrase-based statistical machine translation ( <ref type="bibr" target="#b2">Cho et al., 2014;</ref><ref type="bibr">Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015</ref>). An NMT system is a single neural network that reads a source sentence and generates its translation. Using the bilingual corpus, the whole neural network is jointly trained to maximize the conditional probability of a correct translation given a source sentence. NMT has several advantages over the existing statistical machine translation systems such as the phrase-based system. First, NMT uses minimal domain knowledge. Second, the NMT system is jointly trained to maximize the translation performance, unlike the existing phrase-based system which consists of many separately trained features. Third, the NMT system removes the need to store explicit phrase tables and language models. Lastly, the decoder of an NMT system is easy to implement. Despite these advantages and promising results, NMT has a limitation in handling a larger target vocabulary, as the complexity of training and decoding increases proportionally to the number of target words.</p><p>In this paper, we propose a new approach to avoid the large target vocabulary problem by preprocessing the target word sequences, encoding them as a longer character sequence drawn from a small character vocabulary. The proposed approach removes the need to replace rare words with the unknown word symbol. Our approach is simpler than other methods recently proposed to address the same issue ( <ref type="bibr">Luong et al., 2015;</ref><ref type="bibr">Jean et al., 2015</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model</head><p>In this paper, we use our in-house software of NMT that uses an attention mechanism, as recently proposed by <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>. The encoder of NMT is a bi-directional recurrent neural network such that</p><formula xml:id="formula_0">h t = [ 񮽙 h t ; 񮽙 h t ]<label>(1)</label></formula><formula xml:id="formula_1">񮽙 h t = f GRU (W s we x t , 񮽙 h t+1 )<label>(2)</label></formula><formula xml:id="formula_2">񮽙 h t = f GRU (W s we x t , 񮽙 h t−1 )<label>(3)</label></formula><p>where h t is a hidden state of the encoder, x t is a one-hot encoded vector indicating one of the words in the source vocabulary, W s we is a weight matrix for the word embedding of the source language, and f GRU is a gated recurrent unit (GRU) ( <ref type="bibr" target="#b2">Cho et al., 2014</ref>). At each time, the decoder of NMT computes the context vector c t as a convex combination of the hidden states (h 1 ,. . . ,h T ) with the alignment weights α 1 ,. . . ,α T :</p><formula xml:id="formula_3">c t = T 񮽙 i=1 α ti h i (4) α ti = exp(e tj ) 񮽙 T j=1 exp(e tj )<label>(5)</label></formula><formula xml:id="formula_4">e ti = f F F N N (z t−1 , h i , y t−1 )<label>(6)</label></formula><p>where f F F N N is a feedforward neural network with a single hidden layer, z t−1 is a previous hidden state of the decoder, and y t−1 is a previous generated target word (one-hot encoded vector).</p><p>A new hidden state z t of the decoder which uses GRU is computed based on z t−1 , y t−1 , and c t :</p><formula xml:id="formula_5">z t = f GRU (y t−1 , z t−1 , c t )<label>(7)</label></formula><p>The probability of the next target word y t is then computed by <ref type="bibr">(8)</ref> </p><formula xml:id="formula_6">p(y t |y &lt;t , x) = y T t f sof tmax {W z 񮽙 y z 񮽙 t + W zy z t + W cy c t + W yy (W t we y t−1 ) + b y } z 񮽙 t = f ReLU (W zz 񮽙 z t )<label>(9)</label></formula><p>where f sof tmax is a softmax function, f ReLU is a rectified linear unit (ReLU), W t we is a weight matrix for the word embedding of the target language, and b y is a target word bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>We constructed the source word vocabulary with the most common words in the source language corpora. For the target character vocabulary, we used a BI (begin/inside) representation (e.g., 񮽙/B, 񮽙/I), because it gave better accuracy in preliminary experiment. The sizes of the source vocabularies for English and Korean were 245K and 60K, respectively, for the En-Ja and Ko-Ja tasks. The sizes of the target character vocabularies for Japanese were 6K and 5K, respectively, for the En-Ja and Ko-Ja tasks. We chose the dimensionality of the source word embedding and the target character embedding to be 200, and chose the size of the recurrent units to be 1,000. Each model was optimized using stochastic gradient descent (SGD). We did not use dropout. Training was early-stopped to maximize the performance on the development set measured by BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>All scores of this section are reported in experiments on the official test data; test.txt of the ASPEC-JE corpus. <ref type="table">Table 1</ref> shows the evaluation results of our En-Ja traditional SMT system. The first row in the table indicates the baseline of the tree-to-string systaxbased model. The second row shows the system that reflects the tree modification described in section 2.3. The augmentation method drastically increased both the number of rules and the BLEU score. Our OOV handling methods described in  The decoding time of the rule-augmented treeto-string SMT is about 1.3 seconds per a sentence in our 12-core machine. Even though it is not a terrible problem, we are required to improve the decoding speed by pruning the rule table or using the incremental decoding method <ref type="bibr">(Huang and Mi, 2010)</ref>. <ref type="table" target="#tab_1">Table 2</ref> shows the evaluation results of our Ko-Ja traditional SMT system. We obtained the best result in the combination of two phrase-based SMT systems. <ref type="table" target="#tab_3">Table 3</ref> shows effects of our NMT model. "Human" indicates the pairwise crowdsourcing evaluation scores provided by WAT 2015 organizers. In the table, "T2S/PBMT only" is the final T2S/PBMT systems shown in section 5.1 and section 5.2. "NMT only" is the system using only RNN encoder-decoder without any traditional SMT methods. The last row is the combined system that reranks T2S/PBMT n-best translations by NMT. Our T2S/PBMT system outputs 100,000-best translations in En-Ja and 10,000-best translations in Ko-Ja. The final output is 1-best translation selected by considering only NMT score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">En-Ja SMT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ko-Ja SMT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">NMT</head><p>NMT outperforms the traditional SMT in EnJa, while it does not in Ko-Ja. This result means that NMT produces a strong effect in the language pair with long linguistic distance. Moreover, the reranking system achieved a great synergy of T2S/PBMT and NMT in both task, even  if "NMT only" is not effective in Ko-Ja. From the human evaluation, we can be clear that our NMT model produces successful results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper described NAVER machine translation system for En-Ja and Ko-Ja tasks at WAT 2015. We developed both the traditional SMT and NMT systems and integrated NMT into the traditional SMT in both tasks by reranking n-best translations of the traditional SMT. Our evaluation results showed that a combination of the NMT and traditional SMT systems outperformed two independent systems.</p><p>For the future work, we try to improve the space and time efficiency of both the tree-to-string SMT and the NMT model. We also plan to develop and evaluate the NMT system in other language pairs. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Liang</head><label></label><figDesc>Huang and Haitao Mi. 2010. Efficient in- cremental decoding for tree-to-string translation. In Proceedings of the 2010 Conference on Em- pirical Methods in Natural Language Processing (EMNLP). Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proceedings of the 7th bi- ennial conference of the Association for Machine Translation in the Americas (AMTA). Sébastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL- IJCNLP). Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree- to-string alignment template for statistical machine translation. In Proceedings of the joint conference of the International Committee on Computational Lin- guistics and the Association for Computational Lin- guistics (Coling-ACL). Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Address- ing the rare word problem in neural machine transla- tion. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference of the Asian Federation of Natural Language Processing (ACL- IJCNLP). Toshiaki Nakazawa, Hideya Mino, Isao Goto, Gra- ham Neubig, Sadao Kurohashi, and Eiichiro Sumita. 2015. Overview of the 2nd workshop on asian trans- lation. In Proceedings of the 2nd Workshop on Asian Translation (WAT2015), Kyoto, Japan, October. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural net- works. In Proceedings of the Twenty-eighth Annual Conference on Neural Information Processing Sys- tems (NIPS). Andreas Zollmann and Ashish Venugopal. 2006. Syn- tax augmented machine translation via chart pars- ing. In Proceedings of the Workshop on Statistical Machine Translation (WMT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Ko-Ja SMT. 

section 2.4 also caused a positive effect as shown 
in the last row of the table. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>72 SYS</head><label>72</label><figDesc></figDesc><table>En-Ja 
Ko-Ja 
BLEU Human BLEU Human 
T2S/PBMT only 
32.76 
N/A 
70.91 
6.75 
NMT only 
33.14 
48.50 
65.72 
N/A 
T2S/PBMT + NMT reranking 34.60 
53.25 
71.38 
14.75 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Effect of NMT. 

</table></figure>

			<note place="foot" n="1"> https://github.com/slavpetrov/ berkeleyparser</note>

			<note place="foot" n="2"> http://aspell.net/ the shortest edit distance between an original word and its suggestion word.</note>

			<note place="foot" n="3"> https://bitbucket.org/eunjeon/ mecab-ko/ 4 http://nlp.ist.i.kyoto-u.ac.jp/EN/ index.php?JUMAN</note>

			<note place="foot" n="5"> Jamo is Korean alphabet letters which represent consonants and vowels. A Korean character can be usually separated to three jamoes; choseong, jungseong and jongseong.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Kevin Cho, who has visited for the summer internship program in NAVER LABS, for his contribution for development of the English spell corrector.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A hierarchical phrase-based model for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
