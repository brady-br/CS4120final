<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The seven deadly sins of cloud computing research</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Computer Laboratory ‡ Microsoft Research Silicon Valley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Computer Laboratory ‡ Microsoft Research Silicon Valley</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Computer Laboratory ‡ Microsoft Research Silicon Valley</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The seven deadly sins of cloud computing research</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Research into distributed parallelism on &quot;the cloud&quot; has surged lately. As the research agenda and methodology in this area are being established, we observe a tendency towards certain common simplifications and shortcuts employed by researchers, which we provocatively term &quot;sins&quot;. We believe that these sins, in some cases, are threats to the scientific integrity and practical applicability of the research presented. In this paper, we identify and discuss seven &quot;deadly sins&quot; (many of which we have ourselves committed!), present evidence illustrating that they pose real problems, and discuss ways for the community to avoid them in the future. Sin 1 Unnecessary distributed parallelism Parallel computing is an old concept, but has recently become a hot topic again due to two factors: multi-core CPUs, and the need to process very large data sets. The latter in particular has led the cloud community to adopt distributed parallelism as a default, often based on models such as MapReduce [11]. Unlike existing HPC frameworks for parallel processing, MapReduce offers an appealingly simple interface, and manages many aspects of the parallel coordination-such as synchronization , data motion and communication-as part of framework code. However, it does not always make sense to paral-lelize a computation. Firstly, doing so almost inevitably comes with an additional overhead, both in runtime performance and in engineering time. Secondly, although MapReduce and similar data-flow frameworks aim to migitate the engineering overhead, this transparency often results in programmers being ignorant of the potential performance impact. This may arise from communicating large amounts of intermediate data across machine boundaries, or from using algorithms which inherently involve large amounts of frequently mutated shared state. Thus, when designing a parallel implementation, its performance should always be compared to an optimized serial implementation, even if only for a small input data set, in order to understand the overheads involved. If we satisfy ourselves that parallel processing is indeed necessary or beneficial, it is also worth considering whether distribution over multiple machines is required. As Rowstron et al. recently pointed out, the rapid increase in RAM available in a single machine combined with large numbers of CPU cores per machine can make it economical and worthwhile to exploit local, rather than distributed, parallelism [36]. This observation is especially pertinent given that use of MapReduce for processing relatively small data sets appears to be commonplace [3]. Harnessing locally available parallelism has attractive benefits: it is possible to exploit shared memory and fast communication primitives; and data motion , which is often a bottleneck in distributed data processing [9], is avoided. It is also worth noting that the original MapReduce system was developed and used at Google before multi-core CPUs were widely used. Modern many-core machines, however, can easily apply 48 or more CPUs to processing a 100+ GB dataset entirely in memory, which already covers many practical use cases. There are, of course, many situations that warrant distributed parallelism: some data sets are too large for a single machine, or suffer performance degradation due to contention on shared I/O resources when running locally. The precise cross-over points between serial, locally parallelized and distributed implementation performance depend on the exact workload, and it thus makes sense to establish the need for distributed parallelism on a case-by-case basis, rather than assume it by default. Sin 2 Assuming performance homogeneity Systems for cloud computing usually run on, and are evaluated on, clusters of physical or virtual machines, with production systems comprising as many as thou</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 1 Unnecessary distributed parallelism</head><p>Parallel computing is an old concept, but has recently become a hot topic again due to two factors: multicore CPUs, and the need to process very large data sets. The latter in particular has led the cloud community to adopt distributed parallelism as a default, often based on models such as MapReduce <ref type="bibr" target="#b10">[11]</ref>. Unlike existing HPC frameworks for parallel processing, MapReduce offers an appealingly simple interface, and manages many aspects of the parallel coordination-such as synchronization, data motion and communication-as part of framework code.</p><p>However, it does not always make sense to parallelize a computation. Firstly, doing so almost inevitably comes with an additional overhead, both in runtime performance and in engineering time. Secondly, although MapReduce and similar data-flow frameworks aim to migitate the engineering overhead, this transparency often results in programmers being ignorant of the potential performance impact. This may arise from communicating large amounts of intermediate data across machine boundaries, or from using algorithms which inherently involve large amounts of frequently mutated shared state.</p><p>Thus, when designing a parallel implementation, its performance should always be compared to an optimized serial implementation, even if only for a small input data set, in order to understand the overheads involved.</p><p>If we satisfy ourselves that parallel processing is indeed necessary or beneficial, it is also worth considering whether distribution over multiple machines is required. As Rowstron et al. recently pointed out, the rapid increase in RAM available in a single machine combined with large numbers of CPU cores per machine can make it economical and worthwhile to exploit local, rather than distributed, parallelism <ref type="bibr" target="#b35">[36]</ref>. This observation is especially pertinent given that use of MapReduce for processing relatively small data sets appears to be commonplace <ref type="bibr" target="#b2">[3]</ref>. Harnessing locally available parallelism has attractive benefits: it is possible to exploit shared memory and fast communication primitives; and data motion, which is often a bottleneck in distributed data processing <ref type="bibr" target="#b8">[9]</ref>, is avoided. It is also worth noting that the original MapReduce system was developed and used at Google before multi-core CPUs were widely used. Modern many-core machines, however, can easily apply 48 or more CPUs to processing a 100+ GB dataset entirely in memory, which already covers many practical use cases.</p><p>There are, of course, many situations that warrant distributed parallelism: some data sets are too large for a single machine, or suffer performance degradation due to contention on shared I/O resources when running locally. The precise cross-over points between serial, locally parallelized and distributed implementation performance depend on the exact workload, and it thus makes sense to establish the need for distributed parallelism on a case-by-case basis, rather than assume it by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 2 Assuming performance homogeneity</head><p>Systems for cloud computing usually run on, and are evaluated on, clusters of physical or virtual machines, with production systems comprising as many as thousands of machines <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>. In academic research, rented clusters of virtual machines in "the cloud" are popular evaluation testbeds, since most academics do not have access to large physical clusters. However, it has been shown that virtualized cloud environments exhibit highly variable and sometimes unpredictable performance <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Any results obtained from such environments should hence be treated with care: it is absolutely essential to base reported results on multiple benchmark runs, and to report the performance variance over both a reasonable number of runs and a reasonable time period. Surprisingly many publications omit either <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, are inconsistent <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref>, or do not explicitly qualify parameters, such as the number of repeats, whether mean or median values are used, or what error bars on plots mean.</p><p>Variance may also exist for reasons other than the underlying multi-tenant cloud fabric: since cloud computing systems operate at a high level of abstraction, and frequently ship large amounts of data over the network, they are particularly vulnerable to performance interference from external effects such as network congestion or OS-level contention on shared resources. Again, many publications that use local clusters neglect to report on variance, or the absence thereof <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Finally, variance can also be caused by the system itself. For example, data loaded into the Hadoop Distributed File System (HDFS) is randomly distributed across the cluster <ref type="bibr" target="#b38">[39]</ref>, leading to a variable distribution of blocks corresponding to any single file. This particular issue has been ameliorated in some recent systems by the use of two-random-choice load balancing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, but some imbalance remains, and other issues inherent to system construction may exist, making measurement of performance variance in experiments a necessity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 3 Picking the low-hanging fruit</head><p>The Hadoop ecosystem is a highly popular basis for research in cloud computing-partly due to its open source nature, partly because of its generality and widespread use in production environments, and partly because it can easily be used as a benchmark for comparative evaluation. It is the last quality, in particular, that has turned out to be rather tempting to many.</p><p>Plenty of research projects have found ways of achieving a speedup over Hadoop for different kinds of workload, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, ranging from double-digit percentages to order-of-magnitude speedups <ref type="bibr">[2, 4, 6, 7, 12-14, 16, 18, 24, 29, 41, 43-45]</ref>. While impressive, the sheer magnitude and abundance of these speedup results does raise the question of their significance. To put it differently, one might ask how hard it is to achieve a  speedup over Hadoop? The answer is, of course, that it depends. Hadoop, as a consequence of its MapReduce roots, is an exceptionally general framework, while many of the research efforts mentioned are specialized to a particular application domain or class of algorithm. For example, iterative processing systems exploit in-memory caching <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, and systems from the database community trade off additional processing at data load time for faster job execution times <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>It is unsurprising that it is easy to beat a general system, unoptimized for particular application requirements, by specializing it. Hence, a better comparison for any specialized system is with a domain-specific optimized solution, which may not be as scalable, faulttolerant or easy-to-use as Hadoop, but provides a relevant performance baseline. Depending on the metric, we should not necessarily expect to beat the domain-specific solution-instead, the goal is to illustrate the cost of distributed processing, fault-tolerance or ease-of-use.</p><p>For example, we evaluated our CIEL system using the iterative k-means clustering algorithm. In our initial evaluation, we found CIEL to outperform Hadoop by about 160s per iteration using the same compute kernel <ref type="bibr" target="#b28">[29]</ref> ( <ref type="figure" target="#fig_1">Figure 2a)</ref>. However, the margin is constant as the input data size and number of workers increase, meaning that CIEL has less constant overhead than Hadoop, but does not scale any better. Subsequently, we also built a more efficient k-means compute kernel using arrays instead of Hadoop-like record I/O, and compared its performance against an MPI-based implementation (see <ref type="figure" target="#fig_1">Figure 2b)</ref>. We found that with this implementation, CIEL outperforms Hadoop by orders of magnitude, and scales better-but yet still performs, and scales, worse than MPI, which should be no surprise given its additional fault-tolerance overheads.</p><p>Furthermore, few researchers comment on the composability of their solutions. For example, if it were possible to compose all of the speedups depicted in <ref type="figure" target="#fig_0">Fig- ure 1</ref>, a Hadoop job that previously took 24 hours to complete would be reduced to a duration of a mere 20 milliseconds! Clearly, some improvements proposed are very similar to, or encompass, others, while some might be mutually exclusive. If optimizations that have previously been published are used, this fact should be noted, and speedups should be reported relative to the existing work, rather than to "vanilla" Hadoop. PrIter <ref type="bibr" target="#b44">[45]</ref> and Hadoop++ <ref type="bibr" target="#b11">[12]</ref> are model citizens in this regard, quantifying the speedup over iMapReduce <ref type="bibr" target="#b43">[44]</ref> and HadoopDB <ref type="bibr" target="#b0">[1]</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 4 Forcing the abstraction</head><p>MapReduce is a versatile paradigm, covering a wide range of algorithms and problems, but it is not a panacea or silver bullet. Yet it is often used over other, more appropriate, approaches because it is simple and regarded as a de-facto "standard". As we note with Sin 3, the MapReduce paradigm has been applied to anything from databases to iterative processing.</p><p>As we consider this, it is worth recalling that MapReduce was designed with a primary design goal of scalably processing huge data sets in a fault-tolerant way. Indeed, one main design focus of MapReduce was to alleviate the I/O bottleneck of disk spindle performance by parallelizing over many disks <ref type="bibr" target="#b10">[11]</ref>, and dealing with the consequent additional failure scenarios. Furthermore, jobs were assumed to be processing so much data that they would take a long time, even when parallelized over many machines. Hadoop inherited this goal, and it is reflected in its design choices. <ref type="bibr" target="#b0">1</ref> There is, however, evidence that, at least in some places, many Hadoop jobs are quite short-median job lengths of around 90s have been reported <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43]</ref>, although it is unclear whether these are representative of production environments. It does, however, seem to be the case that much research finding high speedups over Hadoop considers relatively small datasets (see <ref type="figure" target="#fig_2">Figure  3)</ref>, which typically correspond to relatively small and short jobs.</p><p>This effect is compounded in the case of high-level languages that translate a program into a set of MapReduce jobs: Hive <ref type="bibr" target="#b39">[40]</ref>, Pig Latin <ref type="bibr" target="#b30">[31]</ref> and FlumeJava <ref type="bibr" target="#b7">[8]</ref> are recent examples. While we certainly agree that there is a case to be made for languages lending more expressivity to data-parallel paradigms, we believe that it is more appropriate to consider execution models that can <ref type="bibr" target="#b0">1</ref> For example, task dispatch messages are piggy-backed onto Hadoop's periodic ping messages, so that a job can take up to 30s to start-this is insignificant for long-running jobs, but not for short ones. perform iterations within a single job, instead of chaining MapReduce jobs, as this allows for whole program optimization and avoids having many short jobs.</p><p>In some application domains, it is unclear if a MapReduce-like approach can offer any benefits, and indeed, some have argued that it is fruitless as a research direction <ref type="bibr" target="#b32">[33]</ref>.</p><p>In other application domains, the research community has a more pleasing track record of coming up with new, domain-specific systems. Examples of these include iterative processing <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45]</ref>, stream processing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b31">32]</ref>, and graph processing <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. Ideally, future research in these application domains should build on these best-ofbreed solutions, rather than on the MapReduce paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 5 Unrepresentative workloads</head><p>Cloud computing workloads are evaluated on clusters of machines, and usually a particular type of cluster setup is implicitly assumed. Popular setups appear to be (i) large, shared, multi-framework, multi-job, multi-user clusters; (ii) single-purpose, but multi-user and multi-job clusters; or (iii) per-job (possibly virtual) clusters. There is evidence that some large data centre operators use the first option <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>, and research on fair sharing of a cluster between multiple frameworks is emerging <ref type="bibr" target="#b17">[18]</ref>. Furthermore, short batch computations can make up a large portion of a multi-job cluster's jobs, while using only a fraction of the resources <ref type="bibr" target="#b34">[35]</ref>. Nonetheless, the common assumption in academic research systems is that the cluster workload is relatively homogeneous-that is, of the same job type and general nature-or that different kinds of workload do not interfere, or both. This assumption may stem from using workload traces with a single job type <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43]</ref>, or the assumption that in a "cloud" world of rented resources, clusters are spun up and down on demand. However, even in an environment such as EC2, interference exists, manifesting itself e.g. as contention on I/O resources such as disks and NICs, and resulting in variable performance (see <ref type="bibr">Sin 2)</ref>.</p><p>In a cluster shared between heterogeneous jobs, we may wish to prioritize some workloads over others. A common example of this is mixing revenue-critical web applications with backened data-analysis jobs, which are more tolerant of failures and extended runtimes <ref type="bibr" target="#b34">[35]</ref>. This may necessitate preemptions of running tasks, which can also be used to improve locality <ref type="bibr" target="#b19">[20]</ref>.</p><p>Having established the widespread sharing of clusters between jobs in the "real world", we note that most research evaluations measure performance by running a single job on an otherwise idle cluster. This may be defended by noting that, at least with experiments on EC2, the effect of other jobs running manifests itself as performance variance (cf. Sin 2), but due to the random and uncontrollable nature of the variance, this affects the reproducibility of results, and it is unclear how closely it resembles the situation in large commercial clusters.</p><p>Instead, running a single job on an idle cluster is the moral equivalent of an OS micro-benchmark, and should be complemented by appropriate macro-benchmarks. In the OS community, the lack of a representative "desktop mix" benchmark prompted a call for better multi-core benchmarks <ref type="bibr" target="#b21">[22]</ref>, and in a similar vein, we believe the cloud computing community needs representative cluster benchmarks. Hadoop GridMix 2 is a good starting point, but only covers MapReduce jobs. Comprehensive cluster traces covering multiple job types, such as recent Google traces, 3 may aid generating synthetic workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 6 Assuming perfect elasticity</head><p>The cloud paradigm is closely associated with the concept of utility computing and its promise of an unlimited supply of computation. The notion that one can use 10 machines for 100 hours or 1000 machines for one hour at the same price <ref type="bibr" target="#b4">[5]</ref>, and get the same work done, is an oft-misquoted canard.</p><p>As researchers, we know that this is, of course, a fallacy, because workloads do not exhibit infinite parallel speedup. That said, even ignoring the issues of algorithmic scalability and financial concerns, the scalability and supply of compute resources are not in fact infinite. For example, as the size of a compute cluster grows, it runs increasingly many tasks, which are scheduled by a single cluster scheduler in contemporary architecturesclearly, scheduler throughput will eventually become a bottleneck. Similarly, there are limits to the scalability of data center communication infrastructure and logic. TCP incast is a well-known issue in large-scale distributed systems <ref type="bibr" target="#b29">[30]</ref>, and especially affects systems that operate a single "master" node, as is the case with most existing frameworks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29]</ref>. <ref type="bibr" target="#b3">4</ref> Another reason for diminishing returns from parallelization is the increasing likelihood of failures, and greater vulnerability to performance interference, resulting in, e.g. "straggler" tasks <ref type="bibr" target="#b40">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sin 7 Ignoring fault tolerance</head><p>Two of the key challenges in the cloud are scale and fault tolerance. Indeed, the original MapReduce paper emphasizes ease-of-use and automatic handling of failures via task re-execution <ref type="bibr" target="#b10">[11]</ref>. This is not a surprise: as jobs scale to thousands of tasks, the chance of the job being affected by a failure increases.</p><p>Yet despite this, many recent systems neglect to account for the performance implications of fault tolerance, or indeed of faults occurring <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>. This may be another reason for observing high speedups over Hadoop (cf. Sin 3): for example, a popular optimization is to store intermediate data in memory, but this obviously reduces fault tolerance and may necessitate re-running tasks.</p><p>Instead, for each system, we should ask the question whether fault tolerance is relevant or required for the application considered. If it is, it makes sense to check precisely what level is required, and what faults are to be protected against. The evaluation in this case should explicitly consider and ideally quantify the cost of fault tolerance. If it is not deemed to be required-e.g. because the job is running within a single fault tolerance domain-then this should, again, be argued explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion and conclusions</head><p>We believe that the "sins" we discussed in this paper are widely committed-not maliciously, but for reasons of unwareness, or to simplify matters. While we do not believe that this (necessarily) invalidates existing research, we see a danger of these sins becoming entrenched in the research agenda and methodology.</p><p>Thus, we propose a set of remedies for the individual sins. Most of these can be best enforced by reviewers and shepherds, but we believe that, as a community, we should not require force-instead, we should cultivate awareness and avoid the sins by default, by following the suggestions below or otherwise.</p><p>1. Compare serial and distributed implementation performance of algorithms, or alternatively provide a formal or informal derivation of the maximum parallel speedup. Justify, if non-obvious, why the algorithm demands more parallelism than can be attained locally on a single machine. 2. Perform repeated runs and indicate performance variance on benchmarks, especially those executed in a virtualized environments. State clearly how many repeated runs the data is based on, and what the error bars signify (σ , max-min, percentiles, etc.). 3. Do not use or accept speedup over Hadoop as an indication of quality; instead, compare with relevant alternatives or justify why comparison with Hadoop is appropriate. The parallel speedup over a serial execution is a good alternative metric, ideally compared to the maximum attainable speedup. 4. Use common sense, rather than ideology or ease-ofimplementation, to decide if MapReduce is an appropriate paradigm for solving a problem. If it is not, but a MapReduce-based solution is significantly cheaper to implement, quantify the loss in performance by comparing with an appropriate alternative. 5. Consider different job types, priorities and preemption! Instead of, or in addition to, benchmarking individual jobs, benchmark cluster "job mixes". 6. When talking about elasticity, clearly qualify the assumptions made and note scalability bottlenecks. 7. Clearly state the fault tolerance characteristics of the proposed system, and justify why they are appropriate and satisfy the requirements.</p><p>We do realise that it may not be possible to avoid these sins all the time, but we believe that the onus should be on us as researchers to show that committing the sin is either unavoidable, or does not matter to answering the research question addressed. For some of the sins, such as the fifth and the sixth, this may be easy to argue; for others, such as the second, there really is no excuse ;-).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Maximum speedups over Hadoop claimed by a selection of research endeavours; N.B. log-scale y-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: k-means clustering with CIEL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Max. input data set sizes considered in various research systems' evaluations; N.B. log-scale y-axis.</figDesc></figure>

			<note place="foot" n="2"> http://goo.gl/brd6y 3 http://goo.gl/XO5YQ 4 Notably, Hadoop avoids this problem by only having workers communicate indirectly with the job tracker.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">HadoopDB: An architectural hybrid of MapReduce and DBMS technologies for analytical workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abouzeid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endowment</title>
		<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="922" to="933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reining in the outliers in map-reduce clusters using Mantri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI 2010</title>
		<meeting>OSDI 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Disk-Locality in Datacenter Computing Considered Irrelevant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotOS</title>
		<meeting>HotOS</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scarlett: coping with skewed content popularity in mapreduce clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="287" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Above the clouds: a Berkeley view of cloud computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armbrust</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<pubPlace>Berkeley</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hyracks: A flexible and extensible foundation for data-intensive computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borkar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1151" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">HaLoop: Efficient iterative data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endowment</title>
		<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="285" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">FlumeJava: Easy, efficient data-parallel pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chambers</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PLDI</title>
		<meeting>PLDI</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="363" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Managing data transfers in computer clusters with Orchestra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chowdhury</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGCOMM</title>
		<meeting>SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">98</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MapReduce online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Condie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI (2010)</title>
		<meeting>NSDI (2010)</meeting>
		<imprint>
			<biblScope unit="page" from="21" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hadoop++: Making a yellow elephant run like a cheetah (without it even noticing)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dittrich</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB</title>
		<meeting>VLDB</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Twister: a runtime for iterative mapreduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekanayake</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HPDC</title>
		<meeting>HPDC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="810" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">iHadoop: asynchronous iterations for MapReduce</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elnikety</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CloudCom</title>
		<meeting>CloudCom</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TidyFS: a simple and small distributed filesystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fetterly</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX ATC</title>
		<meeting>USENIX ATC</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sector and Sphere: the design and implementation of a high-performance data cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phil. trans. A, Math., phys., eng. sciences</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="page" from="2429" to="2474" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Comet: batched stream processing for data intensive distributed computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SoCC</title>
		<meeting>SoCC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mesos: A platform for fine-grained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hindman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dryad: distributed data-parallel programsfrom sequential building blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS OSR</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Quincy: fair scheduling for distributed computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOSP</title>
		<meeting>SOSP</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An Analysis of Traces from a Production MapReduce Cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavulya</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCGRID</title>
		<meeting>CCGRID</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multicore OS benchmarks: we can do better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotOS</title>
		<meeting>HotOS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CloudCmp: comparing public cloud providers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IMC</title>
		<meeting>IMC</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cloud MapReduce: A MapReduce Implementation on Top of a Cloud Operating System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CCGRID</title>
		<meeting>CCGRID</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="464" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed GraphLab: a framework for machine learning and data mining in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Low</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malewicz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD (2010)</title>
		<meeting>SIGMOD (2010)</meeting>
		<imprint>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards characterizing cloud backend workloads: insights from Google compute clusters. SIGMET-RICS Perf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mishra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eval. Review</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="34" to="41" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A distributed execution engine supporting datadependent control flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>Univ. of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CIEL: a universal execution engine for distributed data-flow computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI</title>
		<meeting>NSDI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Panasas ActiveScale storage cluster: Delivering scalable high bandwidth storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SC</title>
		<meeting>SC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pig Latin: a not-so-foreign language for data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1099" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nova: continuous Pig/Hadoop workflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparison of approaches to large-scale data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="165" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Piccolo: building fast, distributed programs with partitioned tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Power</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Characterizing a large, heterogeneous cluster trace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reiss</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nobody ever got fired for using Hadoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowstron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotCDP</title>
		<meeting>HotCDP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Runtime measurements in the cloud: observing, analyzing, and reducing variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endowment</title>
		<meeting>VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="460" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling and synthesizing task placement constraints in Google compute clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SoCC</title>
		<meeting>SoCC</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Hadoop distributed file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shvachko</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE MSST</title>
		<meeting>IEEE MSST</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hive -a petabyte scale data warehouse using Hadoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thusoo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDE</title>
		<meeting>ICDE</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="996" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving MapReduce performance in heterogeneous environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. OSDI</title>
		<meeting>OSDI</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Delay scheduling: A simple technique for achieving locality and fairness in cluster scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EuroSys</title>
		<meeting>EuroSys</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="265" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaharia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. HotCloud</title>
		<meeting>HotCloud</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">iMapReduce: A Distributed Computing Framework for Iterative Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPDPS</title>
		<meeting>IPDPS</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PrIter: a distributed framework for prioritized iterative computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SOCC</title>
		<meeting>SOCC</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
