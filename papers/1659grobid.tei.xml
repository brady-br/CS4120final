<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AmazingStore: Available, Low-cost Online Storage Service Using Cloudlets *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">U. C. Santa Barbara</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjian</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Xiao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafei</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AmazingStore: Available, Low-cost Online Storage Service Using Cloudlets *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>&quot;Cloud-based&quot; Internet services rely on the availability and reliability of managed data centers. Recent events indicate that data centers tend to create centralized points of failure, and providing resilience to large-scale faults remains a significant challenge for both providers and users of cloud in-frastructures. Running data centers also incurs high hardware and network costs, particularly for storage-intensive applications such as data synchronization and backup. In this paper, we show how to improve data availability while reducing costs in storage clouds, by augmenting centralized clouds with an efficient client-side storage system. We introduce AmazingStore, a low-cost cloud storage system that provides high data availability while protecting against correlated failures. We describe our initial experiences with an already deployed prototype and outline opportunities in this modified cloud model.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Always available and highly durable storage is one of the driving applications of the cloud computing model. The ability to store or backup files onto highly reliable data centers promises to greatly simplify data management at the edge, a growing challenge for users with rapidly increasing number of files. Users of these popular services can use them as online storage services (e.g. Amazon's S3 and Microsoft's SkyDrive and LiveMesh), or purely for online backup (e.g. Symantec's SwapDrive and EMC's Mozy).</p><p>There are two primary challenges facing cloud-based services, especially the storage services. First, cloud infrastructures are generally centralized for manageability and economy. Therefore, they are vulnerable to central point of failures caused by fires, power outages, natural disasters, etc. In fact, a quick scan of recent news shows that in the first half of 2009 alone, cloud computing providers such as Google, Amazon, and Rackspace have been hit by several major outages attributed to numerous causes, including network hardware failure <ref type="bibr" target="#b11">[12]</ref>, fires <ref type="bibr" target="#b5">[6]</ref>, network management <ref type="bibr" target="#b6">[7]</ref>, power failure <ref type="bibr" target="#b12">[13]</ref>, and other undisclosed causes. Clearly, cloud providers have not embraced replication across distributed data centers for fault-tolerance, perhaps due to challenges of performance and data consistency <ref type="bibr" target="#b14">[15]</ref>. A second challenge lies in the cost. For cloud storage providers, highly available storage means more storage arrays and power backups, escalating the cost of network bandwidth, data center cooling, and well-trained IT management personnel <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this paper, we take the first step to address these challenges by proposing Cloudlets, an alternative model for building available, reliable and economically efficient cloud-based storage services. In this model, we augment the centralized cloud-based storage service with a P2P storage service built using storage resources at its users. Each peer provides a cloudlet of storage for a number of object replicas in the system. Data access requests are forwarded to replicas on cloudlets whenever possible. Our objective is to improve the cloud's resilience to correlated failures while decreasing storage and bandwidth costs. Our key insight is that since the on/off availability dynamics of cloudlets are independent from failures at data centers, the peer-based storage layer complements the central cloud and maintains data availability during data center outages. In addition, available data replicas on peers can divert traffic away from the data center, thereby reducing its workload.</p><p>We instantiate the Cloudlets architecture in AmazingStore, a highly available, reliable, general purpose storage system. While purely peer-based storage systems have achieved limited success in the past <ref type="bibr" target="#b1">[2]</ref>, AmazingStore incorporates several novel insights and lessons from prior systems that make it well-suited to support today's online storage demands. First and foremost, since AmazingStore stores at least a "master" replica of each data object in the data center, its peer-based storage layer does not need to guarantee perfect availability. Second, it incorporates the advantages of both centralized and P2P architecture, thus simplifying the data management while protecting the service against single-point failures. Third, AmazingStore uses a group availability estimator to carefully navigate the cost-availability tradeoff at its peer level, by reducing replication costs as much as possible while maintaining the desired level of data availability. Finally, AmazingStore leverages the fact that requests for data <ref type="figure">Figure 1</ref>: The Cloudlets architecture. Data is kept at a central cloud, but is replicated on distributed cloudlets for improved availability and durability.</p><formula xml:id="formula_0">C l o u d l e t C e n t r a l S t o r a g e / M a n a g e m e n t C l o u d C l o u d l e t C l o u d l e t C l o u d l e t</formula><p>access correlates positively with peer availability. Thus periods of high demand for data access is also the time when the most peers (and the data replicas they host) are available in the system. This means its peer level can effectively reduce the costs at the data center by allowing the number of available replicas to fluctuate with the request load. We integrate these insights into the first prototype of AmazingStore, which has been open to users since April 3 rd , 2009. As of early October 2009, AmazingStore has a growing population of over 7100 registered users and 705GB allocated storage. Given a requirement of two nines availability, AmazingStore's cloudlet peers are able to improve overall data availability from 93.22% to 99.13% and divert over 90% request traffic away from the cloudlet servers, while reducing cloudlet replication traffic to a fixed maximum of 2KB/s. Moreover, cloudlet peers provide high data durability (an average of 0.07% files lost per month on the peer side), hence the replication costs required for maintaining high data durability can be further reduced.</p><p>In the remainder of this paper, we describe in detail the Cloudlets architecture, as well as the design and motivation behind the AmazingStore prototype. We present preliminary measurement results from our deployment of AmazingStore in Section 4 and discuss opportunities for future exploration in Section 5. Finally, we conclude the paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CLOUDLET MODEL</head><p>At a high level, cloudlet includes both a centralized cloud of servers in a data cluster ("cloudlet servers"), and distributed "cloudlet peers" at the network edge, as shown in <ref type="figure">Figure 1</ref>. Cloudlet servers serve two purposes, to provide a highly available copy of every object, and to monitor and manage replicas stored on cloudlet peers. In contrast, cloudlet peers are instances of the cloudlet clients running on user machines. Each peer allocates a fixed amount of local disk space for replica storage, maintains periodic heartbeats with cloudlet servers, and creates or deletes object replicas according to the servers' instructions.</p><p>This model exploits the different nature of faults in data center and P2P systems. In a storage system where each Cloud Server Cloudlet Peer Synchronization <ref type="figure">Figure 2</ref>: The AmazingStore design. Objects are organized into groups, each of which has a master node (locatable by searching for the group ID in the DHT) which monitors all replicas of objects in the group, and resolves client queries.</p><p>object is replicated in both a "small" data center and a P2P storage layer, neither has to be perfect. When an object's P2P replicas are all offline, it will be available in the data center; on the rare occasion when the data center is offline, the P2P replicas are still available. The probability that failures coincide on both sides is low enough to guarantee high availability. And of course, this hybrid model has the potential to lower the cost for providers by turning each of its users into part of the cloud.</p><p>Notice that Cloudlet is orthogonal to peer-assisted content distribution applications (including content distribution <ref type="bibr" target="#b3">[4]</ref>, file sharing <ref type="bibr" target="#b15">[16]</ref> and Video-on-Demand <ref type="bibr" target="#b4">[5]</ref>) due to different design issues. Specifically, Cloudlet pertains to the reliability aspect of storage and must exhibit high availability and data persistence characteristics. Efficient replication/repair strategies are the most important design decisions when deploying a peer-assisted storage system. However, Cloudlet can also adopt P2P file-sharing technology (e.g., BitTorrent protocol or DHT-based file sharing approach) to speedup the downloading of popular files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AMAZINGSTORE</head><p>In this section, we briefly describe the design rationale behind AmazingStore, showing how to turn cloudlet model into a practical system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">System Architecture</head><p>To form a highly available architecture, all nodes (servers and peers) are organized in a structured P2P overlay (DHT). Objects are organized into object "groups". Each group has its own group ID and a "master node" that manages replicas of all objects within the group. The group ID (GID) maps to its "master" node on a cloudlet server, as shown in <ref type="figure">Figure 2</ref>. A group master maintains pointers to all nodes with replicas of objects in the group, and monitors their liveness using heartbeats. A client seeking to read an object has its twopart ID. It first locates the object's group master using DHT routing on its GID, then queries the master with the object's Object ID (OID). If a replica on a cloudlet peer is available, the master directs the client to that peer. Otherwise, the master returns a reference to a server replica. In this way, the system congregates the master roles to the cloudlet servers, enabling insensitivity to peer dynamics.</p><p>For fault-tolerance, each master has a backup node with which it periodically synchronizes all metadata (references to replicas). The master and backup both have system-chosen nodeIDs based on the groups they manage. The backup's ID is assigned so that if the master fails, DHT routing maps any requests for the group directly to the backup. In the worstcase scenario, e.g., the centralized cloud goes down due to correlated failures, both nodes fail, and a cloudlet peer P whose nodeID is next closest to GID receives both requests for the group and heartbeats from nodes with object replicas. Within one heartbeat period, P will receive data on the location of all replicas for the group, and can service any read requests like the original master. When the cloudlet server returns, peers can automatically switch back to the original master by periodically looking up the GID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Availability Maintenance</head><p>To efficiently achieve the desired level of data availability, we must carefully determine the degree of redundancy used to tolerate availability transients and how quickly the system reacts to peer departures, a significant challenge in highly dynamic environment like AmazingStore.</p><p>Recall that in cloudlet, each object is replicated on both cloudlet servers and peers. Let µ S and µ P represent mean server availability and peer availability respectively. Given a target level of availability A (where A represents the probability a file can be accessed at any time), we can calculate the number of required replicas in the peer layer, c, as follows:</p><formula xml:id="formula_1">A = 1 − (1 − µ S )(1 − µ P ) c<label>(1)</label></formula><p>Notice that replicas placed at cloudlet servers would suffer correlated failures, so we use 1−µ S as a conservative prediction of unavailability experienced in the server layer. Since peers's online behaviors are independent <ref type="bibr" target="#b0">[1]</ref>, we predict unavailability experienced in the peer level as</p><formula xml:id="formula_2">(1 − µ P ) c . Solv- ing for c, c = log(1 − A) − log(1 − µ S ) log(1 − µ P )<label>(2)</label></formula><p>However, maintaining the c replicas over longer periods is the real challenge, which is complicated by the fact that peer failures can be either transient or permanent. Notice an object with sufficient replicas (e.g. ≥ c) can tolerate some transient failures without sacrificing data availability. Thus, it is preferable to accurately distinguish failures, and only pay the cost of object recreation under permanent failures. Carbonite <ref type="bibr" target="#b2">[3]</ref> tried to address this challenge by reintegrating returning replicas, but the approach is ineffective in dynamic environments with low peer availability.</p><p>Having recognized that permanent failure detection is difficult on a per-peer basis, we improve detection accuracy by making replica liveness detections across groups of replicas (e.g. all peers hosting replicas of the same object), balancing false positives for some peers against false negatives for others. In particular, we uses a probabilistic method to estimate the number of alive replicas (e.g. replicas residing on online peers or peers experiencing transient failures). The first step of this method is to obtain the failure probability F (d) that a peer has permanently failed given an observed failure of d time units, which can be extracted from historical traces of peer failure events (details can be found in <ref type="bibr" target="#b16">[17]</ref>). Next, we aggregate failure probabilities across peers in the replica group and apply the maximum a posteriori (MAP) estimation rule to derive an estimate. For instance, let x be the number of remaining replicas at the time of estimation. Given F (d i ) (1 ≤ i ≤ n) for peers in the group, we can compute the probability of x = k using the following approximation (with accuracy/complexity tradeoff):</p><formula xml:id="formula_3">P (x = k) = 0 k &lt; n up n−nup k−nup F n−k (1 − F ) k−nup n up ≤ k ≤ n<label>(3)</label></formula><p>where n up is the number of available peers and F is the average failure probability of n − n up unavailable peers in the replica group. We pick the m that maximizes P (x = m), e.g., P (X = m) = max{P (x = k) | k = 0, 1, 2, . . . , n}, and use m as the estimated number of remaining replicas. Since this estimate is the most likely one that occurs in reality, it provides the highest prediction rate (the probability that the estimate correctly matches the reality).</p><p>Periodically (e.g. every hour), the system uses the estimator to determine whether data recovery is necessary to reach the desired replica target. The system would create new replicas only when the estimated replica number m falls below the target c: If m &lt; c, then the replication layer generates c − m new replicas randomly among online peers in the system, otherwise, no action are necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PRELIMINARY MEASUREMENTS</head><p>A first full prototype of AmazingStore was released for public use on April 3rd, 2009. As of early October 2009, more than 7100 users had registered accounts. The daily peak of simultaneous online users is 780, and is increasing on a daily basis. By default, each user allocates 1GB storage and 2KB/s replication bandwidth (a resource unit defined in the system). In total, the number of files currently stored in the system is above 28600 and continuously rising (not including files such as popular movies that users share each other with file-sharing approach). The total amount of storage currently occupied in the system is roughly 705GB. While the AmazingStore is still growing, we present some initial measurements that show that the system is successfully meeting our expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Guaranteed System Availability</head><p>We first examine the data availability gained on the cloudlet server side. Overall, the system experiences a high data availability of 93.22% (in terms of the fraction of available time) in this layer. As expected, the correlated server failures result in unavailability. We illustrate the longest one (an 8-hour power failure from 9:00 am to 5:00 pm on May 25, 2009) in <ref type="figure">Figure 3</ref>. Clearly, we see that one cannot further improve availability by distributing data across more servers due to correlated failures. On the other hand, our analysis reveals that the average peer is only online 27% of each day. Given two nines availability requirement, the peer layer maintains c = 6 replicas (derived based on equation <ref type="formula" target="#formula_2">(2)</ref>) and provides an average availability of 83.76%. In addition, <ref type="figure">Figure 3</ref> shows data availability in the peer level exhibits a strong diurnal pattern. As we will show in Section 4.2, this actually improves the offloading efficiency and user experience in that periods of high data availability (9 a.m. to 12 p.m.) is also those of high demand for data access on a daily basis.</p><p>We finally see whether the overall data availability (also defined as system availability) meets the target two nines availability. By adding a P2P layer, our analysis shows that the system availability jumps from 93.22% to 99.13% (measured over the whole deployment period). <ref type="figure">Figure 3</ref> illustrates the availability compensation of the P2P layer. Notice that the system availability is only slightly better than the target, which implies the system incurs very little extra replication cost over long periods. In fact, with a target replica  To show how the system achieves a good cost-availability tradeoff on a long-term basis, we also take frequent snapshots during a 3 week period of our deployment <ref type="bibr">(May 11-30, 2009)</ref>, and look at whether the estimator is accurately predicting the number of alive replicas for each object. At each snapshot, we compute the ratio of data objects for which the estimator was accurate, underestimated, or overestimated the actual replica count. The result in <ref type="figure">Figure 4</ref> shows that our estimator is highly accurate. As a result, the system is very efficient in repairing failures, and only creates new replicas when replicas are truly lost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Low-cost Storage Service</head><p>AmazingStore offloads a majority of bandwidth cost by forwarding the requests to the cloudlet peers. Overall, 90.38% of user requests were handled by replicas stored on cloudlet peers, diverting 90.57% traffic load from centralized servers. For the remaining requests, no cloudlet peer replicas were available, and the clients read the data from the centralized servers. <ref type="figure" target="#fig_1">Figure 5</ref> shows the fractions of request served on both sides per day over a 2-week period <ref type="bibr">(June 19-22, 2009)</ref>, in which the requests were highly concentrated (on average 305 requests arrived per day).</p><p>A key reason for the high offloading efficiency of the P2P component is that requests and peer availability have a strong positive correlation. We measure the frequency of data access requests and peer online times on a daily basis, and plot both as CDFs averaged over different time-slices throughout each 24 hour period. As shown in <ref type="figure" target="#fig_2">Figure 6</ref>, there is a clear correlation between accesses and peer availability, with a correlation coefficient 0.78.</p><p>Recall that a centralized cloud still has to maintain multiple replicas (e.g., 3) for high data durability, even though availability cannot be effectively improved due to correlated failures. Thus, the addition of the P2P layer can further reduce the storage overhead of the centralized cloud. Over a half year of deployment, the observed data loss rate in the P2P component of AmazingStore is only 0.07% per month on average, corresponding to a MTTF (Mean Time To Failure) of about 1.05 × 10 6 hours, which falls within the range of MTTFs claimed by today's highest quality disks <ref type="bibr" target="#b10">[11]</ref>. Thus, by taking the P2P layer as a durable disk, the number of replicas maintained in the server layer can be reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">OPPORTUNITIES &amp; APPLICATIONS</head><p>In addition to the above fundamental advantages, the Cloudlets architecture hints to promising directions for further exploration, as well as novel applications deployable on a real platform like AmazingStore. We describe some here.</p><p>Network-and Bandwidth-aware Cloudlet Selection. Current replica replacement in AmazingStore is random. By utilizing knowledge of the network through network measurements or an information plane <ref type="bibr" target="#b9">[10]</ref>, we can optimize applications for different objectives, including resilience to network failures, improved performance through local Cloudlets, and the ability to handle flash crowds and network hotspots.</p><p>Socially-aware Cloudlet Management. Integrating social links into the Cloudlets provides another potential dimension for optimization. Prior work has proposed using social links to improve online backup <ref type="bibr" target="#b8">[9]</ref>. In our context, using social links can be used to localize data requests within certain communities and improve user privacy.</p><p>Lyra: a Distributed Online Social Network. There are a number of disadvantages to centralized social networks such as Facebook and MySpace, including loss of data ownership and lack of data privacy <ref type="bibr" target="#b7">[8]</ref>. On the other hand, it is very difficult for"pure" distributed social networks to guarantee data availability, which is critical for friends to browse and share content on social networks. As an alternative solution, we are building a distributed social network, Lyra, on the Cloudlet architecture. The Lyra system encrypts all private data at the cloud server, and makes private data only accessible after decryption. Key distribution bypasses the cloud server, ensuring that no third party has access to private data. Lyra also uses network coordinates to detect locality in data accesses, and biases Cloudlet selection to optimize performance for frequent readers. The Cloudlets infrastructure simplifies data consistency in Lyra: all writes are encrypted and sent to the master replica on the cloudlet server before propagating out to cloudlets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSIONS</head><p>Empirical evidence has shown us that centralized cloud infrastructure are vulnerable to massive failures. In this paper, we propose an alternative architecture for cloud-based infrastructures that retains the simplicity of management in cloud systems while gaining the failure resiliency available in P2P systems. We advocate that servers in the data center and peers at the network edge complement each other, resulting in highly available and durable systems with lower costs than traditional cloud-based designs. We describe a deployed prototype of AmazingStore, an online storage system adopting the Cloudlets hybrid architecture. While AmazingStore is not yet a mature system, initial results show that its cloudlet peers can improve system resilience by complementing the centralized storage servers. We believe that the properties demonstrated by Cloudlets open up new opportunities for improving storage services and enabling new distributed applications in the cloud infrastructures. The latest version of AmazingStore is available for download at our website at http://en.amazingstore.org/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The fraction of available objects over a failure period (May 25-29, 2009), with the details on both server and peer sides. The ticks on the x-axis correspond to midnight on the days that are labeled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The fraction of request load on both sides over a two-week period (June 19-22, 2009).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Correlation between peer availability and read requests for data, computed on a daily basis.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding availability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhagwan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IPTPS</title>
		<meeting>of IPTPS</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High availability, scalable storage, dynamic peer networks: Pick two</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HotOS</title>
		<meeting>of HotOS</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient replica maintenance for distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NSDI</title>
		<meeting>of NSDI</meeting>
		<imprint>
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Democratizing content publication with coral</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freedman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NSDI</title>
		<meeting>of NSDI</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Challenges, design and analysis of a large-scale p2p-vod system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGCOMM</title>
		<meeting>of SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Seattle fire knocks out service to bing travel, other sites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CNet News</title>
		<imprint>
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Google networking error caused outage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krazit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CNet News</title>
		<imprint>
			<date type="published" when="2009-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building a social networking future without big brother</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">POMI 2020 Workshop</title>
		<meeting><address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">F2f: reliable storage in open networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dabek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IPTPS</title>
		<meeting>of IPTPS<address><addrLine>Santa Barbara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">iplane: An information plane for distributed services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madhyastha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI</title>
		<meeting>of OSDI</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Disk failures in the real world: What does an mttf of 1000000 hours mean to you?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maymounkov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of FAST</title>
		<meeting>of FAST</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Minor outage at facebook monday. CNet News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Needleman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rackspace suffers outage. CNet News</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Needleman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The real cost of storage. Enterprise Storage Forum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Scaling out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<ptr target="http://www.facebook.com/notes.php?id=9445547199" />
		<imprint>
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fs2you: Peer-assisted semi-persistent online storage at a large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of INFOCOM</title>
		<meeting>of INFOCOM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic failure detection for efficient distributed storage maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SRDS</title>
		<meeting>of SRDS</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
