<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Thread and Memory Placement on NUMA Systems: Asymmetry Matters Thread and Memory Placement on NUMA Systems: Asymmetry Matters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 8-10. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Quéma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grenoble</forename><surname>Inp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Simon Fraser University</orgName>
								<address>
									<addrLine>Vivien Quéma</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Grenoble INP</orgName>
								<orgName type="institution" key="instit2">Alexandra Fedorova</orgName>
								<orgName type="institution" key="instit3">Simon Fraser University</orgName>
								<orgName type="institution" key="instit4">USENIX Association</orgName>
								<orgName type="institution" key="instit5">Simon Fraser University</orgName>
								<orgName type="institution" key="instit6">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Thread and Memory Placement on NUMA Systems: Asymmetry Matters Thread and Memory Placement on NUMA Systems: Asymmetry Matters</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2015 USENIX Annual Technical Conference (USENIC ATC &apos;15)</title>
						<meeting>the 2015 USENIX Annual Technical Conference (USENIC ATC &apos;15) <address><addrLine>Santa Clara, CA, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page">277</biblScope>
							<date type="published">July 8-10. 2015</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2015 USENIX Annual Technical Conference (USENIX ATC &apos;15) is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>It is well known that the placement of threads and memory plays a crucial role for performance on NUMA (Non-Uniform Memory-Access) systems. The conventional wisdom is to place threads close to their memory, to collocate on the same node threads that share data, and to segregate on different nodes threads that compete for memory bandwidth or cache resources. While many studies addressed thread and data placement, none of them considered a crucial property of modern NUMA systems that is likely to prevail in the future: asymmetric interconnect. When the nodes are connected by links of different bandwidth, we must consider not only whether the threads and data are placed on the same or different nodes, but how these nodes are connected. We study the effects of asymmetry on a widely available x86 system and find that performance can vary by more than 2× under the same distribution of thread and data across the nodes but different inter-node connectiv-ity. The key new insight is that the best-performing con-nectivity is the one with the greatest total bandwidth as opposed to the smallest number of hops. Based on our findings we designed and implemented a dynamic thread and memory placement algorithm in Linux that delivers similar or better performance than the best static placement and up to 218% better performance than when the placement is chosen randomly.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Typical modern CPU systems are structured as several CPU/memory nodes connected via an interconnect. These architectures are usually characterized by nonuniform memory access times (NUMA), meaning that the latency of data access depends on where (which CPU-cache or memory node) the data is located. For this reason, the placement of threads and memory plays a crucial role in performance. This property inspired many NUMA-aware algorithms for operating systems. Their insight is to place threads close to their memory <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b7">9]</ref>, spread the memory pages across the system to avoid the overload on memory controllers and interconnect links <ref type="bibr" target="#b10">[12]</ref>, to collocate data-sharing threads on the same node <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref> while avoiding memory controller contention <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b8">10]</ref>, and to segregate threads competing for cache and memory bandwidth on different nodes <ref type="bibr" target="#b34">[34]</ref>.</p><p>Further, modern operating systems aim to reduce the number of hops used for thread-to-thread and threadto-memory communication. When balancing the load across CPUs, Linux first uses CPUs on the same node, then those one hop apart and lastly two or more hops apart. These techniques assume that the interconnect between nodes is symmetric: given any pair of nodes connected via a direct link, the links have the same bandwidth and the same latency. On modern NUMA systems this is not the case. <ref type="figure" target="#fig_0">Figure 1</ref> depicts an AMD Bulldozer NUMA machine with eight nodes (each hosting eight cores). Interconnect links exhibit many disparities: (i) Links have different bandwidths: some are 16-bit wide, some are 8-bit wide; (ii) Some links can send data faster in one direction than in the other (i.e., one side sends data at 3/4 the speed of a 16-bit link, while the other side can only send data at the speed of an 8-bit link). We call these links 16/8-bit links; (iii) Links are shared differently. For instance the link between nodes 4 and 3 is only used by these two nodes, while the link between nodes 2 and 3 is shared by nodes 0, 1, 2, 3, 6 and 7; (iv) Some links are unidirectional. For instance node 7 sends requests directly to node 3, but node 3 routes its answers via node 2. This creates an asymmetry in read/write bandwidth: node 7 can write at 4GB/s to node 3, but can only read at 2GB/s.</p><p>The asymmetry of interconnect links has dramatic and at times surprising effects on performance. <ref type="figure" target="#fig_2">Figure 2</ref> shows the performance of 20 different applications on Machine A <ref type="figure" target="#fig_0">(Figure 1</ref>) <ref type="bibr" target="#b0">1</ref> . Each application runs with 24 threads and so it needs three nodes to run on. We vary which three nodes are assigned to the application and hence the connectivity between the nodes. The relative placement of threads and memory on those nodes is identical in all configurations. The only difference is how the chosen nodes are connected. The figure shows the performance on the best-performing and the worstperforming subset of nodes for that application compared to the average (obtained by measuring the performance on all 336 unique subsets of nodes and computing the mean). We make several observations. First, the performance on the best subset is up to 88% faster than the average, and the performance on the worst subset is up to 44% slower. Second, the maximum performance difference between the best and the worst subsets is 237% (for facerec). Finally, the mean difference between the best and worst subsets is 40% and the median 14%. In the following section we demonstrate that these performance differences are caused by the asymmetry of the interconnect between the nodes. This work makes the following contributions:</p><p>• We quantify and characterize the effects of asymmetric interconnect on a commercial x86 NUMA system. The key insight is that the best-performing connectivity is the one with the greatest total bandwidth as opposed to the smallest number of hops.</p><p>• We design, implement and evaluate a new algorithm that dynamically picks the best subset of nodes for applications requiring more than one node. This algorithm places the clusters of threads and their memory to ensure that the most intensive CPU-to-CPU or CPU-to-memory communication occurs between the best-connected nodes. Our evaluation shows that this algorithm performs as well as or better than the best set of nodes chosen statically.</p><p>• Our implementation revealed a limitation in hardware counters, which prevented us from having certain flexibilities in the algorithm. We discuss them and make suggestions for improvements.</p><p>The paper is structured as follows. Section 2 studies the impact of interconnect asymmetry and discusses challenges in catering to this phenomenon in an operating system. Section 3 discusses current architectural trends and shows that machines are becoming increasingly asymmetric. Section 4 presents our algorithm, and Section 5 reports on the evaluation. Section 6 discusses related work, and Section 7 provides a summary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Impact of Interconnect Asymmetry on Performance</head><p>To explain the reasons behind the performance reported in <ref type="figure" target="#fig_2">Figure 2</ref>, <ref type="figure">Figure 3</ref> shows the memory latency measured when the application runs on the best and worst node subsets relative to the latency averaged across all 336 possible subsets. We can see that memory accesses performed by facerec are approximately 600 cycles faster when running on the best subset of nodes relative to the average, and 1400 cycles faster relative to the worst. We can see that the latency differences are tightly correlated with the performance difference between configurations. The applications that are the most affected by the choice of nodes on which to run are also those with the highest difference in the memory latencies.</p><p>To further understand the cause of very high latencies on "bad" configurations we analyzed streamcluster -an application from the Parsec <ref type="bibr" target="#b24">[26]</ref> benchmark suite, which was among the most affected by the placement of its threads and memory. In the following experiment we run streamcluster with 16 threads on two nodes. <ref type="table" target="#tab_5">Table 1</ref> presents the salient metrics for each possible two-node subset. Depending on which two nodes we chose, we observe large (up to 133%) disparities in performance. The data in <ref type="table" target="#tab_5">Table 1</ref> leads to several crucial observations:</p><p>• As shown earlier, performance is correlated with the latency of memory accesses.</p><p>• Surprisingly, the latency of memory accesses is not correlated with the number of hops between the nodes: some two-hop configurations (shown in bold) are faster than one-hop configurations.</p><p>• The latency of memory accesses is actually correlated with the bandwidth between the nodes. Note that this makes sense: the difference between onehop vs. two-hop latency is only 80 cycles when the interconnect is nearly idle. So a higher number of hops alone cannot explain the latency differences of thousands of cycles.</p><p>Bandwidth between the nodes matters more than the distance between them.</p><p>So the problem of choosing a "good" subset of nodes is essentially the problem of the placement of threads and memory pages on a well-connected subset of nodes. When an application executes on only two nodes on a machine similar to the one used in the aforementioned experiments, the placement on the nodes connected with the widest (16-bit) link is always the best because it maximizes the bandwidth and minimizes the latency between the nodes. However, when an application needs more than two nodes to run, no configuration exists with 16-bit links between every pair of nodes, so we must decide       <ref type="table" target="#tab_5">Table 1</ref>: Performance of streamcluster executing with 16 threads on 2 nodes on machine A. The performance depends on the connectivity between the nodes on which streamcluster is executing and on the node on which the master thread is executing. Numbers in bold indicate 2-hops configurations that are as fast or faster than some 1-hop configurations.</p><p>which nodes to pick. When there is more than one application running, we need to decide how to allocate the nodes among multiple applications.</p><p>Nodes % perf. relative to best subset streamcluster SPECjbb 0, 1, 3, 4, and 7 -64% 0% (best) 2, 3, 4, 5, and 6 0% (best) -9.4% <ref type="table">Table 2</ref>: Performance of streamcluster and SPECjbb on two different set of nodes on machine A, relative to the best set of nodes for the respective application.</p><p>In this paper, we present a new thread and memory placement algorithm. Designing such an algorithm for asymmetrically connected NUMA systems is challenging for the following reasons:</p><p>Efficient online measurement of communication patterns is challenging: The algorithm must measure the volume of CPU-to-CPU and CPU-to-memory communication for different threads in order to determine the best placement when we cannot run the entire application on the best connected nodes. This measurement process must be very efficient, because it must be done continuously in order to adapt to phase changes.</p><p>Changing the placement of threads and memory may incur high overhead: Frequent migration of threads may be costly, because of the associated CPU overhead, but most importantly because cache affinity is not preserved. Moreover, when threads are migrated to "better" nodes, it might be necessary to migrate their memory in order to avoid the overhead of remote accesses and overloaded memory controllers. Migrating large amounts of memory can be extremely costly. Thus, thread migration must be done in a way that minimizes memory migration.</p><p>Accomodating multiple applications simultaneously is challenging: Applications have different communication patterns and are thus differently impacted by the connectivity between the nodes they run on. As an illustration, <ref type="table">Table 2</ref> presents the performance of streamcluster and SPECjbb executing on two different sets of five nodes (the best set of nodes for the two applications, respectively). The two applications behave differently on these two sets of nodes: streamcluster is 64% slower on the best set of nodes for SPECjbb than on its own best set. The algorithm must, therefore, determine the best set of nodes for every application. Furthermore, it cannot always place each application on its best set of nodes, because applications may have conflicting preferences.</p><p>Selecting the best placement is combinatorially difficult: The number of possible application placements on an eight-node machine is very large (e.g., 5040 possible configurations for four applications executing on two nodes). So, (i) it is not possible to try all configurations online by migrating threads and then choosing the best configurations, and (ii) doing even the simplest computation involving "all possible placements" can still add a significant overhead to a placement algorithm.</p><p>Before describing how we addressed these challenges, we briefly discuss architectural trends and the increasing impact of interconnect asymmetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architectural trends</head><p>Asymmetric interconnect is not a new phenomenon. Nevertheless, we show in this section that its effects on performance are increasing as machines are built with more nodes and cores. For that purpose, we measured the performance of streamcluster on four different asymmetric machines: two recent machines with 64 and 48 cores respectively, and 8 nodes (Machines A and B, <ref type="figure" target="#fig_0">Fig- ure 1)</ref>, and two older machines with 24 and 16 cores respectively, and 4 nodes (Machines C and D, not depicted). Machines A and B have highly asymmetric interconnect; <ref type="table" target="#tab_5">Table 1</ref> lists all possible interconnect configurations between 2 nodes. Machines C and D have a less pronounced asymmetry. Machine C has full connectivity, but two of the links are slower than the rest. Machine D has links with equal bandwidth, but two nodes do not have a link between them. <ref type="table" target="#tab_3">Table 3</ref> shows the performance of streamcluster with 16 threads on the best-performing and the worstperforming set of nodes on each machine. The performance difference between the best and worst configurations increases with the number of cores in the machine: from 3% for the 16-core machine to 133% for the 64-core machine. We explain this as follows: (i) On the 16-core Machine D, the only difference between configurations is the longer wire delay between the nodes that are not connected via a direct link. This delay is not significant compared to the extra latency induced by bandwidth contention on the interconnect. (ii) The CPUs on 24-core Machine C have a low frequency compared to the other machines. As a result, the impact of longer memory latency is not as pronounced. More importantly, the network on this machine is still a fully connected mesh, so there is less asymmetry than on Machines A and B. (iii) The 48-and 64-core Machines B and A offer a wider range of bandwidth configurations, which increases the difference between the best and the worst placements. The 64-core machine is more affected than the 48-core machine because it has more cores per node, which increases the effects of bandwidth contention. If this trend holds across different machines and architectures, then it is clear that the effects of asymmetry can no longer be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine</head><p>Best  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Solution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We designed AsymSched, a thread and memory placement algorithm that takes into account the bandwidth asymmetry of asymmetric NUMA systems. AsymSched's goal is to maximize the bandwidth for CPUto-CPU communication, which occurs between threads that exchange data, and CPU-to-memory communication, which occurs between a CPU and a memory node upon a cache miss. To that end, AsymSched places threads that perform extensive communication on relatively well-connected nodes and places the frequently accessed memory pages such that the data requests are either local or travel across high-bandwidth paths.</p><p>AsymSched is implemented as a user level process and interacts with the kernel and the hardware using system calls and /proc file system, but could also be easily integrated with the kernel scheduler if needed.</p><p>AsymSched continuously monitors hardware counters to detect opportunities for better thread placements. The thread placement decision occurs every second, and AsymSched only migrates threads when the benefits of migration is expected to exceed its overhead. The placement of memory pages follows the placement of threads.</p><p>AsymSched relies on three main techniques to manage threads and memory: (i) Thread migration: changing the node where a thread is running. (ii) Full memory migration: migrating all pages of an application from one node to another. Full memory migration is performed using a new system call that we present in Section 4.3. (iii) Dynamic memory migration: migrating only the pages that an application actively accesses. Dynamic memory migration uses Instruction-Based Sampling (IBS), a profiling mechanism available in AMD processors 2 , to sample memory accesses and to identify the most frequently accessed pages. Then, the pages that are not shared are migrated to the node that accesses them. Shared pages are spread across multiple nodes. We use the same algorithm and techniques described in <ref type="bibr" target="#b10">[12]</ref>; we therefore omit further details on dynamic memory migration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm</head><p>AsymSched relies on 3 components. The measurement component continuously computes salient metrics. The decision component uses these metrics to periodically compute the best thread placements. The migration component migrates threads and memory. <ref type="table" target="#tab_4">Table 4</ref> presents the definitions relevant to AsymSched. Algorithm 1 summarizes the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per cluster (C) statistics C rbw</head><p>Remote bandwidth: the number of memory accesses performed by threads in the cluster to another node, i.e., remote accesses. C weight "Weight" of the cluster. Clusters with the highest weights are scheduled on the nodes with the highest interconnect bandwidth. By default C weight = log(C rbw ). C bw (P)</p><p>Maximum bandwidth of C threads on placement P. Per placement (P) statistics P <ref type="bibr">wbw</ref> Weighted total bandwidth of P. Is equal to the sum of the C bw (P) * C weight for every placed cluster C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P mm</head><p>Amount of memory that has to be migrated to use this placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per application (A) statistics A tm</head><p>Time already spent migrating memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A tt</head><p>Dynamic running time of the application. A mm [node] Resident set size of the application, per node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A oldA</head><p>Percentage of memory accesses performed on nodes on which the application was scheduled but is no longer scheduled on. Measurement. AsymSched continuously gathers the metrics characterizing the volume of CPU-to-CPU and CPU-to-memory communication. On our experimental system there is a single counter that captures both: it measures the number of data accesses performed by a CPU to a given node and includes both the accesses to cached data (CPU-to-CPU communication) and to the data located in RAM (CPU-to-memory communication). Ideally, we would like to measure the communication volume from every CPU to every other CPU, however the counters available on AMD systems do not offer this opportunity. One alternative is to use AMD's Instruction Based Sampling (IBS) <ref type="bibr" target="#b1">3</ref> . Unfortunately, to accurately track CPU-to-CPU communication, IBS requires a high sampling rate, and that introduces too much overhead. Lightweight Profiling (LWP), a new profiling facility of AMD processors, has a smaller overhead, but is only partially implemented in current processors. Despite these limitations, we believe that it is only a matter of time until they are addressed in the mainstream hardware, so for the time being we use the following work-around.</p><p>The algorithm described below relies on detecting which threads share data. Since we can only practically measure the communication between a CPU and a remote node, but not CPU-to-CPU communication (either across or within nodes), we make the following simplifying assumptions: (a) a thread may share data with any other thread running on the same node, (b) if there is a high volume of communication between a CPU and a node, a thread running on that CPU may share data with any thread of the same application on that node. To reduce the occurrence of situations where we assume data sharing while in reality there is none, we initially collocate threads from the same application on the same node, to the extent possible. Data sharing is far more common between threads from the same application than between threads from different applications.</p><p>The downside of this simplifying assumption is that we may unnecessarily keep a group of threads collocated on the same node even if they do not share data. But there is also an important benefit: characterizing the communication in terms of CPU-to-node keeps the number of sharing relationships to consider small and reduces the complexity of the algorithm.</p><p>Decision. The following description relies on definitions in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>Step 1: AsymSched groups threads of the same application that share data in virtual clusters. A cluster is simply a list of threads that share data. It then assigns a weight C weight to each cluster; clusters with the highest weights will be scheduled on the nodes with the best connectivity. By default clusters are weighed by the logarithm of the number of remote memory accesses performed by their threads (C weight = log(C rbw )). The logarithm deemphasizes small differences in C rbw between the clusters, while preserving large differences. This makes it much easier for the algorithm to pick out the clusters with a relatively high C rbw and place them on well-connected nodes.</p><p>Step 2: AsymSched computes possible placements for all the clusters. A placement is an array mapping clusters to nodes. It works at the node granularity, so the number of possible placements is equal to the number of node permutations (i.e., migrating all threads of node X to node Y and vice versa). As this number can be very large, it is important that AsymSched not test all possible placements. Section 4.3 details how AsymSched avoids testing all possible placements. For each placement P, AsymSched computes the maximum bandwidth C bw (P) that each cluster C would receive if it were put in this placement. Each placement is assigned a performance metric, P wbw , the weighted bandwidth of P, defined as P wbw = ∑ C∈clusters C bw (P) * C weight . The higher P wbw , the higher the bandwidth available to clusters that perform a lot of remote communications. The definition of C weight implies that our algorithm aims to optimize the overall communication bandwidth across all applications. The algorithm can be easily changed to optimize a different metric, e.g., one that takes into account application priorities, by changing the definition of C weight .</p><p>Step 3: AsymSched filters placements to keep only those that have a weighted bandwidth value at least equal to 90% of the maximum weighted bandwidth. Among these remaining placements AsymSched chooses those that will minimize the number of page migrations.</p><p>Step 4: For each application, AsymSched estimates the overhead of memory migration assuming the cost of 0.3s per GB, which was derived on our system using simple experiments. If the overhead is deemed too high, the new placement will not be applied. Another goal here is to avoid migrating the applications back and forth because of recurring changes in communication patterns and accumulating a high overhead. To that end, AsymSched keeps track of the total time already spent doing memory migration for the application: A tm . If that time plus the estimated cost of additional migration ( ∑ n∈migrated nodes A mm [n] * 0.3) exceeds 5% of the running time of the application (A tt ), then AsymSched does not apply the new thread placement. We chose 5% as a reasonable maximum overhead value. In practice, the highest overhead we observed was around 3%. Migration.</p><p>Step 1: AsymSched migrates threads using system calls that are available in the Linux kernel.</p><p>Step 2: AsymSched relies on dynamic migration to migrate the subset of pages that the application uses. If, after two seconds, the application still performs more than 90% of its memory accesses on the nodes where it was previously running (A oldA &gt; 90%), then AsymSched concludes that dynamic migration was not able to migrate the working set of the application and performs a full memory migration.</p><p>The Measurement, Decision and Migration phases described above are performed continuosly to account for phase changes in applications and other dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Optimizations and tricks</head><p>We integrated several optimizations within AsymSched to ensure that it runs accurately and with low overhead.</p><p>Fast memory migration. When AsymSched performs full memory migration, all the pages located on one node are migrated to another node. The applications we tested have large working sets (up to 15GB per node), and migrating pages is costly. We measured that migrating 10GB of data using the standard migrate pages system call takes 51 seconds on average, making migration of large applications impractical.</p><p>Therefore, we designed a new system call for memory migration. This system call performs memory migration without locks in most cases, and exploits the parallelism end if <ref type="bibr" target="#b25">27</ref>: end for available on multicore machines. Using our system call, migrating memory between two nodes is on average 17× faster than using the default Linux system call and is only limited by the bandwidth available on interconnect links. Unlike the Linux system call, our system call can migrate memory from multiple nodes simultaneously. So if we are migrating the memory simultaneously between two pairs of nodes that do not use the same interconnect path, our system call will run about 34 times faster.</p><p>Fast migration works as follows. (i) First, we "freeze" the application by sending SIGSTOP to all its threads. Freezing the application is done to ensure that the application does not allocate or free pages during migration. This allows removing many locks taken by the Linux memory migration mechanism, and since up to 80% of migration time can be wasted waiting on locks, the resulting performance improvements are significant. (ii) Second, we parse the memory map of the application and store all pages in an array. We then launch worker threads on the node(s) on which the application is scheduled. Worker threads process pages stored in the array in chunks of 30 thousand. The old page is unmapped, data is copied to a new page, the new page is remapped, and the old page is freed. Shared pages or pages that are currently swapped are ignored.</p><p>Avoiding evaluation of all possible placements. The number of all possible thread placements on a machine can be very large. We use two techniques to avoid computing all thread placements: (i) A lot of thread placement configurations are "obviously" bad. For instance, when a communication-intensive application uses two nodes, we only consider configurations with nodes connected with a 16-bit link. (ii) Several configurations are equivalent (e.g., the bandwidth between nodes 0 and 1 and between nodes 2 and 3 is the same). To avoid estimating the bandwidth of all placements, we create a hash for each placement. The hash is computed so that equivalent configurations have the same hash. Using simple dynamic programming techniques, we only perform computations on non-equivalent configurations.</p><p>These two techniques allow skipping between 67% and 99% of computations in all tested configurations with clusters of 2, 3 or 5 nodes (e.g., with 4 clusters of 2 nodes, we only evaluate 20 configurations out of 5040).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Our goal is to evaluate the impact of asymmetry-aware thread placement in isolation from other effects, such as those stemming purely from collocating threads that share data on the same node. Performance benefits of sharing-aware thread clustering are well known <ref type="bibr" target="#b30">[30]</ref>. AsymSched clusters threads that share data as described in the Section 4; the Linux thread scheduler, however, does not. We experimentally observed that Linux performed worse than clustered configurations. E.g., when graph500 and specjbb are scheduled simultaneously, both run 23% slower on Linux than on an average clustered placement. Since comparing Linux to AsymSched would not be meaningful because of that, we instead compare AsymSched 4 to the best and the worst static placements of data-sharing thread clusters. We also compare the average performance achieved under all static placements that are unique in terms of connectivity. We obtain all unique static placements with respect to connectivity by examining the topology of the machine. There are 336 placements for single-application scenarios and 560 placements for multi-application scenarios.</p><p>Further, we want to isolate the effects of thread placement with AsymSched from the effects of dynamic mem-ory migration. To that end, we compare AsymSched to the subset of our algorithm that performs the dynamic placement of memory only, turning off the parts performing thread placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental platform</head><p>We evaluate AsymSched on machine A. It is equipped with four AMD Opteron 6272 processors, each with two NUMA nodes and 8 cores per node (64 cores in total). The machine has 256GB of RAM and uses HyperTransport 3.0. It runs Linux 3.9.</p><p>We used several benchmark suites: the NAS Parallel Benchmarks suite <ref type="bibr" target="#b4">[6]</ref> which is composed of numeric kernels, MapReduce benchmarks from Metis <ref type="bibr" target="#b23">[25]</ref>, parallel applications from Parsec <ref type="bibr" target="#b24">[26]</ref>, Graph500 <ref type="bibr" target="#b0">[1]</ref>, a graph processing application with a problem size of 21, FaceRec from the ALPBench benchmark suite <ref type="bibr" target="#b9">[11]</ref>, and SPECjbb [2] running on OpenJDK7. From the NAS and Parsec benchmark suites we picked the benchmarks that run for at least 15 seconds, and that can be executed with arbitrary numbers of threads. The memory usage of the benchmarks ranges from 518MB for EP from the NAS suite to 34,291MB for IS from NAS. Except for SPECjbb, we use the execution time of applications as performance indicator. SPECjbb runs during a fixed amount of time; we use the throughput (measured in SPECjbb bops) as performance indicator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Single application workloads</head><p>The results are presented in <ref type="figure" target="#fig_4">Figure 4</ref>. AsymSched always performs close to the best static thread placement. In a few cases where it does not, the difference is not statistically significant. For applications that produce the highest degree of contention on the interconnect links (streamcluster, pca, and facerec), AsymSched achieves much better performance than the best thread placement, because the dynamic memory migration component balances memory accesses across nodes, thus reducing contention on interconnect links and memory controllers.</p><p>We also observe that dynamic memory migration without the migration of threads is not sufficient to achieve the best performance. More precisely, dynamic memory migration alone often achieves performance close to average. Moreover, it produces a high standard deviation for many benchmarks: the minimum and maximum performance often being the same as that of the best and worst static thread placement. For instance, on SPECjbb, the difference between the minimum and maximum performance with dynamic memory migration alone is 91%.</p><p>In contrast, AsymSched produces a very low standard deviation for most benchmarks. Two exceptions are is.D   and SPECjbb. This is because in both cases, AsymSched migrates a large amount of memory. Both applications become memory intensive after an initialization phase, and AsymSched starts migrating memory only after the entire working set has been allocated. For instance, in the case of is.D, AsymSched migrates between 0GB and 20GB, depending on the initial placement of threads. <ref type="figure" target="#fig_5">Figure 5</ref> shows the latency of memory accesses compared to the average. For most applications, the dynamics of latency closely matches that of the performance. A few exceptions are is.D, lu.B and kmeans. For is.D, the latency is drastically improved by AsymSched but the impact on performance is not visible because of the time lost performing memory migrations. Lu.B is extremely memory intensive during its first seconds of execution, but performs very few memory accesses thereafter; AsymSched improves this initial phase but has no impact on the rest of the running time. Kmeans is very bursty; placing its threads has a huge impact on the latency of memory accesses performed during bursts of memory accesses but not on the rest of the execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multi application workloads</head><p>We evaluate several multi-application workloads using the applications studied in section 5.2. We chose four applications that benefit to various degrees from AsymSched: streamcluster (benefits to a high degree), SPECjbb (benefits to a moderate degree), graph500 (benefits to a small degree), and matrixmultiply (does not benefit). Some of these applications have different phases during their execution; for instance, streamcluster processes its input set in five distinct rounds, and SPECjbb spends significant amount of time initializing data before emulating a three-tier client/server system. <ref type="figure">Figure 6</ref> presents the performance on multi application workloads. We chose two different clustering configurations: (i) Three applications executing on three, three and two nodes, respectively; (ii) Two applications executing on five and three nodes respectively.</p><p>In all workloads, AsymSched achieves performance that is close or better than the best static thread placement on Linux. Furthermore, it produces a very low standard deviation. In constrast, dynamic memory migration alone exhibits high standard deviation and, like with single application workloads, is unable to improve performance for Graph500 and SPECjbb.</p><p>AsymSched significantly improves the latency of applications that benefit from thread and memory migration <ref type="figure">(Figure 7</ref>), in particular for streamcluster. This is because AsymSched chooses configurations in which the links used by streamcluster are not shared with any other application.  <ref type="table">Table 5</ref>: Average amount of migrated memory for various applications running on 3 nodes and required time to perform the migration using the standard Linux system call and using fast memory migration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Overhead</head><p>The main overhead of AsymSched is due to memory migration. This explains why we implemented a custom system call (see Section 4.3). <ref type="table">Table 5</ref> compares the migration time when running the standard Linux system call and when running our custom system call. For instance, for is.D, migration takes 101 seconds using the Linux system call (50% overhead), but only 3 seconds using our custom system call (1.5% overhead). To keep the overhead low, AsymSched performs migrations only if the predicted overhead is below 5%. In practice, the maximum migration overhead we observed was 3%.</p><p>The cost of collecting metrics and computing cluster placement is below 0.5% on all studied applications. Moreover, AsymSched requires less than 2MB of RAM.</p><p>The overhead of thread migration is negligible and we did not observe any noticeable effect of thread migrations on cache misses. Finally, when dynamic memory placement is used, IBS sampling incurs a light overhead (within 2% in our experiments) and statistics on memory accesses are stored in about 20MB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Discussion -Applicability on future NUMA machines</head><p>We believe that the findings and the solution presented in this paper are likely to be applicable on future NUMA systems. First, we believe that the clustering and placement techniques used in AsymSched can scale on machines with a much larger number of nodes. With very simple heuristics we were able to avoid computing up to 99% of the possible thread placements. Such optimizations will still likely be possible on future machines, as machines are usually made of multiple identical cores/sockets (e.g., our 64-core machine has 4 identical sockets). On machines that offer a wider diversity of thread placements, a possibility is to use statistical approaches, such as that of Radojkovic et al. <ref type="bibr" target="#b25">[27]</ref> to find good thread placements with a bounded overhead. Furthermore, AsymSched can easily be adapted to different optimization goals. On current NUMA machines, maximizing the bandwidth between threads was the key to achieving good performance, but our solution could be easily adapted to take other metrics into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>NUMA optimizations and contention management: Optimizing thread and memory placement on NUMA systems has been extensively studied <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b8">10]</ref>. However, as shown in <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b21">23]</ref>, contention on interconnect links and memory controllers remains a major source of inefficiencies on modern NUMA machines. Our work complements these previous studies by minimizing contention on interconnect links on asymmetrically connected NUMA systems. We adopt a dynamic memory management algorithm presented in <ref type="bibr" target="#b10">[12]</ref> for memory placement, but the key contribution of our work is the algorithm that efficiently computes the placement of threads on nodes to maximize the bandwidth between communicating threads.</p><p>Several extensions to Linux improve data-access locality on NUMA systems, but do not improve the bandwidth for communicating threads and do not address interconnect asymmetry. For example, Sched/NUMA <ref type="bibr" target="#b32">[32]</ref> adds the notion of a "home node": when scheduling threads, Linux will try to collocate threads and data on the "home node" of the corresponding process. While this may improve communication bandwidth for applications with the number of threads not exceeding the number of cores in a node, it does not address applications spanning several nodes. Another extension, called AutoNUMA <ref type="bibr" target="#b2">[4]</ref>, implements locality optimizations by migrating pages on the nodes from which they are accessed. AsymSched uses a similar dynamic algorithm to migrate memory, but unlike AutoNUMA it also places threads so as to optimize communication bandwidth.</p><p>Several studies addressed contention for the memory hierarchy of UMA systems <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b34">34</ref>] by segregating competing threads on different nodes. None of these systems, however, addressed contention on the interconnect.</p><p>Scheduling on asymmetric architectures: Several thread schedulers catered to the asymmetry of CPUs <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b15">17]</ref>. They optimize thread placement on processors with asymmetric characteristics (e.g., different frequencies, or different hardware features). The techniques used to address processor asymmetry are fundamentally different than those needed to address interconnect asymmetry, so there is no overlap with our study.</p><p>Thread clustering: Pusukuri et al. <ref type="bibr" target="#b16">[18]</ref> cluster threads based on lock contention and memory access latencies. <ref type="bibr">Kamali [14]</ref> and Tam <ref type="bibr" target="#b30">[30]</ref> proposed algorithms that cluster threads that share data on the same shared cache. AsymSched uses a similar high-level idea: it samples hardware counters to detect communicating threads and place them onto a well-connected nodes. However, the problem addressed in AsymSched (asymmetric interconnect) and the specific algorithm proposed is quite different from those in the aforementioned studies.</p><p>Radojkovic et al.</p><p>[28] present a scheduler that takes into account resource sharing inside a processor. They model the benefits and drawbacks of data and instruction cache sharing between threads, and they schedule threads on the the set of cores that will maximize performance. Their solution explores all possible thread placements. Their follow-up work <ref type="bibr" target="#b25">[27]</ref> refines the solution to use a statistical approach to find the optimal placement. Our solution could benefit from a similar technique on machines with a much larger number of dissimilar nodes.</p><p>Network optimizations: Network traffic optimization is a well studied problem. Machines are often interconnected with asymmetric Ethernet links; optimizing the bandwidth on asymmetric NUMA systems shares a lot of similarities with optimizations problems found in these systems. For instance, Volley <ref type="bibr" target="#b1">[3]</ref> proposed an algorithm to place data used by Cloud services. As AsymSched, this algorithm takes into account the available bandwidth between nodes (that are geographically distributed computers in their case) in order to optimize performance.</p><p>Hermenier et al.</p><p>[13] present a consolidation manager for distributed systems. The goal of their system is to minimize energy consumption in a cluster. To this end, they place virtual machines on the smallest possible number of physical machines while meeting certain performance constraints. They model the problem as the multiple knapsack problem and use a constraintsatisfaction solver to find good placements. AsymSched could use a similar technique, but we found that on existing systems a much simpler solution was sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We showed that the asymmetry of the interconnect in modern NUMA systems drastically impacts performance. We found that the performance is more affected by the bandwidth between nodes than by the distance between them. We developed AsymSched, a new thread and memory placement algorithm that maximizes the bandwidth for communicating threads.</p><p>As the number of nodes in NUMA systems increases, the interconnect is less likely to remain symmetric. AsymSched design principles will, therefore, be of growing importance in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Modern NUMA systems, with eight nodes. The width of links varies, some paths are unidirectional (e.g., between 7 and 3) and links may be shared by multiple nodes. Machine A has 64 cores (8 cores per node -not represented in the picture) and machine B has 48 cores (6 cores per node). Not shown in the picture: the links between nodes 4 and 1 and between nodes 2 and 7 are bidirectional on machine B. This changes the routing of requests from node 7 to 2 and node 1 to 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance difference between the best, and worst thread placement with respect to the average thread placement on Machine A. Applications run with 24 threads on three nodes. Graph500, specjbb, streamcluster, pca and facerec are highly affected by the choice of nodes and are shown separately with a different y-axis range.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>-</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance difference between the best and worst static thread placement, dynamic memory placement, AsymSched and the average thread placement on machine A. Applications run with 24 threads on 3 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Memory latency under the best and worst static thread placement, dynamic memory placement, AsymSched and the average thread placement on machine A. Applications run with 24 threads on 3 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Performance of streamcluster executing on 2 
nodes on machine A, B, C, and D. The performance of 
streamcluster depends on the placement of its threads. 
The impact of thread placement is more important on re-
cent machines (A and B) than on older ones (C and D). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Definitions relevant to AsymSched. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Algorithm 1 AsymSched algorithm 1: if Threads of nodes N 1 and N 2 access a common memory controller and threads of N 1 and N 2 have the same pid then 2: Put all threads running on N 1 and N 2 in a cluster C and Increase C rbw 3: end if 4: Compute relevant cluster placements 5: Max wbw = 0 6: for all P ∈ computed placements do 7: P wbw = ∑ C∈clusters C bw (P) * C weight 8: Max wbw = max(Max wbw , P wbw ) 9: end for 10: for all P ∈ computed placements do 11: Skip if P wbw &lt;90%*Max wbw 12: Compute P mm 13: end for 14: Choose the placement with the lowest P mm 15: for all A ∈ migrated applications do 16: if A tm + ∑ n∈migrated nodes A mm [n] * 0.3 &gt; 0.05*A tt then 17:all A ∈ migrated applications do 24: if A oldA &gt; 90% then 25:</head><label>1</label><figDesc></figDesc><table>Do not change thread placement 

18: 

end if 
19: end for 
20: Migrate threads 
21: Use dynamic memory migration 
22: After 2 seconds: 
23: for Fully migrate memory of A 

26: 

</table></figure>

			<note place="foot" n="1"> Additional details about the applications and the machine are provided in Section 5.</note>

			<note place="foot" n="2"> Intel processors have a similar mechanism called Precise EventBased Sampling (PEBS).</note>

			<note place="foot" n="3"> PEBS, Precise Event-Based Sampling, is a similar feature on Intel systems.</note>

			<note place="foot" n="4"> When running AsymSched, thread clusters are initially placed on a randomly chosen set of nodes.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.graph500.org/referencecode" />
		<title level="m">Graph500 reference implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Volley: Automated data placement for geo-distributed cloud services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharad</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Dunagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navendu</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Saroiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Wolman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harbinder</forename><surname>Bhogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;10</title>
		<meeting>the 7th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;10<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">AutonNUMA: the other approach to NUMA scheduling. LWN.net</title>
		<ptr target="http://lwn.net/Articles/488709/" />
		<imprint>
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Handling the problems and opportunities posed by multiple on-chip memory controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sudan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The nas parallel benchmarks summary and preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Browning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fatoohi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Frederickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lasinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkatakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Weeratunga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing, 1991. Supercomputing &apos;91. Proceedings of the 1991 ACM/IEEE Conference on</title>
		<imprint>
			<date type="published" when="1991-11" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Case for NUMA-aware Contention Management on Multicore Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhuravlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple but Effective Techniques for NUMA Memory Management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the Importance of Parallel Application Placement in NUMA Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Brecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX SEDMS</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data distribution, migration and replication on a ccnuma architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth European Workshop on OpenMP</title>
		<meeting>the Fourth European Workshop on OpenMP</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<ptr target="http://www.cs.colostate.edu/evalfacerec/index10.php" />
		<title level="m">CSU Face Identification Evaluation System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Traffic management: A holistic approach to memory placement on numa systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Quéma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth international conference on Architectural support for programming languages and operating systems</title>
		<meeting>the eighteenth international conference on Architectural support for programming languages and operating systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="381" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Entropy: A consolidation manager for clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Hermenier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Lorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Marc</forename><surname>Menaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Lawall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE &apos;09</title>
		<meeting>the 2009 ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments, VEE &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sharing aware scheduling on multicore systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Kamali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSc Thesis</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Simon Fraser Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aash: An asymmetry-aware scheduler for hypervisors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Kazempour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments</title>
		<meeting>the 6th ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Using OS Observations to Improve Performance in Multicore Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Knauerhase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Hohlt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bias scheduling in heterogeneous multi-core architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Computer Systems, EuroSys &apos;10</title>
		<meeting>the 5th European Conference on Computer Systems, EuroSys &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adapt: A framework for coscheduling multithreaded programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Kumar Pusukuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laxmi</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
		<idno>45:1-45:24</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MemProf: A Memory Profiler for NUMA Multicore Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Quéma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluation of NUMA Memory Management Through Modeling and Measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Larowe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carla</forename><surname>Schlatter Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Holliday</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="686" to="701" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient operating system scheduling for performance-asymmetric multi-core architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Baumberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koufaty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing, 2007. SC &apos;07. Proceedings of the 2007 ACM/IEEE Conference on</title>
		<imprint>
			<date type="published" when="2007-11" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memory management in numa multicore systems: Trapped between cache contention and interconnect overhead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Majo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Memory System Performance in a NUMA Multicore Multiprocessor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan</forename><surname>Majo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SYSTOR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Resource-Conscious Scheduling for Energy Efficiency on Multicore Processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Merkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Stoess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Bellosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metis Mapreduce Library</surname></persName>
		</author>
		<ptr target="http://pdos.csail.mit.edu/metis/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parsec Benchmark Suite</surname></persName>
		</author>
		<ptr target="http://parsec.cs.princeton.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Radojkovi´cradojkovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimiř</forename><surname>Cakarevi´ccakarevi´c</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimal task assignment in multithreaded processors: A statistical approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Moretó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Verdú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Pajuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="248" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Radojkovi´cradojkovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimiř</forename><surname>Cakarevi´ccakarevi´c</surname></persName>
		</author>
		<imprint>
			<pubPlace>Javier</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Thread to strand binding of parallel network applications in massive multi-threaded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Verdú</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><forename type="middle">J</forename><surname>Pajuelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cazorla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;10</title>
		<meeting>the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPoPP &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="191" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A comprehensive scheduler for asymmetric multicore systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Saez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Blagodurov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Computer Systems, EuroSys &apos;10</title>
		<meeting>the 5th European Conference on Computer Systems, EuroSys &apos;10<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="139" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thread Clustering: Sharing-Aware Scheduling on SMP-CMP-SMT Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimizing google&apos;s warehouse scale computers: The numa experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingjia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hagmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA2013)</title>
		<imprint>
			<date type="published" when="2013-02" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
	<note>IEEE 19th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Toward better NUMA scheduling. Linux Weekly News</title>
		<ptr target="http://lwn.net/Articles/486858/" />
		<imprint>
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Operating system support for improving data locality on CC-NUMA compute servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Verghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Devine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Addressing Contention on Multicore Processors via Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zhuravlev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
