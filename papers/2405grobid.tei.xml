<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST &apos;17). Crystal: Software-Defined Storage for Multi-Tenant Object Stores Crystal: Software-Defined Storage for Multi-tenant Object Stores</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 27-March 2, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Gracia-Tinedo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Sampé</surname></persName>
							<email>josep.sampe@urv.cat</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Zamora</surname></persName>
							<email>edgar.zamora@urv.cat</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sánchez-Artigas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>García-López</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Universitat</forename><surname>Rovira</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virgili</forename><forename type="middle">;</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosef</forename><surname>Moatti</surname></persName>
							<email>moatti@il.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Rom</surname></persName>
							<email>eranr@il.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Gracia-Tinedo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Sampé</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Zamora</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sánchez-Artigas</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>García-López</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosef</forename><surname>Moatti</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Rom</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">IBM Research-Haifa</orgName>
								<orgName type="institution" key="instit2">Universitat Rovira i Virgili (Tarragona</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country>USA, Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research (Haifa</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST &apos;17). Crystal: Software-Defined Storage for Multi-Tenant Object Stores Crystal: Software-Defined Storage for Multi-tenant Object Stores</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February 27-March 2, 2017</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 15th USENIX Conference on File and Storage Technologies is sponsored by USENIX. https://www.usenix.org/conference/fast17/technical-sessions/presentation/gracia-tinedo</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Object stores are becoming pervasive due to their scalability and simplicity. Their broad adoption, however , contrasts with their rigidity for handling heterogeneous workloads and applications with evolving requirements , which prevents the adaptation of the system to such varied needs. In this work, we present Crystal, the first Software-Defined Storage (SDS) architecture whose core objective is to efficiently support multi-tenancy in object stores. Crystal adds a filtering abstraction at the data plane and exposes it to the control plane to enable high-level policies at the tenant, container and object granularities. Crystal translates these policies into a set of distributed controllers that can orchestrate filters at the data plane based on real-time workload information. We demonstrate Crystal through two use cases on top of OpenStack Swift: One that proves its storage automation capabilities, and another that differentiates IO bandwidth in a multi-tenant scenario. We show that Crystal is an ex-tensible platform to deploy new SDS services for object stores with small overhead.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object stores are becoming an increasingly pervasive storage building block due to their scalability, availability and usage simplicity via HTTP RESTful APIs <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">8]</ref>. These desirable features have spurred the adoption of object stores by many heterogeneous applications and systems, ranging from Personal Clouds <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b21">25]</ref>, Big Data companies such as DataBricks <ref type="bibr" target="#b1">[2]</ref> and Mirantis <ref type="bibr" target="#b3">[6]</ref> and analytics frameworks <ref type="bibr" target="#b18">[22,</ref><ref type="bibr" target="#b13">17]</ref>, and Web applications <ref type="bibr" target="#b15">[19]</ref>, to name a few.</p><p>Despite their growing popularity, object stores are not well prepared for heterogeneity. Typically, a deployment of an object store in the cloud uses a monolithic configuration setup, even when the same object store acts as a substrate for different types of applications with timevarying requirements <ref type="bibr" target="#b13">[17,</ref><ref type="bibr" target="#b12">16]</ref>. This results in all applications experiencing the same service level, though the workloads from different applications can vary dramatically. For example, while a social network application such as Facebook would have to store a large number of small-medium sized photos (KB-to MB-sized objects), a Big Data analytics framework would probably generate read and write requests for large files. It is clear that using a static configuration inhibits optimization of the system to such varying needs.</p><p>But not only this; beyond the particular needs of a type of workload, the requirements of applications can also vary greatly. For example, an archival application may require of transparent compression, annotation, and encryption of the archived data. In contrast, a Big Data analytics application may benefit from the computational resources of the object store to eliminate data movement and enable in-place analytics capabilities <ref type="bibr" target="#b18">[22,</ref><ref type="bibr" target="#b13">17]</ref>. Supporting such a variety of requirements in an object store is challenging, because in current systems, custom functionality is hard-coded into the system implementation due to the absence of a true programmable layer, making it difficult to maintain as the system evolves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Scope and Challenges</head><p>In this paper, we argue that Software-Defined Storage (SDS) is a compelling solution to these problems. As in SDN, the separation of the "data plane" from the "control plane" is the best-known principle in SDS <ref type="bibr" target="#b37">[41,</ref><ref type="bibr" target="#b30">34,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr" target="#b19">23,</ref><ref type="bibr" target="#b34">38]</ref>. Such separation of concerns is the cornerstone of supporting heterogeneous applications in data centers. However, the application of SDS fundamentals on cloud object stores is not trivial. Among other things, it needs to address two main challenges:</p><p>A flexible control plane. The control plane should be the key enabler that makes it possible to support multiple applications separately using dynamically configurable functionalities. Since the de facto way of expressing management requirements and objectives in SDS is via policies, they should also dictate the management rules for the different tenants in a shared object store. This is not easy since policies can be very distinct. They can be as simple as a calculation on an object such as compression, and as complex as the distributed enforcement of per-tenant IO bandwidth limits. Further, as a singular attribution of object storage, such policies have to express objectives and management rules at the tenant, container and object granularities, which requires of a largely distinct form of policy translation into the data plane compared with prior work <ref type="bibr" target="#b37">[41,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr" target="#b34">38]</ref>. Identifying the necessary abstractions to concisely define the management policies is not enough. If the system evolves over time, the control plane should be flexible enough to properly describe the new application needs in the policies.</p><p>An extensible data plane. Although the controller in all SDS systems is assumed to be easy to extend <ref type="bibr" target="#b37">[41,</ref><ref type="bibr" target="#b35">39,</ref><ref type="bibr" target="#b34">38]</ref>, data plane extensibility must be significantly richer for object storage; for instance, it must enable to perform "on the fly" computations as the objects arrive and depart from the system to support application-specific functions like sanitization, Extract-Transform-Load (ETL) operations, caching, etc. This entails the implementation of a lightweight, yet versatile computing layer, which do not exist today in SDS systems. Building up an extensible data plane is challenging. On the one hand, it requires of new abstractions that enable policies to be succinctly expressed. On the other hand, these abstractions need to be flexible enough to handle heterogeneous requirements, that is, from resource management to simple automation, which is not trivial to realize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>To overcome the rigidity of object stores we present Crystal: The first SDS architecture for object storage to efficiently support multi-tenancy and heterogeneous applications with evolving requirements. Crystal achieves this by separating policies from implementation and unifying an extensible data plane with a logically centralized controller. As a result, Crystal allows to dynamically adapt the system to the needs of specific applications, tenants and workloads.</p><p>Of Crystal, we highlight two aspects, though it has other assets. First, Crystal presents an extensible architecture that unifies individual models for each type of resource and transformation on data. For instance, global control on a resource such as IO bandwidth can be easily incorporated as a small piece of code. A dynamic management policy like this is materialized in form of a distributed, supervised controller, which is the Crystal abstraction that enables the addition of new control algorithms (Section 5.2). In particular, these controllers, which are deployable at runtime, can be fed with pluggable per-workflow or resource metrics. Examples of metrics are the number of IO operations per second and the bandwidth usage. An interesting property of Crystal is that it can even use object metadata to better drive the system towards the specified objectives.</p><p>Second, Crystal's data plane abstracts the complexity of individual models for resources and computations through the filter abstraction. A filter is a piece of programming logic that can be injected into the data plane to perform custom calculations on object requests. Crystal offers a filter framework that enables the deployment and execution of general computations on objects and groups of objects. For instance, it permits the pipelining of several actions on the same object(s) similar to stream processing frameworks <ref type="bibr" target="#b26">[30]</ref>. Consequently, practitioners and systems developers only need to focus on the development of storage filters, as their deployment and execution is done transparently by the system (Section 5.1). To our knowledge, no previous SDS system offers such a computational layer to act on resources and data.</p><p>We evaluate the design principles of Crystal by implementing two use cases on top of OpenStack Swift: One that demonstrates the automation capabilities of Crystal, and another that enforces IO bandwidth limits in a multitenant scenario. These uses cases demonstrate the feasibility and extensibility of Crystal's design. The experiments with real workloads and benchmarks are run on a 13-machine cluster. Our experiments reveal that policies help to overcome the rigidity of object stores incurring small overhead. Also, defining the right policies may report performance and cost benefits to the system. In summary, our key contributions are:</p><p>• Design of Crystal, the first SDS architecture for object storage that efficiently supports multi-tenancy and applications with evolving requirements;</p><p>• A control plane for multi-tenant object storage, with flexible policies and their transparent translation into the enforcement mechanisms at the data plane;</p><p>• An extensible data plane that offers a filter abstraction, which can encapsulate from arbitrary computations to resource management functionality, enabling concise policies for complex tasks;</p><p>• The implementation and deployment of policies for storage automation and IO bandwidth control that demonstrate the design principles of Crystal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Crystal Design</head><p>Crystal seeks to efficiently handle workload heterogeneity and applications with evolving requirements in shared object storage. To achieve this, Crystal separates highlevel policies from the mechanisms that implement them at the data plane, to avoid hard-coding the policies in the system itself. To do so, it uses three abstractions: filter, metric, and controller, in addition to policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Abstractions in Crystal</head><p>Filter. A filter is a piece of code that a system administrator can inject into the data plane to perform custom computations on incoming object requests 1 . In Crystal, this concept is broad enough to include computations on object contents (e.g., compression, encryption), data management like caching or pre-fetching, and even resource management such as bandwidth differentiation <ref type="figure" target="#fig_0">(Fig. 1)</ref>. A key feature of filters is that the instrumented system is oblivious to their execution and needs no modification to its implementation code to support them. Inspection trigger. This abstraction represents information accrued from the system to automate the execution of filters. There are two types of information sources. A first type that corresponds to the real-time metrics got from the running workloads, like the number of GET operations per second of a data container or the IO bandwidth allocated to a tenant. As with filters, a fundamental feature of workload metrics is that they can be deployed at runtime. A second type of source is the metadata from the objects themselves. Such metadata is typically associated with read and write requests and includes properties like the size or type of objects.</p><p>Controller. In Crystal, a controller represents an algorithm that manages the behavior of the data plane based on monitoring metrics. A controller may contain a simple rule to automate the execution of a filter, or a complex algorithm requiring global visibility of the cluster to control a filter's execution under multi-tenancy. Crystal builds a logically centralized control plane formed by supervised and distributed controllers. This allows an administrator to easily deploy new controllers on-the-fly that cope with the requirements of new applications.</p><p>Policy. Our policies should be extensible for really allowing the system to satisfy evolving requirements. This means that the structure of policies must facilitate the incorporation of new filters, triggers and controllers.</p><p>To succinctly express policies, Crystal abides by a structure similar to that of the popular IFTTT (If-ThisThen-That) service <ref type="bibr">[5]</ref>. This service allows users to express small rule-based programs, called "recipes", using triggers and actions. For example: <ref type="bibr" target="#b0">1</ref> Filters work in an online fashion. To explore the feasibility of batch filters on already stored objects is matter of future work.  An IFTTT-like language can reflect the extensibility capabilities of the SDS system; at the data plane, we can infer that triggers and actions are translated into our inspection triggers and filters, respectively. At the control plane, a policy is a "recipe" that guides the behavior of control algorithms. Such apparently simple policy structure can express different policy types. On the one hand, <ref type="figure" target="#fig_0">Fig. 1</ref> shows storage automation policies that enforce a filter either statically or dynamically based on simple rules; for instance, P1 enforces compression and encryption on document objects of tenant T1, whereas P2 applies data caching on small objects of container C1 when the number of GETs/second is &gt; 5. On the other hand, such policies can also express objectives to be achieved by controllers requiring global visibility and coordination capabilities of the data plane. That is, P3 tells a controller to provide at least 30MBps of aggregated GET bandwidth to tenant T2 under a multi-tenant workload. Control Plane. Crystal provides administrators with a system-agnostic DSL (Domain-Specific Language) to define SDS services via high-level policies. The DSL "vocabulary" can be extended at runtime with new filters and inspection triggers. The control plane includes an API to compile policies and to manage the life-cycle and metadata of controllers, filters and metrics (see <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">System Architecture</head><p>Moreover, the control plane is built upon a distributed model. Although logically centralized, the controller is, in practice, split into a set of autonomous micro-services, each running a separate control algorithm. Other microservices, called workload metric processes, close the control loop by exposing monitoring information from the data plane to controllers. The control loop is also extensible, given that both controllers and workload metric processes can be deployed at runtime.</p><p>Data Plane. Crystal's data plane has two core extension points: Inspection triggers and filters. First, a developer can deploy new workload metrics at the data plane to feed distributed controllers with new runtime information on the system. The metrics framework runs the code of metrics and publishes monitoring events to the messaging service. Second, data plane programmability and extensibility is delivered through the filter framework, which intercepts object flows in a transparent manner and runs computations on them. A developer integrating a new filter only needs to contribute the logic; the deployment and execution of the filter is managed by Crystal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Control Plane</head><p>The control plane allows writing policies that adapt the data plane to manage multi-tenant workloads. It is formed by the DSL, the API and distributed controllers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Crystal DSL</head><p>Crystal's DSL hides the complexity of low-level policy enforcement, thus achieving simplified storage administration <ref type="figure" target="#fig_0">(Fig. 1</ref>). The structure of our DSL is as follows:</p><p>Target: The target of a policy represents the recipient of a policy's action (e.g., filter enforcement) and it is mandatory to specify it on every policy definition. To meet the specific needs of object storage, targets can be tenants, containers or even individual data objects. This enables high management and administration flexibility.</p><p>Trigger clause (optional): Dynamic storage automation policies are characterized by the trigger clause. A policy may have one or more trigger clauses -separated by AND/OR operands-that specify the workload-based situation that will trigger the enforcement of a filter on the target. Trigger clauses consist of inspection triggers, operands (e.g, &gt;, &lt;, =) and values. The DSL exposes both types of inspection triggers: workload metrics (e.g., GETS SEC) and request metadata (e.g., OBJECT SIZE&lt;512).</p><p>Action clause: The action clause of a policy defines how a filter should be executed on an object request once the policy takes place. The action clause may accept parameters after the WITH keyword in form of key/value pairs that will be passed as input to customize the filter execution. Retaking the example of a compression filter, we may decide to enforce compression using a gzip or an lz4 engine, and even their compression level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Crystal Controller Calls Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>add policy delete policy list policies</head><p>Policy management API calls. For storage automation policies, the add policy call can either to directly enforce the filter or to deploy a controller to do so. For globally coordinated policies, the call sets an objective at the metadata layer. register keyword delete keyword</p><p>Calls that interact with Crystal registry to associate DSL keywords with filters, inspection triggers or coin new terms to be used as trigger conditions (e.g., DOCS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>deploy controller kill controller</head><p>These calls are used to manage the life-cycle of distributed controllers and workload metric processes in the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter Framework Calls Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>deploy filter undeploy filter list filters</head><p>Calls for deploying, undeploying and listing filters associated to a target. deploy/undeploy filter calls interact with the filter framework at the data plane for enabling/disabling filter binaries to be executed on a specific target. update slo list slo delete slo Calls to manage "tenant objectives" for coordinated resource management filters. For instance, bandwidth differentiation controllers take as input this information in order to provide an aggregated IO bandwidth share at the data plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workload Metric Calls Description</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>deploy metric delete metric</head><p>Calls for managing workload metrics at the data plane. These calls also manage workload metric processes to expose data plane metrics to the control plane. *For the sake of simplicity, we do not include call parameters in this table. <ref type="table" target="#tab_1">Table 1</ref>: Main calls of Crystal controller, filter framework and workload metrics management APIs.</p><p>To cope with object stores formed by proxies/storage nodes (e.g., Swift), our DSL enables to explicitly control the execution stage of a filter with the ON keyword. Also, dynamic storage automation policies can be persistent or transient; a persistent action means that once the policy is triggered the filter enforcement remains indefinitely (by default), whereas actions to be executed only during the period where the condition is satisfied are transient (keyword TRANSIENT, P2 in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>The vocabulary of our DSL can be extended on-the-fly to accommodate new filters and inspection triggers. That is, in <ref type="figure" target="#fig_0">Fig. 1</ref> we can use keywords COMPRESSION and DOCS in P1 once we associate "COMPRESSION" with a given filter implementation and "DOCS" with some file extensions, respectively (see <ref type="table" target="#tab_1">Table 1</ref>).</p><p>The Crystal DSL has other features: i) specialization of policies based on the target scope, so that if several policies apply to the same request, only the most specific one is executed (e.g., container-level policy is more specific than a tenant-level one), ii) pipelining several filters on a single request (e.g., compression + encryption) ordered as they are defined in the policy, similar to stream processing frameworks <ref type="bibr" target="#b26">[30]</ref>, and iii) grouping, which enables to enforce a single policy to a group of targets; that is, we can create a group like WEB CONTAINERS to represent all the containers that serve Web pages.</p><p>As visible in <ref type="table" target="#tab_1">Table 1</ref>, Crystal offers a DSL compilation service via API calls. Crystal compiles simple automation policies as target→filter relationships at the metadata layer. Next, we show how dynamic policies (i.e., with WHEN clause) use controllers to enforce filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distributed Controllers</head><p>Crystal resorts to distributed controllers, in form of supervised micro-services, which can be deployed in the system at runtime to extend the control plane <ref type="bibr" target="#b11">[15,</ref><ref type="bibr" target="#b14">18,</ref><ref type="bibr" target="#b36">40]</ref>.</p><p>We offer two types of controllers: automation and global controllers. On the one hand, the Crystal DSL compiles dynamic storage automation policies into au- tomation controllers (e.g., P2 in <ref type="figure" target="#fig_0">Fig. 1</ref>). Their life-cycle consists of consuming the appropriate monitoring metrics and interact with the filter framework API to enforce a filter when the trigger clause is satisfied.</p><p>On the other hand, global controllers are not generated by the DSL; instead, by simply extending a base class and overriding its computeAssignments method, developers can deploy controllers that contain complex algorithms with global visibility and continuous control of a filter at the data plane (e.g., P3 in <ref type="figure" target="#fig_0">Fig. 1</ref>). To this end, the base global controller class encapsulates the logic i) to ingest monitoring events, ii) to disseminate the computed assignments across nodes 2 , and iii) to get Service-Level Objectives (SLO) to be enforced from the metadata layer (see <ref type="table" target="#tab_1">Table 1</ref>). This allowed us to deploy distributed IO bandwidth control algorithms (Section 5).</p><p>Extensible control loop: To close the control loop, workload metric processes are micro-services that provide controllers with monitoring information from the data plane. While running, a workload metric process consumes and aggregates events from one workload metric at the data plane. For the sake of simplicity <ref type="bibr" target="#b36">[40]</ref>, we advocate to separate workload metrics not only per metric type, but also by target granularity.</p><p>Controllers and workload metrics processes interact in a publish/subscribe fashion <ref type="bibr" target="#b17">[21]</ref>. For instance, <ref type="figure" target="#fig_3">Fig.  3</ref> shows that, once initialized, an automation controller subscribes to the appropriate workload metric process, taking into account the target granularity. The subscription request of a controller specifies the target to which it is interested in, such as tenant T1 or container C1; this ensures that controllers do not receive unnecessary monitoring information from other targets. Once the workload metric process receives the subscription request, it adds the controller to its observer list. Periodically, it notifies the activity of the different targets to the interested controllers that may trigger the execution of filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Data Plane</head><p>At the data plane, we offer two main extension hooks: Inspection triggers and a filter framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inspection Triggers</head><p>Inspection triggers enable controllers to dynamically respond to workload changes in real time. Specifically, we consider two types of introspective information sources: object metadata and monitoring metrics.</p><p>First, some object requests embed semantic information related to the object at hand in form of metadata. Crystal enables administrators to enforce storage filters based on such metadata. Concretely, our filter framework middleware (see Section 4.2) is capable of analyzing at runtime HTTP metadata of object requests to execute filters based on the object size or file type, among others.</p><p>Second, Crystal builds a metrics middleware to add new workload metrics on the fly. At the data plane, a workload metric is a piece of code that accounts for a particular aspect of the system operation and publishes that information. In our design, a new workload metric can inject events to the monitoring service without interfering with existing ones <ref type="table" target="#tab_1">(Table 1)</ref>. Our metrics framework allows developers to plug-in metrics that inspect both the type of requests and their contents (e.g., compressibility <ref type="bibr" target="#b25">[29]</ref>). We provide the logic (i.e., AbstractMetric class) to abstract developers from the complexity of request interception and event publishing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Filter Framework</head><p>The Crystal filter framework enables developers to deploy and run general-purpose code on object requests. Crystal borrows ideas from active storage literature <ref type="bibr" target="#b32">[36,</ref><ref type="bibr" target="#b31">35]</ref> as a mean of building filters to enforce policies.</p><p>Our framework achieves flexible execution of filters. First, it enables to easily pipeline several filters on a single storage request. Currently, the execution order of filters is set explicitly by the administrator, although filter metadata can be exploited to avoid conflicting filter ordering errors <ref type="bibr" target="#b16">[20]</ref>. Second, to deal with object stores composed by proxies/storage nodes, Crystal allows administrators to define the execution point of a filter.</p><p>To this end, the Crystal filter framework consists of i) a filter middleware, and ii) filter execution environments.</p><p>Filter middleware: Our filter middleware intercepts data streams and classifies incoming requests. Upon a new object request, the middleware at the proxy performs a single metadata request to infer the filters to be executed on that request depending on the target. If the target has associated filters, the filter middleware sets the appropriate metadata headers in the request for triggering the execution of filters through the read/write path.</p><p>Filters that change the content of data objects may receive a special treatment (e.g., compression, encryption). To wit, if we create a filter with the reverse flag enabled, it means that the execution of the filter when the object was stored should be always undone upon a GET request.</p><p>That is, this yields that we may activate data compression on certain periods, but tenants will always download decompressed objects. To this end, prior to storing an object, we tag it with extended metadata that keeps track of the enforced filters with reverse flag set. Upon a GET request, the filter middleware fetches such metadata from the object itself to trigger the reverse transformations on it prior to the execution of regular filters.</p><p>Filter execution environments: Currently, our middleware can support two filter execution environments:</p><p>Isolated filter execution: Crystal provides an isolated filter execution environment to perform general-purpose computations on object streams with high security guarantees. To this end, we extended the Storlets framework <ref type="bibr" target="#b4">[7]</ref> with pipelining and stage execution control functionalities. Storlets provide Swift with the capability to run computations close to the data in a secure and isolated manner making use of Docker containers <ref type="bibr">[3]</ref>. Invoking a Storlet on a data object is done in an isolated manner so that the data accessible by the computation is only the object's data and its user metadata. Also, a Docker container only runs filters of a single tenant.</p><p>Native filter execution: The isolated filter execution environment trades-off higher security for lower communication capabilities and interception flexibility. For this reason, we also contribute an alternative way to intercept and execute code natively. As with Storlets, a developer can deploy code modules as native filters at runtime by following simple implementation guidelines. However, native filters can i) execute code at all the possible points of a request's life-cycle, and ii) communicate with external components (e.g, metadata layer), as well as to access storage devices (e.g., SSD). As Crystal is devised to execute trusted code from administrators, this environment represents a more flexible alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Hands On: Extending Crystal</head><p>Next, we show the benefits of Crystal's design by extending the system with data management filters and distributed control of IO bandwidth for OpenStack Swift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">New Storage Automation Policies</head><p>Goal: To define policies that enforce filters, like compression, encryption or caching, even dynamically: Data plane (Filters): To enable such storage automation policies, we first need to develop the filters at the data plane. In Crystal this can be done using either native or isolated execution environments.</p><p>The next code snippet shows how to develop a filter for our isolated execution environment. A system developer only needs to create a class that implements an interface (IStorlet), providing the actual data transformations on the object request streams (iStream, oStream) inside the invoke method. To wit, we implemented the compression (gzip engine) and encryption (AES-256) filters using storlets, whereas the caching filter exploits SSD drives at proxies via our native execution environment. Then, once these filters were developed, we installed them via the Crystal filter framework API.  <ref type="table" target="#tab_1">Ta- ble 1</ref>), we deployed metrics that capture various workload aspects (e.g., PUTs/GETs per second of a tenant) to satisfy policies like P2. Similarly, we deployed the corresponding workload metrics processes (one per metric and target granularity) that aggregate such monitoring information to be published to controllers. Also, our filter framework middleware is already capable of enforcing filters based on object metadata, such as object size (OBJECT SIZE) and type (OBJECT TYPE).</p><p>Control Plane: Finally, we registered intuitive keywords for both filters and workload metrics at the metadata layer (e.g., CACHING, GET SEC TENANT) using the Crystal registry API. To achieve P1, we also registered the keyword DOCS, which contains the file extensions of common documents (e.g, .pdf, .doc). At this point, we can use such keywords in our DSL to design new storage policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Global Management of IO Bandwidth</head><p>Goal: To provide Crystal with means of defining policies that enforce a global IO bandwidth SLO on GETs/PUTs:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P3:FOR TENANT T1 DO SET BANDWIDTH WITH GET BW=30MBps</head><p>Data plane (Filter). To achieve global bandwidth SLOs on targets, we first need to locally control the bandwidth of object requests. Intuitively, bandwidth control in Swift may be performed at the proxy or storage node stages. At the proxy level this task may be simpler, as fewer nodes should be coordinated. However, this approach is agnostic to the background tasks (e.g., replication) executed by storage nodes, which impact on performance <ref type="bibr" target="#b29">[33]</ref>. We implemented a native bandwidth control filter that enables the enforcement at both stages.</p><p>Our filter dynamically creates threads that serve and control the bandwidth allocation for individual tenants, 17: end function either at proxies or storage nodes. Our filter garbagecollects control threads that are inactive for a certain timeout. Moreover, it has a consumer process that receives bandwidth assignments from a controller to be enforced on a tenant's object streams. Once the consumer receives a new event, it propagates the assignments to the filter that immediately take effect on current transfers.</p><p>Data plane (Monitoring): For building the control loop, our bandwidth control service integrates individual monitoring metrics per type of traffic (i.e., GET, PUT, REPLICATION); this makes it possible to define policies for each type of traffic if needed. In essence, monitoring events contain a data structure that represents the bandwidth share that tenants exhibit at proxies or per storage node disk. We also deployed workload metric processes to expose these events to controllers.</p><p>Control plane. We deployed Algorithm 1 as a global controller to orchestrate our bandwidth differentiation filter. Concretely, we aim at satisfying three main requirements: i) A minimum bandwidth share per tenant, ii) Work-conservation (do not leave idle resources), and iii) Equal shares of spare bandwidth across tenants. The challenge is to meet these requirements considering that we do not control neither the data access of tenants nor the data layout of Swift <ref type="bibr" target="#b24">[28,</ref><ref type="bibr" target="#b40">44]</ref>.</p><p>To this end, Algorithm 1 works in three stages. First, the algorithm tries to ensure the SLO for tenants specified in the metadata layer by resorting to function minSLO (requirement 1, line 6). Essentially, minSLO first assigns a proportional bandwidth share to tenants with guaranteed bandwidth. Note that such assignment is done in descending order based on the number of parallel transfers per tenant, provided that tenants with fewer transfers have fewer opportunities of meeting their SLOs. Moreover, minSLO checks whether there exist overloaded nodes in the system. In the affirmative case, the algorithm tries to reallocate bandwidth of tenants with multiple transfers from overloaded nodes to idle ones. In case that no reallocation is possible, the algorithm reduces the bandwidth share of tenants with SLOs on overloaded nodes.</p><p>In second place, once Algorithm 1 has calculated the assignments for tenants with SLOs, it estimates the spare bandwidth available to achieve full utilization of the cluster (requirement 2, line 8). Note that the notion of spare bandwidth depends on the cluster at hand, as the bottleneck may be either at the proxies or storage nodes.</p><p>Algorithm 1 builds a new assignment data structure in which the spare bandwidth is equally assigned to all tenants. The algorithm proceeds by calling function spareSLO to calculate the spare bandwidth assignments (requirement 3, line 15). Note that spareSLO receives the SLOAssignments data structure that keeps the already reserved node bandwidth according to the SLO tenant assignments. The algorithm outputs the combination of SLO and spare bandwidth assignments per tenant. While more complex algorithms can be deployed in Crystal <ref type="bibr" target="#b23">[27]</ref>, our goal in Algorithm 1 is to offer an attractive simplicity/effectiveness trade-off, validating our bandwidth differentiation framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Prototype Implementation</head><p>We tested our prototype in OpenStack Kilo version. The Crystal API is implemented with the Django framework. The API manages the system's metadata from Redis 3.0 in-memory store <ref type="bibr" target="#b7">[10]</ref>. We found that co-locating both Redis and the Swift proxies in the same servers is a suitable deployment strategy. As we show next, this is specially true as only the filter middleware in proxies accesses the metadata layer (once per request).</p><p>We resort to PyActive <ref type="bibr" target="#b42">[46]</ref> for building distributed controllers and workload metric processes that can communicate among them (e.g., TCP, message brokers). For fault tolerance, the PyActive supervisor is aware of all the instantiated remote micro-services (either at one or many servers) and can spawn a new process if one dies.</p><p>We built our metrics and filter frameworks as standard WSGI middlewares in Swift. The code of workload metrics is dynamically deployed on Swift nodes, intercepts the requests and periodically publishes monitoring information (e.g., 1 second) via RabbitMQ 3.6 message broker. Similarly, the filter framework middleware intercepts a storage request and redirects it via a pipe either to the Storlets engine or to a native filter, depending on the filter pipeline definition. As both filters and metrics can run on all Swift nodes, in the case of server failures they can be executed in other servers holding object replicas.</p><p>The code of Crystal is publicly available 3 and our contributions to the Storlets project are submitted for acceptance to the official OpenStack repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Next, we evaluate a prototype of Crystal for OpenStack Swift in terms of flexibility, performance and overhead.</p><p>Objectives: Our evaluation addresses the challenges of Section 1.1 by showing: i) Crystal can define policies at multiple granularities, achieving administration flexibility; ii) The enforcement of storage automation filters can be dynamically triggered based on workload conditions; iii) Crystal achieves accurate distributed enforcement of IO bandwidth SLOs on different tenants; iv) Finally, Crystal has low execution/monitoring overhead.</p><p>Workloads: We resort to well-known benchmarks and replays of real workload traces. First, we use ssbench <ref type="bibr" target="#b8">[11]</ref> to execute stress-like workloads on Swift. ssbench provides flexibility regarding the type (CRUD) and number of operations to be executed, as well as the size of files generated. All these parameters can be specified in form of configuration "scenarios".</p><p>To evaluate Crystal under real-world object storage workloads, we collected the following traces <ref type="bibr" target="#b2">4</ref> : ii) The first trace captures 1.28TB of a write-dominated (79.99% write bytes) document database workload storing 817K car testing/standardization files (mean object size is 0.91MB) for 2.6 years at Idiada; an automotive company. i) The second trace captures 2.97TB of a readdominated (99.97% read bytes) Web workload consisting of requests related to 228K small data objects (mean object size is 0.28MB) from several Web pages hosted at Arctur datacenter for 1 month. We developed our own workload generator to replay a part of these traces (12 hours), as well as to perform experiments with controllable rates of requests. Our workload generator resorts to SDGen <ref type="bibr" target="#b20">[24]</ref> to create realistic contents for data objects based on the file types described in the workload traces.</p><p>Platform: We ran our experiments in a 13-machine cluster formed by 9 Dell PowerEdge 320 nodes (Intel Xeon E5-2403 processors); 2 of them act as Swift proxy nodes (28GB RAM, 1TB HDD, 500GB SSD) and the rest are Swift storage nodes (16GB RAM, 2x1TB HDD). There are 3 Dell PowerEdge 420 (32GB RAM, 1TB HDD) nodes that were used as compute nodes to execute workloads. Also, there is 1 large node that runs the OpenStack services and the Crystal control plane (i.e., API, controllers, messaging, metadata store). Nodes in the cluster are connected via 1 GbE switched links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Evaluating Storage Automation</head><p>Next, we present a battery of experiments that demonstrate the feasibility and capabilities of storage automation with Crystal. To this end, we make use of synthetic workloads and real trace replays (Idiada, Arctur). These <ref type="bibr" target="#b2">4</ref> Available at http://iostack.eu/datasets-menu. experiments have been executed at the compute nodes against 1 swift proxy and 6 storage nodes.</p><p>Storage management capabilities of Crystal. <ref type="figure" target="#fig_6">Fig. 4</ref> shows the execution of several storage automation policies on a workload related to containers C1 and C2 belonging to tenant T1. Specifically, we executed a writeonly synthetic workload (4PUT/second of 1MB objects) in which data objects stored at C1 consist of random data, whereas C2 stores highly redundant objects.</p><p>Due to the security requirements of T1, the first policy defined by the administrator is to encrypt his data objects (P1). <ref type="figure" target="#fig_6">Fig. 4</ref> shows that the PUT operations of both containers exhibit a slight extra overhead due to encryption, given that the policy has been defined at the tenant scope. There are two important aspects to note from P1: First, the execution of encryption on T1's requests is isolated from filter executions of other tenants, providing higher security guarantees <ref type="bibr" target="#b4">[7]</ref> (Storlet filter). Second, the administrator has the ability to enforce the filter at the storage node in order to do not overload the proxy with the overhead of encrypting data objects (ON keyword).</p><p>After policy P1 was enforced, the administrator decided to optimize the storage space of T1's objects by enforcing compression (P2). P2 also enforces compression at the proxy node to minimize communication between the proxy and storage node (ON PROXY). Note that the enforcement of P1 and P2 demonstrates the filter pipelining capabilities of our filter framework; once P2 is defined, Crystal enforces compression at the proxy node and encryption at storage nodes for each object request. Also, as shown in Section 4, the filter framework tags objects with extended metadata to trigger the reverse execution of these filters on GET requests (i.e., decryption and decompression, in that order).</p><p>However, the administrator realized that the compression filter on C1's requests exhibited higher latency and provided no storage space savings (incompressible data). To overcome this issue, the administrator defined a new policy P3 that essentially enforces only encryption on C1's requests. After defining P3, the performance of C1's requests exhibits the same behavior as before the enforcement of P2. Thus, the administrator is able to manage storage at different granularities, such as tenant or container. Furthermore, the last policy also proves the usefulness of policy specialization; policies P1 and P2  apply to C2 at the tenant scope, whereas the system only executes P3 on C1's requests, as it is the most specialized policy. Dynamic storage automation. <ref type="figure" target="#fig_7">Fig. 5</ref> shows a dynamic caching policy (P1) on one tenant. The filter implements LRU eviction and exploits SSD drives at the proxy to improve object retrievals. We executed a synthetic oscillatory workload of 1MB objects (gray area) to verify the correct operation of automation controllers.</p><p>In <ref type="figure" target="#fig_7">Fig. 5</ref>, we show the average latency of PUT/GET requests and the intensity of the workload. As can be observed, the caching filter takes place when the workload exceeds 5 GETs per second. At this point, the filter starts caching objects at the proxy SSD on PUTs, as well as to lookup the SSD to retrieve potentially cached objects on GETs. First, the filter provides performance benefits for object retrievals; when the caching filter is activated, object retrievals are in median 29.7% faster compared to non-caching periods. Second, we noted that the costs of executing asynchronous writes on the SSD upon PUT requests may be amortized by offloading storage nodes; that is, the average PUT latency is in median 2% lower when caching is activated. A reason for this may be that storage nodes are mostly free to execute writes, as a large fraction of GETs are being served at the proxy's cache.</p><p>In conclusion, Crystal's control loop enables dynamic enforcement of storage filters under variable workloads. Moreover, native filters in Crystal allow developers to build complex data management filters.</p><p>Managing real workloads. Next, we show how Crystal policies can handle real workloads (12 hours). That is, we compress and encrypt documents (P1 in <ref type="figure" target="#fig_0">Fig. 1</ref>) on a replay of the Idiada trace (write-dominated), whereas we enforce caching of small files (P2 in <ref type="figure" target="#fig_0">Fig. 1</ref>) on a replay of Arctur workload (read-dominated). <ref type="figure" target="#fig_10">Fig. 6(a)</ref> shows the request bandwidth exhibited during the execution of the Idiada trace. Concretely, we executed two concurrent workloads, each associated to a different tenant. We enforced compression and encryption only on tenant T2. Observably, tenant T2's transfers are over 13% and 7% slower compared to T1 for GETs and PUTs, respectively. This is due to the computation overhead of enforcing filters on T2's document objects. As a result, T2's documents consumed 65% less space compared to T1 with compression and they benefited from   higher data confidentially thanks to encryption. <ref type="figure" target="#fig_10">Fig. 6(b)</ref> shows tenants T1 and T2, both concurrently running a trace replay of Arctur. By executing a dynamic caching policy, T2's GET requests are in median 1.9x faster compared to T1. That is, as the workload of Arctur is intense and almost read-only, caching was enabled for tenant T2 for most of the experiment. Moreover, because the requested files fitted in the cache, the SSDbased caching filter was very beneficial to tenant T2. The median write overhead of T2 compared to T1 was 4.2%, which suggests that our filter efficiently intercepts object streams for doing parallel writes at the SSD.</p><p>Our results with real workloads suggest that Crystal is practical for managing multi-tenant object stores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Achieving Bandwidth SLOs</head><p>Next, we evaluate the effectiveness of our bandwidth differentiation filter. To this end, we executed a ssbench workload (10 concurrent threads) in each of the 3 compute nodes in our cluster, one of each representing an individual tenant. As we study the effects of replication separately (in <ref type="figure" target="#fig_13">Fig. 7(d)</ref> we use 3 replicas), the rest of experiments were performed using one replica rings.</p><p>Request types. <ref type="figure" target="#fig_13">Fig. 7(a)</ref> plots two different SLO enforcement experiments on three different tenants for PUT and GET requests, respectively (enforcement at proxy node). Appreciably, the execution of Algorithm 1 exhibits a near exact behavior for both PUT and GET requests. Moreover, we observe that tenants obtain their   SLO plus an equal share of spare bandwidth, according to the expected policy behavior defined by colored areas. This demonstrates the effectiveness of our bandwidth control middleware for intercepting and limiting both requests types. We also observe in <ref type="figure" target="#fig_13">Fig. 7(a)</ref> that PUT bandwidth exhibits higher variability than GET bandwidth. Concretely, after writing 512MB of data, Swift stopped the transfers of tenants for a short interval; we will look for the causes of this in our next development steps.</p><p>Impact of enforcement stage. An interesting aspect to study in our framework are the implications of enforcing bandwidth control at either the proxies or storage nodes. In this sense, <ref type="figure" target="#fig_13">Fig. 7(b)</ref> shows the enforcement SLOs on GET requests at both stages. At first glance, we observe in <ref type="figure" target="#fig_13">Fig. 7</ref>(b) that our framework makes it possible to enforce bandwidth limits at both stages. However, <ref type="figure" target="#fig_13">Fig. 7</ref>(b) also illustrates that the enforcement on storage nodes presents higher variability compared to proxy enforcement. This behavior arises from the relationship between the number of nodes to coordinate and the intensity of the workload at hand. That is, given the same workload intensity, a fewer number of nodes (e.g., proxies) offers higher bandwidth stability, as a tenant's requests are virtually a continuous data stream, being easier to control. Conversely, each storage node receives a smaller fraction of a tenant's requests, as normally storage nodes are more numerous than proxies. This yields that storage nodes have to deal with shorter and discontinuous streams that are harder to control.</p><p>But enforcing bandwidth SLOs at storage nodes enables to control background tasks like replication. Thus, we face a trade-off between accuracy and control that may be solved with hybrid enforcement schemes.</p><p>Mixed tenant activity, variable file sizes. Next, we execute a mixed read/write workload using files of different sizes; small (8MB to 16MB), medium (32MB to 64MB) and large (128MB to 256MB) files. Besides, to explore the scalability, in this set of experiments we resort to a cluster configuration that doubles the size of the previous one (2 proxies and 6 storage nodes).</p><p>Appreciably, <ref type="figure" target="#fig_13">Fig. 7(c)</ref> shows that our enforcement controller achieves bandwidth SLOs under mixed workloads. Moreover, the bandwidth differentiation framework works properly when doubling the storage cluster size, as the policy provides tenants with the desired SLO plus a fair share of spare bandwidth, specially for T1 and T2. However, <ref type="figure" target="#fig_13">Fig. 7</ref>(c) also illustrates that the PUT bandwidth provided to T1 is significantly more variable than for other tenants; this is due to various reasons. First, we already mentioned the increased variability of PUT requests, apparently due to write buffering. Second, the bandwidth filter seems to be less precise when limiting streams that require an SLO close to the node/link capacity. Moreover, small files make the workload harder to handle by the controller as more node assignments updates are potentially needed, specially as the cluster grows. In the future, we plan to continue the exploration and mitigation of these sources of variability.</p><p>Controlling background tasks. An advantage of enforcing bandwidth SLOs at storage nodes is that we can also control the bandwidth of background processes via policies. To wit, <ref type="figure" target="#fig_13">Fig. 7(d)</ref> illustrates the impact of replication tasks on multi-tenant workloads. In <ref type="figure" target="#fig_13">Fig. 7(d)</ref>, we observe that during the first 60 seconds of this experiment (i.e., no SLOs defined) tenants are far from having a sustained GET bandwidth of ≈ 33MBps, meaning that they are importantly affected by the replication process. The reason is that, internally, storage nodes trigger hundreds of point-to-point transfers to write copies of already stored objects to other nodes belonging to the ring. Note that the aggregated replication bandwidth within the cluster reached 221MBps. Furthermore, even though we enforce SLOs from second 60 onwards, the objectives are not achieved -specially for tenants T2 and T3-until replication bandwidth is under control. As soon as we deploy a controller that enforces a hard limit of 5MBps to the aggregated replication bandwidth, the SLOs of tenants are rapidly achieved. We conclude that Crystal has potential as a framework to define fine-grained policies for managing bandwidth allocation in object stores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Crystal Overhead</head><p>Filter framework latency overheads. A relevant question to answer is the performance costs that our filter framework introduces to the regular operation of the system. Essentially, the filter framework may introduce overhead at i) contacting the metadata layer, ii) intercepting the data stream through a filter 5 and iii) managing extended object metadata. We show this in <ref type="figure">Fig. 8</ref>.</p><p>Compared to vanilla Swift (SW), <ref type="figure">Fig. 8</ref> shows that the metadata access of Crystal incurs a median latency penalty between 1.5ms and 3ms (MA boxplots). For 1MB objects, this represents a relative median latency overhead of 3.9% for both GETs and PUTs. Naturally, this overhead becomes slightly higher as the object size decreases, but is still practical (8% to 13% for 10KB objects). This confirms that our filter framework minimizes communication with the metadata layer (i.e., 1 query per request). Moreover, <ref type="figure">Fig. 8</ref> shows that an in-memory store like Redis fits the metadata workload of Crystal, specially if it is co-located with proxy nodes.</p><p>Next, we focus on the isolated interception of object requests via Storlets, which trades off performance for higher security guarantees (see Section 4). <ref type="figure">Fig. 8</ref> illustrates that the median isolated interception overhead of a void filter (NOOP) oscillates between 3ms and 11ms (e.g., 5.7% and 15.7% median latency penalty for 10MB and 1MB PUTs, respectively). This cost mainly comes from injecting the data stream into a Docker container to achieve isolation. We also may consider filter implementation effects, or even the data at hand. To wit, columns CZ and CR depict the performance of the compression filter for highly redundant (zeros) and random data objects. Visibly, the performance of PUT requests changes significantly (e.g., objects ≥ 1MB) as compression algorithms exhibit different performance depending on the data contents <ref type="bibr" target="#b20">[24]</ref>. Conversely, decompression in <ref type="bibr">5</ref> We focus on isolated filter execution, as native execution has no additional interception overhead. GET requests is not significantly affected by data contents. Hence, to improve performance, filters should be enforced in the right conditions. Finally, our filter framework enables managing extended metadata of objects to store a sequence of data transformations to be undone on retrievals (see Section 4). We measured that reading/writing extended object metadata takes 0.3ms/2ms, respectively, which constitutes modest overhead.</p><p>Filter pipelining throughput. Next, we want to further explore the overhead of isolated filter execution. Specifically, <ref type="figure" target="#fig_14">Fig. 9</ref> depicts the latency overhead of pipeling multiple NOOP Storlet filters. As pipelining is a new feature of Crystal, it required a separate evaluation. <ref type="figure" target="#fig_14">Fig. 9</ref> shows that the latency costs of intercepting a data stream through a pipeline of isolated filters is acceptable. To inform this argument, each additional filter in the pipeline incurs 3ms to 9ms of extra latency in median. This is slightly lower than passing the stream through the Docker container for the first time. The reason is that pipelining tenant filters is done within the same Docker container, so the costs of injecting the stream into the container are present only once. Therefore, our filter framework is a feasible platform to dynamically compose and pipeline several isolated filters.</p><p>Monitoring overheads. To understand the monitoring costs of Crystal, we provide a measurement-based estimation of various configurations of monitoring nodes, workload metrics and controllers. To wit, the monitoring traffic overhead O related to |W | workload metrics is produced by a set of nodes N . Each node in N periodically sends monitoring events of size s to the MOM broker, which are consumed by |W | workload metric processes. Then, each workload metric process aggregates the messages of all nodes in N into a single monitoring message. The aggregated message is then published to a set of subscribed controllers C . Therefore, we can do a worst case estimation of the total generated traffic per monitoring epoch (e.g., 1 second) as:</p><formula xml:id="formula_0">O = |W | · [s · (2 · |N | + |C |)]</formula><p>. We also measured simple events (e.g., PUT SEC) to be s = 130 bytes in size. <ref type="figure" target="#fig_0">Fig. 10</ref> shows that the estimated monitoring overhead of a single metric is modest; in the worst case, a single workload metric generates less than 40KBps in a 100-machine cluster with |C | = 100 subscribed controllers. Clearly, the dominant factor of traffic generation is the number of workload metrics. However, even for a large number of workload metrics (|W | = 20), the monitoring requirements in a 50-machine cluster do not exceed 520KBps. These overheads seem lower than existing SDS systems with advanced monitoring <ref type="bibr" target="#b29">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>SDS Systems.</p><p>IOFlow <ref type="bibr" target="#b37">[41]</ref>, now extended as sRoute <ref type="bibr" target="#b34">[38]</ref>, was the first complete SDS architecture. IOFlow enables end-to-end (e2e) policies to specify the treatment of IO flows from VMs to shared storage. This was achieved by introducing a queuing abstraction at the data plane and translating high-level policies into queuing rules. The original focus of IOFlow was to enforce e2e bandwidth targets, which was later augmented with caching and tail latency control in <ref type="bibr" target="#b34">[38,</ref><ref type="bibr" target="#b35">39]</ref>.</p><p>Crystal, however, targets a different scenario. Simply put, it pursues the configuration and optimization of object stores to the evolving needs of tenants/applications, for it needs a richer data plane and a different suite of management abstractions and enforcement mechanisms. For example, tenants require mechanisms to inject custom logic to specify not only system activities but also application-specific transformations on objects.</p><p>Retro <ref type="bibr" target="#b29">[33]</ref> is a framework for implementing resource management policies in multi-tenant distributed systems. It can be viewed as an incarnation of SDS, because as IOFlow and Crystal, it separates the controller from the mechanisms needed to implement it. A major contribution of Retro is the development of abstractions to enable policies that are system-and resource-agnostic. Crystal shares the same spirit of requiring low develop effort. However, its abstractions are different. Crystal must abstract not only resource management; it must enable the concise definition of policies that enable high levels of programmability to suit application needs. Retro is only extensible to handle custom resources.</p><p>IO bandwidth differentiation. Enforcing bandwidth SLOs in shared storage has been a subject of intensive research over the past 10 years, specially in block storage <ref type="bibr" target="#b22">[26,</ref><ref type="bibr" target="#b23">27,</ref><ref type="bibr" target="#b39">43,</ref><ref type="bibr" target="#b41">45,</ref><ref type="bibr" target="#b28">32,</ref><ref type="bibr" target="#b37">41,</ref><ref type="bibr" target="#b29">33]</ref>. For instance, mClock <ref type="bibr" target="#b23">[27]</ref> achieves IO resource allocation for multiple VMs at the hypervisor level, even in distributed storage environments (dmClock). However, object stores have received much less attention in this regard; vanilla Swift only provides a non-automated mechanism for limiting the "number of requests" <ref type="bibr">[12]</ref> per tenant, instead of IO bandwidth. In fact, this problem resembles the one stated by <ref type="bibr">Wang et al. [44]</ref> where multiple clients access a distributed storage system with different data layout and access patterns, yet the performance guarantees required are global. To our knowledge, Wu et al. <ref type="bibr" target="#b41">[45]</ref> is the only work addressing this issue in object storage. It provides SLOs in Ceph by orchestrating local rate limiters offered by a modified version of the underlying file system (EBOFS). However, this approach is intrusive and restricted to work with EBOFS. In contrast, Crystal transparently intercepts and limits requests streams, enabling developers to design new algorithms that provide distributed bandwidth enforcement <ref type="bibr" target="#b33">[37,</ref><ref type="bibr" target="#b24">28]</ref>.</p><p>Active storage. The early concept of active disk <ref type="bibr" target="#b32">[36,</ref><ref type="bibr" target="#b10">14,</ref><ref type="bibr" target="#b27">31,</ref><ref type="bibr" target="#b38">42]</ref>, i.e., a HDD with computational capacity, was borrowed by distributed file system designers in HPC environments two decades ago to give birth to active storage. The goal was to diminish the amount of data movement between storage and compute nodes <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b6">9]</ref>. Piernas et al. <ref type="bibr" target="#b31">[35]</ref> presented an active storage implementation integrated in the Lustre file system that provides flexible execution of code near to data in the user space. Crystal goes beyond active storage. It exposes through the filter abstraction a way to inject custom logic into the data plane and manage it via policies. This requires filters to be deployable at runtime, support sandbox execution <ref type="bibr" target="#b4">[7]</ref>, and be part of complex workflows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>Crystal is a SDS architecture that pursues an efficient use of multi-tenant object stores. Crystal addresses unique challenges for providing the necessary abstractions to add new functionalities at the data plane that can be im-mediately managed at the control plane. For instance, it adds a filtering abstraction to separate control policies from the execution of computations and resource management mechanisms at the data plane. Also, extending Crystal requires low development effort. We demonstrate the feasibility of Crystal on top of OpenStack Swift through two use cases that target automation and bandwidth differentiation. Our results show that Crystal is practical enough to be run in a shared cloud object store.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Structure of the Crystal DSL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High-level overview of Crystal's architecture materialized on top of OpenStack Swift. TRIGGER: compressibility of an object is &gt; 50% ACTION: compress RECIPE: IF compressibility is &gt; 50% THEN compress</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 presents Crystal's architecture, which consists of: Control Plane. Crystal provides administrators with a system-agnostic DSL (Domain-Specific Language) to define SDS services via high-level policies. The DSL "vocabulary" can be extended at runtime with new filters and inspection triggers. The control plane includes an API to compile policies and to manage the life-cycle and metadata of controllers, filters and metrics (see Table 1). Moreover, the control plane is built upon a distributed model. Although logically centralized, the controller is, in practice, split into a set of autonomous micro-services,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Interactions among automation controllers, workload metric processes and the filter framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>P1:FOR TENANT T1 WHEN OBJECT TYPE=DOCS DO SET COMPRESSION ON PROXY, SET ENCRYPTION ON STORAGE NODE P2:FOR CONTAINER C1 WHEN GETS SEC &gt; 5 DO SET CACHING</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Data plane (Monitoring): Via the Crystal API (see</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Enforcement of compression/encryption filters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Dynamic enforcement of caching filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(a) Idiada workload (file sizes in inner plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(b) Arctur workload (file sizes in inner plot).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Policy enforcement on real trace replays.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Bandwidth per tenant (MBps) Mixed workload − Heterogeneous file sizes T1 (SLO=70, expected=86.67) No SLOs defined T2 (SLO=50, expected=66.67) T3 (SLO=30, expected=120MBps) T2 (SLO=50, expected=110) T3 (SLO=30, expected=90) T3 (SLO=30, expected=46.67) (c) 2 proxy/6 storage nodes, bandwidth control at proxies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>and replication bandwidth control at storage nodes − 128MB files Time (seconds) Bandwidth per tenant (MBps) It is very difficult to guarantee SLOs until replication bandwidth is controlled. Intra−cluster Replication (SLO=5) No SLOs defined T3 (SLO=5, expected=100) T3 (SLO=5, expected=42.5) T2 (SLO=20, expected=57.5) T3 (SLO=5, expected=6.67) T1 (SLO=70, expected=71.67) T2 (SLO=20, expected=21.67) (d) 1 proxy/3 storage nodes, bandwidth control at storage nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of the Crystal bandwidth differentiation service (SLOs per tenant are in MBps).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 8: Performance overhead of filter framework metadata interactions and isolated filter enforcement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Traffic overhead of Crystal depending on the number of nodes, controllers and workload metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Algorithm 1 computeAssignments pseudo-code embedded into a bandwidth differentiation controller 1: function COMPUTEASSIGNMENTS(info): 2: /* Retrieve the defined tenant SLOs from the metadata layer */ 3:</head><label>1</label><figDesc></figDesc><table>SLOs ← getMetadataStoreSLOs(); 

4: 

/* Compute assignments on current tenant transfers to meet SLOs */ 

5: 

SLOAssignments ← minSLO(info, SLOs); 

6: /* Estimate spare bw at proxies/storage nodes based on current usage */ 
7: 

spareBw ← min(spareBwProxies(SLOAssignments), spareBwStor-
ageNodes(SLOAssignments)); 

8: 

spareBwSLOs ← {}; 

9: 

/* Distribute spare bandwidth equally across all tenants */ 

10: 

for tenant in info do 

11: 

spareBwSLOs[tenant] ← 

spareBW 

numTenants(in f o) ; 

12: 

end for 

13: 

/* Calculate assignments to achieve spare bw shares for tenants */ 

14: 

spareAssignments ← spareSLO(SLOAssignments, spareBwSLOs); 

15: 

/* Combine SLO and spare bw assignments on tenants */ 

16: 

return SLOAssignments ∪ spareAssignments; 

</table></figure>

			<note place="foot" n="2"> For efficiency reasons, global controllers disseminate assignments to data plane filters also via the messaging service.</note>

			<note place="foot" n="3"> https://github.com/Crystal-SDS</note>

			<note place="foot" n="250"> 15th USENIX Conference on File and Storage Technologies USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our shepherd Ajay Gulati and the anonymous reviewers. This work has been partly funded by the EU project H2020 "IOStack: Software-Defined Storage for Big Data" (644182) and Spanish research project "Cloud Services and Community Clouds" (TIN2013-47245-C2-2-R) funded by the Ministry of Science and Innovation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Amazon s3</title>
		<ptr target="https://aws.amazon.com/en/s3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Databricks</surname></persName>
		</author>
		<ptr target="https://databricks.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropbox</surname></persName>
		</author>
		<ptr target="https://www.dropbox.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirantis</surname></persName>
		</author>
		<ptr target="https://www.mirantis.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openstack</forename><surname>Storlets</surname></persName>
		</author>
		<ptr target="https://github.com/openstack/storlets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openstack</forename><surname>Swift</surname></persName>
		</author>
		<ptr target="http://docs.openstack.org/devel-oper/swift" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pvfs</forename><surname>Project</surname></persName>
		</author>
		<ptr target="http://www.pvfs.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Redis</surname></persName>
		</author>
		<ptr target="https://www.redis.io" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ssbench</surname></persName>
		</author>
		<ptr target="https://github.com/swiftstack/ssbench" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<ptr target="http://www.panasas.com/products/panfs" />
		<title level="m">The Panasas activescale file system (PanFS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Active disks: Programming model, algorithms and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Actors: A model of concurrent computation in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Agha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taming the cloud object storage with mos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Parallel Data Storage Workshop</title>
		<meeting>the 10th Parallel Data Storage Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mos: Workload-aware elasticity for cloud object stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM HPDC&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Programming Erlang: software for a concurrent world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Armstrong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Pragmatic Bookshelf</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finding a needle in haystack: Facebook&apos;s photo storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vajgel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX OSDI&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Processing flows of information: From data stream to complex event processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cugola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Margara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The many faces of publish/subscribe</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Eugster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Felber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Kermarrec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="114" to="131" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The perfect match: Apache spark meets swift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Factor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xin</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">IOStack: Software-defined object storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gracia-Tinedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>García-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanchezartigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sampé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Oppermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">SDGen: mimicking datasets for content generation in storage benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gracia-Tinedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Naor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sotnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zuck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="317" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dissecting ubuntuone: Autopsy of a global-scale personal cloud back-end</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gracia-Tinedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sampé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lenton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>García-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sánchez-Artigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vukolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM IMC&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Proportional allocation of resources for distributed storage access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Waldspurger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST&apos;09</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">mclock: handling throughput variability for hypervisor io scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX OSDI&apos;10</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lexicographic qos scheduling for parallel i/o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SPAA&apos;05</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">To zip or not to zip: Effective resource usage for real-time compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sotnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Traeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Margalit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="229" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A pipelined framework for online cleaning of sensor data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Jeffery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDE&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="140" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A case for intelligent disks (idisks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keeton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="42" to="52" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pslo: enforcing the x th percentile latency and throughput slos for consolidated vm storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Eurosys&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Retro: Targeted resource management in multitenant distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Musuvathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX NSDI&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flexstore: A software defined, energy adaptive distributed storage framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murugan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MAS-COTS&apos;14</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation of active storage strategies for the lustre parallel file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piernas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nieplocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Felix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Supercomputing&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active storage for large-scale data mining and multimedia applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB&apos;98</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fairness and isolation in multi-tenant storage as optimization decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="21" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">sRoute: treating the storage stack like a network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stefanovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>O&amp;apos;shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST&apos;16</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="197" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Software-defined caching: Managing caches in multi-tenant data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stefanovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>O&amp;apos;shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Talpey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SoCC&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="174" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Experience with rules-based programming for distributed, concurrent, fault-tolerant code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stutsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ioflow: a software-defined storage architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>O&amp;apos;shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Talpey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SOSP&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="182" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of active disks for decision support databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uysal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE HPCA&apos;00</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="337" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cake: enabling high-level slos on shared storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SoCC&apos;</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Proportional-share scheduling for distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Providing quality of service support in object-based file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MSST&apos;07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="157" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Continuation complexity: A callback hell for distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zamora-Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>García-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mondéjar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LSDVE@Euro-Par&apos;15</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="286" to="298" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
