<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Profile Source-side Deduplication for Virtual Machine Backup</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Agun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Santa Barbara † Pure Storage Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Santa Barbara † Pure Storage Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California at Santa Barbara † Pure Storage Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Profile Source-side Deduplication for Virtual Machine Backup</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a source-side backup scheme with low-resource usage through collaborative deduplication and approximated lazy deletion when frequent virtual machine snapshot backup is required in a large-scale cloud cluster. The key ideas are to orchestrate multi-round duplicate detection batches among machines in a partitioned asynchronous manner and remove most un-referenced content chunks with approximated snapshot deletion. This paper discusses the challenges, main design and strategies, and evaluation results.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Frequent backup of virtual machine (VM) snapshots increases the reliability of VMs hosted in a cloud. For example, Aliyun <ref type="bibr" target="#b0">[1]</ref>, the largest cloud service provider by Alibaba in China, intends to provide automatic frequent backup of all VM images <ref type="bibr" target="#b17">[17]</ref>. The challenge is the sheer size of backup data to be transmitted and stored. Source-side deduplication <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b7">7]</ref> eliminates duplicates before backup data is transmitted; however, its computing resource usage can impact other co-located cloud services. Using a simple dirty-bit method to detect version <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b15">15]</ref> can identify duplicates among snapshots <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b7">7]</ref>. Still there is a large amount of network transmission for sending undeduped data. For example, our experimental data with Alibaba datasets show that after dirty bit detection each VM can still send over 24% of raw data chunks to the back end storage. In a cluster with 100,000 VMs with average size of VM snapshot size as 40GB, the total amount of dirty data sent over the network could exceed 0.96 petabyte. Backup products such as ones from NetApp or EMC typically deploy advanced deduplication techniques <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b12">12]</ref> at the target side. These techniques are memory-intensive even with optimization or approximation <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">6]</ref> and are not ideal for source-level deduplication.</p><p>Since backup is a secondary service, cloud providers normally do not want it to contend for resources with other collocated primary services, and it is an open problem how to exploit aggressive source-side deduplication without impacting other collocated cloud services. The focus of this paper is to address this challenge for scalable cloud VM backup.</p><p>Most previous work on deduplication <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b16">16]</ref> is an inline approach which uses relatively extensive resources (e.g. a few gigabytes of memory per machine) to optimize the performance of each individual chunk backup operation. The trade-off we play is to develop a low-cost deduplication that optimizes the average backup time of a VM snapshot instead of individual chunk backup operations.</p><p>Our previous work on source-side deduplication uses multi-stage synchronous processing <ref type="bibr" target="#b18">[18]</ref> with a key disadvantage is that multi-stage synchronization is vulnerable when some participating machine is abnormally slow. The slowness can be caused by a failure or load imbalance due to uneven VM image size distribution. For example, the VM size distribution of production clusters at Alibaba is highly skewed. <ref type="figure" target="#fig_1">Figure 1</ref> shows two such clusters, with the left and right clusters having 4200 and 8000 VMs respectively, and skews (Max/Average VM size) of 20 and 45. The deduplication cost in <ref type="bibr" target="#b16">[16]</ref> depends on the the shared index size and as the number of VMs increase, the solution can become expensive in terms of memory and CPU usage. Another key weakness in <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b16">16]</ref> is that snapshot deletion is not addressed. When deleting unused snapshots, a grouped mark-and-sweep approach <ref type="bibr" target="#b8">[8]</ref> is effective, but still carries a significant cost in a distributed setting (see Section 5). To provide a full low-cost solution, this paper also addresses the scalable deletion challenge.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Strategies and Architecture</head><p>Figure 2 illustrates a cloud cluster platform targeted by our scheme. Each server hosts multiple VMs and the co-located backup service collects and fulfills snapshot backup requests for VMs every day. The backup data can be transmitted to a separate secondary storage or to a distributed storage system in this cluster. While VM data is stored in a distributed file system, each machine typically caches actively used virtual images; the backup service can exploit the cached copy so that reading modified backup data can be conducted locally. Strategies. For inner-VM deduplication, each machine can conduct local deduplication using the dirty-bit method. For the remaining duplicates, cross-VM deduplication is necessary and we adopt the VM-centric strategy <ref type="bibr" target="#b16">[16]</ref> which simplifies the cross-VM deduplication by focusing on top-k popular duplicates. This is because global deduplication that detects the appearance of a chunk in any VM requires a substantial resource for cross-machine fingerprint comparison. Our experimental results show that choosing the top 2-4% most popular items (called PDS) for cross-VM deduplication can accomplish close to 98% of deduplication efficiency compared to full deduplication.</p><p>We propose two new strategies to accomplish aggressive source-side deduplication with minimal resource usage. 1) Distribute the signature index such as PDS index to a cluster of cloud machines and compare the signatures of candidate chunks with distributed index asynchronously in a multi-round collaborative manner. The asynchronous elimination is necessary to better tolerate load imbalance and straggling tasks. Such a scheme requires a significant amount of buffer space and also some stragglers slow down the entire process and increase the average VM backup time. We play a trade-off through multi-round batch scheduling to limit the size of buffer memory usage and detect the stragglers more aggressively while still allowing a good load balancing.</p><p>2) Snapshot deletions can occur frequently since old snapshots become less useful. To filter out unreferenced data from shared duplicates, it would require either maintaining expensive live schemas or conducting global reference counting <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b13">13]</ref>. We propose an approximation strategy and take advantages of separating popular data chunks from unpopular ones to minimize resource utilization. For chunks used by different snapshots within the same VM, we use a bloom-filter for approximated reference counting with periodic repair. For chunks that are shared among VMs as popular items, we delay reference counting as much as possible, assuming other VMs still use such chunks.</p><p>Software architecture. Fingerprint index for popular chunks is partitioned and distributed among the cloud machines that participate in collaborative deduplication. Each physical machine that hosts a VM reads dirty chunks, performs inner-VM duplicate detection, and then sends the signatures of the remaining dirty data to other machines that host popular signature partitions. Thus each machine that hosts a VM and a deduplication service runs the two agents asynchronously. The backup agent at each machine leverages the dirty-bit change tracking and inner VM deduplication, and then runs three concurrent threads. The request thread schedules the backup in batches, and initiates duplicate detection requests for each scheduled VM. It reads the dirty documents, divides them into chunks, and computes chunk fingerprints <ref type="bibr" target="#b11">[11]</ref>. Then it sends a duplicate detection request for each chunk to a proper machine. The second thread is to accumulate responses from duplicate detection agents, and the third thread performs the real backup of non-duplicate chunks. The duplicate detection agent manages three threads to accumulate detection requests and compares them to local index data periodically. It also updates the index when new fingerprints are added the system. The main thread performs multiple rounds of fingerprint comparisons and the multi-round setting provides a flexibility to handle the skewed workload so that small VM data backup can be handled as early as possible.</p><p>When some machines fail or respond slowly, a backup agent sets up a timeout and activates a detection task in another machine or temporarily considers these unprocessed requests as non-duplicates. The system conducts global cleanup and removes undetected duplicates periodically (e.g. every few months).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scheduling and Resource Control</head><p>Each backup agent conducts k rounds of backup batches and it selects the request initiation of a VM with a smaller data size first. The objective is to complete the backup of small VMs first and to shorten the average backup time per VM. <ref type="figure" target="#fig_3">Figure 3</ref> shows an example of scheduling in which 3 rounds of fingerprint comparison are triggered at each machine.</p><p>To control memory usage, we divide the entire fingerprint index evenly and distribute to p machines. For simplicity, we assume each machine hosts VMs and also participates in collaborative deduplication. Each machine further divides the local index to q partitions so that the duplicate detection agent loads one index partition at a time during comparison with a small memory need. The main memory usage is buffering for communication among machines. 1) Each backup agent uses p request send buffers and V p * k response receive buffers per machine where V is the total number of VMs hosted in a cluster; 2) Each duplicate detection agent uses p request receive buffers and p send buffers. The fingerprint comparison thread needs memory to load one of q on-disk index partitions and the requests for that partition. The total memory usage is about</p><formula xml:id="formula_0">D * V r * p [ 1 k * q + bµ q + 1 k ]</formula><p>where D is the amount of modified data per VM that needs backup after inner VM deduplication; µ is the percentage of unique chunk entries among dirty data accumulated in all snapshots; b is the average number of snapshot versions maintained for each VM; r is the ratio of average chunk size over the index entry size. When the overall index size increases, the memory cost can still be well controlled by increasing q value. This is a key advantage over <ref type="bibr" target="#b16">[16]</ref>.</p><p>Large k value reduces the overall space significantly, but it increases the chance of load imbalance, the cost of synchronization, and also the cost of disk I/O in repeated reading of signature index data for k-round comparisons. Our rule is to allocate less than 100MB of memory usage per machine. The above formula then leads to the estimation of k. For our test data with k=12 which means 9% of VM is handled at a time for each machine.</p><p>When a VM is extremely large, special handling is needed to control memory usage. The backup agent divides this VM into a number of sub-VMs and the size of each sub-VM is the same as the average VM size in the cluster. The response accumulation thread buffers the detection response for a chunk based on its corresponding sub-VM ID. The backup data output thread reads one sub-VM at a time for this VM, and checks duplicate status from the corresponding sub-VM response buffer. The metadata for the VM can then be reconstructed by appending each sub-VM's metadata.</p><p>For local disk storage usage, cost for buffering messages and storing distributed fingerprint index is relatively small. In our tested dataset, the total disk overhead is under 10GB per machine. The CPU usage is also small, less than 15% of one core during our experiments. The main resource usage in our asynchronous scheme that may create a resource contention is memory and disk I/O bandwidth. For disk I/O bandwidth usage, we set a bandwidth limit with I/O throttling so that other cloud services are minimally impacted. In our experiment, the limit is 60MB/s, which is about 20% of the peak local storage bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Approximated Lazy Snapshot Deletion</head><p>For chunks that are shared among VMs as popular items, we do not need to remove them if other VMs still use the chunk. The system periodically gathers usage information in each index partition, recomputes the top popular items, and adjusts the deletion decisions since each machine maintains the usage information of each chunk per partition. For chunks which are not shared by other VMs, we need to quickly identify if they are used by other snapshots of the same VM. Since the VM size is highly skewed in practice, a large VM may still require a substantial amount of memory for the mark-and-sweep process of data chunks used by all snapshots of this VM. We use a Bloom filter per snapshot to quickly check if the chunks are still referenced by living snapshots. The approximate deletion algorithm contains three aspects.</p><p>Computation for snapshot reference summary. Every time there is a new snapshot created, we compute a Bloom-filter with z bits as the reference summary vector for all non-popular chunks used in this snapshot. The items we put into the summary vector are all the references appearing in the metadata of the snapshot.</p><p>Fast approximate deletion with summary comparison. To approximately identify if chunks are still used by other undeleted snapshots, we compare the reference of deleted chunks with the merged reference summary vectors of other live snapshots. The merging of live snapshot Bloom-filter vectors uses the bit-wise OR operator. Since the number of live snapshots h is limited for each VM, the time and memory cost of this comparison is small, linear to the number of chunks to be deleted. If a chunk's reference is not found in the merged summary vector, this chunk is not used by any live snapshots. Thus it can be deleted safely. However, among all the chunks to be deleted, there are a small percentage of unused chunks which are misjudged as being in use, resulting in storage leakage.</p><p>One advantage of the above fast method is that it can finish and free storage usage immediately, while other off-line methods (e.g. <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b2">3]</ref>) can't. That is important for storage accounting as users pay for used storage and delayed deletion affects the accounting.</p><p>Periodic repair of leakage. Leakage repair is conducted periodically to fix the above approximation error by comparing the live chunks for each VM with what are truly used in snapshot recipes. Since it is a VMspecific procedure, the space cost is proportional to the number of chunks within each VM. This is much less expensive than the VM-oblivious mark-and-sweep which scans snapshot chunks from all VMs, even with optimization <ref type="bibr" target="#b8">[8]</ref>. We have conducted an analysis to estimate on how often leak repair should be conducted. Assume that a VM keeps h snapshots in backup storage, creates and deletes one snapshot every day. Let u be the number of chunks brought by initial backup for a VM, ∆u be the average number of additional chunks added from one version to next snapshot version. is the misjudgment rate of being in use caused by the merged Bloom filter. Each Bloom filter vector has z bits for each snapshot and let j be the number of hash functions used by the Bloom filter. Then = (1 − (1 − 1 z ) jU ) j where U = u + (h − 1)∆u. The number of snapshot deletions when reaching a storage leakage ratio τ can be derived as: τ × u+(h−1)∆u ∆u . For the test data in Section 5, h = 10, ∆u/u = 2.5%. To control the leakage under the desired threshold τ = 0.1, leak repair is needed every 19.6 days following the above formula.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We have evaluated our prototype implementation on a Linux cluster with 8-core 3.1GHz AMD FX-8120using a production dataset from Alibaba Aliyun's cloud platform <ref type="bibr" target="#b0">[1]</ref>. There are 2500 VMs running on 100 physical machines, each machine hosts up to 25 VMs, and each VM keeps 10 snapshots in backup storage. Each VM has about 40GB of storage data on average. The fingerprint for variable-sized chunks is computed using their SHA-1 hash. We have also included some synthetic traces based on VM size distributions from larger Alibaba clusters (see <ref type="figure" target="#fig_1">Figure 1)</ref>. We compare three source-side deduplication schemes. 1) Pure dirty-bit detection. All data are divided into 2MB fix-sized segments and only dirty segments are sent to backup storage. 2) Synchronous multi-stage scheme <ref type="bibr" target="#b18">[18]</ref>. 3) Asynchronous scheme.</p><p>Resource usage. <ref type="table">Table 1</ref> compares resource consumption. Column 2 shows the memory required during deduplication and backup per physical machine. Fingerprint comparison does need more memory than the pure dirty-bit method. The multi-round collaborative scheme uses concurrent thread processing and thus requires more memory than the synchronous scheme. However we limit its memory usage to 90MB, which is a small fraction of the available memory on a server. Column 3 of <ref type="table">Table 1</ref> shows the total size of local disk IO required. Our scheme reads backup data twice, thus doubling the I/O size. The collaborative scheme does incur slightly more I/O than the synchronous one because of k rounds of fingerprint comparison. Column 4 lists the final size of output data sent to the backup storage for 2500 VMs with p = 100. The dirty-bit method reduces the data size by 75.86%. The final data after our collaborative deduplication is 4.55x smaller. Column 5 shows the network communication size including backup data transmission, and inter-machine deduplication message exchange. The pure dirty-bit approach does not have inter-machine deduplication, but communicates 4x more data because of more undetected duplicates.</p><p>Processing time. <ref type="table">Table 2</ref> shows the total job time (job span in hours) in Column 2 for a synthetic 2500 VM dataset with a skewed VM size distribution (max/average=20) following <ref type="figure" target="#fig_1">Figure 1</ref>  <ref type="table">Table 2</ref>: Job span and average per-VM backup time average per-VM backup time for this dataset with a relatively even size distribution. Column 4 shows the per-VM time when VM size is skewed. Our scheme reads VM data twice, and thus doubles the job span. Collaborative processing uses 12 rounds and is much faster than the synchronous scheme. In the skewed case, backup time per VM is very high for the synchronous scheme because all VMs must wait for the completion of large VMs at each synchronized stage. Impact of k rounds. <ref type="figure">Figure 4</ref> shows the average backup time per VM and job span in the asynchronous scheme. As k increases, the average backup reduces because a large k value provides more opportunities for earlier VM output and the job span increases slightly because more multi-round processing overhead.  <ref type="table" target="#tab_3">Table 3</ref> lists a comparison of processing time and memory usage using the four deletion methods when the number of physical machines p = 100 and p = 50. These four methods are 1) the standard mark-and-sweep method. 2) Grouped mark-and-sweep <ref type="bibr" target="#b8">[8]</ref>. 3) local without using summary vectors (Row 4). 4) approximate local with summary vectors. Last row of <ref type="table" target="#tab_3">Table 3</ref> is the cost of the leakage repair for local with summary vectors. The mark-andsweep process requires all machines read snapshot metadata for usage comparison. The I/O read speed for the backend distributed file system is about 50MB/second and there is some throughput contention when all machines read data simultaneously: the speed drops to about 30MB/second when p = 50 and 25MB/s with  four deletion methods p = 100. We explain the results for p = 100 below. The explanation for p = 50 is similar and the result difference for p = 100 and p = 50 shows our deletion method scales well when p increases.</p><p>For the mark-and-sweep method (Row 2 of <ref type="table" target="#tab_3">Table 3</ref>) on p = 100, we conduct p phases of the mark-and-sweep process. At each phase, a physical machine reads 1/p of the non-deduplicated chunk metadata and keeps a reference table in the memory. Then all machines read the meta data of snapshots in parallel and mark the used chunks in the above reference table. The above phase is repeated 100 times (one for for each physical machine). The memory allocated at each physical machine is for the chunk reference table at each phase. the average size is 1.2GB and the maximum is 3GB due to data skew. There is a trade-off between memory usage of a reference table in terms of the size and the total processing time. If we reduce the size of the reference table at each phase, then there are more phases to mark all data and the whole process will take more time. For the grouped mark-and-sweep (Row 3), about 50% of snapshot metadata reading can be avoided by actively tracking the reference usage of non-duplicate chunks. Thus it takes 50% less time, which is about 43 hours, but the memory requirement does not decrease. Notice that in our setting because snapshot deletion occurs frequently, the grouped mark-and-sweep approach becomes less effective in reducing metadata I/O.</p><p>For the local deletion without summary vectors (Row 4 of <ref type="table" target="#tab_3">Table 3</ref>), all physical machines conduct the markand-sweep process in parallel, but each machine only handles one VM at time and the scope of meta data comparison is controlled within the single VM. Popular chunks are excluded. The average memory usage is the index size of non-deduplicated VM chunks, which is about 50MB on average and the largest size is 1.96GB. For approximate deletion with summary vectors (Row 5), each physical machine loads the VM snapshots and only needs to compare with the summary vectors. The memory usage is controlled around 15MB for hosting the summary vectors and small buffers. The deletion time is reduced to less than 1 minute. The periodic leakage repair (Row 6) still takes about 0.83 hours while using an average of 50MB memory. For few big VMs due to data skew, their repair uses upto 1.9GB memory and lasts about 1 minute. Such a repair does not occur often Deletions 1 3 5 7 9 Estimated .02% .06% .10% .14% .18% Measured .01% .055% .09% .12% .15% <ref type="table">Table 4</ref>: Accumulated storage leakage by approximate snapshot deletions (∆u/u = 0.025) (e.g. every 19.6 days). <ref type="table">Table 4</ref> shows percentage of unused storage space per VM misjudged as "in use" due to approximate deletion. In this experiment, we select 105 VMs and let all the VMs accumulate 10 snapshot versions, then start to delete those snapshots one by one in reverse order. Row 1 in <ref type="table">Table 4</ref> is the number of snapshot versions deleted. Entry value 3 in this row means that snapshot versions of all VMs from 1 to 3 are deleted. Row 2 is based on a predicted leakage analysis briefly discussed in Section 4. given ∆u/u = 0.025, while row 3 lists the actual average leakage measured during the experiment for all the VMs. The Bloom filter setting is based on ∆u/u = 0.025. After 9 snapshot deletions, the actual leakage ratio reaches 0.0015 and this means that there is only 1.5MB space leaked for every 1GB of stored data. The actual leakage can reach 4.1% after 245 snapshot deletions for all VMs. This experiment shows that the leakage of our approximate snapshot deletion is very small, below the estimated number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Concluding Remarks</head><p>The contribution of this work is a scalable solution with multi-round source-side deduplication and approximated deletion for frequent VM snapshot backup. For the tested dataset, the network cost is reduced by 4x and storage cost is reduced by 4.55x compared to a pure dirty-bit-based method. The multi-round deduplication is an order of magnitude faster than a synchronous scheme, when some machines are very slow or have a skewed load. Approximate snapshot deletion only requires 15MB per machine within 1 minute in the tested cases, which is over 3114x faster than the grouped markand-sweep method. Leakage repair is 53x faster with 35% to 96% less memory usage. If we were handling the case mentioned in Section 1 with 100,000 VMs using 4000 machines, the size of all VM snapshots sent could be reduced from 0.96 petabyte with a dirty-bit method to 196 terabytes while each physical machine uses about 90MB memory, 10GB disk space, and less than 1% of CPU and sends about 6.6GB metadata for low-profile duplication in less than 3 hours.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Skewed VM size distribution in two production clusters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Targeted cloud cluster architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: 3-round comparison batches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>k</head><label></label><figDesc>Figure 4: Job span and average per-VM backup time Effectiveness of Approximate Deletion. Table 3 lists a comparison of processing time and memory usage using the four deletion methods when the number of physical machines p = 100 and p = 50. These four methods are 1) the standard mark-and-sweep method. 2) Grouped mark-and-sweep [8]. 3) local without using summary vectors (Row 4). 4) approximate local with summary vectors. Last row of Table 3 is the cost of the leakage repair for local with summary vectors. The mark-andsweep process requires all machines read snapshot metadata for usage comparison. The I/O read speed for the backend distributed file system is about 50MB/second and there is some throughput contention when all machines read data simultaneously: the speed drops to about 30MB/second when p = 50 and 25MB/s with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Processing time and per-machine memory usage of 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alibaba</forename><surname>Aliyun</surname></persName>
		</author>
		<ptr target="http://www.aliyun.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extreme Binning: Scalable, parallel deduplication for chunk-based file backup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhagwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D E</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lillibridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MASCOTS &apos;09</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Memory efficient sanitization of a deduplicated storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;13</title>
		<imprint>
			<biblScope unit="page" from="81" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">USENIX</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Decentralized deduplication in san cluster file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vilayannur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno>ATC&apos;09. USENIX</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tradeoffs in scalable data routing for deduplication clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shilane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;11</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Design tradeoffs for data deduplication performance in backup workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of USENIX FAST 2015</title>
		<meeting>of USENIX FAST 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Application-aware local-global source deduplication for cloud backup services of personal storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1155" to="1165" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a highperformance deduplication system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Efstathopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ATC&apos;11</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="25" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse Indexing: Large Scale, Inline Deduplication Using Sampling and Locality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lillibridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhagwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Deolalikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trezis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Camble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;09</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="111" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Venti: A New Approach to Archival Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;02</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="89" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Rabin</surname></persName>
		</author>
		<title level="m">Fingerprinting by random polynomials. Center for Research in Computing Techn</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
		<respStmt>
			<orgName>Aiken Computation Laboratory, Univ.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">idedup: latency-aware, inline data deduplication for primary storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Voruganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;12</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="24" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Concurrent deletion in a distributed contentaddressable storage system with global deduplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Strzelczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adamczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Herman-Izycka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sakowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Slusarczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubnicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="161" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SAM: A semantic-aware multi-tiered source de-duplication framework for cloud backup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">39th International Conference on Parallel Processing, ICPP 2010</title>
		<meeting><address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cumulus: Filesystem backup to the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vrable</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;09</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="225" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Vm-centric snapshot deduplication for cloud data backup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 31st Int. Conf. on Massive Storage Systems and Technologies</title>
		<meeting>of 31st Int. Conf. on Massive Storage Systems and Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-level selective deduplication for vm snapshots in cloud storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CLOUD&apos;12</title>
		<imprint>
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Low-cost data deduplication for virtual machine backup in cloud storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Narayanasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotStorage&apos;13. USENIX</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Avoiding the disk bottleneck in the data domain deduplication file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST&apos;08</title>
		<imprint>
			<publisher>USENIX</publisher>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
