<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tackling Parallelization Challenges of Kernel Network Stack for Container Overlay Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">State University of New York (SUNY</orgName>
								<orgName type="institution" key="instit2">Binghamton + University of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Suo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">State University of New York (SUNY</orgName>
								<orgName type="institution" key="instit2">Binghamton + University of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">State University of New York (SUNY</orgName>
								<orgName type="institution" key="instit2">Binghamton + University of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">State University of New York (SUNY</orgName>
								<orgName type="institution" key="instit2">Binghamton + University of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tackling Parallelization Challenges of Kernel Network Stack for Container Overlay Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Overlay networks are the de facto networking technique for providing flexible, customized connectivity among distributed containers in the cloud. However, overlay networks also incur non-trivial overhead due to its complexity, resulting in significant network performance degradation of containers. In this paper, we perform a comprehensive empirical performance study of container overlay networks which identifies unrevealed, important parallelization bottlenecks of the kernel network stack that prevent container overlay networks from scaling. Our observations and root cause analysis cast light on optimizing the network stack of modern operating systems on multi-core systems to more efficiently support container overlay networks in light of high-speed network devices.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As an alternative to virtual machine (VM) based virtualization, containers offer a lightweight process-based virtualization method for managing, deploying and executing cloud applications. Lightweight containers lead to higher server consolidation density and lower operational cost in cloud data centers, making them widely adopted by industry -Google even claims that "everything at Google runs in containers" <ref type="bibr">[4]</ref>. Further, new cloud application architecture has been enabled by containers: services of a large-scale distributed application are packaged into separate containers, automatically and dynamically deployed across a cluster of physical or virtual machines with orchestration tools, such as Apache Mesos <ref type="bibr" target="#b0">[1]</ref>, Kubernetes <ref type="bibr" target="#b5">[12]</ref>, and Docker Swarm <ref type="bibr">[7]</ref>.</p><p>Container overlay networks are the de facto networking technique for providing customized connectivity among these distributed containers. Various container overlay network approaches are becoming available, such as Flannel <ref type="bibr">[8]</ref>, Weave <ref type="bibr">[20]</ref>, Calico <ref type="bibr" target="#b1">[2]</ref> and Docker Overlay <ref type="bibr">[6]</ref>. They are generally built upon the tunneling approach which enables container traffic to travel across physical networks via encapsulating container packets with their host headers (e.g., with the VxLAN protocol <ref type="bibr" target="#b10">[19]</ref>). With this, containers belonging to a same virtual network can communicate in an isolated address space with their private IP addresses, while their packets are routed through "tunnels" using their hosts public IP addresses.</p><p>Constructing overlay networks in a container host can be simply achieved by stacking a pipeline of in-kernel network devices. For instance, for a VxLAN overlay, a virtual network device is created and assigned to a container's network namespace, while a tunneling VxLAN network device is created for packet encapsulation/decapsulation. These two network devices are further connected via a virtual switch (e.g., Open vSwitch <ref type="bibr">[15]</ref>). Such a container overlay network is also extensible: various network policies (e.g., isolation, rate limiting, and quality of service) can be easily added to either the virtual switch or the virtual network device of a container.</p><p>Regardless of the above-mentioned advantages, container overlay networks incur additional, non-trivial overhead compared to the native host network (i.e., without overlays). Recent studies report that overlay networks achieve 50% less throughput than the native and suffer much higher packet processing latency <ref type="bibr" target="#b19">[28,</ref><ref type="bibr" target="#b20">29]</ref>. The prolonged network packet processing path in overlay networks can be easily identified as the main culprit. Indeed, as an example in the above VxLAN overlay network, a packet traverses three different namespaces (i.e., the container, overlay and host) and two kernel network stacks (the container and host) in both sending and receiving ends, leading to high per-packet processing cost and long end-to-end latency. However, our investigation reveals that the causes of high-overhead and low-efficiency of container network overlays are much complicated and multifaceted:</p><p>First, the high-performance, high-speed physical network devices (e.g., 40 and 100 Gbps Ethernet) require the kernel to quickly process each packet (e.g., 300 ns for a 40 Gbps network link). However, as stated above, the prolonged packet path in container overlay networks slows down the per-packet processing speed with multiple network devices involved. More critically, we observe that modern OSes only provide parallelization of packet processing at the per-flow level (instead of per-packet); thus, the maximum network throughput of a single container flow is limited by the processing capability of a single core (e.g., 6.4 Gbps for TCP in our case). Further, the combination of multi-core CPUs and multiqueue network interface cards (NIC) allows packets of different flows to route to separate CPU cores for parallel processing. Unfortunately, container overlay networks are observed to produce poor scalability -the network throughput increases by 4x with 6x number of flows. In addition, under the same throughput (e.g., <ref type="bibr">40</ref> Gbps with 80 flows), overlay networks consume much more CPU resources (e.g., 2 âˆ¼ 3 times). Our investigation finds that this severe scalability issue is largely due to the inefficient interplay by kernel among pipelined, asynchronous packet processing stages -an overlay packet traverses among the contexts of one hardware interrupt, three software interrupts and the user-space process. With more flows, the hardware also becomes inefficient with poor cache efficiency and high memory bandwidth.</p><p>Last, research has long observed inefficiency in the kernel network stack for flows with small packet sizes. We observe that such inefficiency becomes more severe in container overlay networks which achieve as low as 50% packet processing rate of that in the native host (e.g., for UDP packets). We find that, in addition to prolonged network path processing path, the high interrupt request (IRQ) rate and the associated high software interrupt (softirq) rate (i.e., 3x of IRQs) impair the overall system efficiency by frequently interrupting running processes with enhanced context switch overhead.</p><p>In this paper, we perform a comprehensive empirical performance study of container overlay networks and identify the above-stated new, critical parallelization bottlenecks in the kernel network stack. We further deconstruct these bottlenecks to locate their root causes. We believe our observations and root cause analysis will cast light on optimizing the kernel network stack to well support container network stacks on multi-core systems in light of high-speed network devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Related Work</head><p>In this section, we introduce the background of network packet processing (under Linux) and the existing optimizations for network packet processing.</p><p>Network Packet Processing. Packet processing traverses NICs, kernel space, and user space. Taking receiving a packet as an example <ref type="figure" target="#fig_0">(Figure 1</ref>): When a packet arrives at the NIC, it is copied (via DMA) to the kernel ring buffer and triggers a hardware interrupt (IRQ). The kernel responds to the interrupt and starts the receiving path. The receiving process in kernel is divided into two parts: the top half and the bottom half. The top half runs in the context of a hardware interrupt, which simply inserts the packet in the per-CPU packet queue and triggers the bottom half. The bottom half is executed in the form of a software interrupt (softirq), scheduled by the kernel at an appropriate time later and is the main routine that the packet is processed through the network protocol stack. After being processed at various protocol layers, the packet is finally copied to the user space buffer and passed to the application. Container Overlay Networks. Overlay networks are a common way to virtualize container networks and provide customized connectivity among distributed containers. Container overlay networks are generally based on a tunneling technique (e.g., VxLAN): When sending a container packet, it encapsulates the packet in a new packet with the (source and destination) host headers; when receiving an encapsulated container packet, it decapsulates the received packet to recover the original packet and finally delivers it to the target container application by its private IP address.</p><p>As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the overlay network is created by adding additional devices, such as a VxLAN network device for packet encapsulation and decapsulation, virtual Ethernet ports (veth) for network interfaces of containers, and a virtual bridge to connect all these devices. Intuitively, compared to the native host network, a container overlay network is more complex with longer data path. As an example in <ref type="figure" target="#fig_0">Figure 1</ref>, receiving one container packet raises one IRQ and three softirqs (by the host NIC, the VxLAN, the veth separately). In consequence, the container packet traverses three network namespaces (host, overlay and container) and two network stacks (container and host). Inevitably, it leads to high overhead of packet processing and low efficiency of container networking. Optimizations for Packet Processing. There is a large body of work targeting at optimizing the kernel for efficient packet processing. We categorize them into two groups:</p><p>(1) Mitigating per-packet processing overhead: Packet processing cost generally consists of two parts: per-packet cost and per-byte cost. In modern OSes, per-packet cost dominates in packet processing. Thus, a bunch of optimizations have been proposed to mitigate per-packet processing including interrupt coalescing and polling-based approaches which reduce the number of interrupts <ref type="bibr" target="#b11">[21,</ref><ref type="bibr" target="#b13">23,</ref><ref type="bibr" target="#b17">27]</ref>; packet coalescing which reduces the number of packets that need to be processed by kernel network stacks (e.g., Generic Receive Offload <ref type="bibr" target="#b2">[9]</ref> and Large Receive Offload <ref type="bibr" target="#b6">[13]</ref>); user-space network stacks which bypass the OS kernel thus reducing context switches <ref type="bibr" target="#b3">[10]</ref>; and data path optimizations <ref type="bibr" target="#b12">[22,</ref><ref type="bibr" target="#b14">[24]</ref><ref type="bibr" target="#b15">[25]</ref><ref type="bibr" target="#b16">[26]</ref>.</p><p>(2) Parallelizing packet processing path: High-speed network devices can easily saturate one CPU core even with the above optimizations. This is especially true in virtualized overlay networks. To leverage multi-core systems, a set of hardware and software optimizations have been proposed to parallelize packet processing. Parallelism can be achieved using the hardware approach -a single physical NIC with multi-queues, each mapping IRQs to one separate CPU core with Receive Side Scaling (RSS) <ref type="bibr" target="#b9">[18]</ref>. Even without the NIC support, Receive Packet Steering (RPS) <ref type="bibr" target="#b8">[17]</ref> can achieve the same RSS functionality in a software manner. Both RSS an RPS use hash functions (based on packet IP addresses and protocol ports) to determine the target CPU cores for packets of different flows. As we will show shortly, none of these approaches work effectively in container overlay networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation of Container Overlay Networks</head><p>In this section, we perform empirical studies to illustrate parallelization bottlenecks of the kernel network stack for container overlay networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>We conducted experiments with three network configurations as follows:</p><p>â€¢ The Native Case. Applications were running in the native host (i.e., no containers), and communicated with each other using the host IP addresses associated with the physical network interface -the traditional configuration in a non-virtualization, non-overlay environment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Results and Analysis</head><p>A Single Flow. First, we measure the TCP and UDP throughput using a single pair of iperf client and server residing on two machines separately. <ref type="figure">Figure 3</ref> shows the TCP throughput, while <ref type="figure">Figure 5</ref> shows the UDP throughput. More specifically, the native case can reach around 23 Gbps for TCP and 9.3 Gbps for UDP. The Linux overlay performs a little better than the Docker overlay: in the Linux overlay, the TCP throughput reaches 6.5 Gbps, and the UDP reaches 4.7 Gbps. In comparison, in the Docker overlay case, the TCP throughput reaches around 6.4 Gbps, while the UDP throughput reaches only 3.9 Gbps. Compared to the native case, the throughput of the Docker overlay drops by 72% for TCP and 58% for UDP. As the packet processing path gets longer, the single pair bandwidth performance gets lower for both TCP and UDP cases.</p><p>The reason why the Docker overlay achieves much lower throughput than the native shows that: it consumes much higher CPU cycles for processing each packet. As plotted in <ref type="figure">Figure 4</ref> (CPU usage breakdown for TCP) and <ref type="figure" target="#fig_3">Figure 6</ref> (CPU usage breakdown for UDP), in the single flow case, the docker overlay consumes the same (or more) CPU usage with much less throughput, compared to the native case <ref type="bibr" target="#b0">1</ref> . <ref type="figure" target="#fig_1">Figure 2</ref> shows the function call stack along the TCP receiving path -the highlighted areas refer to the extra time spent in the functions of the overlay networks. It clearly demonstrates that the network processing path in the docker overlay network is much longer than the native case leading to extra CPU usage.</p><p>A question arises after we observe that the iperf client and server in the user space consume little CPU far away from occupying one single core: why cannot the throughput keep scaling by consuming more CPU resources? Upon deeper investigation, we found that existing parallelization approaches <ref type="bibr" target="#b0">1</ref> Each machine has in total 20 virtual cores -5% CPU usage means that a single core has been exhausted.   (e.g., RSS or RPS) work at the per-flow level, as they decide the core for packet processing based on the flow-level information (i.e., IP addresses and/or port number). Hence, packets of the same flow are processed by the kernel on the same core -including all the softirqs triggered by all the network devices (i.e., the host interface, VxLAN and veth). As the docker overlay incurs longer packet processing path, it easily saturates one CPU core -as shown in <ref type="figure">Figure 4</ref> and <ref type="figure" target="#fig_3">Figure 6</ref>, the CPU consumed by the kernel (i.e., the sum of the system and softirq parts) saturates one core. Multiple Flows. As a single flow is far away from fully utilizing a 40 Gbps network link in the Docker overlay case, we tried to use multiple flows to saturate the network bandwidth by scaling the number of flows -we ran multiples pairs of iperf clients and servers from 1 to 80; each iperf client or server was running in a separate container.</p><p>As shown in <ref type="figure">Figure 3</ref>, we observe that the native case quickly reaches the peak throughput, âˆ¼37 Gbps under TCP with only two pairs. However, the TCP throughput in the two overlay cases grows slowly as the pair number increasesthe throughput increase by 4x (6.4 Gbps to 25 Gbps) with 6x number of pairs (1 pair to 6 pairs). Though all three cases can saturate the whole 40 Gbps network bandwidth (with 80 flows), under the same throughput (e.g., 40 Gbps) overlay networks consume much more CPU resources (e.g., around 2.5 times) than the native case.</p><p>This raises another question: why does the overlay network not scale well with multiple flows given that in this situation both RSS and RPS take effect (i.e., we did observe that packets of different flows were assigned with different CPU cores)? Our investigation shows that such a bad scalability is largely due to the inefficient interplay of many packet processing tasks -IRQs, three different softirq contexts, and user-space processes. Too frequent context switches among these tasks greatly hurt the CPU cache efficiency, resulting in much higher memory bandwidth. For example, the Docker overlay case consumes 2x memory bandwidth with 50% network throughput with 7 pairs (not depicted in the figures). Such inefficiency can also be observed in <ref type="figure">Figure 4</ref>, though the total throughput does not scale, the CPU usage keeps increasing as the number of flow pairs increases -the kernel is just busy with juggling numerous tasks.</p><p>We observe the similar (and even worse) scalability in the UDP case 2 as illustrated in <ref type="figure">Figure 5</ref> with the exception that the throughput of the native case keeps flat regardless of the flow numbers. The reason is that, in the native case, all UDP flows share the same flow-level information (i.e., same source and destination IP addresses); the RSS and RPS cannot distinguish them and assign all flows on the same core which is fully occupied. In contrast, in the overlay networks, the RSS and RPS can distinguish the packets of different flows by looking at the inner header information containing the private IP addresses of containers which are different among flows.</p><p>Small Packets. It is evident that most packets in the real world have small sizes (e.g., 80% â‰¤ 600 bytes <ref type="bibr">[16]</ref>). The inefficient packet processing will negatively impact the performance of real-world applications. We conducted experiments to show the performance impact of overlay networks on small packets by varying the packet sizes of a single flow from 64 bytes to 8 KB. As illustrated in <ref type="figure">Figure 7</ref>, the Docker overlay performs a bit worse with small packet sizes (64 bytes to 1    KB) than the native under TCP in terms of packet processing rate; the gap becomes wider as the packet size increases. Further, as shown in <ref type="figure">Figure 8</ref>, the Docker overlay consumes less CPU due to lower packet processing rate <ref type="bibr">3</ref> .</p><p>The more significant inefficiency is observed in the UDP case: In <ref type="figure">Figure 9</ref>, we observe that the Docker overlay achieves as low as 50% packet processing rate of that in the native case with lower CPU usage ( <ref type="figure" target="#fig_0">Figure 10</ref>). The Linux overlay case performs better than the Docker overlay but still worse than the native. Correspondingly, we observe that the IRQ number increases dramatically in the Docker overlay case10x of that in the TCP case. In addition, much more softirqs are observed in <ref type="figure" target="#fig_0">Figure 11</ref>, âˆ¼3x of the IRQs. It is because again, one IRQ can trigger (at most) three softirqs in the Docker overlay case. Note that, multiple softirqs can"merge" within one softirq, as long as they are processed in a timely manner (i.e., all processed under the context of one softirq and counted once). Notice that, more softirqs indicate that <ref type="bibr">3</ref> The Docker overlay is more CPU efficient than the Linux overlay under small packet sizes, as (we observed that) the kernel CPU scheduler intends to put the user-space iperf processes on the same core -that also performs kernel-level packet processing -more often in the Docker overlay case.</p><p>either the IRQ number is large or the process of softirqs is frequently interrupted (multiple softirqs cannot merge) -the Docker overlay case falls in the latter category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Insights and Conclusions</head><p>We have presented the performance study of container overlay networks on a multi-core system with high-speed network devices, and identified three critical parallelization bottlenecks in the kernel network stack which prevent overlay networks from scaling: (1) the kernel does not provide per-packet level parallelization preventing a single container flow from achieving high network throughput; (2) the kernel does not efficiently handle various packet processing tasks preventing multiple container flows from easily saturating a 40 Gbps network link; and (3) the above two parallelization bottlenecks become more severe for small packets, as the kernel fails to handle a large number of interrupts which disrupts the overall system efficiency.</p><p>These parallelization bottlenecks urge us to develop a more efficient kernel network stack for overlay networks by considering the following several questions: (1) Is it feasible to provide packet-level parallelization for a single network flow? Though probably not necessary in the native case, it becomes imperative in the overlay networks as the achieved throughput of a single flow is still very low (limited by a single CPU core). (2) How can the kernel perform a better isolation among multiple flows especially for efficiently utilizing shared hardware resources (e.g., CPU caches and memory bandwidth). This is particularly important as one server can host tens or even hundreds of light-weight containers. It becomes more challenging to handle small packets under overlay networks. (3) Can the packets be further coalesced with optimized network path for reduced interrupts and context switches?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion Topic</head><p>By presenting our observations in container overlay networks, we are looking to receive feedback that can gauge the importance of these observed bottlenecks considering real cloud containerized applications. We are aware of that there is a large body of work addressing the inefficient network packet processing issue with either optimizing existing operating systems (OS), or renovating OSes with a clean-slate design, or completely bypassing the OSes with a user-space approach. However, in our work, we aim to first have a thorough understanding about the inefficiencies of conventional OSes particularly for container overlay networks. With this, we plan to generate discussions about whether we should keep improving the conventional kernel network stack following an evolutionary concept by retrofitting existing OSes with the new technology for better adoptability and compatibility.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of data receiving path in Linux kernel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Function call graph along the TCP receiving path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 3: TCP throughput.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: CPU usage breakdown on the iperf server side (the receiver) for UDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: CPU usage breakdown on the iperf server side (the receiver) for UDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Interrupt number with varying packet sizes (UDP).</figDesc></figure>

			<note place="foot" n="2"> We cannot collect performance data after 7 pairs for the Docker overlay case, as the system becomes very unstable due to high packet drop rate.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Mesos</surname></persName>
		</author>
		<ptr target="http://mesos.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Calico</surname></persName>
		</author>
		<ptr target="https://www.projectcalico.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Surviving 10gbp/s with cycles to spare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gro</surname></persName>
		</author>
		<ptr target="https://events.static.linuxfound.org/images/stories/slides/jls09/jls09_xu.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dpdk</forename><surname>Intel</surname></persName>
		</author>
		<ptr target="https://www.dpdk.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iperf3</surname></persName>
		</author>
		<ptr target="https://iperf.fr" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kubernetes</surname></persName>
		</author>
		<ptr target="https://kubernetes.io" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Offload</forename><surname>Large Receive</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/243949/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Linux Overlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Network</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/networking/vxlan.txt" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<ptr target="https://lwn.net/Articles/362339/" />
	</analytic>
	<monogr>
		<title level="j">Receive Packet Steering (RPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<ptr target="https://www.kernel.org/doc/Documentation/networking/scaling.txt" />
		<title level="m">Receive Side Scaling</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vxlan</surname></persName>
		</author>
		<ptr target="https://tools.ietf.org/html/rfc7348" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Measurement of processing and queuing delays introduced by a software router in a single-hop network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leopoldo</forename><surname>Angrisani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Peluso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annarita</forename><surname>Tedesco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Ventre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Instrumentation and Measurement Technology Conference, 2005. IMTC 2005</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1797" to="1802" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Performance analysis of system overheads in tcp/ip workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">R</forename><surname>Nathan L Binkert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">G</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">G</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven K</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 14th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>14th International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Routebricks: exploiting parallelism to scale software routers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Dobrescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norbert</forename><surname>Egi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Argyraki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung-Gon</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Fall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianluca</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Knies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Manesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvia</forename><surname>Ratnasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unikernels: Library operating systems for the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Madhavapeddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Mortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charalampos</forename><surname>Rotsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balraj</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gazagnaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Crowcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Arrakis: The operating system is the control plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Woos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roscoe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Netmap: a novel framework for fast packet i/o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 21st USENIX Security Symposium (USENIX Security)</title>
		<meeting>21st USENIX Security Symposium (USENIX Security)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond softnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamal Hadi</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kuznetsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th</title>
		<meeting>the 5th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">ALS &apos;01</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="18" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An analysis and empirical study of container networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2018-IEEE Conference on Computer Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Slim:{OS} kernel support for a low-overhead container overlay network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqiang</forename><forename type="middle">Harry</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Rockett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 19)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="331" to="344" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
