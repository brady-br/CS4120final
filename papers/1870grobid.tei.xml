<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Design Guidelines for High Performance RDMA Systems Design Guidelines for High Performance RDMA Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 22-24. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuj</forename><surname>Kalia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Michael Kaminsky</orgName>
								<orgName type="institution" key="instit3">Intel Labs</orgName>
								<orgName type="institution" key="instit4">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit5">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Design Guidelines for High Performance RDMA Systems Design Guidelines for High Performance RDMA Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16)</title>
						<meeting>the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) <address><addrLine>Denver, CO, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page">437</biblScope>
							<date type="published">June 22-24. 2016</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) is sponsored by USENIX. https://www.usenix.org/conference/atc16/technical-sessions/presentation/kalia USENIX Association</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modern RDMA hardware offers the potential for exceptional performance, but design choices including which RDMA operations to use and how to use them significantly affect observed performance. This paper lays out guidelines that can be used by system designers to navigate the RDMA design space. Our guidelines emphasize paying attention to low-level details such as individual PCIe transactions and NIC architecture. We empirically demonstrate how these guidelines can be used to improve the performance of RDMA-based systems: we design a networked sequencer that outperforms an existing design by 50x, and improve the CPU efficiency of a prior high-performance key-value store by 83%. We also present and evaluate several new RDMA optimizations and pitfalls, and discuss how they affect the design of RDMA systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Remote Direct Memory Access (RDMA)-capable networks have dropped in price and made substantial inroads into datacenters. Despite their newfound popularity, using their advanced capabilities to best effect remains challenging for software designers. This challenge arises because of the nearly-bewildering array of options a programmer has for using the NIC1, and because the relative performance of these operations is determined by complex low-level factors such as PCIe bus transactions and the (proprietary and often confidential) details of the NIC architecture.</p><p>Unfortunately, finding an efficient match between RDMA capabilities and an application is important: As we show in Section 5, the best and worst choices of RDMA options vary by a factor of seventy in their overall throughput, and by a factor of 3.2 in the amount of host CPU they consume. Furthermore, there is no onesize-fits-all best approach. Small changes in application requirements significantly affect the relative performance of different designs. For example, using general-purpose 1In this paper, we refer exclusively to RDMA-capable network interface cards, so we use the more generic but shorter term NIC throughout.</p><p>RPCs over RDMA is the best design for a networked key-value store (Section 4), but this same design choice provides lower scalability and 16% lower throughput than the best choice for a networked "sequencer" (Section 4; the sequencer returns a monotonically increasing integer to client requests). This paper helps system designers address this challenge in two ways. First, it provides guidelines, backed by an open-source set of measurement tools, for evaluating and optimizing the most important system factors that affect end-to-end throughput when using RDMA NICs. For each guideline (e.g., "Avoid NIC cache misses"), the paper provides insight on both how to determine whether this guideline is relevant (e.g., by using PCIe counter measurements to detect excess traffic between the NIC and the CPU), and a discussion of which modes of using the NICs are most likely to mitigate the problem.</p><p>Second, we evaluate the efficacy of these guidelines by applying them to both microbenchmarks and real systems, across three generations of RDMA hardware. Section 4.2 presents a novel design for a network sequencer that outperforms an existing design by 50x. Our best sequencer design handles 122 million operations/second using a single NIC and scales well. Section 4.3 applies the guidelines to improve the CPU efficiency and throughput of the HERD key-value cache <ref type="bibr" target="#b17">[20]</ref> by 83% and 35% respectively. Finally, we show that today's RDMA NICs handle contention for atomic operations extremely slowly, rendering designs that use them <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b8">11]</ref> very slow.</p><p>A lesson from our work is that low-level details are surprisingly important for RDMA system design. Our underlying goal is to provide researchers and developers with a roadmap through these details without necessarily becoming RDMA gurus. We provide simple models of RDMA operations and their associated CPU and PCIe costs, plus open-source software to measure and analyze them (https://github.com/efficient/rdma _ bench).</p><p>We begin our journey into high-performance RDMAbased systems with a review of the relevant capabilities of RDMA NICs, and the PCIe bus that frequently arises as a bottleneck.  <ref type="figure" target="#fig_0">Figure 1</ref> shows the relevant hardware components of a machine in an RDMA cluster. A NIC with one or more ports connects to the PCIe controller of a multi-core CPU. The PCIe controller reads/writes the L3 cache to service the NIC's PCIe requests; on modern Intel servers <ref type="bibr" target="#b2">[4]</ref>, the L3 cache provides counters for PCIe events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">PCI Express</head><p>The current fastest PCIe link is PCIe "3.0 x16," the 3rd generation PCIe protocol, using 16 lanes. The bandwidth of a PCIe link is the per-lane bandwidth times the number of lanes. PCIe is a layered protocol, and the layer headers add overhead that is important to understand for efficiency. RDMA operations generate 3 types of PCIe transaction layer packets (TLPs): read requests, write requests, and read completions (there is no transaction-layer response for a write). <ref type="figure" target="#fig_1">Figure 2a</ref> lists the bandwidth and header overhead for the PCIe generations in our clusters. Note that the header overhead of 20-26 bytes is comparable to the common size of data items used in services such as memcached <ref type="bibr" target="#b22">[25]</ref> and RPCs <ref type="bibr" target="#b12">[15]</ref>.</p><p>MMIO writes vs. DMA reads There are important differences between the two methods of transferring data from a CPU to a PCIe device. CPUs write to mapped device memory (MMIO) to initiate PCIe writes. To avoid generating a PCIe write for each store instruction, CPUs use an optimization called "write combining," which combines stores to generate cache line-sized PCIe transactions. PCIe devices have DMA engines and can read from DRAM using DMA. DMA reads are not restricted to cache lines, but a read response larger than the CPU's read completion combining size (C rc ) is split into multiple completions. C rc is 128 bytes for the Intel CPUs used in our measurements <ref type="table" target="#tab_7">(Table 2)</ref>; we assume 128 bytes for the AMD CPU <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b1">3]</ref>. A DMA read always uses less hostto-device PCIe bandwidth than an equal-sized MMIO; <ref type="figure" target="#fig_1">Figure 2b</ref> shows an analytical comparison. This is an important factor, and we show how it affects performance of higher-layer protocols in the subsequent sections.  insights.2 For each counter, the number of captured events per second is its counter rate. Our analysis primarily uses counters for DMA reads (PCIeRdCur) and DMA writes (PCIeItoM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PCIe counters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RDMA</head><p>RDMA is a network feature that allows direct access to the memory of a remote computer. RDMA-providing networks include InfiniBand, RoCE (RDMA over Converged Ethernet), and iWARP (Internet Wide Area RDMA Protocol). RDMA networks usually provide high bandwidth and low latency: NICs with 100 Gbps of per-port bandwidth and ∼ 2µs round-trip latency are commercially available. The performance and scalability of an RDMAbased communication protocol depends on several factors including the operation (verb) type, transport, optimization flags, and operation initiation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">RDMA verbs and transports</head><p>RDMA hosts communicate using queue pairs (QPs); hosts create QPs consisting of a send queue and a receive queue, and post operations to these queues using the verbs API. We call the host initiating a verb the requester and the destination host the responder. For some verbs, the responder does not actually send a response. On completing a verb, the requester's NIC optionally signals completion by DMA-ing a completion entry (CQE) to a completion queue (CQ) associated with the QP. Verbs can be made unsignaled by setting a flag in the request; these verbs do not generate a CQE, and the application detects completion using application-specific methods. and atomic operations. These verbs specify the remote address to operate on and bypass the responder's CPU. Messaging verbs include the send and receive verbs. These verbs involve the responder's CPU: the send's payload is written to an address specified by a receive that was posted previously by the responder's CPU. In this paper, we refer to RDMA read, write, send, and receive verbs as READ, WRITE, SEND, and RECV respectively. RDMA transports are either reliable or unreliable, and either connected or unconnected (also called datagram). With reliable transports, the NIC uses acknowledgments to guarantee in-order delivery of messages. Unreliable transports do not provide this guarantee. However, modern RDMA implementations such as InfiniBand use a lossless link layer that prevents congestion-based losses using link layer flow control <ref type="bibr">[1]</ref>, and bit error-based losses using link layer retransmissions <ref type="bibr" target="#b5">[8]</ref>. Therefore, unreliable transports drop packets very rarely. Connected transports require one-to-one connections between QPs, whereas a datagram QP can communicate with multiple QPs. We consider two types of connected transports in this paper: Reliable Connected (RC) and Unreliable Connected (UC </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">RDMA WQEs</head><p>To initiate RDMA operations, the user-mode NIC driver at the requester creates Work Queue Elements (WQEs) in host memory; typically, WQEs are created in a preallocated, contiguous memory region, and each WQE is individually cache line-aligned. (We discuss methods of transferring WQEs to the device in Section 3.1.) The WQE format is vendor-specific and is determined by the NIC hardware.</p><p>WQE size depends on several factors: the type of RDMA operation, the transport, and whether the payload is referenced by a pointer field or inlined in the WQE (i.e., the WQE buffer includes the payload). <ref type="table">Table 1</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Terminology and default assumptions</head><p>We distinguish between inbound and outbound verbs because their performance differs significantly (Section 5): memory verbs and SENDs are outbound at the requester and inbound at the responder; RECVs are always inbound. <ref type="figure">Figure 3</ref> summarizes this. As our study focuses on small messages, all WRITEs and SENDs are inlined by default. We define the padding-to-cache-line-alignment function x 񮽙 := 񮽙x/64񮽙 * 64. We denote the per-lane bandwidth, request header size, and completion header size of PCIe 3.0 by P bw , P r , and P c , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RDMA design guidelines</head><p>We now present our design guidelines. Along with each guideline, we present new optimizations and briefly describe those presented in prior literature. We make two assumptions about the NIC hardware that are true for all currently available NICs. First, we assume that the NIC is PCIe-based device. Current network interfaces (NIs) are predominantly discrete PCIe cards; vendors are beginning to integrate NIs on-die <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b3">5]</ref>, or on-package <ref type="bibr" target="#b4">[6]</ref>, but these NIs still communicate with the PCIe controller using the PCIe protocol, and are less powerful than discrete NIs. Second, we assume that the NIC has internal parallelism using multiple processing units (PUs)-this is generally true of high-speed NIs <ref type="bibr" target="#b16">[19]</ref>. As in conventional parallel programming, this parallelism provides both optimization opportunities (Section 3.3) and pitfalls (Section 3.4).</p><p>To discuss the impact on CPU and PCIe use of the optimizations below, we consider transferring N WQEs of size D bytes from the CPU to the NIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Reduce CPU-initiated MMIOs</head><p>Both CPU efficiency and RDMA throughput can improve if MMIOs are reduced or replaced with the more CPU-   In this paper, we refer to Doorbell batching as batching-a batched WQE transfer via WQE-by-MMIO is identical to a sequence of individual WQE-by-MMIOs, so batching is only useful for Doorbell.</p><p>WQE shrinking Reducing the number of cache lines used by a WQE can improve throughput drastically. For example, consider reducing WQE size by only 1 byte from 129 B to 128 B, and assume that WQE-by-MMIO is used. CPU: CPU-generated MMIOs decreases from 3 to 2. PCIe: Number of PCIe transactions decreases from 3 to 2. Shrinking mechanisms include compacting the application payload, or overloading unused WQE header fields with application data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reduce NIC-initiated DMAs</head><p>Reducing DMAs saves NIC processing power and PCIe bandwidth, improving RDMA throughput. Note that the batching optimization above adds a DMA read, but it avoids multiple MMIOs, which is usually a good tradeoff. Figure 6: Optimizations for RECVs with small SENDs.</p><p>.</p><p>Known optimizations to reduce NIC-initiated DMAs include unsignaled verbs which avoid the completion DMA write (Section 2.2.1), and payload inlining which avoids the payload DMA read (Section 2.2.2). The two optimizations in Section 3.1 affect DMAs, too: batching with large N requires fewer DMA reads than smaller N; WQE shrinking further makes these reads smaller. NICs must DMA a completion queue entry for completed RECVs <ref type="bibr">[1]</ref>; this provides an additional optimization opportunity, as discussed below. Unlike CQEs of other verbs that only signal completion and are dispensible, RECV CQEs contain important metadata such as the size of received data. NICs typically generate two separate DMAs for payload and completion, writing them to application-and driver-owned memory respectively. We later show that the corresponding performance penalty explains the rule-of-thumb that messaging verbs are slower than memory verbs, and using the DMA-avoiding optimizations below challenges this rule-of-thumb for some payload sizes. We assume here that the corresponding SEND for a RECV carries an X-byte payload.</p><p>Inline RECV If X is small (∼ 64 for Mellanox's NICs), the NIC encapsulates the payload in the CQE, which is later copied by the driver to the application-specified address. CPU: Minor overhead for copying the small payload. PCIe: Uses 1 DMA instead of 2.</p><p>Header-only RECV If X = 0 (i.e., the RDMA SEND packet consists of only a header and no payload), the payload DMA is not generated at the receiver. Some information from the packet's header is included in the DMA-ed CQE, which can be used to implement application protocols. We call SENDs and RECVs with X = 0 header-only, and regular otherwise. PCIe: Uses 1 DMA instead of 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Engage multiple NIC PUs</head><p>Exploiting NIC parallelism is necessary for high performance, but requires explicit attention. A common RDMA programming decision is to use as few queue pairs as possible, but doing so limits NIC parallelism to the number of QPs. This is because operations on the same QP have ordering dependencies and are ideally handled by the same NIC processing unit to avoid cross-PU synchronization. For example, in datagram-based communication, one QP per CPU core is sufficient for communication with all remote cores. Using one QP consumes the least NIC SRAM to hold QP state, while avoiding QP sharing among CPU cores. However, it "binds" a CPU core to a PU and may limit core throughput to PU throughput. This is likely to happen when per-message application processing is small (e.g., the sequencer in Section 4.2) and a high-speed CPU core overwhelms a less powerful PU. In such cases, using multiple QPs per core increases CPU efficiency; we call this the multi-queue optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Avoid contention among NIC PUs</head><p>RDMA operations that require cross-QP synchronization introduce contention among PUs, and can perform over an order of magnitude worse than uncontended operations. For example, RDMA provides atomic operations such as compare-and-swap and fetch-and-add on remote memory. To our knowledge, all NICs available at the time of writing (including the recently released ConnectX-4 <ref type="bibr">[7]</ref>) use internal concurrency control for atomics: PUs acquire an internal lock for the target address and issue read-modify-write over PCIe. Note that atomic operations contend with non-atomic verbs too. Future NICs may use PCIe's atomic transactions for higher performing, cache coherence-based concurrency control.</p><p>Therefore, the NIC's internal locking mechanism, such as the number of locks and the mapping of atomic addresses to these locks, is important; we describe experiments to infer this in Section 5.4. Note that due to the limited SRAM in NICs, the number of available locks is small, which amplifies contention in the workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Avoid NIC cache misses</head><p>NICs cache several types of information; it is critical to maintain a high cache hit rate because a miss translates to a read over PCIe. Cached information includes (1) virtual to physical address translations for RDMA-registered memory, (2) QP state, and (3) a work queue element cache. While the first two are known <ref type="bibr" target="#b10">[13]</ref>, the third is undocumented and was discovered in our experiments. Address translation cache misses can be reduced by using  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Improved system designs</head><p>We now demonstrate how these guidelines can be used to improve the design of whole systems. We consider two systems: networked sequencers, and key-value stores.</p><p>Evaluation setup We perform our evaluation on the three clusters described in  <ref type="figure">Figure 7</ref>: Impact of optimizations on HERD RPC-based sequencer (blue lines with circular dots), and throughput of Spec-S0 and the atomics-based sequencer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview of HERD RPCs</head><p>We use HERD's RPC protocol for communication between clients and the sequencer/key-value server. HERD RPCs have low overhead at the server and high numberof-clients scalability. Protocol clients use unreliable WRITEs to write requests to a request memory region at the server. Before doing so, they post a RECV to an unreliable datagram QP for the server's response. A server thread (a worker) detects a new request by polling on the request memory region. Then, it performs application processing and responds using a UD SEND posted via WQE-by-MMIO.</p><p>We apply the following two optimizations to HERD RPCs in general; we present specific optimizations for each system later.</p><p>• Batching Instead of responding after detecting one request, the worker checks for one request from each of the C clients, collecting N ≤ C requests. Then, it SENDs N responses using a batched Doorbell.</p><p>• Multi-queue Each worker alternates among a tuneable number of UD queue pairs across the batched SENDs. Note that batching does not add significant latency because we do it opportunistically <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b18">21]</ref>; we do not wait for a number of requests to accumulate. We briefly discuss the latency added by batching in Section 4.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Networked sequencers</head><p>Centralized sequencers are useful building blocks for a variety of network applications, such as ordering operations in distributed systems via logical or real timestamps <ref type="bibr" target="#b8">[11]</ref>, or providing increasing offsets into a linearly growing memory region <ref type="bibr" target="#b7">[10]</ref>. A centralized sequencer can be the bottleneck in high-performance distributed systems, so building a fast sequencer is an important step to improving whole-system performance.</p><p>Our sequence server runs on a single machine and provides an increasing 8-byte integer to client processes running on remote machines. The baseline design uses HERD RPCs. The worker threads at the server share an 8-byte counter; each client can send a sequencer request to   <ref type="figure">Figure 7</ref> shows the effect of batching and multi-queue on the HERD RPC-based sequencer's throughput with an increasing number of server CPU cores. We run 1 worker thread per core and use 70 client processes on 5 client machines. Batching increases single-core throughput from 7.0 million requests per second (Mrps) to 16.6 Mrps. In this mode, each core still uses 2 response UD queue pairs-one for each NIC port-and is bottlenecked by the NIC processing units handling the QPs; engaging more PUs with multi-queue (3 per-port QPs per core) increases core throughput to 27.4 Mrps. With 6 cores and both optimizations, throughput increases to 97.2 Mrps and is bottlenecked by DMA bandwidth: The DMA bandwidth limit for the batched UD SENDs used by our sequencer is 101.6 million operations/s (Section 5.2.1). At 97.2 Mrps, the sequencer is within 5% of this limit; we attribute the gap to PCIe link-and physical-layer overheads in the DMA-ed requests, which are absent in our SEND-only benchmark. When more than 6 cores are used, throughput drops because the response batch size are smaller: With 6 cores (97.2 Mrps), there are 15.9 responses per batch; with 10 cores (84 Mrps), there are 4.4 responses per batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sequencer-specific optimizations</head><p>The above design is a straightforward adoption of generalpurpose RPCs for a sequencer, and inherits the limitations of the RPC protocol. First, the connected QPs used for writing requests require state in the server's NIC and limit scalability to a few hundred RPC clients <ref type="bibr" target="#b17">[20]</ref>  <ref type="figure">Figure 8</ref>: Impact of response batching on Spec-S0 latency</p><p>We exploit the specific requirements of the sequencer to overcome these limitations. We use header-only SENDs for both requests and responses to solve both problems:</p><p>1. The client's header-only request SENDs generate header-only, single-DMA RECVs at the server <ref type="figure">(Fig- ure 6</ref>), which are as fast as the WRITEs used earlier. 2. The server's header-only response SEND WQEs use a header field for application payload and fit in 1 cache line ( <ref type="figure" target="#fig_4">Figure 5</ref>), reducing the data DMA-ed per response by 50% to 64 bytes. Using header-only SENDs requires encoding application information in the SEND packet header; we use the 4-byte immediate integer field of RDMA packets <ref type="bibr">[1]</ref>. Our 8-byte sequencer works around the 4-byte limit as follows: Clients speculate the 4 higher bytes of the counter and send it in a header-only SEND. If the client's guess is correct, the server sends the 4 lower bytes in a header-only SEND, else it sends the entire 8-byte value in a regular, non header-only SEND which later triggers an update of the client's guess. Only a tiny fraction (≤ C/2 32 with C clients) of SENDs are regular. We discuss this speculation technique further in Section 5.</p><p>We call this datagram-only sequencer Spec-S0 (speculation with header-only SENDs). <ref type="figure">Figure 7</ref> shows its throughput with increasing server CPU cores. Spec-S0's DMA bandwidth limit is higher than HERD RPCs because of smaller response WQEs; it achieves 122 Mrps and is limited by the NIC's processing power instead of PCIe bandwidth. Spec-S0 has lower single-core throughput than the HERD RPC-based sequencer because of the additional CPU overhead of posting RECVs. <ref type="figure">Figure 8</ref> shows the average end-to-end latency of Spec-S0 with and without response batching. Both modes receive a batch of requests from the NIC; the two modes differ only in the method used to send responses. The non-batched mode sends responses one-by-one using WQE-by-MMIO whereas the batched mode uses Doorbell when multiple responses are available to send. We batch atomic increments to the shared counter in both modes. We use 10 server CPU cores, which is the minimum required to achieve peak throughput. We measure throughput with increasing client load by adding more clients, and by increasing the number of outstanding requests per client. Batching adds up to 1 µs of latency because of the additional DMA read required with the Doorbell method. We believe that the small additional latency is acceptable because of the large throughput and CPU efficiency gains from batching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Latency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Atomics-based sequencers</head><p>Atomic fetch-and-add over RDMA is an appealing method to implement a sequencer: Binnig et al. <ref type="bibr" target="#b8">[11]</ref> use this design for the timestamp server in their distributed transaction protocol. However, lock contention for the counter among the NIC's PUs results in poor performance. The effects of contention are exacerbated by the duration for which locks are held-several hundred nanoseconds for PCIe round trips. Our RPC-based sequencers have lower contention and shorter lock duration: the programmability of general-purpose CPUs allows us to batch updates to the counter which reduces cache line contention, and proximity to the counter's storage (i.e., core caches) makes these updates fast. <ref type="figure">Figure 7</ref> shows the throughput of our atomics-based sequencer: it achieves only 2.24 Mrps, which is 50x worse than our optimized design, and 12.2x worse than our single-core throughput. <ref type="table" target="#tab_10">Table 3</ref> summarizes the performance of our sequencers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Key-value stores</head><p>Several designs have been proposed for RDMA-based key-value storage. The HERD key-value cache uses HERD RPCs for all requests and does not bypass the remote CPU; other key-value designs bypass the remote CPU for key-value GETs (Pilaf <ref type="bibr" target="#b21">[24]</ref> and FaRM-KV <ref type="bibr" target="#b10">[13]</ref>), or for both GETs and PUTs (DrTM-KV <ref type="bibr" target="#b27">[30]</ref> and Nessie <ref type="bibr" target="#b24">[27]</ref>). Our goal here is to demonstrate how our guidelines can be used to optimize or find flaws in RDMA system designs in general; we do not compare across different key-value systems. We first present performance improvements for HERD. Then, we show how the performance of atomics-based key-value stores is adversely affected by lock contention inside NICs.  <ref type="figure" target="#fig_0">Figure 10</ref>: Throughput of emulated DrTM-KV with increasing updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Improving HERD's performance</head><p>We apply batching to HERD as follows. After collecting N ≤ C requests, the server-side worker performs the GET or PUT operations on the backing datastore. It uses prefetching to hide the memory latency of accesses to the storage data structures <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b20">23]</ref>. Then, the worker sends the responses either one-by-one using WQE-by-MMIO, or as a single batch using Doorbell. For evaluation, we run a HERD server with a variable number of workers on a CIB machine; we use 128 client threads running on eight client machines to issue requests. We pre-populate the key space partition owned by each worker with 8 million key-value pairs, which map 16-byte keys to 32-byte values. The workload consists of 95% GET and 5% PUT operations, with keys chosen uniformly at random from the inserted keys. <ref type="figure" target="#fig_6">Figure 9</ref> shows the throughput achieved in the above experiment. We also include the maximum throughput achievable by a READ-based key-value store such as Pilaf or FaRM-KV that uses ≥ 2 small READs per GET (one READ for fetching the index entry, and one for fetching the value). We compute this analytically by halving CIB's peak inbound READ throughput (Section 5.3.1). We make three observations:</p><p>• Batching improves HERD's per-core throughput by 83% from 6.7 Mrps to 12.3 Mrps. This improvement is smaller than for the sequencer because the CPU processing time saved from avoiding MMIOs is smaller relative to per-request processing in HERD than in the sequencer.</p><p>• Batching improves peak throughput by 35% from 72.8 Mrps to 98.3 Mrps. Batched throughput is bottlenecked by PCIe DMA bandwidth.</p><p>• With batching, HERD's throughput is up to 63%</p><p>higher than a READ-based key-value store. While HERD's original non-batched design requires 12 cores to outperform a READ-based design, only 7 cores are needed with batching. This highlights the importance of including low-level factors such as batching when comparing RDMA system designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Atomics-based key-value stores</head><p>DrTM-KV <ref type="bibr" target="#b27">[30]</ref> and Nessie <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b25">28]</ref> use RDMA atomics to bypass the remote CPU for both GETs and PUTs. However, these projects do not consider the impact of the NIC's concurrency control on performance, and present performance for either GET-only (DrTM-KV) or GETmostly workloads (Nessie). We now show that locking inside the NIC results in low PUT throughput, and degrades throughput even when only a small fraction of key-value operations are PUTs. We discuss DrTM-KV here because of its simplicity, but similar observations apply to Nessie. DrTM-KV caches some fields of its key-value index at all clients; GETs for cached keys use one READ. PUT operations lock, update, and unlock key-value items; locking and unlocking is done using atomics. Running DrTM-KV's codebase on CIB requires significant modification because CIB's dual-port NIC are connected in a way that does not allow cross-port communication. To overcome this, we wrote a simplified emulated version of DrTM-KV: we emulate GETs with 1 READ and PUTs with 2 atomics, and assume a 100% cache hit rate. <ref type="figure" target="#fig_0">Figure 10</ref> shows the throughput of our emulated DrTM-KV server with different fractions of PUT operations in the workload. The server hosts 16 million items with 16-byte keys and 32-byte values. Clients use randomly chosen keys and we use as many clients as required to maximize throughput. Although throughput for a 100% GET workload is high, adding only 10% PUTs degrades it by 72% on CX3 and 31% on CIB. Throughput with 100% PUTs is a tiny fraction of GET-only throughput: 4% on CX3 and 12% on CIB. Note that the degradation for CIB is more gradual than for CX3 because CIB has a better locking mechanism, as shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Low-level factors in RDMA</head><p>Our guidelines and system designs are based on an improved understanding of low-level factors that affect RDMA performance, including I/O initiation mechanisms, PCIe, and NIC architecture. These factors are complicated and there is little existing literature describing them or studying their relevance to networked systems. We attempt to fill this void by presenting clarifying performance measurements, experiments, and models; <ref type="table" target="#tab_14">Table 4</ref> shows a partial summary for CIB. Additionally, we discuss the importance of these factors to general RDMA system design beyond the two systems in Section 4.</p><p>We divide our discussion into three common use cases that highlight different low-level factors: (1) batched operations, (2) non-batched operations, and (3) atomic operations. For each case, we present a performance analysis focusing on hardware bottlenecks and optimizations, and discuss implications on RDMA system design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">PCIe models</head><p>We have demonstrated that understanding the PCIe behavior is critical for improving RDMA performance. How-   ever, deriving analytical models of PCIe behavior without access to proprietary/confidential NIC manuals and our limited resources-per-cache line PCIe counters and undocumented driver software-required extensive experimentation and analysis. Our derived models are presented in a slightly simplified form at several points in this paper ( <ref type="figure" target="#fig_0">Figures 5, 6, 13)</ref>. The exact analytical models are complicated and depend on several factors such as the NIC, its PCIe capability, the verb and transport, the level of Doorbell batching, etc. To make our models easily accessible, we instrumented the datapath of two Mellanox drivers (ConnectX-3 and Connect-IB) to provide statistics about PCIe bandwidth use (https: //github.com/efficient/rdma _ bench). Our models and drivers are restricted to requester-side PCIe behavior. We omit responder-side PCIe behavior because it is the same as described in our previous work <ref type="bibr" target="#b17">[20]</ref>: inbound READs and WRITEs generate one PCIe read and write, respectively; inbound SENDs trigger a RECV completion-we discuss the PCIe transactions for the RECV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Batched operations</head><p>A limitation of batching on current hardware makes it useful mainly for datagram transport: all operations in a batch must use the same queue pair because Doorbells are per QP. This limitation seems fundamental to the parallel architecture of NICs: In a hypothetical NIC design where Doorbells contained information relevant for multiple queue pairs (e.g., a compact encoding of "2 and 1 new WQEs for QP 1 and QP 2, respectively"), sending the Doorbell to the NIC processing units handling these QPs would require an expensive selective broadcast inside the NIC. These PUs would then issue separate DMAs for WQEs, losing the coalescing advantage of batching. This limitation makes batching less useful for connected QPs, which provide only one-to-one communication between two machines: the chances that a process has multiple messages for the same remote machine are low in large deployments. We therefore discuss batching for UD transport only.   <ref type="figure" target="#fig_0">Figure 11</ref> shows the throughput and PCIe bandwidth limit of batched and non-batched UD SENDs on CIB. We use one server to issue SENDs to multiple client machines. With batching, we use batches of size 16 (i.e., the NIC DMAs 16 work queue elements per Doorbell). Otherwise, the CPU writes WQEs by the WQE-by-MMIO method. We use as many cores as required to maximize throughput. Batching improves peak SEND throughput by 27% from 80 million operations/s (Mops) to 101.6 Mops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">UD SENDs</head><p>Bottlenecks Batched throughput is limited by DMA bandwidth. For every DMA completion of size C rc bytes, there is header overhead of P c bytes (Section 2.1), leading to 13443 MB/s of useful DMA read bandwidth on CIB. As UD WQEs span at least 2 cache lines, the maximum WQE transfer rate is 13443/128 = 105 million/s, which is within 5% of our achieved throughput; we attribute the difference to link-and physical-layer PCIe overheads.</p><p>Non-batched throughput is limited by MMIO bandwidth. The write-combining MMIO rate on CIB is (16 * P bw )/(64 + P r ) = 175 million cache lines/s. UD SEND WQEs with non-zero payload span at least 2 cache lines <ref type="table">(Table 1)</ref>, and achieve up to 80 Mops. This is within 10% of the 87.5 Mops bandwidth limit.</p><p>Multi-queue optimization <ref type="figure" target="#fig_0">Figure 11b</ref> shows singlecore throughput for batched and non-batched 60-byte UD SENDs-the largest payload size for which the WQEs fit in 2 cache lines. Interestingly, batching decreases core throughput if only one QP is used: with one QP, a core is coupled to a NIC processing unit (Section 3.3), so throughput depends on how the PU handles batched and non-batched operations. Batching has the expected effect when we break this coupling by using multiple QPs. Batched throughput increases by ∼ 2x on all clusters with 2 QPs, and between 2-3.2x with 4 QPs. Nonbatched (WQE-by-MMIO) throughput does not increase with multiple QPs (not shown in graph), showing that it is CPU-limited.  Design implications RDMA-based systems can often choose between CPU-bypassing and CPU-involving designs. For example, clients can access a key-value store either by READing directly from the server's memory <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b10">13,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b25">28]</ref>, or via RPCs as in HERD <ref type="bibr" target="#b17">[20]</ref>. Our results show that achieving peak performance on even the most powerful NICs does not require a prohibitive amount of CPU power: only 4 cores are needed to saturate the fastest PCIe links. Therefore, CPU-involving designs will not be limited by CPU processing power, provided that their application-level processing permits so. <ref type="table" target="#tab_16">Table 5</ref> compares the throughput of header-only and payload-carrying regular RECVs ( <ref type="figure">Figure 6</ref>). In our experiment, multiple client machines issue SENDs to one server machine that posts RECVs. On CIB, avoiding the payload DMA with header-only RECVs increases throughput by 49% from 82 Mops to 122 Mops, and makes them as fast as inbound WRITEs <ref type="figure" target="#fig_0">(Figure 12a</ref>).4 <ref type="table" target="#tab_16">Table 5</ref> also compares header-only and regular SENDs ( <ref type="figure" target="#fig_4">Figure 5</ref>). Header-only SENDs use single-cache line WQEs and achieve 54% higher throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">UD RECVs</head><p>Design implications Developers avoid RECVs at performance critical machines as a rule of thumb, favoring the faster READ/WRITE verbs <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b17">20]</ref>. Our work provides the exact reason: RECVs are slow due to the CQE DMA; they are as fast as inbound WRITEs if it is avoided.</p><p>Speculation Current RDMA implementations allow 4 bytes of application data in the packet header of header4Inline-receive improves regular RECV throughput from 22 Mops to 26 Mops on CX3, but is not yet supported for UD on CIB.</p><p>only SENDs. For applications that require larger messages, header-only SEND/RECV can be used if speculation is possible; we demonstrated such a design for an 8-byte sequencer in Section 4.2. In general, speculation works as follows: clients transmit their expected response along with requests, and get a small confirmation response in the common case. For example, in a key-value store with client-side caching, clients can send GET requests with the key and its cached version number (using a WRITE or regular SEND). The server replies with a header-only "OK" SEND if the version is valid.</p><p>There are applications for which 4 bytes of per-message data suffices. For example, some database tables in the TPC-C <ref type="bibr" target="#b26">[29]</ref> benchmark have primary key size between 2 and 3 bytes. A table access request can be sent using a header-only SEND (using the remaining 1 byte to specify the table ID), while the response may need a larger SEND. <ref type="figure" target="#fig_0">Figure 12</ref> shows the measured throughput of inbound READs and UC WRITEs, and the InfiniBand bandwidth limit of inbound READs. We do not show the InfiniBand limit for WRITEs and the PCIe limits as they are higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Non-batched operations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Inbound READs and WRITEs</head><p>Bottlenecks On our clusters, inbound READs and WRITEs are initially bottlenecked by the NIC's processing power, and then by InfiniBand bandwidth. The payload size at which bandwidth becomes a bottleneck depends on the NIC's processing power relative to bandwidth. For READs, the transition point is approximately 128 bytes, 256 bytes, and 64 bytes for CX, CX3, and CIB, respectively. CIB NICs are powerful enough to saturate 112 Gbps with 64-byte READs, whereas CX3 NICs require 256-byte READs to saturate 56 Gbps.</p><p>Implications The transition point is an important factor for systems that make tradeoffs between the size and number of READs: For key-value lookups of small items (∼32 bytes), FaRM's key-value store <ref type="bibr" target="#b10">[13]</ref> can use one large (∼256-byte) READ. In a client-server design where inbound READs determine GET performance, this design performs well on CX3 because 32-and 256-byte READs have similar throughput; other designs such as DrTM-KV <ref type="bibr" target="#b27">[30]</ref> and Pilaf <ref type="bibr" target="#b21">[24]</ref> that instead use 2-3 small READs may provide higher throughput on CIB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Outbound READs and WRITEs</head><p>For brevity, we only present a summary of the performance of non-batched outbound operations on CIB. Outbound UC WRITEs larger than 28 bytes, i.e., WRITEs with WQEs spanning more than one cache line (Table 1), achieve up to 80 Mops and are bottlenecked by PCIe MMIO throughput, similar to non-batched outbound SENDs <ref type="figure" target="#fig_0">(Figure 11a)</ref>  <ref type="figure" target="#fig_0">Figures 13b and 13c</ref>, we show the cumulative RDMA request rate, and the extent of WQE cache misses using the PCIeRdCur counter rate. Each thread waits for the N requests to complete before issuing the next window. We use all 14 cores on the server to generate the maximum possible request rate, and RC transport to include cache misses generated while processing ACKs for WRITEs. We make the following observations, showing the importance of the WQE cache in improving and understanding RDMA throughput:</p><p>• The optimal window size for maximum throughput is not obvious: throughput does not always increase with increasing window size, and is dependent on the NIC. For example, N = 16 and N = 512 maximize READ throughput on CX3 and CIB respectively.</p><p>• Higher RDMA throughput may be obtained at the cost of PCIe reads. For example, on CIB, both READ throughput and PCIe read rate increases as N increases. Although the largest N is optimal for a machine that only issues outbound READs, it may be suboptimal if it also serves other operations. • CIB's NIC can handle the CPU's peak WQE injection rate for WRITEs and never suffers cache misses. This is not true for READs, indicating that they require more NIC processing than reliable WRITEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Atomic operations</head><p>NIC processing units contend for locks during atomic operations (Section 3.4). The performance of atomics depends on the amount of parallelism in the workload with respect to the NIC's internal locking scheme. To vary the amount of parallelism, we create an array of Z 8-byte counters in a server's memory, and multiple remote client processes issue atomic operations on counters chosen randomly at each iteration. <ref type="figure" target="#fig_0">Figure 14</ref> shows the total client throughput in this experiment. For CX3, it remains 2.7 Mops irrespective of Z; for CIB, it rises to 52 Mops.</p><p>Inferring the locking mechanism The flatness of CX3's throughput graph indicates that it serializes all atomic operations. For CIB, we measured performance with randomly chosen pairs of addresses and observed lower performance for pairs where both addresses have the same 12 LSBs. This strongly suggests that CIB uses 4096 buckets to slot atomic operations by address-a new operation waits until its slot is empty.</p><p>Bottlenecks and implications Throughput on CX3 is limited by PCIe latency because of serialization. For CIB, buffering and computation needed for PCIe read-modifywrite makes NIC processing power the bottleneck. The abysmal throughput for Z = 1 on both NICs reaffirms that atomics are a poor choice for a sequencer; our optimized sequencer in Section 4 provides 12.2x higher performance with a single server CPU core. A lock service for data stores, however, might use a larger Z. Atomics could perform well if such an application used CIB, but they are very slow with CX3, which is the NIC used in prior work <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b27">30]</ref>. With CIB, careful lock placement is still necessary. For example, if page-aligned data records have their lock variables at the same offset in the record, all lock requests will have the same 12 LSBs and will get serialized. A deterministic scheme that places the lock at different offsets in different records, or a scheme that keeps locks separate from the data will perform better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>High-performance RDMA systems Designing highperformance RDMA systems is an active area of research. Recent advances include several key-value storage systems <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b10">13,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b25">28]</ref> and distributed transaction processing systems <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b9">12,</ref><ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b8">11]</ref>. A key design decision in each of these systems is the choice of verbs, made using a microbenchmark-based performance comparison. Our work shows that there are more dimensions to these comparisons than these projects explore: two verbs cannot be exhaustively compared without exploring the space of low-level factors and optimizations, each of which can offset verb performance by several factors.</p><p>Low-level factors in network I/O Although there is a large body of work that measures the throughput and CPU utilization of network communication <ref type="bibr" target="#b15">[18,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b10">13,</ref><ref type="bibr" target="#b17">20]</ref>, there is less existing literature on understanding the lowlevel behavior of network cards. NIQ <ref type="bibr" target="#b12">[15]</ref> presents a highlevel picture of the PCIe interactions between an Ethernet NIC and CPUs, but does not discuss the more subtle interactions that occur during batched transfers. Lee et al. <ref type="bibr" target="#b19">[22]</ref> study the PCIe behavior of Ethernet cards using a PCIe protocol analyzer, and divide the PCIe traffic into Doorbell traffic, Ethernet descriptor traffic, and actual data traffic. Similarly, analyzing RDMA NICs using a PCIe analyzer may reveal more insights into their behavior than what is achievable using PCIe counters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Designing high-performance RDMA systems requires a deep understanding of low-level RDMA details such as PCIe behavior and NIC architecture: our best sequencer is ∼50x faster than an existing design and scales perfectly, our optimized HERD key-value store is up to 83% faster than the original, and our fastest transmission method is up to 3.2x faster than the commonly-used baseline. We believe that by presenting clear guidelines, significant optimizations based on these guidelines, and tools and experiments for low-level measurements on their hardware, our work will encourage researchers and developers to develop a better understanding of RDMA hardware before using it in high-performance systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Hardware components of a node in an RDMA cluster</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PCIe background</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Inbound and outbound verbs at the server.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Optimizations for issuing two 4-byte UD SENDs. A UD SEND WQE spans 2 cache lines on Mellanox NICs because of the 68-byte header; we shrink it to 1 cache line by using a 4-byte header field for payload. Arrow notation follows Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 and</head><label>5</label><figDesc>Figure 5 and Figure 6 summarize these two guidelines for UD SENDs and RECVs, respectively. These two verbs are used extensively in our evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Improvement in HERD's throughput with 5% PUTs</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Inbound READ and UC WRITE throughput, and the InfiniBand limit for READs. Note the different scales for Y axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: PCIe model showing possible WQE cache misses, and measurement of WQE cache misses for READs and RC WRITEs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Our contributions rely on understanding the PCIe interaction between NICs and CPUs. Although precise PCIe analysis requires expensive PCIe analyzers or proprietary/confidential NICs manuals, PCIe counters available on modern CPUs can provide several useful</figDesc><table>Gen Bitrate Per-lane b/w 
Request 
Completion 
2.0 
5 GT/s 
500 MB/s 
24 B 
20 B 
3.0 
8 GT/s 
984.6 MB/s 
26 B 
22 B 

(a) Speed and header sizes for PCIe generations. Lane band-
width excludes physical layer encoding overhead. 

�� 

���� 

���� 

���� 

���� 

�� 
��� ���� ���� ���� ���� ���� ���� ���� 

����������������� 

��������� 

��� 
���� 

(b) CPU-to-device PCIe traffic for an x-byte transfer with 
DMA and MMIO, assuming PCIe 3.0 and C rc = 128 bytes. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>3In this paper, we assume that the NIC reads all new</head><label>3In</label><figDesc></figDesc><table>WQEs in one 
DMA, as is done by Mellanox's Connect-IB and newer NICs. Older 
Mellanox NICs read one or more WQEs per DMA, depending on the 
NIC's proprietary prefetching logic. 

1, 2 

Dbell MMIO 

WQE DMA 

CPU 
NIC 

Payload, CQE 

1 

1 

Payload, CQE 

2 

2 

Dbell MMIO 

WQE DMA 

CPU 
NIC 

CQE 1 

CQE 2 

Header-only RECV: CQE contains application data 
from SEND header; saves a PCIe transaction 

Inline RECV: 
CQE contains 
payload; saves a 
PCIe transaction 

Dbell MMIO 

WQE DMA 

CPU 
NIC 

CQE 1 

CQE 2 

1, 2 

1, 2 
1, 2 

1, 2 
1, 2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Measurement clusters. CX is NSF PRObE's Nome 
cluster [17], CX3 is Emulab's Apt cluster [31], and CIB is a 
cluster at NetApp. CX uses PCIe 2.0 at 2.5 GT/s. 

large (e.g., 2 MB) pages, and QP state cache misses by 
using fewer QPs [13]. We make two new contributions in 
this context: 

Detecting cache misses All types of NIC cache misses 
are transparent to the application and can be difficult to de-
tect. We demonstrate how PCIe counters can be leveraged 
to accomplish this, by detecting and measuring WQE 
cache misses (Section 5.3.2). In general, subtracting the 
application's expected PCIe reads from the actual reads 
reported by PCIe counters gives an estimate of cache 
misses. Estimating expected PCIe reads in turn requires 
PCIe models of RDMA operations (Section 5.1). 

WQE cache misses The initial work queue element 
transfer from CPU to NIC triggers an insertion of the 
WQE into the NIC's WQE cache. When the NIC eventu-
ally processes this WQE, a cache miss can occur if it was 
evicted by newer WQEs. In Section 5.3.2, we show how 
to measure and reduce these misses. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>We name the clusters 
with the initials of their NICs, which is the main hardware 
component governing performance. CX3 and CIB run 
Ubuntu 14.04 with Mellanox OFED 2.4; CX runs Ubuntu 
12.04 with Mellanox OFED 2.3. Throughout the paper, 
we use WQE-by-MMIO for non-batched operations and 
Doorbell for batched operations. However, when batching 
is enabled but the available batch size is one, WQE-by-
MMIO is used. (Doorbell provides little CPU savings for 
transferring a single small WQE, and uses an extra PCIe 
transaction.) For brevity, we primarily use the state-of-
the-art CIB cluster in this section; Section 5 evaluates our 
optimizations on all clusters. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Baseline +RPC opts Spec-S0 Atomics</head><label>Baseline</label><figDesc></figDesc><table>Throughput 26 
97.2 
122 
2.24 
Bottleneck CPU 
DMA bw 
NIC 
PCIe RTT 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="true"><head>Table 3 : Sequencer throughput (Mrps) and bottlenecks on CIB</head><label>3</label><figDesc></figDesc><table>any worker. The worker's application processing consists 
of atomically incrementing the shared counter by one. 
When Doorbell batching is enabled, we use an additional 
application-level optimization to reduce contention for 
the shared counter: after collecting N requests, a worker 
atomically increments the shared counter by N, thereby 
claiming ownership of a sequence of N consecutive inte-
gers. It then sends these N integers to the clients using a 
batched Doorbell (one integer per client). 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head></head><label></label><figDesc>. Higher scal- ability necessitates exclusive use of datagram transport which only supports SEND/RECV verbs. The challenge then is to use SEND/RECV instead of WRITEs for se- quencer requests without sacrificing server performance. Second, it uses PCIe inefficiently: UD SEND work queue elements on Mellanox's NICs span ≥ 2 cache lines be- cause of their 68-byte header (Table 1); sending 8 bytes of useful sequencer data requires 128 bytes (2 cache lines) to be DMA-ed by the NIC.</figDesc><table>�� 

�� 

�� 

�� 

�� 

��� 

�� 
��� 
��� 
��� 
��� 
���� 
���� 
���� 

�������������������������� 

����������������� 

����������� 
�������� 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Throughput and bottleneck of different modes of RDMA verbs on CIB. HO denotes the header-only optimization. 

�� 

��� 

���� 

���� 

���� 

�� 
��� 
���� 
���� 
���� 
���� 

������������������� 

�������������������� 

���������������������� 
����������������� 
������������ 

(a) CIB 

�� 

��� 

��� 

��� 

��� 

��� 

�� 
��� 
���� 
���� 
���� 
���� 

�������������������� 

(b) CX3 

�� 

��� 

��� 

��� 

�� 
��� 
���� 
���� 
���� 
���� 

�������������������� 

(c) CX 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15" validated="false"><head>WQE cache misses for READs �� ��� ��� ��� ��� ���� ����</head><label></label><figDesc></figDesc><table>Payload, CQE DMA 

WQE DMA: cache miss 
while generating request 

RDMA request 
RDMA response 

WQE DMA: cache miss 
while servicing response 

CPU 
NIC 

WQE MMIO: inserts 
WQE into cache 

1 

1 

1 

1 

1 

(a) PCIe model for reliable verbs 

�� 

��� 

��� 

��� 

��� 

���� 

�� 
�� 
�� 
�� ��� ��� ��� ���� ���� ���� 

����������������� 

��������������������������������� 

������ 
���������� 
������ 
���������� 

(b) �� 
�� 
�� 
�� ��� ��� ��� ���� ���� ���� 
��������������������������������� 

������ 
���������� 
������ 
���������� 

(c) WQE cache misses for RC WRITEs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Per-NIC rate (millions/s) for header-only (0) and regu-
lar (≥ 1) SENDs and RECVs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17" validated="false"><head>. READs achieve up to 88 Mops and</head><label></label><figDesc></figDesc><table>�� 
��� 
��� 
��� 
��� 
��� 
��� 

�� 
�� 
��� 
��� 
���� 
����� 
����� 

����������������� 

����������������������������� 

��� 
��� 

Figure 14: Atomics throughput with increasing concurrency 

are bottlenecked by NIC processing power. 

Achieving high outbound throughput requires main-
taining multiple outstanding requests via pipelining. 
When the CPU initiates an RDMA operation, the work 
queue element is inserted into the NIC's WQE cache. 
However, if the CPU injects new WQEs faster than the 
NIC's processing speed, this WQE can be evicted by 
newer WQEs. This can cause cache misses when the 
NIC eventually processes this WQE while generating its 
RDMA request packets, while servicing its RDMA re-
sponse, or both. Figure 13a summarizes this model. 
To quantify this effect, we conduct the following ex-
periment on CIB: 14 requester threads on a server issue 
windows of N 8-byte READs or WRITEs over reliable 
transport to 14 remote processes. In </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. WQE-by-MMIO and Doorbell PCIe use</head><p>We denote the doorbell size by d. The total data transmitted from CPU to NIC with the WQE-by-MMIO method is T b f = 10 * (񮽙65/64񮽙 * (64 + P r )) bytes. With cache line padding, 65-byte WQEs are laid out in 128-byte slots in host memory; assuming C rc = 128, T db = (d +P r )+(10 * (128+P c )) bytes. We ignore the PCIe linklayer traffic since it is small compared to transaction-layer traffic: it is common to assume 2 link-layer packets (1 flow control update and 1 acknowledgment, both 8 bytes) per 4-5 TLPs <ref type="bibr" target="#b6">[9]</ref>, making the link-layer overhead &lt; 5%. Substituting d = 8 gives T b f = 1800, and T db = 1534.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.intel.in/content/dam/www/public/us/en/documents/datasheets/atom-c2000-microserver-datasheet.pdf" />
		<title level="m">Intel Atom Processor C2000 Product Family for Microserver</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno>E5-1600/2400/2600/4600</idno>
		<ptr target="http://www.intel.com/content/dam/www/public/us/en/documents/datasheets/xeon-e5-v3-datasheet-vol-2.pdf" />
		<title level="m">Intel Xeon Processor</title>
		<imprint/>
	</monogr>
	<note>v3 Product Families</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Intel Xeon Processor E5-1600/2400/2600/4600 (E5-Product Family) Product Families</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://www.intel.in/content/dam/www/public/us/en/documents/product-briefs/xeon-processor-d-brief.pdf" />
		<title level="m">Intel Xeon Processor D-1500 Product Family</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Intel Xeon Phi Processor Knights Landing Architectural Overview</title>
		<ptr target="https://www.nersc.gov/assets/Uploads/KNL-ISC-2015-Workshop-Keynote.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofed</forename><surname>Mellanox</surname></persName>
		</author>
		<ptr target="http://www.mellanox.com/related-docs/" />
		<title level="m">prod _ software/Mellanox _ OFED _ Linux _ User _ Manual _ v2</title>
		<imprint/>
	</monogr>
	<note>.2-1.0.1.pdf</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding Performance of PCI Express Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CORFU: a shared log design for flash clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th USENIX NSDI</title>
		<meeting>9th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The end of slow networks: It&apos;s time for a redesign</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Binnig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Çetintemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crotty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galakatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zamanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Zdonik</surname></persName>
		</author>
		<idno>abs/1504.01048</idno>
		<ptr target="http://arxiv.org/abs/1504.01048" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast and general distributed transactions using RDMA and HTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM European Conference on Computer Systems (EuroSys)</title>
		<meeting>11th ACM European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">FaRM: Fast remote memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dragojevi´cdragojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th USENIX NSDI</title>
		<meeting>11th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">No compromises: Distributed transactions with consistency, availability, and performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dragojevi´cdragojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Renzelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>25th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Network interface design for low latency request-response protocols</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Flajslik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Annual Technical Conference</title>
		<meeting>USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Comparison of frameworks for high-performance packet io</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gallenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Emmerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wohlfart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ANCS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">PRObE: A Thousand-Node Experimental</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Grider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>Cluster for Computer Systems Research</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">PacketShader: a GPU-accelerated software router</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2010-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Packet processing at 100 Gbps and beyond -challenges and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hauger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirstaedter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohlendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Scharf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Photonic Networks, 2009 ITG Symposium on</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using RDMA efficiently for key-value services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2014-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Andersen. Raising the bar for using GPUs in software packet processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th USENIX NSDI</title>
		<meeting>12th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Platform io dma transaction acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CACHES. ACM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MICA: A holistic approach to fast in-memory keyvalue storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th USENIX NSDI</title>
		<meeting>11th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using one-sided RDMA reads to build a fast, CPU-efficient key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Annual Technical Conference</title>
		<meeting>USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling Memcache at Facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fugal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcelroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th USENIX NSDI</title>
		<meeting>10th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">netmap: a novel framework for fast packet I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX conference on Annual Technical Conference</title>
		<meeting>the 2012 USENIX conference on Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Designing a low-latency cuckoo hash table for writeintensive workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szepesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSRC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Nessie: A decoupled, client-driven, keyvalue store using RDMA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szepesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cassell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno>CS- 2015-09</idno>
		<imprint>
			<date type="published" when="2015-06" />
			<pubPlace>Waterloo, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo, David R. Cheriton School of Computer Science</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tpc-C</forename><surname>Tpc</surname></persName>
		</author>
		<ptr target="http://www.tpc.org/tpcc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast in-memory transaction processing using RDMA and HTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 25th Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An integrated experimental environment for distributed systems and networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lepreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guruprasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Newbold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joglekar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th USENIX OSDI</title>
		<meeting>5th USENIX OSDI</meeting>
		<imprint>
			<date type="published" when="2002-12" />
			<biblScope unit="page" from="255" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High Performance Ethernet Forwarding with CuckooSwitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)</title>
		<meeting>9th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
