<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effects of Data Center Vibration on Compute System Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2009-09-21">September 21, 2009 until October 2nd, 2009,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Turner</surname></persName>
							<email>julian.turner@qassociates.com</email>
							<affiliation key="aff0">
								<orgName type="institution">CTO -Q Associates</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Effects of Data Center Vibration on Compute System Performance</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-09-21">September 21, 2009 until October 2nd, 2009,</date>
						</imprint>
					</monogr>
					<note>Abstract From</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Q Associates performed a series of tests and benchmarks to determine the effect of data center and system vibration on the I/O performance of an end-to-end compute environment. These tests revealed that ambient vibration inherent in a world-class, raised floor data center caused performance degradation of up to 246% for random reads and up to 88% degradation for random writes for an enterprise class storage system. This loss of performance due to environmental vibration results in a commensurate increase in energy usage for equivalent work to be performed. A prototype anti-vibration rack was tested within the same environment and shown to significantly reduce or eliminate the detrimental vibration effects resulting in significantly increased performance. A &quot;latent performance effect&quot; was also discovered and analyzed associated with the testing. This effect is a potential source of traditional benchmark error and would likely not be detected by normal benchmark procedures and tests. The study of this effect further collaborates the impact that vibration has on overall system performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The reduction of vibration within the IT environment has long been considered an important attribute both at the macro-data center level <ref type="bibr" target="#b0">[1]</ref> and the micro-component level <ref type="bibr">[2]</ref>. At the macro data center level, the reduction of vibration primarily centers on personal comfort and safety <ref type="bibr" target="#b1">[3]</ref> and impact on the physical components such as cable connectors or system attachment. On the micro level, there are countless methods for reducing vibration associated with individual hard drives <ref type="bibr" target="#b2">[4]</ref>. These methods and commercially available products are primarily aimed at reducing disc drive failure or reducing perceived audible noise <ref type="bibr">[5]</ref>  <ref type="bibr">[6]</ref>. None of the researched products imply increased performance as a primary benefit.</p><p>The first relevant reference on the potential impact of vibration on hard drive performance was an IEEE paper by Ruwart and Lu in 2005 <ref type="bibr" target="#b3">[7]</ref>. In this paper, a number of consumer and enterprise-grade hard drives were tested. In general, it was shown that 3.5" consumer grade hard drives were moderately to significantly impacted by vibration with with 2.5" enterprise grade typically being much less susceptible. Since 2005, hard drives have become faster and much higher capacity <ref type="bibr">[8]</ref>. In addition, the combination of hard drives into large arrays of tens or hundreds of TeraBytes has become much more common <ref type="bibr">[9]</ref>  <ref type="bibr">[10]</ref>.</p><p>The first demonstration of the impact of vibration on one of these high capacity arrays was done by Brendan Gregg of Sun Microsystems in a YouTube video commonly referred to as the "Yell Test" <ref type="bibr">[11]</ref>. In this video, he demonstrated the direct impact on performance caused by yelling at a running storage system. By using the real-time graphical analytic tool built into the array, he visually showed the reduction in I/O during the period in which he yelled. It was this video that led to questions about the potential impact of typical data center vibration on storage system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Compute Environment and Testing Procedures</head><p>The intent of the testing was to determine the impact of "typical" data center vibration on the performance of a current generation storage array. A secondary goal was to determine if any performance degradation found could be reduced or eliminated within the data center environment using a specifically designed antivibration rack by Green Platform Corporation <ref type="bibr">[12]</ref>.</p><p>The test plan called for running a number of freely available benchmarks within two well defined environments: a specially designed sound room having virtually no external vibration as well as a production raised-floor data center. Within the data center, tests were to be performed in an industry standard metal rack enclosure as well as a specialized rack that incorporated full-frequency anti-vibration modules.</p><p>The storage array selected for the tests was a Sun 7110 array with 16 x 300GB, 10k 2.5" Seagate enterprise class SAS hard drives. The Sun array was selected due to its manageable size and the inclusion of the real-time GUI D-Trace Analytics software that comes with the system <ref type="bibr">[13]</ref>. The benchmark source system was a Sun X4440 16 core system with 64GB of RAM. These two systems were connected using four Gigabit Ethernet connections running NFSV4 via a non-blocking switch. With this configuration, network performance of a sustained 280 MBytes/sec (2.2 Gbits/sec) could be maintained with temporary spikes in excess of 320 MBytes/sec (2.6 Gbits/sec) recorded. Additional optimization was not performed since it was felt that this bandwidth was more than sufficient for testing purposes.</p><p>For all tests, the Sun Analytics tool was used to capture 12 different system characteristics including disk I/O broken down by disk, disk I/O broken down by latency, and network interface bytes per second broken down by interface. Information on CPU utilization, memory utilization, and cache misses was also recorded.</p><p>Two configurations of the 7110 were tested. For configuration #1, the 7110 utilized a mirrored file system across all 16 drives. Two drives were used for OS and the remaining 14 drives for data. For this phase, all drives were originally blank and data was deleted after each test. For configuration #2, 10 drives were taken off-line. The remaining 6 drives were in a mirrored configuration with 2 drives for OS and the remaining 4 for data. In this configuration, there was a total of 570GB available for data storage. The entire 570GB was filled with files ranging in size from 1k to 75GB. Approximately 400 files were semi-randomly deleted to give back 37 GB of storage for use in testing. The smallest file size deleted was 50k with the largest being 1GB. Approximately ten 1GB files were deleted, thirty 250MB or 500MB files, and the remainder were 100MB or less.</p><p>The use of configuration #2 was viewed as justified as a means to better approximate "real world" conditions where there would be a much closer match of Host devices to Target hard drives within a typical NAS or HPC environment. In addition, it was concluded that very seldom would their be a "blank disk" condition in a "real world" environment so adding data to the drives in a random manner would be a better approximation of actual working conditions. The expectation was also that a smaller number of hard drives would make it easier to see performance anomalies via the GUI if the anomalies existed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Physical Environments</head><p>The first test environment was a specially constructed sound room in Bastrop, Texas. Ambient noise was less than 40dB and there were no commercial sources of vibration within 2 miles or pedestrian sources of vibration (traffic, etc.) within Â¼ mile of the location. Equipment was placed on a rubber pad directly on a concrete foundation.</p><p>The second test environment was a Tier 1 data center located in Houston, Texas. This Tier 1 data center is a world-class raised floor data center specializing in hosted systems. This environment is representative of most large enterprise data centers and included vibration sources from other compute systems, air handling/AC equipment, and UPS units. Ambient noise was measured at approximately 82dB at a location 2 feet from the rack being tested. Vibration was measured at several locations both on servers and racks through-out the data center and typically were .3 m/s2 RMS (.03 GRMS) or below for any single vibration axis. The Tier 1 data center furnished a standard CPI (APC style) metal stand-alone rack with 40 Amps of power. The general layout for the environment is shown in <ref type="figure">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Test Environment #1</head><p>Since the time available at the Tier 1 data center was limited and it was critical to have a non-vibration baseline, initial configuration, performance testing, and baseline benchmark testing were performed in a  environment. This facility allowed all outside vibration and sound influences to be greatly reduced/eliminated. The ambient sound pressure with no systems on was measured at 38.2dB. At a distance of 1 foot from the front of the Sun 7110, the pressure was 84.7dB at initial system start-up with all fans working at maximum, and 68.5dB at typical system idle. Temperature in the room ranged from 80-85 degrees through-out the testing.</p><p>The first step was to perform an initial baseline and determine the optimal configuration for testing. The Sun X4440 was configured as an NFS4 Client with all 4 GigE ports trunked together to form a single virtual connection. The Sun 7110 was configured as a NFS4 server with each port assigned a separate IP address. With four separate mount points on the X4440 client, separate 10GigaByte files could be sent to each mount point. The Sun Analytics feature of the 7110 was then used to view the NFS throughput coming into each 7110 port as well as CPU utilization and raw disk throughput and utilization.</p><p>With sustained average throughput for serial transfer of over 280Mbytes/sec via the NFSV4 ports, the CPU utilization of the 7110 remained in the 75%-85% range and never went above 85%. With the data being spread over 16 hard drives, actual hard drive throughput for any individual hard drive was extremely low (&lt;5% utilization on average) and very spiky in nature due to RAM caching on the 7110. Maximum disk drive latency was typically less than 120 milliseconds and less than 10 milliseconds was typical. Due to the low disk utilization and the initial focus on serial versus random I/O, the storage system was then reconfigured as explained in Section 2.</p><p>IOZone had originally been selected as the primary benchmarking tool. However, after several tests it was evident via Analytics that IOZone was not sufficiently stressing the hard drives. As such, Solaris FileBench was then selected since it had both Micro and Macro benchmark capabilities and could be easily modified if required.</p><p>Prior to moving the system to Tier 1 data center, the following FileBench scripts were run:</p><p>FileMacro: Consists of File Server, Varmail, Web Proxy, and Web Server workloads. Large database OLTP workloads were also available but were not run due to an error in the sub-script causing system lockup.</p><p>FileIO: Consists of RandomRead2k, RandomRead8k, RandomRead1m, RandomWrite2k, RandomWrite8k, RandomWrite1m, SingleStreamRead1m, SingleStreamReadDirect1m, SingleStreamWrite1m, and SingleStreamWriteDirect1m workloads. Due to target disk space limitations, multi-stream reads and writes were not performed. Prior testing had shown that multistream reads and writes were roughly equivalent to single stream reads and writes for this system so removal of them from the script was viewed as acceptable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Test Environment #2</head><p>After completion of the initial baseline testing at Environment #1, all systems were moves to the Tier 1 data center in Houston. Due to a shipping error, only one slide-rail kit was available for attaching either the X4440 or 7110 to the rack. This added an additional variable to the testing but ultimately worked out well since it allowed the top system to be moved from one rack to the other without being turned off.</p><p>For the first round of testing, the slide-rail kit was attached to the 7110 and mounted in the metal CPI rack. The X4440 was placed on top of the 7110 with a rubber no-slip pad between them. The D-Link switch was mounted directly to the metal rack and was not moved for the duration of the testing.</p><p>The amount of measured vibration in the Tier 1 data center was actually lower than expected. Even though you could physically feel considerable vibration of the metal racks, the actual measured vibration was typically .2 m/s2 RMS (.02 GRMS) or less in all three axis. Since the Reed vibration meter only measured up to 1kHz, the belief is that there is an additional vibration component above 1kHz that was not captured. During power-up, both the 7110 and X4400 were measured at 1.6 m/s2 RMS (.16 GRMS) along a single vertical axis. At steady state, both servers were less than .4 m/s2 RMS (.04 GRMS) depending on where the measurement was taken.</p><p>Testing with the anti-vibration rack was done in one of two manners. The 7110 was either sat on a plexiglass shelf that was supported by an anti-vibration module or was supported by a composite slide-rail that slid on an anti-vibration module. Since the antivibration module was roughly equivalent in both cases, it was assumed that the anti-vibration characteristics would be roughly equivalent. Testing indicated that the properties were not exactly the same but close enough to demonstrate the objectives of these tests. Since the rack was a prototype, testing the exact performance difference between the two methods of placement was not warranted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FileBench FileIO Test</head><p>The FileBench FileIO test is a publicly available multi-part benchmark consisting of several sub microbenchmarks. Ten separate runs were performed in Environment #2 with the results shown in <ref type="figure">Figure 2</ref> above.</p><p>For this data-set, single stream read performance was better in the anti-vibration rack by approximately 13%. Single stream write was better in the antivibration rack by approximately 4% which corresponds and collaborates the testing done in other test cases. However, there was a shocking 246% performance difference for the RandomRead1m test and a 56% and 61% difference for the Random Read 2k and 8k tests respectively. The performance difference of Random Writes was similarly compelling with 52% difference for the 2k case, 88% difference for the 8k case, and 34% difference for 1m test. While the number of runs comprising the data-set in <ref type="figure">Figure 2</ref> was relatively small by statistical standards, this data is loosely correlated and confirmed by over 500 runs performed during the 2 weeks of testing. Conservatively, an average performance increase of Sequential Write = 2%+, Sequential Read = 7%+, Random Read = 80%+, and Random Write = 50%+ can be easily supported and defended by the full test data.</p><p>Since the original focus had been on sequential data transfer, the obtained random I/O results were quite surprising. Upon observing the much higher impact that vibration was having on the random I/O, test scripts were modified to remove the sequential portion. This significantly reduced the amount of time required for each test run and allowed other trends to become more visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Performance Effect</head><p>Prior to analyzing the data, one of the most puzzling aspects of testing was that the performance of the "next run" was never what was predicted. A trending was noticed so test scripts were optimized to look specifically at this trending. For these tests, the 7110 was not turned off or rebooted between moves.</p><p>As shown in the numbers within <ref type="figure">Figure 3</ref>, there exists a "latent performance effect" by which once a system is moved, the performance remains roughly equivalent to the previous system for some amount of time. Rebooting the system seems to erase the "bad performance effect" and thus allow the system to work at optimal speed for a short period of time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C o n tro l M etal M etal M etal AV R AV R AV R M etal M etal M etal AV R % D iff</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure #3: Modified FileIO Operations per Second</head><p>Looking at this graphically over a long series of runs as shown in <ref type="figure">Figure 4</ref>, it is easy to see the trend. The graph above shows 19 consecutive runs for Random Write with 2k block size. In this figure, the runs on the anti-vibration rack are shown in black (runs 1 and 6-11) and the runs using the standard metal rack are shown in light gray.</p><p>A review of the raw data in the Analytics tool seems to indicate that the system uses some pre-set algorithm to determine the appropriate system speed relative to experienced disk latency. A fast run that has significant disk I/O latency will be followed by a slower run with much lower latency. If the latency is below some threshold, the disk I/O speed is increased by the system by some factor dependent on the latency of the previous run. This explains why there is not a gradual up or down slope but a very ragged, oscillating slope as the system finds equilibrium.</p><p>The graph in <ref type="figure" target="#fig_2">Figure 5</ref> below is a good example of that oscillating slope phenomenon. Run #5 was a steady-state run by which the disk latency was at equilibrium for being in the vibrating metal rack. When the 7110 was then moved to the anti-vibration rack, the system kept the same reduced performance but experienced much lower disk I/O latency. Thus, for Run #7 the system was allowed to go "full throttle" but again experienced latency and this was throttled back for Run #8. Had the system been allowed to stay in the anti-vibration rack, it would have likely reached equilibrium close to the performance of Run #12. Run #12 had very good performance but saw significant latency above 200ms. As such, the disk I/O was significantly throttled back for run #13 where it reached equilibrium at a much lower performance than on the anti-vibration rack.</p><p>A key point is that if this latency effect is not taken into account, it could be a source of significant error in standard benchmarks. Worse yet, if someone did not know to be looking for the effect, it likely would not be caught by standard benchmark test cases.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Dissipation versus Transmittal</head><p>One observation is that performance was consistently worse in the situation where the X4440 was sitting on the 7110 versus the other way around. One possible explanation is that the X4440 transferred vibration directly to the 7110 when it was sitting on top of it but transmitted some of the this vibration to the rails when it was in the slide-rail kit and the 7110 was sitting on top. In a full rack, this could potentially mean significant vibration being transferred to the rails and thus other systems within the same rack. Another possible explanation is that the rack transfers vibration directly to the 7110 when it is attached directly via the slide-rails.</p><p>To test the theory on the effect of server placement, additional tests were conducted in the control environment. The FileIO benchmark was run a minimum of 4 times for three separate cases: 1) 7110 sitting on an unpowered server on a concrete floor with X4440 sitting next to it 2) 7110 sitting on an unpowered server on a concrete floor with X4440 sitting on top of the 7110 3) 7110 sitting on an power server on a concrete floor with X4440 sitting on top of the 7110 A small but measurable performance reduction could be seen in both transitions. A roughly 2%-3% performance reduction could be seen when the X4440 was placed on the 71110 and an additional 1%-2% was seen when the bottom server was turned on. While there were some fluctuations from run to run, the average of all runs was roughly equivalent to the performance of the 7110 in the anti-vibration rack while at the Tier 1 data center. This is significant since it provides further proof that it is the data center environment and the interaction of vibration via the metal rack that causes the most significant performance degradation. This also shows that a "simulated" vibration environment using other servers as vibration source is not sufficient for adequately testing performance degradation associated with external vibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Analytics Assessment</head><p>The use of the Sun D-trace based Analytics tool provided essential clues as to the detrimental impact that vibration was causing. Analysis of the Analytics output shows that only a small difference in disk latency can cause a huge difference in overall system performance. Within the FileBench FileMacro benchmark, there is a steady-state performance of 300-350 disk I/O ops/sec with regular bursts of 700-725 ops/sec. During this state, CPU usage is less than 3%, NFSV4 latency is less than 19 milliseconds, and memory cache usage is less than 2% for both high performance and low performance runs. The differentiator between high and low performance runs is a small percentage of disk I/O latency above 200 milliseconds. For a high performance run, approximately 11% of I/O will have a value between 100-190 milliseconds. As long as the maximum disk I/O remains below 200 milliseconds, no stoppage in network I/O is seen. The low performance runs look fairly similar to the high performance runs with approximately 12%-15% of the I/O over 100 milliseconds (similar to high performance case) and 4%-6% being over 200 milliseconds (average of approximately 234 milliseconds). For each case when a disk I/O latency greater than 200 milliseconds is seen, the network I/O goes to zero for a brief period of time. Therefore, an average increase of disk I/O latency of less than 5% causes an overall system performance decrease of 40%+.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">So Why is this Relevant to "Me"?</head><p>The amount of system compute time required to perform a set amount of work is relevant whether you are a system administrator, application developer, or a data center designer. While the ability to finish a "job" by x% faster is an obvious but subjective benefit of vibration reduction, the impact it has on energy utilization can be a double-edged sword. With Power Utilization Efficiency (PUE) ranging anywhere from 1.19 claimed by Google [14] to 2.5 or greater for legacy raised floor data centers <ref type="bibr" target="#b4">[15]</ref>, every watt saved or generated has a multiplied effect.</p><p>In many Corporate enterprise environments where set jobs are run each day, decreasing the amount of time it takes for a job to run effectively reduces the energy usage. This amount is effectively: time saved * difference in idle system energy versus running system usage * PUE. An additional computational component associated with system fan speed may also be required depending on the system being analyzed. Based on a theoretical 15% performance increase, the overall energy savings for a job having a 1 hour run on a 1kW system in a 2.2 PUE data center would be approximately 138 Watts or roughly 7% energy savings <ref type="bibr">[16]</ref>.</p><p>Within a typical HPC environment, system utilization run-time can be close to 100%. In this case, as soon as one job finishes the next job runs. In this type of environment, energy utilization may actually increase as system performance improves since the workload of the server CPU goes up since there is much less wait-time on storage. While the total power increase may be marginal on a per-server basis, it could be quite significant in an HPC environment with thousands of servers.</p><p>For those that perform benchmarking for a living, the discovered results could be equally compelling for an entirely different reason. In many companies, benchmarking and performance testing is done on development or test systems that are physically separated from production systems. Thus, performance numbers obtained in a "quite" test lab with only the tested system being run might be quite different than what would be obtained if the tests were conducted in a "noisy" raised floor data center. Even if all testing is done in the data center, the time of day might influence the test results because of more or less vibration due to natural usage patterns of IT systems.</p><p>An additional concern to a benchmarking professional would be the latent performance effect. Testing seemed to indicate that the characteristics of the previous run had an effect on the subsequent run. Therefore, a test run that had a tendency to cause high latency would cause the subsequent test to have slower performance than if the first test had a lower latency profile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Community Next Steps</head><p>Given limited time, budget, and resources, only the tip of the iceberg has been discovered in relation to the effect of vibration on system performance. The testing performed answered many questions but created even more. What can be said with certainty is that vibration in a typical data center does impact performance. Whether that impact is relevant depends on the environment as well as the profile of the application.</p><p>So what are the next steps? First, the community needs to perform independent tests to validate the results in their own environment. This may be as simple as running random read/write benchmark 5 times on a standalone server in an office and then running the benchmark another 5 times with the server mounted in a rack in a production data center.</p><p>Second, the true root cause for the performance degradation needs to be analyzed and understood. It is theorized that the difference in performance between the sequential and random cases are due to increased head movement within the hard drive. Since there is significantly more seek head movement in the random read and write cases, vibration takes a greater toll on performance as the seek head tries to lock onto the data bits. Whether this is true or if there a totally unrelated cause for the performance degradation needs to be determined.</p><p>An analysis of the cause of the latent performance effect also needs to be performed. Discussions with engineers at both hard drive and complete systems companies have not resolved the issue as to whether the effect is due to settings within the hard drive BIOS or the System OS. Understanding what is leading to the latent performance effect would be significant in building benchmarks that reduce or mitigate the effect.</p><p>An understanding of the frequency and amplitude required to negatively affect system performance would be of great benefit to the community. It is obvious that an "impact" or "shock" vibration such as dropping or yelling at a server would have a negative effect on performance. However, this effect would be temporary and is not what would normally be experienced in a data center environment. The ability to feel the vibration was also observed to be a poor indicator of the effect it had on the system performance.</p><p>Finally, spread the word that vibration "might" be a performance issue within any given environment. The more people working on this, the faster a resolution will be found. There are likely several ways to resolve the performance degradation "symptom" and even more to resolve the vibration source "problem". Which of these is more cost effective will have to be researched and analyzed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Summary</head><p>The test results clearly demonstrate that random reads and writes are significantly impacted by vibration in the data center environment. Performance improvements for random reads ranged from 56% to 246% while improvements for random writes ranged from 34% to 88% for a defined set of industry benchmarks. Streaming sequential reads and writes had a much smaller performance improvement but were still measurable and potentially relevant to some environments.</p><p>By reviewing the raw data within the Analytics tool, it is clear that it only takes a small increase in disk latency caused by vibration to have a cascading effect that disproportionately effects overall system performance. Additional testing performed in the control environment further indicates that vibration has a cumulative effect and the data center environment itself (CRAC units, air movement, populated metal racks, etc.) is a primary contributor.</p><p>A final finding that was not part of the initial goals was the discovery of the "latent performance effect" associated with vibration and disk I/O latency. This effect seems to be a performance or reliability characteristic of the system and is clearly visible when viewed across a long series of runs. More importantly, this effect shows that point-in-time benchmarks can be misleading or blatantly wrong depending on the amount of time the system has been active and potentially the I/O characteristics of runs performed prior to the benchmark.</p><p>All of these support the very simple premise that data center vibration affects system performance and should be actively mitigated as part of any next-gen data center or system deployment. In the case of random reads and writes, this performance difference is significant and mitigation of vibration would have a significant positive effect on total energy usage in a data center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Author Biography</head><p>Julian Turner is currently serving as the Chief Technology Officer for Q Associates out of Houston, Texas. Mr. Turner's past employment consisted of five years as the Chief Architect of the Southern US for Sun Microsystems, three years as an independent multinational consultant, and nine years with Andersen Consulting's/Accenture's Advanced Technology practice both in the US and Asia. He can be reached at Julian.Turner@QAssociates.com Additional thanks to Sun Microsystems for providing server and storage hardware for this test and Green Platform Corporation for providing the AVR 1000 anti-vibration rack.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure #1: Representative Physical Layout</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 : Consecutive Runs with Oscillating</head><label>5</label><figDesc>Figure 5: Consecutive Runs with Oscillating Performance</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structural and Vibration Guidelines for Datacom Equipment Centers</title>
	</analytic>
	<monogr>
		<title level="j">ASHRE</title>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ASHRAE Publications</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">European Parliament and the Council</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">44</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NoiseMagic&apos;s No-Vibes Hard Drive Suspension Kit</title>
		<ptr target="www.silentpc.com/forums" />
	</analytic>
	<monogr>
		<title level="m">HDD Vibration and Noise Reducing Methods -Ranked</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>5] www.jab-tech.com, Vibration Dampening Hard Drive Screws [6] www.silentpcreview.com</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Performance Impact of External Vibration on Consumer-Grade and Enterprise-Class Disk Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Ruwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What is Power Usage Effectiveness?, ecmweb.com</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szalkus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-12-01" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
