<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST &apos;15). Open access to the Proceedings of the 13th USENIX Conference on File and Storage Technologies is sponsored by USENIX A Tale of Two Erasure Codes in HDFS A Tale of Two Erasure Codes in HDFS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 16-19,</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Xia</surname></persName>
							<email>mingyuan.xia@mail.mcgill.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcgill</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Saxena</surname></persName>
							<email>msaxena@us.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Blaum</surname></persName>
							<email>mmblaum@us.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Pease</surname></persName>
							<email>pease@us.ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Xia</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">† IBM Research Almaden</orgName>
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Saxena</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Blaum</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Pease</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research Almaden</orgName>
								<address>
									<postCode>2015 •</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 13th USENIX Conference on File and Storage Technologies (FAST &apos;15). Open access to the Proceedings of the 13th USENIX Conference on File and Storage Technologies is sponsored by USENIX A Tale of Two Erasure Codes in HDFS A Tale of Two Erasure Codes in HDFS</title>
					</analytic>
					<monogr>
						<title level="m">USENIX Association 13th USENIX Conference on File and Storage Technologies (FAST &apos;15)</title>
						<imprint>
							<biblScope unit="page">213</biblScope>
							<date type="published">February 16-19,</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Distributed storage systems are increasingly transition-ing to the use of erasure codes since they offer higher reliability at significantly lower storage costs than data replication. However, these codes tradeoff recovery performance as they require multiple disk reads and network transfers for reconstructing an unavailable data block. As a result, most existing systems use an erasure code either optimized for storage overhead or recovery performance. In this paper, we present HACFS, a new erasure-coded storage system that instead uses two different erasure codes and dynamically adapts to workload changes. It uses a fast code to optimize for recovery performance and a compact code to reduce the storage overhead. A novel conversion mechanism is used to efficiently up-code and downcode data blocks between fast and compact codes. We show that HACFS design techniques are generic and successfully apply it to two different code families: Product and LRC codes. We have implemented HACFS as an extension to the Hadoop Distributed File System (HDFS) and experimentally evaluate it with five different workloads from production clusters. The HACFS system always maintains a low storage overhead and significantly improves the recovery performance as compared to three popular single-code storage systems. It reduces the degraded read la-tency by up to 46%, and the reconstruction time and disk/network traffic by up to 45%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Distributed storage systems storing multiple petabytes of data are becoming common today <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">15]</ref>. These systems have to tolerate different failures arising from unreliable components, software glitches, machine reboots, and maintenance operations. To guarantee high reliablity and availablity despite these failures, data is * Work done as an intern at IBM Research Almaden replicated across multiple machines and racks. For example, the Google File System <ref type="bibr" target="#b9">[11]</ref> and the Hadoop Distributed File System <ref type="bibr" target="#b3">[4]</ref> maintain three copies of each data block. Although disk storage seems inexpensive today, replication of the entire data footprint is simply infeasible at massive scales of operation. As a result, most large-scale distributed storage systems are transitioning to the use of erasure codes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b18">20]</ref>, which provide higher reliability at significantly lower storage costs.</p><p>The trade-off for using erasure codes instead of replicating data is performance. If a data block is three-way replicated, it can be reconstructed by copying it from one of its available replicas. However, for an erasurecoded system, reconstructing an unavailable block requires fetching multiple data and parity blocks within the code stripe, which results in significant increase in disk and network traffic. Recent measurements on a Facebook's data warehouse cluster <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b18">20]</ref> storing multiple petabytes of erasure-coded data, required a median of more than 180 Terabytes of data transferred to recover from 50 machine-unavailability events per day.</p><p>This increase in the amount of data to be read and transferred during recovery for an erasure-coded system results in two major problems: high degraded read latency and longer reconstruction time. First, a read to an unavailable block requires multiple disk reads, network transfers and compute cycles to decode the block. The application accessing the block waits for the entire duration of this recovery process, which results in higher latencies and degraded read performance. Second, a failed or decommissioned machine, or a failed disk results in significantly longer reconstruction time than in a replicated system. Although, the recovery of data lost from a failed disk or machine can be performed in the background, it severely impacts the total throughput of the system as well as the latency of degraded reads during the reconstruction phase.</p><p>As a result, the problem of reducing the overhead of recovery in erasure-coded systems has received signifi-cant attention in the recent past both in theory and practice <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b24">26]</ref>. Most of the solutions tradeoff between two dimensions: storage overhead and recovery cost. Storage overhead accounts for the additional parity blocks for a coding scheme. Recovery cost is the total number of blocks required to reconstruct a data block after failure.</p><p>In general, most production systems use a single erasure code, which either optimizes for recovery cost or storage overhead. For example, Reed-Solomon <ref type="bibr" target="#b19">[21]</ref> is a popular family of codes used in Google's ColossusFS <ref type="bibr" target="#b1">[2]</ref>, Facebook's HDFS <ref type="bibr" target="#b2">[3]</ref>, and several other storage systems <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b14">16]</ref>. The Reed-Solomon code used in ColossusFS has a storage overhead of 1.5x, while it requires six disk reads and network transfers to recover a lost data block. In contrast, the Reed-Solomon code used in HDFS reduces the storage overhead to 1.4x, but has a recovery cost of ten blocks. The other popular code family is the Local Reconstruction Codes (LRC) <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b24">26]</ref>, and has similar tradeoffs.</p><p>In this paper, we present Hadoop Adaptively-Coded Distributed File System (HACFS), a new erasure-coded storage system, which instead uses two different erasure codes from the same code family. It uses a fast code with low recovery cost and a compact code with low storage overhead. It exploits the data access skew observed in Hadoop workloads <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b4">5]</ref> to decide the initial encoding of data blocks. The HACFS system uses the fast code to encode a small fraction of the frequently accessed data and provide overall low recovery cost for the system. It uses the compact code to encode the majority of less frequently accessed data blocks and maintain a low and bounded storage overhead.</p><p>After initial encoding, the HACFS system dynamically adapts to workload changes by using two novel operations to convert data blocks between the fast and compact codes. Upcoding blocks initally encoded with fast code into compact code enables the HACFS system to reduce the storage overhead. Similarly, downcoding data blocks from compact code to fast code representation lowers the overall recovery cost of the HACFS system. The upcode and downcode operations are very efficient and only update the associated parity blocks while converting blocks between the two codes.</p><p>We have designed and implemented HACFS as an extension to the Hadoop Distributed File System <ref type="bibr" target="#b2">[3]</ref>. We find that adaptive coding techniques in HACFS are generic and can be applied to different code families. We successfully implement adaptive coding in HACFS with upcode and downcode operations designed for two different code families: Product codes <ref type="bibr" target="#b21">[23]</ref> and LRC codes <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b24">26]</ref>. In both cases, HACFS with adaptive coding using two codes outperforms HDFS with a single Reed-Solomon <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> or LRC code <ref type="bibr" target="#b13">[15]</ref>. We evaluate our design on an HDFS cluster with workload distributions obtained from production environments at Facebook and four different Cloudera customers <ref type="bibr" target="#b7">[9]</ref>.</p><p>The main contributions of this paper are as follows:</p><p>• We design HACFS, a new erasure-coded storage system that adapts to workload changes by using two different erasure codes -a fast code to optimize recovery cost of degraded reads and reconstruction of failed disks/nodes, and a compact code to provide low and bounded storage overhead.</p><p>• We design a novel conversion mechanism in HACFS to efficiently up/down-code data blocks between the two codes. The conversion mechanism is generic and we implement it for two code families -Product and LRC codes -popularly used in distributed storage systems.</p><p>• We implement HACFS as an extension to HDFS and demonstrate its efficacy using two case studies with Product and LRC family of codes. We evaluate HACFS by deploying it on a cluster with real-world workloads and compare it against three popular single code systems used in production. The HACFS system always maintains a low storage overhead, while improving the degraded read latency by 25-46%, reconstruction time by 14-44%, and network and disk traffic by 19-45% during reconstruction.</p><p>The remainder of the paper is structured as follows. Section 2 motivates HACFS by describing the different tradeoffs for erasure-coded storage systems and HDFS workloads. Section 3 and 4 present the detailed description of HACFS design and implementation. Finally, we evaluate HACFS design techniques in Section 5, and finish with related work and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>In this section, we describe the different failure modes and recovery methods in erasure-coded HDFS <ref type="bibr" target="#b2">[3]</ref>. We discuss how the use of erasure codes within HDFS reduces storage overhead, however it increases the recovery cost. This motivates the need to design HACFS, which exploits the data access characteristics of Hadoop workloads to achieve better recovery cost and storage efficiency than the existing HDFS architecture.</p><p>Failure Modes and Recovery in HDFS. HDFS has different failure modes, for example, block failure, disk failure, and a decommisioned or failed node. The causes of these failures may be diverse such as hardware failures, software glitches, maintenance operations, rolling upgrades that take certain percentage of nodes offline, and hot-spot effects that overload particular disks. Most of these failures typically result in the unavailability of a single block within an erasure code stripe. An erasure code stripe is composed of multiple data blocks striped across different disks or nodes in an HDFS cluster. Over 98% of all failure modes in Facebook's data-warehouse and other production HDFS clusters require recovery of a single block failure <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b17">19]</ref>. Another 1.87% have two blocks missing, and just less than 0.05% are three or more block failures. As a result, most recent research on erasure-coded storage systems has focused on reducing the recovery cost of single block failures <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b14">16]</ref>.</p><p>The performance of an HDFS client or a MapReduce task can be affected by HDFS failures in two ways: degraded reads and reconstruction of an unavailable disk or node. <ref type="figure" target="#fig_0">Figure 1</ref> shows a degraded read for an HDFS client reading an unavailable data block B 1 , which returns an exception. The HDFS client recovers this block by first retrieving the available data and parity blocks within the erasure-code stripe from other DataNodes. Next, the HDFS client decodes the block B 1 from the available blocks. Overall, the read to a single block B 1 is delayed or degraded by the time it takes to perform several disk reads and network transfers for available blocks, and the time for decoding. Reed-Solomon codes used in two production filesystems -Facebook's HDFS <ref type="bibr" target="#b2">[3]</ref> and Google ColossusFS <ref type="bibr" target="#b1">[2]</ref> -require between 6-10 network transfers and several seconds for completing one degraded read (see Section 5.3).</p><p>A failed disk/node or a decomissioned node typically requires recovery of several lost data blocks. When a DataNode or disk failure is detected by HDFS, several MapReduce jobs are launched to execute parallel recovery of lost data blocks on other live DataNodes. HDFS places data blocks in an erasure-code stripe on different disks and nodes. As a result, the reconstruction of most disk and node failures effectively requires recovery of several single block failures similar to degraded reads. <ref type="figure" target="#fig_1">Figure 2</ref> shows the network transfers required by the reconstruction job running on live DataNodes. An HDFS client trying to access lost blocks B 1 and B 2 during the reconstruction phase encounters degraded reads. Over-  all, the reconstruction of lost data from a failed disk or node results in several disk reads and network transfers, and can take from tens of minutes to hours for complete recovery (see Section 5.4).</p><p>Erasure Coding Tradeoffs. <ref type="figure" target="#fig_2">Figure 3</ref> show the recovery cost and storage overhead for Reed-Solomon family of codes widely used in production systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">25]</ref>. In addition, it also shows the recovery cost and storage overhead of three popular erasure-coded storage systems: Google ColossusFS [2], Facebook HDFS <ref type="bibr" target="#b2">[3]</ref>, and Microsoft Azure Storage <ref type="bibr" target="#b13">[15]</ref>. Google ColossusFS and Facebook HDFS use two different Reed-Solomon codes -RS(6, 3) and RS(10, 4)-that encode six and ten data blocks within an erasurecode stripe with three and four parity blocks respectively. As a result, they have a recovery cost of six and ten blocks, and storage overheads of 1.5x and 1.4x respectively. Microsoft Azure Storage uses an LRC code -LRC comp , which reduces the storage overhead to <ref type="bibr">1.33x</ref> and has a similar recovery cost of six blocks as Google ColossusFS. It encodes twelve data blocks with two global and two local parity blocks (see Section 3.3 for more detailed description on LRC codes). In contrast, three-way data replication provides recovery cost of one block, but a higher storage overhead of 3x. In general, most erasure-codes including Reed-Solomon and LRC codes trade-off between recovery cost and storage overhead, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>In this work, we focus on the blue region in <ref type="figure" target="#fig_2">Figure 3</ref> to achieve recovery cost for HACFS less than that of both Reed-Solomon and LRC codes used in ColossusFS and Azure. We further exploit the data access skew in Hadoop workloads to maintain a low storage overhead for HACFS and keep it bounded between the storage overheads of these two systems. Data Access Skew. Data access skew is a common characteristic of Hadoop storage workloads <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b4">5]</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> shows the frequency of data accessed in production Hadoop clusters at Facebook and four different Cloudera customers <ref type="bibr" target="#b7">[9]</ref>. All workloads show skew in data access frequencies. The majority of the data volume is cold and accessed only a few times. Similarly, the majority of the data accesses go to a small fraction of data, which is hot. In addition, HDFS does not allow inplace block updates or overwrites. As a result, the read accesses primarily characterize this data access skew.</p><p>Why HACFS? The HACFS design aims to achieve the following goals:</p><p>• Fast degraded reads to reduce the latency of reads when accessing lost or unavailable blocks.</p><p>• Low reconstruction time to reduce the time for recovering from failed disks/nodes or decommissioned nodes, and the associated disk and network traffic.</p><p>• Low storage overhead that is bounded under practical system constraints and adjusted based on workload requirements.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the use of a single erasure code tradesoff recovery cost for storage overhead. To achieve the above design goals, the HACFS system uses a combination of two erasure codes and exploits the data access skew within the workload. We next describe HACFS design in more detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Design</head><p>In this section, we first describe how the HACFS system adapts between the two different codes based on workload characteristics to reduce recovery cost and storage overhead. We next discuss the application of HACFS's adaptive coding to two different code families: Product codes with low recovery cost and LRC codes with low storage overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adaptive Coding in HACFS</head><p>The HACFS system is implemented as an extension to the HDFS-RAID module <ref type="bibr" target="#b2">[3]</ref> within HDFS. We show our extensions to HDFS-RAID as three shaded components in <ref type="figure" target="#fig_4">Figure 5</ref>. The adaptive coding module maintains the system states of erasure-coded data and manages state transitions for ingested and stored data. It also interfaces with the erasure coding module, which implements the different coding schemes.</p><p>System States. The adaptive coding module of HACFS manages the system state. The system state tracks the following file state associated with each erasure-coded data file: file size, last modification time, read count and coding state. The file size and last modification time are attributes maintained by HDFS, and used by HACFS to compute the total data storage and write age of the file. The adaptive coding module also tracks the read count of a file, which is the total number of read accesses to the file by HDFS clients. The coding state of a file represents if it is three-way replicated or the erasure coding scheme used for it. The file state can be updated on a create, read or write operation issued to the file from an HDFS client. The adaptive coding module also maintains a global state, which is the total storage used for data and parity. Every block in a replicated data file is replicated at three different nodes in the HDFS cluster and the two replicas account for the parity storage. In contrast, every block in an erasure-coded data file has exactly one copy. Each erasure-coded file is split into different erasure code stripes, with blocks in each stripe distributed across different nodes in the HDFS cluster. Each erasure-coded data file also has an associated parity file whose size is determined by the coding scheme. The global state of the system is updated periodically when the adaptive coding module initiates state transitions for erasure-coded files. A state transition corresponds to a change in the coding state of a file and is invoked by using the following interfaces to the erasure-coding module.</p><p>Coding Interfaces. As shown in <ref type="table">Table 1</ref>, the erasure coding module in HACFS system exports four major interfaces for coding data: encode, decode, upcode and downcode. The encode operation requires a data file and coding scheme as input, and generates a parity file for all blocks in the data file. The decode operation is invoked on a degraded read for a block failure or as part of the reconstruction job for a disk or node failure. It also requires the index of the missing or corrupted block in a file, and reconstructs the lost block from the remaining data and parity blocks in the stripe using the input coding scheme. The adaptive coding module invokes upcode and downcode operations to adapt with workload changes and convert a data file representation between the two coding schemes. As we show later in Section 3.2 and 3.3, both of these conversion operations only update the associated parity file when changing the coding scheme of a data file. The upcode operation transforms data from a fast code to a compact code representation, thus reducing the size of the parity file to achieve lower storage overhead. It does not require reading the data file and is a parity-only transformation. The downcode operation transforms from a compact code to a fast code representation, thus reducing the recovery cost. It requires read-  <ref type="figure">Figure 6</ref>(a), a recently created file in HDFS-RAID is classified as write hot based on its last modified time and therefore three-way replicated. The HDFS-RAID process (shown as RaidNode in <ref type="figure" target="#fig_4">Figure 5</ref>) scans the file system periodically to select write cold files for erasure coding. It then schedules several MapReduce jobs to encode all such candidate files with a ReedSolomon code <ref type="bibr" target="#b2">[3]</ref>. After encoding, the replication level of these files is reduced to one and the coding state changes to Reed-Solomon. As HDFS only supports appends to files, a block is never overwritten and these files are only read after being erasure-coded. <ref type="figure">Figure 6</ref>(b) shows the first extension of the HACFS system. It replicates write hot files similar to HDFS-RAID. In addition, HACFS also accounts for the read accesses to data blocks in a file. All write cold files are further classified based on their read counts and encoded with either of the two different erasure codes. Read hot files with a high read count are encoded with a fast code, which has a low recovery cost. Read cold files with a low read count are encoded with a compact code, which has a low storage overhead.</p><p>However, a read cold file can later get accessed and turn into a read hot file, thereby requiring low recovery cost. Similarly, encoding all files with the fast code may result in a higher total storage overhead for the system. As a result, the HACFS system needs to adapt to the workload by converting files between fast and compact codes (as shown in <ref type="figure">Figure 6(c)</ref>). The conversion for a file is guided by its own file state (read count) as well as the global system state (total storage). When the total storage consumed by data and parity blocks exceeds a configured system storage bound, the HACFS system se-  lects some files encoded with fast code and upcodes them to the compact code. Similarly, it selects some replicated files and encodes them directly into the compact code. The HACFS system selects these files by first sorting them based on their read counts and then upcodes/encodes the files with lowest read counts into compact code to make the total storage overhead bounded again. The downcode operation transitions a file from compact to fast code. As a result, it reduces the recovery cost of a future degraded read to a file, which was earlier compact-coded but has been recently accessed. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, the data access skew for Hadoop workloads result in a small fraction of read hot files and large fraction of read cold files. This skew allows HACFS to reduce the recovery cost by encoding/downcoding the read hot files with a fast code and reduces the storage overhead by encoding/upcoding a large fraction of read cold files with a compact code.</p><p>Fast and Compact Codes. The adaptive coding techniques in HACFS are generic and can be applied to different code families. We demonstrate its application to two code families: Product codes <ref type="bibr" target="#b21">[23]</ref> with low recovery cost and LRC codes <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b22">24]</ref> with low storage overhead. <ref type="table" target="#tab_3">Table 2</ref> shows the three major characteristics useful for selecting fast and compact codes from a code family. The fast code must have a low recovery cost for degraded reads and reconstruction. The compact code must have a low storage overhead. Finally, the reliability of both codes measured in terms of mean-time-to-failure for data loss must be greater than that for three-way replication (3.5 × 10 9 years) <ref type="bibr" target="#b11">[13]</ref>. In addition, the HACFS system requires a storage bound, which can be set from the practical requirements of the system or can be optimally tuned close to the storage overhead of the compact code. We use a storage bound of 1.5x with Product codes and 1.4x with LRC codes in the two case studies of the HACFS system.</p><p>We next describe the design of the erasure coding module in HACFS for Product and LRC codes in Section 3.2 and 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Product Codes</head><p>We now describe the construction and coding interfaces of Product codes used in the HACFS system.</p><p>Encoding and Decoding. <ref type="figure" target="#fig_5">Figure 7</ref> shows the construction of a Product code, P C f ast or P C(2 × 5), which has a stripe with two rows and five columns of data blocks. The encode operation for P C f ast retrieves the ten data blocks from different locations in the HDFS cluster and generates two horizontal, five vertical and one global parity. The horizontal parities are generated by transferring the five data blocks in each row and performing an XOR operation on them. A vertical parity only requires two data block transfers in a column. The global parity can be constructed as an XOR of the two horizontal parities. The decode operation for a Product code is invoked on a block failure. A single failure in any data or parity block of the P C f ast code requires only two block transfers from the same column to reconstruct it.</p><p>As a result, the P C f ast code can achieve a very low recovery cost of two block transfers at the cost of a high storage overhead of eight parity blocks for ten data blocks (1.8x). We choose the P C comp or P C(6 × 5) as the compact code (see <ref type="figure" target="#fig_5">Figure 7)</ref>, which provides a lower storage overhead of 1.4x and higher recovery cost of five block transfers (see <ref type="table" target="#tab_3">Table 2</ref>). In addition, both fast and compact Product codes have reliability better than threeway replication. We select a storage bound of 1.5x for the HACFS system with these Product codes since it is close to the storage overhead of the P C comp code. This bound also matches the practical limits prescribed by the Google ColossusFS <ref type="bibr" target="#b1">[2]</ref>, which uses the Reed-Solomon RS(6, 3) code similarly optimized for recovery cost. Upcoding and Downcoding. <ref type="figure" target="#fig_5">Figure 7</ref> shows upcoding from P C f ast to P C comp and downcoding from P C comp to P C f ast codes. Upcode is a very efficient parity-only conversion operation for Product codes. All data and vertical parity blocks remain unchanged in upcoding from the P C f ast to P C comp code. Further, it only performs XOR over the old horizontal and global parity blocks of the three P C f ast codes to compute the new horizontal and global parity blocks of the P C comp code. As a result, the upcode operation does not require any network transfers of the data blocks from the three P C f ast codes to compute the new parities in the P C comp code.</p><p>The downcode operation converts a P C comp code into three P C f ast codes. Only the horizontal and global parities change between the P C comp code and the three P C f ast codes. However, computing the horizontal and global parities in the first two P C f ast codes requires network transfers and XOR operations over the data blocks in the two horizontal rows of the P C comp code. The horizontal and global parities in the third P C f ast code is computed from the those of the old P C comp code and those newly computed ones of the first two P C f ast codes. This optimization saves on the network transfers of two horizontal rows of data blocks. Similar to the up- code operation, data and vertical parity blocks in the resulting three P C f ast codes remain unchanged from the P C comp code and do not require any network transfers.</p><formula xml:id="formula_0">G G L L L L L L G G L L + + G G L L L L L</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">LRC Codes</head><p>We now describe the construction and coding interfaces of the erasure coding module using LRC codes in HACFS.</p><p>Encoding and Decoding. <ref type="figure" target="#fig_6">Figure 8</ref> shows the construction of the LRC f ast or LRC(12, 6, 2), with twelve data blocks, six local parities, and two global parities. The encode operation for an LRC code computes the local parities by performing an XOR over a group of data blocks. Two data blocks in each column form a different group in LRC f ast . The two global parities are computed by performing a Reed-Solomon encoding over all of the twelve data blocks <ref type="bibr" target="#b22">[24]</ref>. The Reed-Solomon encoding of the global parities has properties similar to the LRC code construct used in Microsoft Azure Storage <ref type="bibr" target="#b13">[15]</ref> for the most prominent single block failure scenarios. The decode operation for LRC f ast code is similar to Product Codes for data and local parity blocks. Any single failure in data or local parity blocks for LRC f ast requires two block transfers from the same column to reconstruct it. However, a failure in a global parity block requires all twelve data blocks to reconstruct it using the ReedSolomon decoding. Degraded reads from an HDFS client only occur on data blocks, while reconstructing a failed disk or node can also require recovering lost global parity blocks. As a result, the degraded read cost for the fast code -LRC f ast or LRC(12, 6, 2) -is very low at two blocks (see <ref type="table" target="#tab_3">Table 2</ref>). Unlike Product codes, the average reconstruction cost for the LRC f ast code is asymmetri-cal to its degraded read cost since reconstruction requires twelve block transfers for global parity failures:</p><formula xml:id="formula_1">(12+6) * 2+2 * 12 12+6+2</formula><p>or 3.25 blocks. However, the storage overhead for an LRC f ast code is 1.66x corresponding to eight parity blocks required for twelve data blocks.</p><p>We use the LRC comp or LRC(12, 2, 2) code used in Azure <ref type="bibr" target="#b13">[15]</ref> as the compact code for adaptive coding in HACFS. The LRC comp code has a lower storage overhead of 1.33x due to fewer local parities. However, each of its two local parities is associated with a group of six data blocks. Thus, recovering a lost data block or local parity requires six block transfers from its group in the LRC comp code. The global parities require twelve data block transfers for recovery. As a result, the LRC comp code also has a lower recovery cost for degraded reads than its reconstruction cost similar to the LRC f ast code. Both LRC codes are more reliable than three-way replication. We select a storage bound of 1.4x for the HACFS system with LRC codes since it is close to the storage overhead of LRC comp code and lower than HACFS with Product codes.</p><p>Upcoding and Downcoding. Upcode and downcode operations for Product codes require merging three P C f ast codes into a P C comp code and splitting a P C comp code into three P C f ast codes respectively. The LRC codes can be upcoded and downcoded in a similar manner. However, such upcoding and downcoding with LRC codes requires several data block transfers to compute the new local and global parities. As a result, we use a more efficient code collapsing technique for the LRC upcode and downcode operations. This does not require computing the global parities again because collapsing converts exactly one LRC f ast code to one LRC comp code (and reverse for downcoding). <ref type="figure" target="#fig_6">Figure 8</ref> shows the LRC upcode operation by computing the new local parities in LRC comp code and preserving the global parities from the LRC f ast code. The two local parities in the LRC comp code are computed as an XOR over three local parities in the LRC f ast code. As a result, the HACFS system requires only six network transfers to compute the two new local parities of the LRC comp code in an upcode operation. The downcode operation computes two of the three new local parities in LRC f ast code from the data blocks in the individual columns of the LRC comp code. The third local parity is computed by performing an XOR over the two new local parities and the old local parity in the LRC comp code. Overall, the downcode operation requires ten block transfers for computing the new local parities. The global parities remain unchanged and do not require any network transfers in the downcode operation as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We have implemented HACFS as an extension to the HDFS-RAID <ref type="bibr" target="#b2">[3]</ref> module in the Hadoop Distributed File System (HDFS). The HDFS-RAID module is implemented by Facebook to support a single erasure code for distributed storage in an HDFS cluster. Our implementation of HACFS spans nearly 2 K lines of code, contained within the HDFS-RAID module, and requires no modification to other HDFS components such as the NameNode or DataNode. Erasure Coding in HDFS. The HDFS-RAID module overlays erasure coding on top of HDFS and runs as a RaidNode process. The RaidNode process periodically queries the NameNode for new data files that need to be encoded and for corrupted files that need to be recovered. The RaidNode launches a MapReduce job to compute the parity files associated with data files on different DataNodes for the encode operation. The decode operation is invoked as part of a degraded read or the reconstruction phase.</p><p>A read from an HDFS client requests the block contents from a DataNode. A degraded read can occur due to failures on DataNodes such as a CRC check error. In those cases, the HDFS client queries the RaidNode for locations of the available blocks in the erasure-code stripe required for recovery. The client then retrieves these blocks and performs decoding itself to recover the failed block. The recovered block is used to serve the read request, but it is not written back to HDFS since most degraded reads are caused by transient failures that do not necessarily indicate data loss <ref type="bibr" target="#b18">[20,</ref><ref type="bibr" target="#b22">24]</ref>.</p><p>When a disk or node failure is detected, the NameNode updates the list of corrupted blocks and lost files. The RaidNode then launches two MapReduce reconstruction jobs, one to recover lost data blocks and the other for lost parity blocks. The reconstruction job retrieves the available blocks for decoding, recovers the lost blocks using the decode operation, writes the recovered blocks to HDFS, and informs the NameNode of successful recovery. If there is a file which has many errors and can not be recovered, then it is marked as permanently lost. HACFS and Two Erasure Codes. <ref type="figure" target="#fig_4">Figure 5</ref> shows the three major components of HACFS implementation: erasure coding module, system states and the adaptive coding module. In addition, we also implement a fault injector to trigger degraded reads and data reconstruction.</p><p>The HDFS-RAID only supports a single ReedSolomon erasure code for encoding data. We implement two new code families as part of the HACFS erasure coding module: Product and LRC codes. The erasure coding module in HACFS exports the same encode/decode interfaces as HDFS-RAID. In addition, the erasure coding module also provides two new upcode/downcode interfaces to the extended state machine implemented in the adaptive coding module of HACFS. The upcode operation either merges three fast codes for Product codes or collapses one fast code for LRC codes into a new compact code of smaller size. Downcoding performs the reverse sequence of steps. Both operations change the coding state of the data file and reduce its replication level to one.</p><p>The adaptive coding module tracks the system states and invokes the different coding interfaces. As desribed earlier, the HDFS-RAID module selects the three-way replicated files which are write cold based on their last modification time for encoding. The extended state machine implemented as part of the adaptive coding module in HACFS further examines these candidate files based on their read counts. It retrieves the coding state of all files classified as read cold and launches MapReduce jobs to upcode them into the compact code. Similarly, if the global system storage exceeds the bound, it upcodes the files with the lowest read counts into the compact code. If the global system storage is lower than the bound or a cold file has been accessed, the adaptive coding module downcodes the file into the fast code and also updates its coding state.</p><p>On a disk or node failure, the RaidNode in HACFS launches MapReduce jobs to recover lost data and parity blocks similar to HDFS-RAID. It prioritizes the jobs for reconstructing data over parities to quickly restore data availability for HDFS clients. There are four different types of reconstruction jobs in HACFS, which recover data and parity files encoded with fast and compact codes. Data files encoded with a fast code have a lower recovery cost, but they are also fewer in number than compact-coded data files. As a result, the reconstruction of data files encoded with a fast code is prioritized first among all reconstruction jobs. This prioritization also helps to reduce the total number of degraded reads during the reconstruction phase since fast-coded files get accessed more frequently.</p><p>We also implement a fault injector outside HDFS to simulate different modes of block, disk and node failures on DataNodes. The fault injector deletes a block from the local file system on a DataNode, which is detected by the HDFS DataNode as a missing block, and triggers a degraded read when an HDFS client tries to access it. The fault injector simulates a disk failure by deleting all data on a given disk of the target DataNode and then restarting the corresponding DataNode process. A node failure is injected by killing the target DataNode process itself. In both disk and node failure, the NameNode updates the list of lost blocks, and then the RaidNode launches the MapReduce jobs for reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>We evaluate HACFS's design techniques along three different axes: degraded read latency, reconstruction time and storage overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methods</head><p>Experiments were performed on a cluster of eleven different nodes, each of which is equipped with 24 Intel Xeon E5645 CPU cores running at 2.4 GHz, six 7.2 K RPM disks each of 2 TB capacity, 96 GB of memory, and 1 Gbps network link. The systems run Red Hat Enterprise Linux 6.5 and HDFS-RAID <ref type="bibr" target="#b2">[3]</ref>. We use the default HDFS filesystem block size of 64 MB.</p><p>The HACFS system uses adaptive coding with fast and compact codes from Product and LRC code families. We refer these two different systems as: HACFS-PC using P C f ast and P C comp codes, and HACFS-LRC using LRC f ast and LRC comp codes. We compare these two HACFS systems against three HDFS-RAID systems using exactly one of these codes for erasure coding: Reed-Solomon RS(6, 3) code, Reed-Solomon RS(10, 4) code, and LRC(12, 2, 2) or LRC comp code. These three codes are used in production storage systems: RS(6, 3) used in Google Colossus FS <ref type="bibr" target="#b1">[2]</ref>, RS(10, 4) used in Facebook HDFS-RAID <ref type="bibr" target="#b2">[3]</ref>, and LRC comp used in Microsoft Azure Storage <ref type="bibr" target="#b13">[15]</ref>. We configure the storage overhead bound for HACFS-PC and HACFS-LRC systems as 1.5x (similar to Colossus FS) and 1.4x (similar to Facebook's HDFS-RAID) respectively.</p><p>We use the default HDFS-RAID block placement scheme to evenly distribute data across the cluster ensuring that no two blocks within an erasure code stripe reside on the same disk. We measure the degraded read latency by injecting single block failures (as described in Section 4) for a MapReduce grep job that is both network and I/O intensive. We measure the reconstruction time by deleting all blocks on a disk. The block placement scheme ensures that the lost disk does not have two blocks from the same stripe. As a result, the NameNode starts the reconstruction jobs in parallel using the remaining available disks. We report the completion time and network bytes transferred for reconstruction jobs averaged over five different executions.</p><p>We use five different workloads collected from production Hadoop clusters in Facebook and four Cloudera customers <ref type="bibr" target="#b7">[9]</ref>. <ref type="table" target="#tab_6">Table 3</ref> shows the distribution of each workload: number of files accessed, percentage of files accounting for 90% of the total accesses, percentage of data volume corresponding to such files, and percentage of reads in all accesses to such files.   Degraded Read Latency. <ref type="table" target="#tab_5">Table 4</ref> shows the percentage improvement of adaptive coding in HACFS with Product and LRC codes averaged over five different workloads. HACFS reduces the degraded latency by 25-46% for Product codes and 21-43% for LRC codes compared to three single-coded systems. This improvement in HACFS primarily comes from the use of fast codes (P C f ast and LRC f ast ) for hot data, which is primarily dominated by read accesses (see <ref type="table" target="#tab_6">Table 3</ref>). As a result, the degraded read latency of HACFS is lower than all of the three other production systems relying on RS(6, 3), RS(10, 4) and LRC comp codes. We describe these results in more detail for each of the five different workloads in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">System Comparison</head><p>Reconstruction Time. HACFS improves the reconstruction time to recover from a disk or node failure by 14-43% for Product codes and up to 32% for LRC codes. The reconstruction time is dominated by the volume of data and parity blocks lost in a disk or node failure. The fast and compact Product codes used in HACFS have a lower reconstruction cost than the two LRC codes. As described in Section 3.3, this is because LRC codes have a higher recovery cost for failures in local and global parity blocks than data blocks. As a result, the HACFS system with LRC codes takes slightly longer to reconstruct a lost disk than ColossusFS, which uses the RS(6, 3) code with a symmetric cost to recover from data and parity failures. We discuss these results in more detail in Storage Overhead. HACFS is designed to provide low and bounded storage overheads. The Azure system using the LRC comp code has the lowest storage overhead (see <ref type="table" target="#tab_3">Table 2</ref>), and is up to 4-10% better than the two HACFS systems. The HDFS-RAID system using RS(10, 4) has about 5% lower storage overhead than HACFS optimized for recovery with Product codes. However, the HACFS system with LRC codes has storage overheads lower or comparable to the three single-coded production systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b22">24]</ref>. This is primarily because adaptive coding in HACFS bounds the storage overhead by 1.5x for Product codes and by 1.4x for LRC codes. We discuss the storage overheads of each system across different workloads in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Degraded Read Latency</head><p>HACFS uses a combination of recovery-efficient fast codes (P C f ast and LRC f ast ) and storage-efficient compact codes (P C comp and LRC comp ). <ref type="figure" target="#fig_7">Figure 9</ref> shows the degraded read latency on y-axis and storage overhead on x-axis for the five different workloads. A normal read from an HDFS client to an available data block can take up to 1.2 seconds since it requires one local disk read and one network transfer if the block is remote. In contrast, a degraded read can require multiple network transfers, and takes between 16-21 seconds for the three single coded systems. These systems do not adapt with the workload and only use a single code. As a result, their degraded read latency and storage overhead is the same across all five workloads. Adaptive coding in HACFS reduces the degraded read latency by 5-10 seconds for three workloads (CC1, CC4 and FB), which have a higher percentage of reads to hot data encoded with the fast code (85-90%, see <ref type="table" target="#tab_6">Table 3</ref>). The two shaded boxes in <ref type="figure" target="#fig_7">Figure 9</ref> demonstrate that HACFS adapts to the characteristics of the different workloads. However, HACFS always outperforms the three single coded systems since all of them require more blocks to be read and transferred over the network to decode a missing block.</p><p>Both HACFS systems have a lower storage overhead for workloads (CC1, CC2 and FB) with a higher percentage of cold files (85-95%) encoded with the compact codes. The lowest possible storage overhead for HACFS is shown by the left boundary of the two shaded regions marked with 1.33x and 1.4x for the compact codes (LRC comp and P C comp codes respectively). In addition, HACFS also bounds the storage overhead by 1.5x for Product codes and 1.4x for LRC codes. As a result, workloads (CC3 and CC4) with fewer cold files still never exceed these storage overheads marked by the right edges in the two shaded regions. If we do not enforce any storage overhead bounds, these two workloads benefit even further by a reduction of 6-20% in their degraded read latencies.  <ref type="figure" target="#fig_0">Figure 10</ref> shows the reconstruction time of the three single code systems and HACFS system with adaptive coding when a disk with 100 GB of data fails. The reconstruction job launches map tasks on different DataNodes to recreate the data and parity blocks from the lost disk. The time to reconstruct a cold file encoded with a compact code is longer than that for a fast code. The HACFS system with Product codes outperforms the three single code systems for all five workloads. It takes about 10-35 minutes less reconstruction time than the three single code systems. This is because both fast and compact Product codes reconstruct faster than the two ReedSolomon codes and the LRC comp code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Reconstruction Time</head><p>The HACFS system with the use of faster LRC f ast code for reconstruction outperforms the LRC comp code with the lowest storage overhead. However, the HACFS system with LRC codes is generally worse for all workloads than the RS(6, 3) code used in the ColossusFS. This is because both LRC codes used in HACFS have a recovery cost for global parities higher than the RS(6, 3) code (see <ref type="table" target="#tab_3">Table 2</ref>). <ref type="figure" target="#fig_0">Figure 11</ref> shows the reconstruction traffic measured as HDFS read and writes incurred by the reconstruction job to recover 100 GB of data and additional parity blocks lost from the failed disk. The reconstruction job reads all blocks in the code stripe for recovering the lost blocks, and then writes the recovered data and parity blocks back to HDFS. The HDFS-RAID system using the RS <ref type="figure" target="#fig_0">(10, 4)</ref> code results in the highest traffic: 1550 GB of reconstruction traffic for 100 GB of lost data. This is close to the theoretical reconstruction traffic of nearly fifteen blocks per lost data block, including ten block reads for data recovery, four block reads for parity recovery, and then writes of the recovered data block and parity blocks (RS(10, 4) uniformly stores 0.4 parity blocks with each data block on a disk). Similarly, LRC comp in Azure and RS <ref type="bibr">(6,</ref><ref type="bibr" target="#b2">3)</ref> in Colossus, require nearly ten HDFS block read/writes for recovering a block from the lost disk.</p><p>The HACFS system with Product codes always requires fewer blocks for reconstruction than the three single code systems: between eight blocks for CC3 and CC4 workloads and nine blocks for CC1, CC2 and FB workloads (with more than 85% cold files) based on the data skew distributions. The HACFS system with LRC codes requires more blocks for global parity recovery than Product codes. As a result, its reconstruction traffic is close to RS(6, 3) and LRC comp codes at nearly ten blocks per lost data block. <ref type="figure" target="#fig_0">Figure 12</ref> shows the encoding cost for initial encoding of three-way replicated data in three single code systems, and the encoding cost for initial encoding and later conversion between the fast and compact codes in the two HACFS systems. We normalize the encoding cost per block to eliminate the differences in dataset sizes across the five workloads. All compared systems are based on HDFS-RAID implementation, which schedules the encoding and conversion operations as MapReduce jobs in background to minimize their impacts on user jobs. As a result, we show the impact of encoding cost for all systems relative to RS <ref type="bibr" target="#b8">(10,</ref><ref type="bibr" target="#b3">4)</ref> used in HDFS-RAID in <ref type="figure" target="#fig_0">Figure 12</ref>. Encoding cost is a function of the coding scheme used for data blocks and does not change with workload for the three single code systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Encoding and Conversion Time</head><p>Reed-Solomon codes used in HDFS-RAID and ColossusFS have the highest encoding cost because of complex Galois Field operations required to compute parity blocks <ref type="bibr" target="#b16">[18]</ref>. LRC code in Azure uses such operations only to compute global parities and uses cheaper XOR operations for all local parities. Similarly, HACFS with Product codes only uses XOR operations for encoding. As a result, the encoding time component of the two HACFS systems is similar to the LRC codes in Azure for all workloads.</p><p>The HACFS system also converts (upcodes/downcodes) data between fast and compact codes. The conversion cost is only high when the HACFS system aggressively converts blocks to limit the storage overhead by upcoding hot files into compact code. As a result, the three workloads (CC2, CC3 and CC4) with a higher percentage of hot data spend up to 18% of total encoding time for conversion operations. For these workloads, the total encoding and conversion cost of the HACFS systems is up to 16% higher than the Azure system using a single LRC code. In general, the encoding cost of the two HACFS systems is about 3-28% lower than the singlecode ColossusFS and HDFS-RAID systems using ReedSolomon codes for all workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Our work builds on past work on distributed storage systems, faster recovery techniques, and tiered storage systems.</p><p>Distributed Storage Systems. Petabytes of storage is becoming common with the fast growing data requirements of modern systems today. Erasure codes offer an attractive alternative to provide lower storage overhead than data replication. As a result, many distributed filesystems such as Google ColossusFS <ref type="bibr" target="#b1">[2]</ref>, Facebook HDFS <ref type="bibr" target="#b2">[3]</ref>, and IBM General Parallel File System <ref type="bibr">[6]</ref> are moving to the use of erasure codes. Many popular object stores used for cloud storage, for example, OpenStack Swift <ref type="bibr" target="#b15">[17]</ref>, Microsoft Azure Storage <ref type="bibr" target="#b13">[15]</ref> and Cleversafe <ref type="bibr" target="#b0">[1]</ref> are also adopting erasure codes for low storage overhead. However, most of these systems use a single erasure code and address the recovery cost by trading for storage overhead. In contrast, HACFS is the first system that uses a combination of two codes to dynamically adapt with workload changes and provide both low recovery cost and storage overhead.</p><p>Faster Recovery for Erasure-Codes. Recently, there has been a growing focus on improving recovery performance for erasure-coded storage systems. ReedSolomon codes <ref type="bibr" target="#b19">[21]</ref> offer optimal storage overhead but generally have high recovery cost. Rotated ReedSolomon codes have been proposed as an alternative con-struction, which requires fewer data reads for faster degraded read recovery <ref type="bibr" target="#b14">[16]</ref>. HitchHiker proposes a new encoding technique by dividing a single Reed-Solomon code stripe into two correlated substripes and improves recovery performance <ref type="bibr" target="#b18">[20]</ref>. However, both of them trade extra encoding time for faster recovery. In contrast, adaptive coding techniques in HACFS provide lower recovery cost without increasing encoding time. In general, adaptive coding can be applied to most code families, which tradeoff between storage overhead and recovery cost. We have found efficient up/downcode operations for applying adaptive coding to different constructs of Reed-Solomon code and other modern storage codes such as PMDS <ref type="bibr" target="#b6">[8]</ref> and HoVer <ref type="bibr" target="#b10">[12]</ref>. For example, we devised up/downcode operations for converting m (n,r) Reed-Solomon codes into a (mn, r) Reed-Solomon code using a parity-only conversion scheme. Tiered Storage Systems. Adaptive coding in HACFS is inspired by tiering in RAID architectures <ref type="bibr" target="#b25">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">10]</ref>. AutoRAID <ref type="bibr" target="#b25">[27]</ref> provides a two-level storage hierarchy within the storage controller. It automatically migrates data between different RAID levels to provide high I/O performance for active data and low storage overhead for inactive data. Similarly, HACFS migrates data between fast and compact erasure codes, however with the objective to reduce extra network transfers for recovery in distributed storage. Facebook's HDFS-RAID <ref type="bibr" target="#b2">[3]</ref> and DiskReduce <ref type="bibr" target="#b8">[10]</ref> propose tiered storage by asynchronously migrating data between replicated and erasure-coded storage tiers. HACFS extends this further by splitting the erasure-coded storage tier into two parts to optimize for both storage overhead and recovery performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Distributed storage systems extensively deploy erasurecoding today for lower storage overhead than data replication. However, most of these systems trade storage overhead for recovery performance. We present a novel erasure-coded storage system, which uses two different erasure codes and dynamically adapts by converting between them based on workload characteristics. It uses a fast code for fast recovery performance and a compact code for low storage overhead. In the future, as we move to cloud storage, it will be important to revisit similar erasure-coding tradeoffs, and extend adaptive coding techniques to large-scale object stores in the cloud.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Degraded reads for HDFS block failure: The figure shows a degraded read for an HDFS client reading an unavailable block B 1 . The HDFS client retrieves available data and parity blocks and decodes the block B 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Reconstruction for HDFS node/disk failure or decomissioned nodes: The figure shows a reconstruction MapReduce job launched to recover from a disk failure on an HDFS DataNode. The reconstruction job executes parallel recovery of the lost blocks B 1 and B 2 on other live DataNodes by retrieving available data and parity blocks. An HDFS client accessing a lost block encounters degraded reads during the reconstruction phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Recovery Cost vs. Storage Overhead: The figure shows the tradeoff between recovery cost and storage overhead for the popular Reed-Solomon family of codes. It also shows three production storage systems each using single erasure code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Data Access Skew in Hadoop Workloads: The figure shows the data access distributions of Hadoop workloads collected from production clusters at Facebook (FB) and four different Cloudera customers (CC-1,2,3,4). Both axes are on log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: HACFS Architecture: The figure shows the different components of the HACFS architecture implemented as extensions to HDFS in the shaded modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Product Code -Upcode and Downcode Operations: The figure shows the upcode and downcode operations on the data and parity blocks for Product codes. The shaded horizontal parity blocks in the output code are only computed, and the remaining blocks remain unchanged from the input code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: LRC Code -Upcode and Downcode Operations: The figure shows the upcode and downcode operations with data and parity blocks for LRC codes. The shaded blocks are only computed during these operations, and remaining blocks remain unchanged.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Degraded Read Latency: The figure shows the degraded read latency and storage overhead for two HACFS systems and three single code systems. Section 5.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Reconstruction Time: The figure shows the reconstruction time to recover from data loss with two HACFS systems and three single code systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Reconstruction Traffic: The figure shows the reconstruction traffic for recovering lost data and parity blocks with two HACFS systems and three single code systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Encoding Cost: The figure shows the encoding time for three single code systems and two HACFS systems normalized over the HDFS-RAID (RS(10, 4)). The black bars show the c-onversion time component of the total e-ncoding time for the two HACFS systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :Reconstruction Cost, SO: Storage Overhead, MTTF: Mean-Time-To-Failure in years).</head><label>2</label><figDesc></figDesc><table>Fast and Compact Codes: This table shows the 
two codes in the Product and LRC code families used for 
adaptive coding in HACFS (DRC: Degraded Read Cost, 
RC: </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :3), Facebook HDFS-RAID using RS(10, 4), and Microsoft Azure Storage using LRC comp codes.</head><label>4</label><figDesc></figDesc><table>System Comparison. The table shows the percentage improvement of two HACFS systems using Product 
and LRC codes for recovery performance and storage overhead over three single code systems: Google ColossusFS 
using RS(6, Workload 
Files 
Hot Files % Hot Data 
% Hot Reads 
CC1 
20.1K 
1.2K 
5.9 
86.1 
CC2 
10.2K 
1.6K 
15.7 
75.9 
CC3 
2.1K 
1.1K 
52.4 
75.5 
CC4 
5.2K 
1.4K 
26.9 
85.2 
FB 
802K 
103K 
12.8 
90.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Workload Characteristics: The table shows the dis-

tributions of five different workloads from Hadoop clusters de-
ployed at four different Cloudera customers (CC1/2/3/4) and 
Facebook (FB). 

time, and provides low and bounded storage overhead. 
We begin with a high-level comparison of HACFS us-
ing adaptive coding on Product and LRC codes with 
three single code systems: ColossusFS, HDFS-RAID 
and Azure. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Jim Hafner for his valuable feedback at different stages of this work. We also thank our shepherd Cheng Huang and the anonymous FAST reviewers for their helpful comments and useful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.cleversafe.com" />
		<title level="m">Cleversafe object storage</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Successor</forename><surname>Colossus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>To Google File</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>System</surname></persName>
		</author>
		<ptr target="http://static.googleusercontent.com/media/research.google.com/en/us/university/relations/facultysummit2010/storage_architecture_and_challenges.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Facebook&apos;s erasure coded hadoop distributed file system (HDFS-RAID)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://wiki.apache.org/hadoop/HDFS" />
		<title level="m">The Hadoop Distributed File System</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What do real-life apache hadoop workloads look like?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cloudera</surname></persName>
		</author>
		<ptr target="http://blog.cloudera.com/blog/2012/09/what-do-real-life-hadoop-workloads-look-like/" />
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A storage-centric analysis of mapreduce workloads: File popularity, temporal locality and arrival patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camp-Bell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
		<idno>IISWC &apos;12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the 2012 IEEE International Symposium on Workload Characterization (IISWC)<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="100" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Partial-MDS codes and their application to RAID type of architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hetzler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="4510" to="4519" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interactive analytical processing in big data systems: A cross-industry study of mapreduce workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2012-08" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DiskReduce: RAID for data-intensive scalable computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tantisiriroj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gib-Son</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Annual Workshop on Petascale Data Storage</title>
		<meeting>the 4th Annual Workshop on Petascale Data Storage<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PDSW &apos;09, ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Google File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Nineteenth ACM Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
	<note>SOSP &apos;03, ACM</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">HoVer erasure codes for disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hafner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Dependable Systems and Networks (Washington</title>
		<meeting>the International Conference on Dependable Systems and Networks (Washington<address><addrLine>DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
	<note>DSN &apos;06</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Notes on reliability models for non-MDS erasure codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hafner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno>RJ10391 (A0610-035</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
<note type="report_type">IBM Tech Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pyramid codes: Flexible schemes to trade space for access efficiency in reliable data storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2013-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Erasure coding in Windows Azure Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yekhanin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX Conference on Annual Technical Conference</title>
		<meeting>the 2012 USENIX Conference on Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
	<note>USENIX ATC&apos;12, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking erasure codes for cloud file systems: Minimizing I/O for recovery and degraded reads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="20" />
		</imprint>
	</monogr>
	<note>FAST&apos;12, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Swift object storage: Adding erasure codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luse</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greenan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<ptr target="http://www.snia.org/sites/default/files/Luse_Kevin_SNIATutorialSwift_Object_Storage2014_final.pdf" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Screaming fast Galois Field arithmetic using Intel SIMD instructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plank</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Greenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>And Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 11th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
	<note>FAST&apos;13, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A solution to the network challenges of data recovery in erasure-coded distributed storage systems: A study on the Facebook warehouse cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Conference on Hot Topics in Storage and File Systems</title>
		<meeting>the 5th USENIX Conference on Hot Topics in Storage and File Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8" to="8" />
		</imprint>
	</monogr>
	<note>HotStorage&apos;13, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A &quot;hitchhiker&apos;s&quot; guide to fast and efficient data reconstruction in erasure-coded data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<idno>SIG- COMM&apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGCOMM</title>
		<meeting>ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Polynomial codes over certain finite fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reed</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Industrial and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="300" to="304" />
			<date type="published" when="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hadoop&apos;s adolescence: An analysis of Hadoop usage in scientific workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2013-08" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="853" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Introduction to Coding Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">XORing elephants: novel erasure codes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sathiamoorthy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asteris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papail-Iopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Vadali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borthakur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th international conference on Very Large Data Bases</title>
		<meeting>the 39th international conference on Very Large Data Bases</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
	<note>PVLDB&apos;13, VLDB Endowment</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">GPFS: A shareddisk file system for large computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schmuck</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haskin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno>FAST &apos;02</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st USENIX Conference on File and Storage Technologies</title>
		<meeting>the 1st USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A family of optimal locally recoverable codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="4661" to="4676" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The HP AutoRAID hierarchical storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilkes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Staelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sullivan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="108" to="136" />
			<date type="published" when="1996-02" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
