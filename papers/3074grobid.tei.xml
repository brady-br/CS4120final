<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Discriminative Model for Semantics-to-String Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2015-07-30">July 30, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
							<email>tamchyna@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague Malostranské náměstí 25, Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
							<email>chrisq@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Mircosoft Research</orgName>
								<address>
									<addrLine>One Microsoft Way Redmond</addrLine>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
							<email>mgalley@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Mircosoft Research</orgName>
								<address>
									<addrLine>One Microsoft Way Redmond</addrLine>
									<postCode>98052</postCode>
									<region>WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Discriminative Model for Semantics-to-String Translation</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation</title>
						<meeting>the 1st Workshop on Semantics-Driven Statistical Machine Translation <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="30" to="36"/>
							<date type="published" when="2015-07-30">July 30, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a feature-rich discriminative model for machine translation which uses an abstract semantic representation on the source side. We include our model as an additional feature in a phrase-based de-coder and we show modest gains in BLEU score in an n-best re-ranking experiment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of machine translation is to take source language utterances and convert them into fluent target language utterances with the same meaning. Most recent approaches learn transformations using statistical techniques on parallel data. Meaning equivalent representations of words and phrases are learned directly from natural data, as are other syntactic operations such as reordering. However, commonly used methods have a very simple view of the linguistic data. Each word is generally modeled independently, for instance, and the relations between words are generally captured only in fixed phrases or as syntactic relationships.</p><p>Recently there has been a resurgence of interest in unified semantic representations: deep analyses with heavy normalization of morphology, syntax, and even semantic representations. In particular, Abstract Meaning Representation (AMR, <ref type="bibr" target="#b0">Banarescu et al. (2013)</ref>) is a novel representation of (sentential) semantics. Such representations could influence a number of natural language understanding and generation tasks, particularly machine translation.</p><p>Deeper models can be used for multiple aspects of the translation modeling problem. Building translation models that rely on a deeper representation of the input allows for a more parsimonious translation model: morphologically related words can be handled in a unified manner; semantically related concepts are immediately adjacent and available for modeling, etc. Language models using deep representations might help us model which interpretations are more plausible.</p><p>We present an initial discriminative method for modeling the likelihood of a target language surface string given source language deep semantics. This approach relies on an automatic parser for source language semantics. We use a system that parses into AMR-like structures <ref type="bibr" target="#b18">(Vanderwende et al., 2015)</ref>, and apply the resulting model as an additional feature in a translation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large body of related work on utilizing deep language representation in NLP and MT in particular. This is not surprising considering that such representations provide abstractions of many language-specific phenomena, effectively bringing different languages closer together.</p><p>A number of machine translation systems starting as early as the 1950s therefore used a form of transfer: the source sentences were parsed, and those parsed representations were translated into target representations. Finally text generation was applied. The level of analysis is somewhat arguable -sometimes it was purely syntactic, but in other cases it reached into the semantic domain.</p><p>One of the earliest architectures was described in 1957 <ref type="bibr" target="#b21">(Yngve, 1957)</ref>. More contemporary examples of such systems include KANT <ref type="bibr" target="#b9">(Nyberg and Mitamura, 1992)</ref>, which used a very deep representation close to an interlingua, early versions of SysTran and Microsoft Translator, or more recently TectoMT (Popel andŽabokrtsk´yandˇandŽabokrtsk´andŽabokrtsk´y, 2010) for English→Czech translation.</p><p>AMR itself has recently been used for abstractive summarization ( <ref type="bibr" target="#b7">Liu et al., 2015)</ref>. In this work, sentences in the document to be summarized are parsed to AMRs, then a decoding algorithm is run to produce a summary graph. The surface realization of this graph then constitutes the final sum- <ref type="figure">Figure 1</ref>: Logical Form (computed tree) for the sentence: I would like to give you a sandwich taken from the fridge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>mary.</head><p>( <ref type="bibr" target="#b5">Jones et al., 2012</ref>) presents an MT approach that can exploit semantic graphs such as AMR, in a continuation of earlier work that abstracted translation away from strings ( <ref type="bibr" target="#b20">Yamada and Knight, 2001;</ref><ref type="bibr" target="#b4">Galley et al., 2004</ref>). While rule extraction algorithms such as ( <ref type="bibr" target="#b4">Galley et al., 2004</ref>) operate on trees and have also been applied to semantic parsing problems ( <ref type="bibr" target="#b6">Li et al., 2013</ref>), <ref type="bibr" target="#b5">Jones et al. (2012)</ref> generalized these approaches by inducing synchronous hyperedge replacement grammars (HRG), which operate on graphs. In contrast to ( <ref type="bibr" target="#b5">Jones et al., 2012)</ref>, our work does not have to deal with the complexities of HRG decoding, which runs in O(n 3 ) ( <ref type="bibr" target="#b5">Jones et al., 2012)</ref>, as our decoder is simply a phrase-based decoder.</p><p>Discriminative models have been used in statistical MT many times. Global lexicon model <ref type="bibr" target="#b8">(Mauser et al., 2009</ref>) and phrase-sense disambiguation <ref type="bibr" target="#b3">(Carpuat and Wu, 2007)</ref> are perhaps the best known methods. Similarly to <ref type="bibr" target="#b3">Carpuat and Wu (2007)</ref>, we use the classifier to rescore phrasal translations, however we do not train a separate classifier for each source phrase. Instead, we train a global model -similarly to Subotin (2011) or more recently <ref type="bibr" target="#b15">Tamchyna et al. (2014)</ref>. Features for our model are very different from previous work because they come from a deep representation and therefore should capture semantic relations between the languages, instead of surface or morpho-syntactic correspondences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Semantic Representation</head><p>Our representation of sentence semantics is based on Logical Form <ref type="bibr" target="#b18">(Vanderwende, 2015)</ref>. LFs are labeled directed graphs whose nodes roughly correspond to content words in the sentence. Edge labels describe semantic relations between nodes.</p><p>Additional linguistic information, such as verb subcategorization frames, definiteness, tense etc., is stored in graph nodes as bits. <ref type="figure">Figure 1</ref> shows a sentence parsed into the logical form. Nodes are represented by word lemmas. Relations include Dsub for deep subject, Dobj and Dind for direct and indirect objects etc. Bits are shown as flags in parentheses. Note that this graph may have cycles -for example, the Dobj of "take" is "sandwich", but "take" is also the Attrib of "sandwich". The verb "take" is also missing its obligatory subject which is replaced by the free variable X.</p><p>The logical form can be converted using a sequence of rules to a representation which conforms to the AMR specification ( <ref type="bibr" target="#b18">Vanderwende et al., 2015</ref>). We do not use the full conversion pipeline in our work, so our semantic graphs are somewhere between the LF and AMR. Notably, we keep the bits which serve as important features for the discriminative modeling of translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph-to-String Translation</head><p>We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM <ref type="bibr">Models 1, 2 (Brown et al., 1993)</ref>, and the HMM translation model ( <ref type="bibr" target="#b19">Vogel et al., 1996</ref>). In particular, we see translation as a process of selecting target words in order conditioned on source language representation as well as prior target words. Similar to the IBM Models, we see each target word as being generated based on source concepts, though in our case the concepts are semantic graph nodes rather than surface words. That is, we assume the existence of an alignment, though it aligns the target words to source semantic graph nodes rather than surface words.</p><p>Our model views translation as generation of the target-side sentence given the source-side semantic graph. We assume a generative process which operates as follows. We begin in the virtual root node of the graph. At each step, we transition to a graph node and we generate a target-side word. We proceed left-to-right on the target side and we stop once the whole target sentence is generated. <ref type="figure" target="#fig_0">Figure 2</ref> shows an example of this process.</p><p>Say we have a source semantic graph G with nodes V = {n 1 ..n S }, edges E ⊂ V × V , and a root node n R for R ∈ 1..S. Then the likelihood of a target string E = (e 1 , ..., e T ) and alignment A = (a 1 , ..., a T ) with a i ∈ 0..S is as follows, with a 0 = R:</p><formula xml:id="formula_0">P (A, E|G) = T i=1 P (a i |a i−1 1 , e i−1 1 , G) P (e i |a i 1 , e i−1 1 , G)<label>(1)</label></formula><p>In this generative story, we first predict each alignment position and then predict each translated word. The transition distribution P (a i | · · · ) resembles that of the HMM alignment model, though the features are somewhat different. The translation distribution P (e i | · · · ) may take on several forms. For the purposes of alignment, we explore a simple categorical distribution as in the IBM models. For translation reranking, we instead use a feature-rich approach conditioned on a variety of source and target context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Alignment of Semantic Graph Nodes</head><p>We have experimented with a number of techniques for aligning source-side semantic graph nodes to target-side surface words.</p><p>Gibbs sampling. We can attempt to directly align the target language words to the source language nodes using a generative HMM-style model. Unlike the HMM word alignment model ( <ref type="bibr" target="#b19">Vogel et al., 1996)</ref>, the likelihood of jumping between nodes is based on the graph path between those nodes, rather than the linear distance.</p><p>Starting from the generative story of Equation 1, we make several simplifying assumptions. First we assume that the alignment distribution P (a i | · · · ) is modeled as a categorical distribution:</p><formula xml:id="formula_1">P (a i |a i−1 , G) ∝ c(LABEL(a i−1 , a i ))</formula><p>The function LABEL(u, v) produces a string describing the labels along the shortest (undirected) path between the two nodes.</p><p>Next, we assume that the translation distribution is modeled as a set of categorical distributions, one for each source semantic node:</p><formula xml:id="formula_2">P (e i |n a i ) ∝ c(LEMMA(n a i ) → e i )</formula><p>This model is sensitive to the order in which source language information is presented in the target language.</p><p>The alignment variables a i are not observed. We use Gibbs sampling rather than EM so that we can incorporate a sparse prior when estimating the parameters of the model and the assignments to these latent alignment variables. At each iteration, we shuffle the sentences in our training data. Then for each sentence, we visit all its tokens in a random order and re-align them. We sample the new alignment according to the Markov blanket, which has the following probability distribution:</p><formula xml:id="formula_3">P (t|n i ) ∝ c(LEMMA(n i ) → t) + α c(LEMMA(n i )) + αL × c(LABEL(n i , n i−1 )) + β T + βP × c(LABEL(n i+1 , n i )) + β T + βP<label>(2)</label></formula><p>L, P stand for the number of lemma/path types, respectively. T is the total number of tokens in the corpus. Overall, the formula describes the probability of the edge coming into the node n i , the token emission and finally the outgoing edge. We evaluate this probability for each node n i in the graph and re-align the token according to the random sample from this distribution.</p><p>α and β are hyper-parameters specifying the concentration parameters of symmetric Dirichlet priors over the transition and emission distributions. Specifying values less than 1 for these hyper-parameters pushes the model toward sparse solutions. They are tuned by a grid search which evaluates model perplexity on a held-out set.</p><p>Direct GIZA++. GIZA++ <ref type="bibr" target="#b10">(Och and Ney, 2000</ref>) is a commonly used toolkit for word alignment which implements the IBM models. In this setting, we linearized the semantic graph nodes using a simple heuristic based on the surface word order and aligned them directly to the target-side sentences. We experimented with different symmetrizations and found that grow-diag-final-and gives the best results.</p><p>Composed alignments. We divided the alignment problem into two stages: aligning semantic graph nodes to source-side words and aligning the source-and target-side words (i.e., standard MT word alignment). We then simply compose the two alignments. For the alignment between source graph nodes and source surface words, we have two options: we can either train a GIZA++ model or we can use gold alignments provided by the semantic parser. For the second stage, we need to train a GIZA++ model.</p><p>We evaluated the different strategies by manually inspecting the resulting alignments. We found that the composition of two separate alignment steps produces clearly superior results, even if it seems arguable whether such division simplifies the task. Therefore, for the remaining experiments, we used the composition of gold alignment and GIZA++, although two GIZA++ steps performed comparably well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model</head><p>For our discriminative model, the alignment is assumed to be given. At training time, it is the alignment produced by the parser composed with GIZA++ surface word alignment. At test time, we compose the alignment between graph nodes and source surface tokens (given by the parser) with the bilingual surface word alignment provided by the MT decoder.</p><p>Turning to the translation distribution, we use a maximum entropy model to learn the conditional probability:</p><formula xml:id="formula_4">P (e i |n a i , n a i−1 , G, e i−1 i−k+1 ) = exp w · f (e i , n a i , n a i−1 , G, e i−1 i−k+1 ) Z (3)</formula><p>where Z is defined as</p><formula xml:id="formula_5">e ∈GEN (na i ) exp(w · f (e , n a i , n a i−1 , G, e i−1 i−k+1 ))</formula><p>The GEN(n) function produces the possible translations of the deep lemma associated with node n. We collect all translations observed in the training data and keep the 30 most frequent ones for each lemma. Our model thus assigns zero probability to unseen translations.</p><p>Because of the size of our training data, we used online learning. We implemented a parallelized (multi-threaded) version of the standard stochastic gradient descent algorithm (SGD). Our learning rate was fixed -using line search, we found the optimal rate to be 0.05. Our batch size was set to one; different batch sizes made almost no difference in model performance. We used online L1 regularization ( <ref type="bibr" target="#b16">Tsuruoka et al., 2009</ref>) with weight 1. We implemented feature hashing to further improve performance and set the hash length to 22 bits. We shuffled our data and split it into five parts which were processed independently and their final weights were averaged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Feature Set</head><p>Our semantic representation enables us to use a very rich set of features, including information commonly used by both translation models and language models. We extract a significant amount of information from the graph node n a i aligned to the generated word:</p><p>• lemma,</p><p>• part of speech,</p><p>• all bits.</p><p>We extract the same features from the previous graph node (n a i−1 ), from the parent node. (If there are multiple parents in the graph, we break ties in a consistent but heuristic manner, picking the leftmost parent node according to its position in the source sentence) We also gather all the bits of the parent and the parent relation. These features may capture agreement phenomena.</p><p>We also look at the shortest path in the semantic graph from the previous node to the current one and we extract features which describe it:</p><p>• path length,</p><p>• relations (edges) along the path.</p><p>We use the lemmas of all nodes in the semantic graph as bag-of-word features, as well as all the surface words in the source sentence. We also extract lemmas of nodes within a given distance from the current node (i.e. graph context), as well as the relation that led to these nodes. Together, these features ground the current node in its semantic context.</p><p>An additional set of features handle the fact that source nodes may generate multiple target words, and the distribution over subsequent words should be different. We have a feature indicating the number of words generated from the current node, both in isolation, conjoined with the lemma, and conjoined with the part of speech. We also have a feature for each word previously generated by this same node, again in isolation, in conjunction with the lemma, and in conjunction with the part of speech. This helps prevent the model from generating multiple copies of same target word given a source node.</p><p>On the target side, we use several previous tokens as features. These may act as discriminative language model features.</p><p>During MT decoding, our model therefore must maintain state, which could present a computational issue. The language model features present similar complexity as conventional MT state, and the features about prior words generated from the same node require greater memory. Were this cost to become prohibitive, a simpler form of the prior word features would likely suffice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We tested our model in an n-best re-ranking experiment.</p><p>We began by training a basic phrase-based MT system for English→French on 1 million parallel sentence pairs and produced 1000-best lists for three test sets provided for the Workshop on Statistical Machine Translation ( <ref type="bibr" target="#b1">Bojar et al., 2013</ref><ref type="bibr">) -WMT 2009</ref><ref type="bibr" target="#b12">, 2010</ref>. This system had a set of 13 commonly used features: four channel model scores (forward and backward MLE and lexical weighting scores), a 5-gram language model, five lexicalized reordering model scores (corresponding to different ordering outcomes), linear distortion penalty, word count, and phrase count. The system was optimized using minimum error rate training <ref type="bibr" target="#b11">(Och, 2003)</ref>   <ref type="table">Table 1</ref>: BLEU scores of n-best reranking in English→French translation.</p><p>For reranking, we gathered 1000-best lists for the development and test sets. We added six scores from our model to each translation in the n-best lists. We included the total log probability, the sum of unnormalized scores, and the rank of the given output. In addition, we had count features indicating the number of words that were not in the GEN set of the model, the number of NULLs (effectively deleted nodes), and a count of times a target word appeared in a stopword list. In the end, each translation had a total of 19 features: 13 from the original features and 6 from this approach.</p><p>Next, we ran one iteration of the MERT optimizer on these 1000-best lists for all of the features. Because this was a reranking experiment rather than decoding, we did not repeatedly gather n-best lists as in decoding. The resulting feature weights were used to rescore the test n-best lists and evaluated the using BLEU; <ref type="table">Table 1</ref> shows the results. We obtained a modest but consistent improvement. Once the model is used directly in the decoder, the gains should increase as it will be able to influence decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented an initial attempt at including semantic features in a statistical machine translation system. Our approach uses discriminative training and a broad set of features to capture morphological, syntactic, and semantic information in a single model. Although our gains are not particularly large yet, we believe that additional ef-fort on feature engineering and decoder integration could lead to more substantial gains.</p><p>Our approach is gated by the accuracy and consistency of the semantic parser. We have used a broad coverage parser with accuracy competitive to the current state-of-the-art, but even the stateof-the-art is rather low. It would be interesting to explore more robust features spanning multiple analyses, or to combine the outputs of multiple parsers. Even syntax-based machine translation systems are dependent on accurate parsers <ref type="bibr" target="#b13">(Quirk and Corston-Oliver, 2006</ref>); deeper analyses are likely to be more dependent on parse quality.</p><p>In a similar vein, it would be interesting to evaluate the impact of morphological, syntactic, and semantic features separately. A careful feature ablation and exploration would help identify promising areas for future research.</p><p>We have only scratched the surface of possible integrations. Even this model could be applied to MT systems in multiple ways. For instance, rather than applying from source to target, we might evaluate in a noisy channel sense. That is, we could predict the source language surface forms given the target language translations. Furthermore, this would allow incorporation of a target semantic language model. This latter approach is particularly attractive, as it would explicitly model the semantic plausibility of the target. Of course, this would require target language semantic analysis: either we would be forced to parse n-best outcomes from some baseline system, or integrate the construction of target language semantics into the MT system. We believe that including such models of semantic plausibility holds great promise in preventing "word salad" outputs from MT systems: sentences that simply cannot be interpreted by humans.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example of the translation process illustrating several first steps of translating the sentence from Figure 1 into German ("Ich möchte dir einen Sandwich..."). Labels in italics correspond to the shortest undirected paths between the nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>on WMT 2009.</figDesc><table>Dataset 
Baseline +Semantics 
WMT 2009 = devset 
17.44 
17.55 
WMT 2010 
17.59 
17.64 
WMT 2013 
17.41 
17.55 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Lucy Vanderwende and Arul Menezes for their helpful suggestions and insights into the logical form and abstract representations of meaning in general.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-08" />
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<title level="m">Findings of the 2013 Workshop on Statistical Machine Translation</title>
		<meeting><address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
	<note>Eighth Workshop on Statistical Machine Translation, WMT</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The mathematics of statistical machine translation: Parameter estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent J Della</forename><surname>Peter F Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen A Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert L</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="311" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving Statistical Machine Translation using Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekai</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>Prague, Czech Republic</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What&apos;s in a translation rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HLT-NAACL</title>
		<meeting>of HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantics-based machine translation with hyperedge replacement grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bevan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING</title>
		<meeting>of COLING</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1359" to="1376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An extended ghkm algorithm for inducing lambda-scfg</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Toward abstractive summarization using semantic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL 2015</title>
		<meeting>NAACL 2015<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Mauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasa</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="210" to="218" />
			<pubPlace>Suntec, Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The kant system: Fast, accurate, high-quality translation in practical domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruko</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference on Computational Linguistics</title>
		<meeting>the 14th Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1069" to="1073" />
		</imprint>
	</monogr>
	<note>COLING &apos;92</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved Statistical Alignment Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Och</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<publisher>Hong Kong. ACL</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimum Error Rate Training in Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Association for Computational Linguistics, Sapporo</title>
		<meeting>of the Association for Computational Linguistics, Sapporo<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TectoMT: Modular NLP Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdeněkzdeněkˇzdeněkžabokrtsk´zdeněkžabokrtsk´y</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hrafn Loftsson, Eirikur Rögnvaldsson, and Sigrun Helgadottir</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">6233</biblScope>
			<biblScope unit="page" from="293" to="304" />
		</imprint>
	</monogr>
	<note>IceTAL 2010. Iceland Centre for Language Technology (ICLT</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The impact of parse quality on syntactically-informed statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon Corston-</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An exponential translation model for target language morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Subotin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="230" to="238" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integrating a discriminative classifier into phrase-based and hierarchical decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabienne</forename><surname>Braune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marine</forename><surname>Carpuat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Prague Bulletin of Mathematical Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Hal Daumé III, and Chris Quirk</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Jun&amp;apos;ichi Tsujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An AMR parser for English, French, German, Spanish and Japanese and new AMR-annotated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arul</forename><surname>Menezes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 NAACL HLT Demonstration Session</title>
		<meeting>the 2015 NAACL HLT Demonstration Session<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nlpwin -an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno>MSR-TR-2015-23</idno>
		<imprint>
			<date type="published" when="2015-03" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hmm-based word alignment in statistical translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Tillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference on Computational Linguistics</title>
		<meeting>the 16th Conference on Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="836" to="841" />
		</imprint>
	</monogr>
	<note>COLING &apos;96</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A syntaxbased statistical translation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="523" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A framework for syntactic translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">H</forename><surname>Yngve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mechanical Translation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="65" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
