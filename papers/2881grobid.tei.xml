<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Determinism Is Overrated: What Really Makes Multithreaded Programs Hard to Get Right and What Can Be Done about It?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyue</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Determinism Is Overrated: What Really Makes Multithreaded Programs Hard to Get Right and What Can Be Done about It?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Our accelerating computational demand and the rise of multicore hardware have made parallel programs, especially shared-memory multithreaded programs, increasingly pervasive and critical. Yet, these programs remain extremely difficult to write, test, analyze, debug, and verify. Conventional wisdom has attributed these difficulties to nondeterminism, and researchers have recently dedicated much effort to bringing determinism into multi-threading. In this paper, we argue that determinism is not as useful as commonly perceived: it is neither sufficient nor necessary for reliability. We present our view on why multithreaded programs are difficult to get right, describe a promising approach we call stable multithreading to dramatically improve reliability, and summarize our last four years&apos; research on building and applying stable mul-tithreading systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multithreaded programs are notoriously difficult to write, test, analyze, debug, and verify, much harder than the sequential versions. These programs are the most widespread parallel programs, yet experts consider reliable parallelism "something of a black art" <ref type="bibr" target="#b5">[6]</ref> and one of the grand challenges in computing <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. Unsurprisingly, widespread parallel programs are plagued with insidious concurrency bugs <ref type="bibr" target="#b11">[12]</ref>, such as data races and deadlocks. Some of the worst of these bugs have killed people in the Therac 25 incidents and caused the 2003 Northeast blackout. Our study also reveals that these bugs may be exploited by attackers to violate confidentiality, integrity, and availability of critical systems <ref type="bibr" target="#b18">[19]</ref>.</p><p>In recent years, two technology trends have made the challenge of reliable multithreading more urgent. The first is the rise of multicore hardware. The speed of a single processor core is limited by fundamental physical constraints, forcing processors into multicore designs. Thus, developers must resort to parallel code for best performance on multicore processors. The second is our accelerating computational demand. Scientific computing, video and image processing, financial simulation, "big data" analytics, web search, and online social networking are all massive computations and often employ multithreading for performance.</p><p>Unlike sequential programs, repeated executions of the same multithreaded program on the same input may yield different (e.g., correct vs. buggy) behaviors, depending on how the threads interleave. Conventional wisdom has long blamed this nondeterminism for the challenges in reliable multithreading <ref type="bibr" target="#b9">[10]</ref>: threads are nondeterministic by default, and it is the (tricky) job of developers to account for this nondeterminism. Nondeterminism has direct implications on reliability. For instance, it makes testing less effective: a program may run correctly on an input in the testing lab because the interleavings tested happen to be correct, but executions on the same exact input may still fail in the field when the program hits a buggy, untested interleaving.</p><p>To eliminate this nondeterminism, several groups of researchers including us have dedicated much effort to building deterministic multithreading (DMT) systems <ref type="bibr">[2-5, 9, 11, 15]</ref>. These systems force a multithreaded program to always execute the same thread interleaving, or schedule, on the same input, thus always resulting in the same behavior. By mapping each input to only one schedule, DMT brings determinism, a key property of sequential computing, into multithreading.</p><p>However, we believe nondeterminism is responsible for only a small piece of the puzzle. We argue that determinism, the cure to nondeterminism, is not as useful as commonly perceived: it is neither sufficient nor necessary for reliability. It is not sufficient because a perfectly deterministic system can map each input to an arbitrary schedule, so that small input perturbations lead to vastly different schedules, artificially reducing the program's robustness and stability. It is not necessary because a nondeterministic system with a small set of schedules for all inputs can be made reliable by exhaustively checking all schedules. (See §2 for more discussion. <ref type="bibr">)</ref> We believe what makes multithreading hard is rather quantitative: multithreaded programs have too many schedules. The number of schedules for each input is already enormous because the parallel threads may interleave in many ways, depending on such factors as hardware timing and operating system scheduling. Aggregated over all inputs, the number is even greater. Finding a few schedules that trigger concurrency errors out of all enormously many schedules (so developers can prevent them) is like finding needles in a haystack. Although DMT reduces schedules for each input, it may map each input to a different schedule, so the total set of schedules for all inputs remains enormous.</p><p>We attacked this root cause by asking: are all the enormously many schedules necessary? Our study reveals that many real-world programs can use a small set of schedules to efficiently process a wide range of inputs <ref type="bibr" target="#b6">[7]</ref>. Leveraging this insight, we envision a new approach we call stable multithreading (SMT) 1 that reuses each schedule on a wide range of inputs, mapping all inputs to a dramatically reduced set of schedules. By vastly shrinking the haystack, it makes the needles much easier to find. By mapping many inputs to the same schedule, it stabilizes program behaviors against small input perturbations. SMT and DMT are not mutually exclusive: a system can be both deterministic and stable.</p><p>To demonstrate the feasibility of SMT, we have built two compiler and runtime systems, TERN <ref type="bibr" target="#b6">[7]</ref> and PERE-GRINE <ref type="bibr" target="#b7">[8]</ref>, to address complementary challenges in implementing SMT systems. To demonstrate key benefits of SMT, we have built a program analysis framework that leverages SMT to greatly improve the precision of static analysis <ref type="bibr" target="#b16">[17]</ref>. These systems are designed to be compatible with existing hardware, operating systems, thread libraries, and programming languages to simplify adoption. They are also mostly transparent to developers.</p><p>Our initial results are promising. Evaluation on a diverse set of widespread multithreaded programs, including the Apache web server and the MySQL database, show that TERN and PEREGRINE dramatically reduce schedules. For instance, under typical setups, they reduce the number of schedules needed by parallel compression utility PBZip2 down to two schedules for each different number of threads, regardless of the file contents. Their overhead is moderate, less than 15% for most programs. Our program analysis framework enables the construction of many program analyses with precision and coverage unmatched by their counterparts. For instance, a race detector we built found previously unknown bugs in extensively checked code with almost no false positives.</p><p>In the rest of this paper, we present our view on why multithreading is hard to get right ( §2); describe our SMT approach, its benefits ( §3), and the three SMT systems we built ( §4 and §5); and conclude ( §6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Why Are Multithreaded Programs So</head><p>Hard to Get Right?</p><p>We start with preliminaries, then describe the challenges caused by nondeterminism and by too many schedules.</p><p>We then explain why nondeterminism is a lesser cause than too many schedules.</p><p>1 Not to be confused with simultaneous multithreading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries: Inputs, Schedules, and Buggy Schedules</head><p>To ease discussion, we use input to broadly refer to the data a program reads from its execution environment, including not only the data read from files and sockets, but also command line arguments, return values of external functions such as gettimeofday, and any external data that can affect program execution. We use schedule to broadly refer to the (partially or totally) ordered set of communication operations in a multithreaded execution, including synchronizations (e.g., lock and unlock operations) and shared memory accesses (e.g., load and store instructions to shared memory). Of all the schedules, most run fine, but some trigger concurrency errors, causing program crashes, incorrect computations, deadlocked executions, and other failures. Consider the toy program below:</p><formula xml:id="formula_0">// thread 1 // thread 2 lock(l); lock(l); *p = . . .; p = NULL; unlock(l); unlock(l);</formula><p>The schedule in which thread 2 gets the lock before thread 1 causes a dereference-of-NULL failure. Consider another example. The toy program below has data races on balance:</p><p>// thread 1 // thread 2 // deposit 100 // withdraw 100 t = balance + 100; balance = balance − 100; balance = t;</p><p>The schedule with the statements executed in the order shown corrupts balance. We call the schedules that trigger concurrency errors buggy schedules. Strictly speaking, the errors are in the programs, triggered by a combination of inputs and schedules. However, typical concurrency errors, such as the ones appeared in previous studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>, depend much more on the schedules than the inputs (i.e., once the schedule is fixed, the bug occurs for all inputs allowed by the schedule). Thus, recent research on testing multithreaded programs (e.g., <ref type="bibr" target="#b12">[13]</ref>) is focused on effectively testing schedules to find the buggy ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Challenges Caused by Nondeterminism</head><p>A multithreaded program is nondeterministic because even with the same program and input, different schedules may still lead to different behaviors. For instance, the two toy programs in the previous subsection do not always run into the bugs. Except for the schedules described, the other schedules lead to correct executions.</p><p>This nondeterminism raises many challenges, especially in testing and debugging. Suppose an input can </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Schedules</head><p>(b) Deterministic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Schedules</head><p>(c) Stable (deterministic).  execute under n schedules. Testing n − 1 schedules is not enough for complete reliability because the single untested schedule may still be buggy. An execution in the field may hit this untested schedule and fail. Debugging is challenging, too. To reproduce a field failure for diagnosis, the exact input alone is not enough. Developers must also manage to reconstruct the buggy schedule out of n possibilities. <ref type="figure" target="#fig_2">Figure 1a</ref> depicts the traditional multithreading approach. Conceptually, it is a many-to-many mapping, where one input may execute under many schedules because of nondeterminism, and many inputs may execute under one schedule because a schedule fixes the order of the communication operations but allows the local computations to operate on any input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inputs Schedules</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenges Caused by Too Many Schedules</head><p>A typical multithreaded program has an enormous number of schedules. For a single input, the number of schedules is asymptotically exponential in the schedule length. For instance, given m threads each competing for a lock k times, each order of lock acquisitions forms a schedule, easily yielding (mk)! k! ≥ (m!) k total schedules-a number exponential in both m and k. Aggregated over all inputs, the number of schedules is even greater.</p><p>Finding a few buggy schedules in these exponentially many schedules raises a series of "needle-in-a-haystack" challenges. For instance, to write correct multithreaded programs, developers must carefully synchronize their code to weed out the buggy schedules. As usual, humans err when they must scrutinize many possibilities to locate corner cases. Various forms of testing tools suffer, too. Stress testing is the common method for (indirectly) testing schedules, but it often redundantly tests the same schedules while missing others. Recent tools (e.g., <ref type="bibr" target="#b12">[13]</ref>) systematically test schedules for bugs, but we seriously lack resources to cover more than a tiny fraction of all exponentially many schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Determinism is Overrated</head><p>To address the challenges raised by nondeterminism, researchers including us have dedicated much effort and built several systems that force a multithreaded program to always run the same schedule on the same input, bringing determinism to multithreading. This determinism does have value for reliability. For instance, one testing execution now validates all future executions on the same input. Reproducing a concurrency error now requires only the input.</p><p>In contrast to this effort, little has been done to solve the needle-in-a-haystack challenges caused by too many schedules. We believe the community has charged nondeterminism more than its share of the guilt and overlooked the main culprit-a rather quantitative cause that multithreaded programs simply have too many schedules. We argue that, although determinism has value for reliability, its value is smaller than commonly perceived. It is neither sufficient nor necessary for reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Determinism</head><p>=⇒ reliability. Determinism is a narrow property: same input + same program = same behavior. It has no jurisdiction if the input or program changes however slightly. Yet, we often expect a program to be robust or stable against slight program changes or input perturbations. For instance, adding a debug printf should in principle not make the bug disappear. A single bit flip of a file should usually not cause a compression utility to crash. Unfortunately, determinism does not provide stability and, if na¨ıvelyna¨ıvely implemented, even undermines it.</p><p>To illustrate, consider a worst-case system depicted in <ref type="figure" target="#fig_2">Figure 1b</ref> that maps each input to an arbitrary schedule. This mapping is perfectly deterministic, but it destabilizes program behaviors on multiple inputs. A single bit flip may force a program to discard a correct schedule and (ad)venture into a vastly different, buggy schedule.</p><p>This instability is counterintuitive at least, and raises new reliability challenges. For instance, testing one input provides little assurance on very similar inputs, despite that the differences in input do not invalidate the tested schedule. Debugging now requires every bit of the bug-inducing input, including not only the data a user typed, but also environment variables, shared libraries, etc. A different user name? Error report doesn't include credit card numbers? The bug may never be reproduced, regardless of how many times developers retry, because the schedule chosen by the deterministic system for the altered input happens to be correct. Note that even a correct sequential program may show very different behaviors for small input changes across boundary conditions, but these conditions are typically infrequent and the different behaviors are intended by developers. In contrast, the instability introduced by the system in <ref type="figure" target="#fig_2">Figure 1b</ref> is artificial and on all inputs.</p><p>Besides inputs, na¨ıvelyna¨ıvely implemented determinism can destabilize program behaviors on minor code changes, so adding a debug printf causes the bug to deterministically disappear. Another problem is that the number of all possible schedules remains enormous, so the coverage of schedule testing tools remains low.</p><p>In practice, to avoid the worst case, researchers have to augment determinism with other techniques. To support debug printf, some propose to temporarily revert to nondeterministic execution <ref type="bibr" target="#b8">[9]</ref>. DMP <ref type="bibr" target="#b8">[9]</ref>, CoreDet <ref type="bibr" target="#b2">[3]</ref>, and Kendo <ref type="bibr" target="#b14">[15]</ref> change schedules only if the inputs change low-level instructions executed. Although better than mapping each input to an arbitrary schedule, they still allow small input perturbations to destabilize schedules unnecessarily when the perturbations change the low-level instructions executed, observed in our experiments <ref type="bibr" target="#b6">[7]</ref>. Our TERN and PEREGRINE systems and the DTHREADS system built subsequently to TERN by others <ref type="bibr" target="#b10">[11]</ref> combine DMT with SMT (elaborated next section) to frequently reuse schedules on a wide range of inputs for stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reliability</head><p>=⇒ determinism. Determinism is a binary property: if an input maps to n &gt; 1 schedules, executions on this input may be nondeterministic, however small n is. Yet, a nondeterministic system with a small set of total schedules can be made reliable easily. Consider an extreme case, the nondeterministic system depicted in <ref type="figure" target="#fig_2">Fig- ure 1d</ref> which maps all inputs to at most two schedules. In this system, the challenges caused by nondeterminism ( §2.2) are easy to solve. For instance, to reproduce a field failure given an input, developers can easily afford to search for one out of only two schedules. To offer an analogy, a coin toss is nondeterministic, but humans have no problem understanding and doing it because there are only two possible outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Shrinking the Haystack with Stable Multithreading</head><p>Motivated by the limitations of determinism and the needle-in-a-haystack challenges caused by exponentially many schedules, we investigated a central research question: are all the exponentially many schedules necessary? A schedule is necessary if it is the only one that can (1) process specific inputs or (2) yield good performance under specific scenarios. Removing unnecessary schedules from the haystack would make the needles easier to find. We investigated this question on a diverse set of popular multithreaded programs, ranging from server programs such as Apache, to desktop utilities such as parallel compression utility PBZip2, to parallel implementations of computation-intensive algorithms such as fast Fourier transformation. They use diverse synchronizations such as locks, semaphores, condition variables, and barriers. Our investigation reveals two insights: First, for many programs, a wide range of inputs share the same equivalent class of schedules. Thus, one schedule out of the class suffices to process the entire input range. Intuitively, an input often contains two types of data: (1) metadata that controls the communication of the execution, such as the number of threads to spawn; and (2) computational data that the threads locally compute on. A schedule requires the input metadata to have certain values, but it allows the computational data to vary. That is, it can process any input that has the same metadata. For instance, consider the aforementioned PBZip2 which splits an input file among multiple threads, each compressing one file block. The communication, i.e., which thread gets which file block, is independent of the thread-local compression. Under a typical setup (e.g., no read failures or signals), for each different number of threads set by a user, PBZip2 can use two schedules (one if the file can be evenly divided by the number of threads and another otherwise) to compress any file, regardless of the file data.</p><p>This loose coupling of inputs and schedules is not unique to PBZip2; many other programs also exhibit this property. <ref type="table">Table 1</ref> shows a sample of our findings. The programs shown include three real-world programs, Apache, PBZip2, and aget (a parallel file download utility) and five implementations of computation-intensive algorithms from two widely used benchmark suites, Stanford's SPLASH2 and Princeton's PARSEC. (We describe how to compute the constraints that a schedule places on the inputs in §4.)</p><p>Second, the overhead of enforcing a schedule on different inputs is low. Presumably, the exponentially many schedules allow the runtime system to react to various timing factors and select an efficient schedule. However, results from the SMT systems we built invalidated this presumption. With carefully designed schedule representations, our systems incurred less than 15% overhead enforcing schedules on different inputs for most evaluated programs <ref type="bibr" target="#b7">[8]</ref>. We believe this moderate overhead is worth the gains in reliability. In general, users most likely prefer a slightly slower program that computes correct results than a faster program that frequently crashes.</p><p>Leveraging these insights, we have invented stable multithreading (SMT), a new multithreading approach that reuses each schedule on a wide range of inputs, mapping all inputs to a dramatically reduced set of schedules. By vastly shrinking the haystack, it addresses all  <ref type="table">Table 1</ref>: Constraints on inputs sharing the same equivalent class of schedules. For each program listed, one schedule out of the class suffices to process any input satisfying the constraints shown in the third column, assuming typical setups (e.g., no system call failures or signals). We describe how to compute such constraints in §4.</p><p>the needle-in-a-haystack challenges at once. In addition, SMT stabilizes program behaviors on inputs that map to the same schedule and minor program changes that do not affect the schedules, providing robustness and stability anticipated by developers and users. SMT and DMT are orthogonal. SMT aims to reduce the set of schedules for all inputs, whereas DMT aims to reduce the schedules for each input (down to one). A SMT system may be either deterministic or nondeterministic. <ref type="figure" target="#fig_2">Figure 1c</ref> and <ref type="figure" target="#fig_2">Figure 1d</ref> depict two SMT systems: the many-to-one mapping in <ref type="figure" target="#fig_2">Figure 1c</ref> is deterministic, while the many-to-few mapping in <ref type="figure" target="#fig_2">Figure 1d</ref> is nondeterministic. A many-to-few mapping improves performance because the runtime system can choose an efficient schedule out of a few for an input based on current timing factors, but it increases the efforts and resources needed for reliability. Fortunately, the choices of schedules are only a few (e.g., a small constant), so the challenges caused by nondeterminism are easy to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benefits</head><p>By vastly reducing schedules, SMT brings numerous reliability benefits to multithreading. We describe several: Testing. SMT automatically increases the coverage of schedule testing tools, with coverage defined as the ratio of tested schedules over all schedules. For instance, consider PBZip2 again which needs only two schedules for each different number of threads under typical setups. Testing 32 schedules effectively covers from 1 to 16 threads. Given that (1) PBZip2 achieves peak performance when the number of threads is identical or close to the number of cores and (2) a typical machine has up to 16 cores, 32 tested schedules can practically cover most schedules executed in the field. Debugging. Reproducing a bug now does not require the exact input, as long as the original and the altered inputs map to the same schedule. It does not require the exact program either, as long as the changes to the program do not affect the schedule. Users may remove private information such as credit card numbers from their bug reports. Developers may reproduce the bugs in different environments or add printf statements. Analyzing and verifying programs. Static analysis can now focus only on the set of schedules enforced in the field, gaining precision. Dynamic analysis enjoys the same benefits as testing. Model checking can now check drastically fewer schedules, mitigating the socalled "state explosion" problem. Interactive theorem proving becomes easier, too, because verifiers need prove theorems only on the set of schedules enforced in the field. We will discuss these benefits in §5. Avoiding errors at runtime. Programs can also adaptively learn correct schedules in the field, then reuse them on future inputs to avoid unknown, potentially buggy schedules. We will discuss this benefit in §4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Caveats</head><p>SMT is certainly not for every multithreaded program. It works well with programs whose schedules are loosely coupled with inputs, but there are also other programs. For instance, a program may decide to spawn threads or invoke synchronizations based on complex conditions involving most bits in the input. Parallel grep-like utility pfscan is an example. It searches for a keyword in a set of files using multiple threads, and for each match, it grabs a lock to increment a counter. A schedule computed on one set of files is unlikely to suit another.</p><p>SMT provides robustness and stability on small input and program perturbations when they do not affect schedules. However, there is still room to improve. For instance, when developers change their programs by adding synchronizations, it may be more efficient to update previously computed schedules rather than to recompute from scratch. We leave this idea for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Building Stable Multithreading Systems</head><p>Although the vision of stable multithreading is appealing, realizing it faces three main challenges:</p><p>• How can we compute the schedules to map inputs to?</p><p>They must be feasible so executions reusing them do not get stuck. They should also be highly reusable.</p><p>• How can we enforce schedules deterministically and efficiently? "Deterministically" so executions cannot deviate from schedules despite data races, and "efficiently" so overhead does not offset the benefits.</p><p>• How can we handle multithreaded server programs?</p><p>They often run for a long time and react to each client request as it arrives, making their schedules very specific to a stream of requests and difficult to reuse. Over the past four years, we have been tackling these challenges and building SMT systems, which resulted in two SMT prototypes, TERN <ref type="bibr" target="#b6">[7]</ref> and PEREGRINE <ref type="bibr" target="#b7">[8]</ref>, that frequently reuse schedules with low overhead. This section describes our solutions to these challenges. Our solutions are by no means the only ones; subsequent to TERN, others have also built a system that stabilizes schedules for general multithreaded programs <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Computing Schedules</head><p>TERN, our first SMT system, computes schedules by recording them from past executions; the recorded schedules can then be reused on future inputs to stabilize program behaviors. It works as follows. At runtime, it maintains a persistent cache of schedules recorded from past executions. When an input arrives, TERN searches the cache for a schedule compatible with the input. If it finds one, it simply runs the program while enforcing the schedule. Otherwise, it runs the program as is while recording a new schedule from the execution, and saves the schedule into the cache for future reuse.</p><p>When recording a schedule, TERN tracks how the synchronizations in the schedule depend on the input. It captures these dependencies into a relaxed, quickly checkable set of constraints called the precondition of the schedule. It then reuses the schedule on all inputs satisfying the precondition.</p><p>A na¨ıvena¨ıve way to compute the precondition is to collect constraints from all input-dependent branches in an execution. For instance, if a branch instruction inspects input variable X and goes down the true branch, we add a constraint that X must be nonzero to the precondition. A precondition computed this way is sufficient, but it contains many unnecessary constraints concerning only threadlocal computations. Since an over-constraining precondition decreases schedule-reuse rates, TERN removes these unnecessary constraints from the precondition.</p><p>The TERN approach to computing schedules has several benefits. First, by reusing schedules shown to work, TERN may avoid potential errors in unknown schedules, improving reliability. A real-world analogy is the natural tendencies in humans and animals to follow familiar routes to avoid possible hazards along unknown routes. (The name TERN comes from the Arctic Tern, a bird species that migrates the farthest among all animals.) Second, TERN explicitly stores schedules, so developers and users can flexibly choose what schedules to record and when. For instance, developers can populate the cache during testing and deploy the cache together with their program, improving testing effectiveness and avoiding the overhead to record schedules on user machines. Moreover, they can run their favorite checking tools and keep only the correct schedules in the cache.</p><p>Lastly, TERN is efficient because it can amortize the cost of computing schedules. Specifically, recording and checking a schedule is more expensive than reusing a schedule, but, fortunately, TERN does it only once for each schedule and then reuses the schedule on many inputs, amortizing the cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Efficiently Enforcing Schedules</head><p>Prior work enforces schedules at two different granularities: shared memory accesses or synchronizations, forcing users to trade off efficiency and determinism. Specifically, memory access schedules make data races deterministic but are prohibitively inefficient (e.g., 1.2X-6X as slow as traditional multithreading <ref type="bibr" target="#b2">[3]</ref>); synchronization schedules are much more efficient (e.g., average 16% slowdown <ref type="bibr" target="#b14">[15]</ref>) because they are coarse grained, but they cannot make programs with data races deterministic, such as our second toy program in §2 and many realworld multithreaded programs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>. This determinism vs. performance challenge has been open for decades in the areas of deterministic execution and replay. Because of this challenge, TERN, our first SMT system, enforces only synchronization schedules.</p><p>To address this challenge, we have built PEREGRINE, our second SMT system <ref type="bibr" target="#b7">[8]</ref>. The insight in PEREGRINE is that although many programs have races, the races tend to occur only within small portions of an execution, and the majority of the execution is still race-free. Intuitively, if a program is full of data races, most of them would have been caught during testing. Empirically, we analyzed the executions of seven real programs with races, and found that, despite millions of memory accesses, only up to 10 data races were detected per execution. Since races occur rarely, we can schedule synchronizations for the race-free portions of an execution, and resort to memory accesses only for the "racy" portions, combining both the efficiency of synchronization schedules and the determinism of memory access schedules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Handling Server Programs</head><p>Server programs present three challenges for SMT. First, they may run continuously, making their schedules effectively infinite and too specific to reuse. Second, they often process inputs, i.e., client requests, as soon as the requests arrive. Each request may arrive at a random moment, causing a different schedule. Third, since requests do not arrive at the same time, PEREGRINE cannot check them against the precondition of a schedule upfront.</p><p>Our observation is that server programs tend to re-turn to the same quiescent states, so PEREGRINE can use these states to split a continuous request stream down to windows of requests. Specifically, PEREGRINE buffers requests as they arrive until it gathers enough requests to keep all worker threads busy. It then runs the worker threads to process the requests, while buffering newly arrived requests to avoid interference between windows.</p><p>If PEREGRINE cannot gather enough requests before a predefined timeout, it proceeds with the partial window to reduce response time. By breaking a request stream into windows, PEREGRINE can record and reuse schedules across windows, stabilizing server programs. Server quiescent states may evolve. For instance, a web server may cache requests in memory. Developers can annotate the functions that query cache, and PEREGRINE treats the return values as inputs and selects proper schedules. Windowing reduces concurrency, but the cost is moderate based on our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applying Stable Multithreading For Better Program Analysis</head><p>As discussed in §3, stable multithreading can be applied in many ways to improve reliability. In this section, we describe a program analysis framework we have built atop PEREGRINE to effectively analyze multithreaded programs, an open challenge in program analysis.</p><p>At the core of this open challenge lies the tradeoff between precision and coverage. Of the two common types of program analysis, static analysis covers all schedules but with poor precision (e.g., many false positives). The reason is that it must over-approximate the enormous number of schedules, and thus it may analyze a much larger set of schedules, including those impossible to occur at runtime. Not surprisingly, it may detect many "bugs" in the impossible schedules. Dynamic analysis precisely identifies bugs because it sees the code's precise runtime effects. However, it has poor coverage because of the exponentially many schedules.</p><p>Fortunately, SMT shrinks the set of possible schedules, enabling a new program analysis approach that gets the best of both static analysis and dynamic analysis. Specifically, our framework statically analyzes a parallel program over only a small set of schedules at compile time, then dynamically enforces these schedules at runtime. By focusing on only a small set of schedules, we vastly improve the precision of static analysis and reduce false positives; by enforcing the analyzed schedules dynamically, we guarantee high coverage. Dynamic analysis benefits, too, because it enjoys automatically increased coverage defined as the ratio of checked schedules.</p><p>Our framework enables the construction of many high coverage and highly precise analyses. For instance, the static race detector we built found seven previously unknown, harmful races in programs extensively checked by previous tools. It emits extremely few false positives, none for 10 out of 18 programs, a huge reduction compared to other static race detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>Through conceiving, building, applying, and evaluating SMT systems, we have demonstrated that SMT is feasible; it can stabilize program behaviors for better reliability, work both efficiently and deterministically, and greatly improve precision of static analysis. We believe SMT offers new promises to solve the grand parallel programming challenge. However, TERN and PEREGRINE are still research prototypes, not yet ready for wide adoption. Moreover, the ideas explored are just the first few in this direction of SMT; the bulk of work lies ahead:</p><p>• At the system level, can we build efficient, lightweight SMT systems that work automatically with all multithreaded programs? TERN and PERE-GRINE require recording executions and analyzing source code, which can be heavyweight. As the number of cores increases, can we build SMT systems that scale to hundreds of cores? • At the application level, we have only scratched the surface: improving program analysis is just one cool application. There are many others, such as improving testing coverage, verifying a program with respect to a small set of dynamically enforced schedules, and optimizing thread scheduling and placement based on a schedule because it effectively predicts the future. Moreover, the idea of stabilizing schedules may apply to other parallel programming methods such as MPI, OpenMP, and Cilk-like tasks.</p><p>• At the conceptual level, can we reinvent parallel programming to greatly reduce the set of schedules? For instance, a multithreading system may disallow schedules by default, and only allow those that developers explicitly write code to enable. Since developers are already of different calibers, we may let only the best programmers decide what schedules to use, reducing the likelihood of programming errors.</p><p>We invite readers to join us in exploring this exciting direction of stable multithreading and reliable parallelism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Different multithreading approaches. Red stars represent buggy schedules.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Al Aho, Tom Bergan, Emery Berger, Luis Ceze, Jim Larus, Ying Xu, and the anonymous reviewers for their many helpful comments. This work was supported in part by AFRL FA8650-11-C-7190, FA8650-10-C-7024, and FA8750-10-2-0253; ONR N00014-12-1-0166; NSF CCF-1162021, CNS-1117805, CNS-1054906 (CAREER award), and CNS-0905246; an AFOSR YIP Award; and a Sloan Research Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A view of the parallel computing landscape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bodik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Keaveny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient system-enforced deterministic parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aviram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="111" to="119" />
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CoreDet: a compiler and runtime system for deterministic multithreaded execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bergan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS &apos;10)</title>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Grace: safe and efficient concurrent programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Novark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Object-Oriented Programming Systems, Languages, and Applications (OOP-SLA &apos;09)</title>
		<imprint>
			<date type="published" when="2009-10" />
			<biblScope unit="page" from="81" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A type and effect system for deterministic parallel java</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Bocchino</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Heumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Komuravelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Overbey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vakilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA &apos;09)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="97" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-world concurrency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cantrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bonwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="34" to="39" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stable deterministic multithreading through schedule memoization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</title>
		<meeting>the Ninth Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient deterministic multithreading through schedule relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP &apos;11)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="page" from="337" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DMP: deterministic shared memory multiprocessing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lucia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS &apos;09)</title>
		<imprint>
			<date type="published" when="2009-03" />
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The problem with threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DTHREADS: efficient deterministic multithreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Curtsinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP &apos;11)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP &apos;11)</meeting>
		<imprint>
			<date type="published" when="2011-10" />
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning from mistakes: a comprehensive study on real world concurrency bug characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on Architecture Support for Programming Languages and Operating Systems (AS-PLOS &apos;08)</title>
		<imprint>
			<date type="published" when="2008-03" />
			<biblScope unit="page" from="329" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding and reproducing heisenbugs in concurrent programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qadeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Basler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Nainar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Neamtiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Symposium on Operating Systems Design and Implementation (OSDI &apos;08)</title>
		<meeting>the Eighth Symposium on Operating Systems Design and Implementation (OSDI &apos;08)</meeting>
		<imprint>
			<date type="published" when="2008-12" />
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A conversation with john hennessy and david patterson</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>O&amp;apos;hanlon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kendo: efficient deterministic multithreading in software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ansel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth International Conference on Architecture Support for Programming Languages and Operating Systems (ASPLOS &apos;09)</title>
		<imprint>
			<date type="published" when="2009-03" />
			<biblScope unit="page" from="97" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bypassing races in live applications with execution filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</title>
		<meeting>the Ninth Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sound and precise analysis of parallel programs through schedule specialization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGPLAN 2012 Conference on Programming Language Design and Implementation (PLDI &apos;12)</title>
		<meeting>the ACM SIGPLAN 2012 Conference on Programming Language Design and Implementation (PLDI &apos;12)</meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ad hoc synchronization considered harmful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</title>
		<meeting>the Ninth Symposium on Operating Systems Design and Implementation (OSDI &apos;10)</meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Concurrency attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sethumadhavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Fourth USENIX Workshop on Hot Topics in Parallelism (HOTPAR &apos;12)</title>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
