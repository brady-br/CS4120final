<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Case for Benchmarking Control Operations in Cloud Native Storage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Merenstein</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Tarasov</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research-Almaden</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Anwar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research-Almaden</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepavali</forename><surname>Bhagwat</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research-Almaden</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Rupprecht</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research-Almaden</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Skourtis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research-Almaden</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Case for Benchmarking Control Operations in Cloud Native Storage</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Storage benchmarking tools and methodologies today suffer from a glaring gap-they are incomplete because they omit storage control operations, such as volume creation and deletion, snapshotting, and volume reattachment and resizing. Control operations are becoming a critical part of cloud storage systems, especially in containerized environments like Kubernetes, where such operations can be executed by regular non-privileged users. While plenty of tools exist that simulate realistic data and metadata workloads, control operations are largely overlooked by the community and existing storage benchmarks do not generate control operations. Therefore, for cloud native environments, modern storage benchmarks fall short of serving their main purpose-holistic and realistic performance evaluation. Different storage provisioning solutions implement control operations in different ways, which means we need a unified storage benchmark to contrast and comprehend their performance and expected behaviors. In this position paper, we motivate the need for a cloud native storage benchmark by demonstrating the effect of control operations on storage provisioning solutions and workloads. We identify the challenges and requirements when implementing such benchmark and present our initial ideas for its design.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Storage benchmarking is a well established and prolific field <ref type="bibr" target="#b23">[32,</ref><ref type="bibr" target="#b27">36]</ref>. Most systems papers conclude with a system evaluation using one or more benchmarks. Industry uses benchmarks for performance and regression testing-and sometimes publishes results to inform and attract customers <ref type="bibr" target="#b0">[1]</ref>. Many storage benchmarks are publicly available and widely used <ref type="bibr">[12,</ref><ref type="bibr" target="#b12">21,</ref><ref type="bibr" target="#b25">34]</ref>.</p><p>Benchmarks (1) generate workloads and (2) measure a system's performance under those workloads-to analyze system behavior and compare to others. Storage benchmarks can be divided into two classes of workload generation: micro and macro <ref type="bibr" target="#b27">[36]</ref>. Micro benchmarks generate a relatively simple workload focusing on a few I/O operations to stress-test one system component or operational mode. Conversely, macro benchmarks try to mimic real-world workloads by generating a complex mix of operations with realistic properties-with the purpose of exercising a system more holistically. Trace analysis is frequently used to create macro benchmarks <ref type="bibr" target="#b24">[33]</ref>.</p><p>Both micro and macro benchmarks focus solely on data operations (e.g., reads, writes) and metadata operations (e.g., file creates and deletes). The third type of operations-control-is largely ignored. Control operations include volume creations, deletions, attachments, detachments, restores, and resizes; partitioning and formatting; and snapshot creations and deletions.</p><p>In the past, control operations were relatively infrequentinvoked only by system administrators. However, in cloud native environments, control operations are routinely used by non-privileged users and even automated users <ref type="bibr" target="#b4">[11,</ref><ref type="bibr" target="#b7">15]</ref>. In the cloud, owners and users control their applications and storage exclusively, and they do not need administrators. Therefore, the frequency of control operations has surged (see Section 2) and is mostly dependent on the number of users and the speed of software development and deployment.</p><p>Modern container clusters contain thousands of physical nodes and can run hundreds of thousands of containers for many users <ref type="bibr">[3]</ref>. A storage system (e.g., Amazon Elastic Block Store or a Ceph cluster) serving such large-scale deployments inevitably experiences a high rate of control operations-volume creations, deallocations, attachments, reattachments, etc.</p><p>The fundamental shift towards cloud native environments creates a high, previously unseen rate of control operations. The situation is further exacerbated by three more reasons. (1) The use of the microservices architecture <ref type="bibr" target="#b26">[35]</ref> in modern applications increases the number of independent containers that may require their own storage volumes <ref type="bibr" target="#b16">[25]</ref>. (2) Control operations can be data intensive: this makes them slow, but also increases their impact on the data and metadata operations. For example, volume creation often requires file system formatting; volume resizing may require data migration; and volume reattachment requires cache flushes and warmups. (3) In the past, control operations were functionally tested but rarely benchmarked. Anecdotally, we know that even high-performance storage systems may be slow because they were not designed to service many control operations.</p><p>Thus, there is a gap in the available storage benchmarking solutions as they ignore cloud native use cases. We propose to fill this gap by introducing a design of a cloud native storage benchmark that focuses on control operations. This position paper makes the following contributions: environments, the lifecycle of resources in a container environment can be shorter, whereas the number of resources, such as containers and volumes, can be greater. The low overhead of running containers and the micro-service architecture breeds an environment of ephemeral, short-lived resource use. For example, there was a reported "2× increase in the number of containers that live for less than five minutes" in 2019 compared to 2018 on one DevOps platform <ref type="bibr" target="#b14">[23]</ref>. Fast container startup times, along with high container churn rate, result in frequent control operations (e.g., creating, attaching, or mounting volumes). Consequently, control operations are becoming latency sensitive, adding significant load to the storage system and affecting I/O operations and user experience.</p><p>Next, we describe how Kubernetes, a popular container orchestrator, manages storage volumes. Although we focus on Kubernetes, similar concepts exist in other orchestrators such as Apache Mesos <ref type="bibr" target="#b1">[2]</ref> and Docker Swarm <ref type="bibr">[10]</ref>.</p><p>Kubernetes is a platform for running containerized applications in a cloud native fashion <ref type="bibr" target="#b6">[14]</ref>. In Kubernetes, if a user wants to deploy a new application that requires persistent storage, the user has to create a Persistent Volume Claim (PVC) to request a new file system or block-based storage volume. A PVC describes the high-level properties of the volume such as size or expected performance. Using the specific storage system's Container Storage Interface (CSI) <ref type="bibr" target="#b3">[7]</ref> driver, Kubernetes automatically communicates with the storage system to allocate a volume. When users start an application that uses a provisioned volume, Kubernetes automatically attaches the volume to the node where the application's container is scheduled to run.</p><p>Kubernetes defines various resources such as the Pod (a group of containers) and the Service (a frontend to a set of Pods) as abstractions for deploying applications. Similar to compute and networking resources, the orchestrators provide storage in the form of volumes for application consumption. A volume in Kubernetes is called a Persistent Volume (PV); it is usually backed by storage hosted on a given storage provider (SP). A PV can be provisioned either dynamically or manually by an administrator.</p><p>A user can request a PV by creating a Persistent Volume Claim (PVC), specifying properties such as the required volume size and access mode. The backing storage solution and its parameters, referred to as the provisioner, can be expressed through a Storage Class (SC). Upon creation of a PVC, Kubernetes tries to find a corresponding PV that matches the request or asks the provisioner to create a PV dynamically. The PV is then bound to the PVC.</p><p>Assuming that a PVC is bound to a PV, a user can consume it through a Pod as a device or file system. During Pod creation, once the Pod has been scheduled to a node, the PV is published or staged onto that node and then published to the Pod. Alternatively, a PVC may be described as part of a Pod specification. In that case, dynamic provisioning takes place during Pod creation. Staging and publishing operations must complete prior to the Pod becoming available to the user, making their performance critical.</p><p>To support different SPs, Kubernetes, as well as other orchestrators, use the Container Storage Interface (CSI) <ref type="bibr" target="#b3">[7]</ref>. CSI enables SPs to develop and maintain a single plugin for all orchestrators that support the CSI specification. There are currently 23 operations specified in the CSI specification, ranging from volume and snapshot creation, to publishing volumes to nodes, and listing volume information.</p><p>Next we illustrate a Kubernetes-specific example that triggers several control operations. Suppose that an application developer wants to deploy multiple web server instances. Furthermore, assume that the local administrator has set up a storage system that provides a CSI plugin and has also created a Storage Class corresponding to the storage system. The developer is given access to the cluster and creates a Pod for their web server and a PVC for the volume, to be used by the web server. The PVC specifies that the volume should be provisioned from the Storage Class created previously by the cluster administrator.</p><p>At this point, Kubernetes creates a PVC resource describing the volume request. This triggers a call to the storage system's CSI plugin's CreateVolume implementation, which creates the volume as intended by the SP. Upon creation of the volume CreateVolume returns a PV specification, which is then created by Kubernetes and is bound to the PVC that initiated the process.</p><p>Once the PV has been bound to the PVC and the web server's Pod is created and scheduled, the CSI plugin's NodeStageVolume operation is invoked. NodeStageVolume prepares the PV to be used by a Pod by formatting it with a file system and mounting its volume on the Node where the Pod is scheduled.</p><p>Finally, the PV's file system needs to be mounted to the specific Pod. This happens through the NodePublishVolume operation. Here, the NodePublishVolume simply bind-mounts the node directory under a path corresponding to the Pod's volume. The web server Pod now starts and the volume becomes visible to the Pod. In practice, a developer would deploy multiple web server instances, potentially pointing to the same volume, leading to multiple volume staging and publishing events.</p><p>How many control operations per second should an SP be able to sustain? We develop an example based on statistics from a DevOps environment <ref type="bibr" target="#b14">[23]</ref>, where (1) each host runs a median of 30 containers; (2) 54% of containers have a lifespan of up to 5 minutes. Furthermore, we assume that the storage system is attached to 1,000 hosts. This results in an estimate of 1000 * 30/(5 * 60)=100 container creations per second, implying that the SP in this case needs to support 100 attach (and detach) operations per second under typical operation. This shows that the number of control operations in cloud native environments can be far larger than is typically seen elsewhere-hence the need to benchmark control operations carefully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Impact of Control Operations</head><p>To better illustrate the need for a cloud native storage benchmark, we demonstrate the importance of control operations using two different experiments: (1) speed of volume creation and attach-   <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b5">13,</ref><ref type="bibr" target="#b8">16,</ref><ref type="bibr" target="#b9">17,</ref><ref type="bibr" target="#b10">19,</ref><ref type="bibr" target="#b13">22,</ref><ref type="bibr" target="#b15">24]</ref>. We used three popular and different-by-design storage providers-OpenEBS, Gluster, and Ceph-to provision Kubernetes volumes.</p><p>OpenEBS <ref type="bibr" target="#b9">[17]</ref> is a cloud native storage provider that follows the Container Attached Storage approach. This means that OpenEBS deploys a separate container-based controller for each PV which handles control operations for that PV. This approach is more flexible as it permits each application to specify different storage parameters (e.g., replication factor). We used OpenEBS v1.15 and the OpenEBS cStor <ref type="bibr">[8]</ref> storage engine, which formats disks with ZFS and then provisions PVs by creating ZFS volumes and formatting them with Ext4.</p><p>Gluster <ref type="bibr" target="#b5">[13]</ref> is a distributed file system: it aggregates file systems on local nodes to form volumes that can be mounted via NFS or a Gluster FUSE driver. We configured a single node Gluster cluster, running Gluster v6.0 on just one of our Kubernetes workers.</p><p>Ceph <ref type="bibr" target="#b2">[4]</ref> uses the RADOS <ref type="bibr" target="#b28">[37]</ref> object store to provide object, block, or file storage systems. We created a Ceph v14.2.7 storage cluster and provisioned PVs by creating Ceph Block Devices from the storage pool and formatting the block devices with Ext4.</p><p>All three storage providers we used support common storage control operations such as provisioning, snapshotting, and resizing volumes; all three expose these operations to Kubernetes through their respective CSI drivers.</p><p>Experiment 1: Volume creation and attachment. We compared the performance of volume attachment and creation for OpenEBS, Gluster, and Ceph.</p><p>To measure volume attachment and creation, we timed how long it took to start a Pod that used a 1 GiB volume provisioned from one of the three storage providers. We used both pre-existing and new volumes: using pre-existing volumes measures volume attachment time; using new volumes measures the combined volume attachment and volume provisioning time. Because we measure the overall time to start a Pod, both cases also include the extra time needed to create the Pod itself; however, this time is small (only a few seconds) compared to the other two operations and is the same regardless of the storage provider used. To see the effect of a large number of simultaneous control operations, we tested both starting (1) only one Pod at a time and also (2) starting 50 Pods at once. <ref type="figure" target="#fig_0">Figure 1</ref> shows the results in CDF form. We can clearly see that there are differences in how storage providers perform these control operations. For example, although OpenEBS is the fastest storage provider at attaching pre-provisioned volumes, it is the slowest at provisioning and then attaching volumes. This could be due to the fact that OpenEBS allocates and starts a controller Pod for each new volume, which adds overhead to volume provisioning not present in Gluster or Ceph.</p><p>Both the variation in and the overall provision times for all three storage providers are surprisingly high. This is in part due to Kubernetes's asynchronous architecture: some components batch updates and others poll for updates periodically. Since provisioning volumes consists of a series of steps, delays at each step can compound, increasing provisioning times and their variance. More experimentation is needed to determine exactly which parts of the provisioning process account for the high delays and variance.</p><p>The speed with which volumes can be provisioned and attached to application Pods mainly affects Pod startup time, which in turn affects failure recovery time and the time required to scale out applications. For example, Pods using OpenEBS volumes would likely recover faster in the event of an application failure, as OpenEBS is fastest at re-attaching volumes to new Pods. Conversely, Pods using Gluster or Ceph volumes could better respond to load increases by scaling out and deploying additional Pods that require additional volumes. Experiment 2: Snapshotting. Next, we aimed to study the impact of snapshotting on I/O workloads. Although many storage solutions now provide low-overhead volume snapshots, the actual overhead incurred by the application workload depends on various factors, including the characteristics of the workload and the storage solution itself. In addition, snapshots have the potential to impact the workloads of other users that are either co-located on the same node or that share the same underlying storage.</p><p>To measure the effects introduced by snapshotting, we used the fio <ref type="bibr">[12]</ref>   on a configuration recommended for evaluating database I/O performance <ref type="bibr" target="#b29">[38]</ref>. We ran five instances of fio for 30 minutes. Each instance was a separate Pod and was attached to a different volume provisioned from the same storage provider. During the 30 minute run, we cycled through each of the five volumes, taking snapshots at regular intervals: every 90 seconds for the 20 snapshot test; and every 45 seconds for the 40 snapshot test. An individual volume therefore was snapshotted every 7.5 and 3.75 minutes for the 20-and 40-snapshot tests, respectively.</p><p>We configured fio to log I/O operation latencies. <ref type="figure" target="#fig_1">Figure 2</ref> shows how these latencies were affected when a different number of snapshots were taken during the test. Again, the difference in storage providers is apparent. Although Gluster and Ceph are minimally affected, OpenEBS is affected more significantly. For example, when 20 snapshots are taken during fio's run, OpenEBS's 99.9 th percentile read-latency grows 3.3× compared to the baseline, and when 40 snapshots are taken it grows to 17×.</p><p>Together, the experiments show that (1) storage providers differ in how they perform and scale with frequent control operations and (2) control operations do influence the I/O latency for workloads running in neighboring containers. Both factors impact the end users of cloud services directly and indirectly. Workflows that rely on creating new containers frequently-e.g., Continuous Integration (CI) services-can be impacted directly by the performance of control operations such as volume creation and attachment. Additionally, the overhead caused by control operations can indirectly reduce the user-visible performance of applications <ref type="bibr" target="#b22">[31]</ref>. Understanding these behaviors allows cloud administrators to make informed decisions when choosing storage solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Benchmark Design</head><p>We believe that the aforementioned trend analysis and exploratory experiments justify the need to create a benchmark that treats control operations as first-class citizens.</p><p>Such a benchmark would be useful to anyone making design or operational decisions regarding their cloud-based services. For instance, an administrator could use the benchmark to examine the relationship between the number of containers deployed concurrently and their startup times. We first discuss the design requirements for a cloud native storage benchmark and then present our initial design.</p><p>Requirements. We identify 9 core requirements a cloud native storage benchmark should address. The requirements cover aspects of workload generation (W), result measurement and visualization (R), and usability (U). W1: The workload of a cloud native storage benchmark is two-dimensional as it needs to evaluate both the performance of control operations and their effect on I/O operations. Hence, the benchmark should generate a mix of both operation types. Each dimension should be separately controllable to allow measuring the performance of control operations in isolation and in combination with I/O operations, under different loads. W2: The benchmark's workload description should allow users to specify the type and frequency of control operations and the amount of work performed. In addition to straightforward control operations (volume creation, snapshot deletion, etc.), the benchmark should also trigger aborting a running I/O workload in order to evaluate fault recovery performance. Generating the faults of the storage system itself, however, is out of scope for the benchmark, but tools such as Chaos Monkey <ref type="bibr">[5]</ref> could be used for that. It should also be possible to compose different control-operation patterns to generate more complex and realistic workloads. W3: I/O operations can be generated by a variety of existing sources (e.g., fio or real-world applications). The benchmark has to support these different sources using a pluggable architecture. The benchmark should also ship with default sources that can be used without additional configuration efforts. W4: The target environment of the benchmark are cloud native setups, which normally host a large number of different tenants, isolated through different QoS mechanisms. The benchmark should allow to evaluate the quality and reliability of the mechanisms used for storage I/O isolation. In particular, the benchmark needs to enable users to configure QoS targets to detect potential QoS violations. R1: As stated in W3, I/O operations can come from a variety of sources. Hence, performance measurements of I/O operations should be decoupled from their workload generation, to be able to capture basic metrics (e.g., IOPS, bandwidth, and latency) for any source. As cloud native environments often already contain elaborate monitoring tools such as Prometheus <ref type="bibr" target="#b11">[20]</ref>, the benchmark should integrate with those tools. It should also allow one to easily export workload-specific metrics, such as transactionsper-second in a DB benchmark, to specific analysis tools. R2: A single run of the benchmark can produce a large amount of measurement data, coming from a variety of different sources at different granularities (cluster nodes, Pods, PVs, etc.). In its raw form, this data will be hard to interpret for end users due to its variety. The benchmark needs to aggregate all the results and present them in a comprehensive, clear, and actionable way. R3: Cloud native clusters can have thousands of nodes <ref type="bibr">[3]</ref>. Hence, the benchmark should scale to large clusters while keeping the overhead of metrics collection low; the benchmark should utilize the deployment management and monitoring capabilities offered by cloud native environments. Experimentation will be needed to measure the overhead and find an acceptable threshold. For large clusters it may be necessary to tune the number and level of detail of the metrics collected, in order to keep the overhead within the desired threshold. U1: The benchmark should support reproducibility as a first-class citizen to allow easy comparison of different results. To support reproducibility, the benchmark has to collect and store enough information on the experiment, the cluster, and the storage configuration. Experimentation will be needed to determine what information is necessary to collect to ensure reproducibility. To avoid any external dependencies, collection should be restricted to information that can be retrieved natively from the environment (e.g., through Kubernetes's API server). U2: The benchmark should be easy to deploy and use. While this is a general requirement for any benchmark, it is especially important in this context due to the scale and complexity of cloud native environments. This means the benchmark needs to package and deploy all required dependencies, seamlessly integrate with the platform by relying on available primitives (e.g., <ref type="bibr">Operators and DaemonSets in Kubernetes)</ref>, and allow for simple workload descriptions in accepted formats (e.g., YAML).</p><p>Although there are many popular I/O benchmarks such as fio or filebench, these tools lack the capabilities and infrastructure necessary to orchestrate running multiple workloads from different sources while also executing control operations. Moreover, we do not view orchestration and control operations as a simple extension to capabilities of I/O storage benchmarks. Further, requirement W3 necessitates being able to use a variety of existing benchmarks to generate I/O workloads. Therefore, we believe a new benchmark is needed that includes orchestration, control operations, and support for a variety of existing I/O benchmarks to generate I/O operations.</p><p>Design. Our preliminary design addresses some of the above requirements (W1-W3, R1, R3, and U2); we plan to address the remaining ones (W4, R2, and U1) in the final implementation. For the purpose of illustration, we describe our design in terms of a possible implementation for Kubernetes. However, this does not mean that the design is applicable only to Kubernetes; the benchmark could be implemented for any cloud platform that can be interfaced with through an API (e.g., OpenStack or VMWare ESX).</p><p>Our design is well suited for Kubernetes' operator design pattern <ref type="bibr">[18]</ref>. This pattern has two components: (i) a custom user object and (ii) a controller that watches object modifications and acts accordingly <ref type="bibr">[18]</ref>.</p><p>The benchmark controller 1 is the centerpiece of our cloud native storage benchmark (see <ref type="figure" target="#fig_2">Figure 3)</ref>. The controller runs in To meet requirements W1-W3, workloads are defined in two parts: (i) ioOpsUnits, which describe the characteristics of the I/O operations to run; and (ii) controlOps, which describe the control operation characteristics. I/O operations are represented as a collection of I/O units-IOUs 4 -each of which corresponds to a single Pod running some I/O workload consisting of both data and metadata operations. IOU Pods start from container images; we plan to provide a curated set of IOU images (e.g., filebench, fio, iozone, specSFS) in a public image registry like Docker Hub 5 . Users will also be able to specify their own IOU images.</p><p>Control operations are specified in the benchmark as part of the controlOps section. These operations are directly executed by the controller on PVCs and their corresponding PVs 6 and Pods. For example, the controller may periodically create several new PVCs, snapshot some PVs, and reschedule IOU Pods to trigger PV reattachment. To fully understand the extent to which users will want to customize their control workloads, more analysis and user feedback is necessary.</p><p>When the controller receives a Benchmark definition, it creates the specified number of Pods. Kubernetes schedules the IOU Pods in the cluster, pulls the necessary images, and starts the Pods. Benchmarks can typically generate a number of different IOUs, configurable with parameters. Users can specify IOU-specific configurations in the benchmark definition through a ConfigMap <ref type="bibr">[6]</ref> or a URL to a config file. PVs used by the benchmark are created in the Storage Class 7 listed by a user in the Benchmark object definition. This allows one to specify which storage provider (e.g., OpenEBS) to benchmark.</p><p>Besides the workload, a separate Pod will be running to collect the system performance, resource utilization, and performance numbers reported by benchmarks 8 . We plan to utilize the ELK stack <ref type="bibr" target="#b19">[28,</ref><ref type="bibr" target="#b20">29]</ref> to analyze and visualize the collected measurements. This addresses R1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>The need to test the performance of cloud storage has motivated academia and industry to develop several micro-benchmarks such as YCSB <ref type="bibr" target="#b17">[26]</ref> and COSBench <ref type="bibr" target="#b30">[39]</ref>. YCSB is an extensible workload generator that evaluates the performance of different cloud serving key-value stores. COSBench measures the performance of Cloud Object Storage services. Unlike YCSB, COSBench targets more generic workloads and is not limited to key-value or object storage.</p><p>TailBench <ref type="bibr" target="#b21">[30]</ref> provides a set of interactive macro-benchmarks: web servers, databases for speech recognition, and machine translation systems to be executed in the cloud. Similarly, DeathStarBench <ref type="bibr" target="#b18">[27]</ref> is a benchmark suite for microservices and their hardware-software implications for cloud and edge systems. Both TailBench and DeathStarBench target cloud applications and are not explicitly storage benchmarks.</p><p>Traeger et al. <ref type="bibr" target="#b27">[36]</ref> conducted an extensive study of file systems and storage benchmarks. Yet, we are not aware of any studies that focused on benchmarking control operations in cloud native storage systems-this position paper's focus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>There are many, diverse cloud native storage solutions but their real-world performance is poorly understood. This is largely due to the shortcomings of existing benchmarks, which are not able to generate control operations. Control operations are an essential part of the cloud native storage workflow and are becoming increasingly more frequent as regular, non-privileged users are able to issue control operations without involving a storage administrator. Therefore, we argue that it is essential for any cloud native storage benchmark to treat control operations as a first-class citizen. In this position paper, we demonstrated this need with two sample studies; we show that control operations can impact the I/O workloads of containers significantly and that they result in large performance variations across different solutions. We presented a set of requirements for cloud native storage benchmarks and an initial design to address those challenges and get one step closer to effectively benchmarking cloud native storage systems.</p><p>To implement this design as a practical benchmark, community input will be critical. Discussions with academics and professionals will help reach the optimal level of versatility, expressiveness, realism, and ease-of-use. Therefore, we envision the benchmark as an open-source project with a community built around it. In addition, as current storage traces lack control operations, we hope to find partners that can share traces or aggregated statistics from their environments with control operations included. This will be important in developing realistic control workloads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CDFs of how long it took storage providers to create and attach a volume to a Pod. Times were measured by attaching (i) a single volume at a time (in a series of 50) and (ii) 50 volumes simultaneously. ment; and (2) volume snapshotting impact on I/O operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Effect of snapshotting on latency of I/O operations. We show 50 th , 95 th , 99 th , and 99.9 th percentiles. Note that the Y axes' scales are different for each plot, because the ranges of latencies varies significantly across storage providers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Proposed high-level system design one or more Pods and subscribes to Kubernetes for the creation, deletion, and modification of Benchmark objects. A Benchmark is a custom object type [9] defined in Kubernetes during the installation of the benchmark. To run a benchmark, a user 2 creates an instance of a Benchmark type, defined as per Kubernetes convention, in YAML format 3 . This instance represents one or more runs of the same workload. Using existing Kubernetes constructs in the benchmark design addresses requirements R3 and U2. To meet requirements W1-W3, workloads are defined in two parts: (i) ioOpsUnits, which describe the characteristics of the I/O operations to run; and (ii) controlOps, which describe the control operation characteristics. I/O operations are represented as a collection of I/O units-IOUs 4 -each of which corresponds to a single Pod running some I/O workload consisting of both data and metadata operations. IOU Pods start from container images; we plan to provide a curated set of IOU images (e.g., filebench, fio, iozone, specSFS) in a public image registry like Docker Hub 5 . Users will also be able to specify their own IOU images. Control operations are specified in the benchmark as part of the controlOps section. These operations are directly executed by the controller on PVCs and their corresponding PVs 6 and Pods. For example, the controller may periodically create several new PVCs, snapshot some PVs, and reschedule IOU Pods to trigger PV reattachment. To fully understand the extent to which users will want to customize their control workloads, more analysis and user feedback is necessary. When the controller receives a Benchmark definition, it creates the specified number of Pods. Kubernetes schedules the IOU Pods in the cluster, pulls the necessary images, and starts the Pods. Benchmarks can typically generate a number of different IOUs, configurable with parameters. Users can specify IOU-specific configurations in the benchmark definition through a ConfigMap [6] or a URL to a config file. PVs used by the benchmark are created in the Storage Class 7 listed by a user</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>benchmark to generate a workload with five fio threads accessing the same 5 GB file with an even mix of sequential reads and writes. This workload is based</figDesc><table>0 

20 
40 
0.00 

0.02 

0.04 

latency (s) 

Gluster 

0 
20 
40 

Number of snapshots 

0.00 

0.02 

0.04 

Ceph 

0 
20 
40 
0.0 

0.5 

1.0 

OpenEBS 

0 
20 
40 
0.000 0.002 0.004 

Gluster 

0 
20 
40 

Number of snapshots 

0.00 0.02 0.04 

Ceph 

0 
20 
40 
0 

1 

2 

OpenEBS 
Write Latencies 
Read Latencies 

p50 
p95 
p99 
p99.9 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spec</forename><surname>All</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sfs</surname></persName>
		</author>
		<ptr target="https://www.spec.org/sfs2014/results/sfs2014.html" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Mesos</surname></persName>
		</author>
		<ptr target="https://mesos.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceph</surname></persName>
		</author>
		<ptr target="https://ceph.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Container Storage Interface (CSI) Specification</title>
		<ptr target="https://bit.ly/3bqQX4b" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dynamic Provisioning and Storage Classes in Kubernetes</title>
		<ptr target="https://bit.ly/2Uh3Qbw" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gluster</surname></persName>
		</author>
		<ptr target="https://www.gluster.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kubernetes</surname></persName>
		</author>
		<ptr target="https://kubernetes.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubernetes</forename><surname>Storage</surname></persName>
		</author>
		<ptr target="https://kubernetes.io/docs/concepts/storage/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netapp</forename><surname>Trident</surname></persName>
		</author>
		<ptr target="https://github.com/NetApp/trident" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openebs</surname></persName>
		</author>
		<ptr target="https://openebs.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Portworx</surname></persName>
		</author>
		<ptr target="https://portworx.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prometheus</surname></persName>
		</author>
		<ptr target="https://prometheus.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spec</forename><surname>Sfs</surname></persName>
		</author>
		<ptr target="https://www.spec.org/sfs2014/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Storageos</surname></persName>
		</author>
		<ptr target="https://storageos.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<ptr target="https://sysdig.com/blog/sysdig-2019-container-usage-report/" />
		<title level="m">Sysdig 2019 Container Usage Report</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="https://github.com/IBM/ibm-spectrum-scale-csi" />
		<title level="m">The IBM Spectrum Scale Container Storage Interface (CSI) project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Millions of tiny databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brooker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Ping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Benchmarking Cloud Serving Systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 1st ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An Open-source Benchmark Suite for Microservices and their Hardwaresoftware Implications for Cloud &amp; Edge Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dailun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankitha</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyal</forename><surname>Rathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayan</forename><surname>Katarki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariana</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ritchken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendon</forename><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 24th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Elasticsearch: The Definitive Guide: A Distributed Real-time Search and Analytics Engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Gormley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuvraj</forename><forename type="middle">Kibana</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essentials</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tailbench: A Benchmark Suite and Evaluation Methodology for Latencycritical Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshad</forename><surname>Kasture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 IEEE International Symposium on Workload Characterization (IISWC)</title>
		<meeting>the 2016 IEEE International Symposium on Workload Characterization (IISWC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Managing Tail Latency in Datacenter-Scale File Systems Under Production Constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Borge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Lebeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth EuroSys Conference</title>
		<meeting>the Fourteenth EuroSys Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Benchmarking File System Benchmarking: It *IS* Rocket Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Tarasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saumitra</forename><surname>Bhanage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margo</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on Hot Topics in Operating Systems (HotOS)</title>
		<meeting>the 13th USENIX Conference on Hot Topics in Operating Systems (HotOS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting Flexible, Replayable Models from Large Block Traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Tarasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santhosh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Hildebrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Povzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Kuenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Filebench: A Flexible Framework for File System Benchmarking. USENIX ;login</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasily</forename><surname>Tarasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Shepler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Johannes Thönes. Microservices. IEEE Software</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Nine Year Study of File System and Storage Benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishay</forename><surname>Traeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolai</forename><surname>Joukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles P</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">RADOS: A Scalable, Reliable Storage Service for Petabyte-scale Storage Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sage</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Maltzahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Workshop on Petascale Data Storage</title>
		<meeting>the 2nd International Workshop on Petascale Data Storage</meeting>
		<imprint>
			<publisher>PDSW</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Filesystem Performance from a Database Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Wong</surname></persName>
		</author>
		<ptr target="https://bit.ly/33M5RiU" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">COSBench: Cloud Object Storage Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangang</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM/SPEC International Conference on Performance Engineering (ICPE)</title>
		<meeting>the 4th ACM/SPEC International Conference on Performance Engineering (ICPE)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
