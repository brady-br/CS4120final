<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Manycore Scalability of File Systems Understanding Manycore Scalability of File Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 22-24. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Maass</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhak</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Maass</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhak</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Georgia Institute of Technology</orgName>
								<orgName type="institution" key="instit2">USENIX Association</orgName>
								<orgName type="institution" key="instit3">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Understanding Manycore Scalability of File Systems Understanding Manycore Scalability of File Systems</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16)</title>
						<meeting>the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) <address><addrLine>Denver, CO, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page">71</biblScope>
							<date type="published">June 22-24. 2016</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We analyze the manycore scalability of five widely-deployed file systems, namely, ext4, XFS, btrfs, F2FS, and tmpfs, by using our open source benchmark suite, FXMARK. FXMARK implements 19 microbenchmarks to stress specific components of each file system and includes three application benchmarks to measure the macroscopic scalability behavior. We observe that file systems are hidden scalability bottlenecks in many I/O-intensive applications even when there is no apparent contention at the application level. We found 25 scal-ability bottlenecks in file systems, many of which are unexpected or counterintuitive. We draw a set of observations on file system scalability behavior and unveil several core aspects of file system design that systems researchers must address.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Parallelizing I/O operations is a key technique to improve the I/O performance of applications <ref type="bibr" target="#b45">[46]</ref>. Today, nearly all applications implement concurrent I/O operations, ranging from mobile <ref type="bibr" target="#b51">[52]</ref> to desktop <ref type="bibr" target="#b45">[46]</ref>, relational databases <ref type="bibr" target="#b9">[10]</ref>, and NoSQL databases <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43]</ref>. There are even system-wide movements to support concurrent I/O efficiently for applications. For example, commercial UNIX systems such as HP-UX, AIX, and Solaris extended the POSIX interface <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b67">68]</ref>, and open source OSes like Linux added new system calls to perform asynchronous I/O operations <ref type="bibr" target="#b16">[17]</ref>.</p><p>Two recent technology trends indicate that parallelizing I/O operations will be more prevalent and pivotal in achieving high, scalable performance for applications. First, storage devices have become significantly faster. For instance, a single NVMe device can handle 1 million IOPS <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b85">86]</ref>, which roughly translates to using 3.5 processor cores to fully drive a single NVMe device <ref type="bibr" target="#b50">[51]</ref>. Second, the number of CPU cores continues to increase <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b61">62]</ref> and large, high-performance databases have started to embrace manycore in their core operations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b89">90]</ref>. These trends seem to promise an implicit scaling of applications already employing concurrent I/O operations.</p><p>In this paper, we first question the practice of concurrent I/O and understand the manycore scalability 1 behavior, that we often take for granted. In fact, this is the right moment to thoroughly evaluate the manycore scalability of file systems, as many applications have started <ref type="bibr" target="#b0">1</ref> We sometimes mention manycore scalability as scalability for short.</p><p>hitting this wall. Moreover, most of the critical design decisions are "typical of an 80's era file system" <ref type="bibr" target="#b36">[37]</ref>. Recent studies on manycore scalability of OS typically use memory file systems, e.g., tmpfs, to circumvent the effect of I/O operations <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. So, there has been no in-depth analysis on the manycore scalability of file systems. In most cases, when I/O performance becomes a scalability bottleneck without saturating disk bandwidth, it is not clear if it is due to file systems, its usage of file systems, or other I/O subsystems.</p><p>Of course, it is difficult to have the complete picture of file system scalability. Nevertheless, in an attempt to shed some light on it, we present an extensive study of manycore scalability on file systems. To evaluate the scalability aspects, we design and implement the FXMARK benchmark suite, comprising 19 microbenchmarks stressing each building block of a file system, and three application benchmarks representing popular I/O-intensive tasks (i.e., mail server, NoSQL key/value store, and file server). We analyze five popular file systems in Linux, namely, ext4, XFS, btrfs, F2FS, and tmpfs, on three storage mediums: RAMDISK, SSD, and HDD.</p><p>Our analysis revealed unanticipated scalability behavior that should be considered while designing I/Ointensive applications. For example, all operations on a directory are sequential regardless of read or write; a file cannot be concurrently updated even if there is no overlap in each update. Moreover, we should revisit the core design of file systems for manycore scalability. For example, the consistency mechanisms like journaling (ext4), copyon-write (btrfs), and log-structured writing (F2FS) are not scalable. We believe that FXMARK is effective in two ways. It can identify the manycore scalability problems in existing file systems and further guide and evaluate the new design of a scalable file system in the future.</p><p>In summary, we make the following contributions:</p><p>• We design an open source benchmark suite, FXMARK, to evaluate the manycore scalability of file systems. The FXMARK benchmark suite is publicly available at https://github.com/sslab-gatech/fxmark.</p><p>• We evaluate five widely-used Linux file systems on an 80-core machine with FXMARK. We also analyze how the design of each file system and the VFS layer affect their scalability behavior.</p><p>• We summarize our insights from the identified scalability bottlenecks to design a new scalable file system for the future. The rest of this paper is organized as follows: §2 pro- Figure 1: Exim throughput (i.e., delivering messages) on six file systems (i.e., btrfs, ext4, F2FS, tmpfs, XFS, and ext4 without journaling, ext4 NJ ) and three storage mediums (i.e., RAMDISK, SSD, HDD at 80-core). We found that the manycore scalability of Exim depends a lot on the file systems (e.g., <ref type="bibr">ext4</ref> is 54× faster than btrfs at 80-core), but does not depend much on storage mediums (e.g., marginal performance difference of ext4 on RAMDISK and HDD). To avoid known scalability bottlenecks in Exim, we modified Exim as in the previous study <ref type="bibr" target="#b20">[21]</ref> and configured it to disable per-message fsync() call.</p><p>vides the motivation for our work with a case study; §3 provides an overview of FXMARK, and §4 describes our evaluation strategies; §5 and §6 show analysis results; §7 summarizes our findings and §8 discusses implications for OS and file system designers; §9 compares our work with previous work; Finally, §10 provides the conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Case Study</head><p>In this section, we show how non-scalable file systems can break the scalability of embarrassingly parallel (i.e., ideally parallelizable) I/O-intensive applications in unexpected ways. The Exim <ref type="bibr" target="#b7">[8]</ref> email server is one such application that is designed to scale perfectly, at least from an application's perspective. For example, Exim delivers messages to appropriate mail boxes in parallel and performs each delivery independently. In fact, the message delivery heavily utilizes I/O operations. It consists of a series of operations ranging from creating, renaming, and deleting small files in the spool directories to appending the message body to the per-user mail file. Unfortunately, Exim fails to scale over 10, 20, or 40-core, at most, among the popular file systems in Linux as shown in <ref type="figure">Figure 1</ref>. File systems kill application scalability. <ref type="figure">Figure 1</ref>(a) shows the throughput of Exim with six different file systems on RAMDISK. Surprisingly, the scalability and performance of Exim are significantly dependent on the file system. The performance gap between the best file system, tmpfs, and the worst, btrfs, is 54× at 80-core. ext4 and tmpfs scale linearly up to 40-core; then the Linux kernel becomes the bottleneck. However, Exim on F2FS is hardly scalable; it is 21× slower than ext4 at 80-core. Faster storage mediums do not guarantee scalability. With a reasonably large memory, the page cache will absorb most read and write operations, and most write operations can be performed in the background. In this case, the in-memory structures in the file systems determine the scalability, rather than the storage medium. <ref type="figure">Figure 1(b)</ref> shows that all file systems, except XFS, show a marginal performance difference among RAMDISK, SSD, and HDD at 80-core. In this case, performance with XFS is largely affected by the storage medium since XFS mostly waits for flushing journal data to disk due to its heavy metadata update operations. Fine-grained locks often impede scalability. We may assume that the worst performing file system, btrfs, will be mostly in idle state, since it is waiting for events from the storage. On the contrary, <ref type="figure">Figure 1</ref>(c) shows that 67% of CPU time is spent in the kernel mode for btrfs. In particular, btrfs spends 47% of CPU time on synchronization to update its root node. In a common path without any contention, btrfs executes at least 10 atomic instructions to acquire a single B-tree node lock (btrfs_tree_lock()) and it must acquire locks of all interim nodes from a leaf to the root. If multiple readers or writers are contending to lock a node, each thread retries this process. Under heavy contention, it is typical to retry a few hundreds times to lock a single node. These frequent, concurrent accesses to synchronization objects result in a performance collapse after 4-core, as there is no upper bound on atomic instructions for updating the root node. Subtle contentions matter. <ref type="figure">Figure 1</ref>(a) shows another anomaly with ext4 NJ (i.e., ext4 with no journaling) performing 44% slower than ext4 itself. We found that two independent locks (i.e., a spinlock for journaling and a mutex for directory update) interleave in an unintended fashion. Upon create(), ext4 first hits the journal spinlock (start_this_handle()) for metadata consistency and then hits the parent directory mutex (path_openat()) to add a new inode to its parent directory. In ext4, the serialization coordinated by the journal spinlock incurs little contention while attempting to hold the directory mutex. On the contrary, the contending directory mutex in ext4 NJ results in expensive side-effects, such as sleeping on a waiting queue after a short period of opportunistic spinning.</p><p>Speculating scalability is precarious. The Exim case study shows that it is difficult for application developers or even file systems developers to speculate on the scalability of file system implementations. To identify such scalability problems in file systems, our community needs a proper benchmark suite to constantly evaluate and guide the design of file systems for scalability. </p><formula xml:id="formula_0">Type Mode Operation Sharing Level LOW MEDIUM HIGH DATA READ BLOCK READ ✓ ✓ ✓ WRITE OVERWRITE ✓ ✓ - APPEND ✓ - - TRUNCATE ✓ - - SYNC ✓ - - META READ PATH NAME READ ✓ ✓ ✓ DIRECTORY LIST ✓ ✓ - WRITE CREATE ✓ ✓ - UNLINK ✓ ✓ - RENAME ✓ ✓ -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FXMARK Benchmark Suite</head><p>There are 19 microbenchmarks in FXMARK that are designed for systematically identifying scalability bottlenecks, and three well-known I/O-intensive application benchmarks to reason about the scalability bottlenecks in I/O-intensive applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Microbenchmarks</head><p>Exploring and identifying scalability bottlenecks systematically is difficult. The previous studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref> on manycore scalability show that most applications are usually stuck with a few bottlenecks, and resolving them reveals the next level of bottlenecks.</p><p>To identify scalability problems in file system implementations, we designed microbenchmarks stressing seven different components of file systems: (1) path name resolution, (2) page cache for buffered I/O, (3) inode management, (4) disk block management, (5) file offset to disk block mapping, (6) directory management, and <ref type="formula">(7)</ref> consistency guarantee mechanism. <ref type="table" target="#tab_0">Table 1</ref> illustrates the way FXMARK categorizes each of these 19 microbenchmarks with varying stressed data types (i.e., file data or file system metadata), a related file system operation (e.g., open(), create(), unlink(), etc.), and its contention level (i.e., low, medium and high).</p><p>A higher contending level means the microbenchmark shares a larger amount of common code with the increasing number of cores, marked as sharing level for clarity.</p><p>For reading data blocks, FXMARK provides three benchmarks based on the sharing level: (1) reading a data block in the private file of a benchmark process (low), (2) reading a private data block in the shared file among benchmark processes (medium), and (3) reading the same data block in the shared file (high). As a convention, we denote each microbenchmark with four letters representing type, mode, etc. For instance, we denote three previous examples with DRBL (i.e., Data, Read, Block read, and Low), DRBM, and DRBH, respectively. <ref type="table">Table 2</ref> shows a detailed description of each benchmark, including its core operation and expected contention.</p><p>To measure the scalability behavior, each benchmark runs its file system-related operations as a separate process pinned to a core; for example, each benchmark runs up to 80 physical cores to measure the scalability characteristics. Note that we use processes instead of threads, to avoid a few known scalability bottlenecks (e.g., allocating file descriptors and virtual memory management) in the Linux kernel <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>. FXMARK modifies the CPU count using CPU hotplug mechanism <ref type="bibr" target="#b75">[76]</ref> in Linux. To minimize the effect of NUMA, CPU cores are assigned on a per socket basis; for example, in the case of 10 cores per socket, the first 10 CPU cores are assigned from the first CPU socket and the second 10 CPU cores are assigned from the second CPU socket. To remove interference between runs, FXMARK formats a testing file system and drops all memory caches (i.e., page, inode, and dentry caches) before each run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Application Benchmarks</head><p>Although a microbenchmark can precisely pinpoint scalability bottlenecks in file system components, scalability problems in applications might be relevant to only some of the bottlenecks. In this regard, we chose three application scenarios representing widely-used I/O-intensive tasks: mail server, NoSQL database, and file server. Mail server. Exim is expected to linearly scale with the number of cores, at least from the application's perspective. But as <ref type="figure">Figure 1</ref> shows, Exim does not scale well even after removing known scalability bottlenecks <ref type="bibr" target="#b20">[21]</ref>. To further mitigate scalability bottlenecks caused by the Linux kernel's process management, we removed one of the two process invocations during the message transfer in Exim. NoSQL database. RocksDB is a NoSQL database and storage engine based on log-structured merge-trees (LSMtrees) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b72">73]</ref>. It maintains each level of the LSM-tree as a set of files and performs multi-threaded compaction for performance, which will eventually determine the write performance. We use db_bench to benchmark RocksDB using the overwrite workload with disabled compression, which overwrites randomly generated key/value-pairs. File server. To emulate file-server I/O-activity, we use the DBENCH tool <ref type="bibr" target="#b4">[5]</ref>. The workload performs a sequence of create, delete, append, read, write, and attribute-change operations, with a specified number of clients processing the workload in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Diagnosis Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Target File Systems</head><p>We chose four widely-used, disk-based file systems and one memory-based file system: ext4, XFS, btrfs, F2FS, and tmpfs. Overwrite a block in a private file None</p><formula xml:id="formula_1">DWOM pwrite("share",b,4K,$ID*4K)</formula><p>Overwrite a private block in a shared file Updating inode attributes (e.g., m_time)</p><formula xml:id="formula_2">DWAL append("$ID/data",b,4K)</formula><p>Append a block in a private file Disk block allocations</p><formula xml:id="formula_3">DWTL truncate("$ID/data",4K)</formula><p>Truncate a private file to a block Disk block frees</p><formula xml:id="formula_4">DWSL pwrite("$ID/data",b,4K,0)</formula><p>Synchronously overwrite a private file Consistency mechanism (e.g., journaling)</p><formula xml:id="formula_5">fsync("$ID/data") META READ MRPL close( open("$ID/0/0/0/0"))</formula><p>Open a private file in five-depth directory Path name look-ups MRPM close( open("$R/$R/$R/$R/$R")) Open an arbitrary file in five-depth directory Path name look-ups</p><formula xml:id="formula_6">MRPH close( open("0/0/0/0/0"))</formula><p>Open the same file in five-depth directory Path name look-ups</p><formula xml:id="formula_7">MRDL readdir("$ID/")</formula><p>Enumerate a private directory None</p><formula xml:id="formula_8">MRDM readdir("share/")</formula><p>Enumerate a shared directory Shared directory accesses</p><formula xml:id="formula_9">WRITE MWCL create("$ID/$N")</formula><p>Create an empty file in a private directory Inode allocations</p><formula xml:id="formula_10">MWCM create("share/$ID.$N")</formula><p>Create an empty file in a shared directory Inode allocations and insertions</p><formula xml:id="formula_11">MWUL unlink("$ID/$N")</formula><p>Unlink an empty file in a private directory Inode frees</p><formula xml:id="formula_12">MWUM unlink("share/$ID.$N")</formula><p>Unlink an empty file in a shared directory Inode frees and deletions</p><formula xml:id="formula_13">MWRL rename("$ID/$N","$ID/$N.2")</formula><p>Rename a private file in a private directory None MWRM rename("$ID/$N","share/$ID.$N") Move a private file to a shared directory Insertions to the shared directory NOTE. $ID: a unique ID of a test process b: a pointer to a memory buffer $R: a random integer between 0 and 7 $N: a test iteration count <ref type="table">Table 2</ref>: Microbenchmarks in FXMARK. Each benchmark is identified by four letters based on type (marked as T), mode (marked as M), operation, and sharing level, as described in <ref type="table" target="#tab_0">Table 1</ref>. Each microbenchmark is designed to stress specific building blocks of modern file systems (e.g., journaling and dcache) to evaluate their scalability on manycore systems.</p><p>Ext4 is arguably the most popular, mature Linux file system. It inherits well-proven features (e.g., bitmap-based management of inodes and blocks) from Fast File System (FFS) <ref type="bibr" target="#b68">[69]</ref>. It also implements modern features such as extent-based mapping, block group, delayed allocation of disk blocks, a hash tree representing a directory, and journaling for consistency guarantee <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b66">67]</ref>. For journaling, ext4 implements write-ahead logging (as part of JBD2). We use ext4 with journaling in ordered mode and without it, marked as ext4 and ext4 NJ , to see its effect on file system scalability. XFS is designed to support very large file systems with higher capacity and better performance <ref type="bibr" target="#b84">[85]</ref>. XFS incorporates B+ trees in its core: inode management, free disk block management, block mapping, directory, etc <ref type="bibr" target="#b82">[83]</ref>. However, using B+ trees incurs huge bulk writes due to tree balancing; XFS mitigates this by implementing delayed logging to minimize the amount of journal writes: (1) logically log the operations rather than tracking physical changes to the pages and (2) re-log the same log buffer multiple times before committing <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. Btrfs is a copy-on-write (CoW) file system that represents everything, including file data and file system metadata, in CoW optimized B-trees <ref type="bibr" target="#b76">[77]</ref>. Since the root node of such B-trees can uniquely represent the state of the entire file system <ref type="bibr" target="#b77">[78]</ref>, btrfs can easily support a strong consistency model, called version consistency <ref type="bibr" target="#b28">[29]</ref>. F2FS is a flash-optimized file system <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63]</ref>. It follows the design of a log-structured file system (LFS) <ref type="bibr" target="#b79">[80]</ref> that generates a large sequential write stream <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b69">70]</ref>. Unlike LFS, it avoids the wandering tree problem <ref type="bibr" target="#b14">[15]</ref> by updating some of its metadata structures in-place. Tmpfs is a memory-based file system that works without a backing storage <ref type="bibr" target="#b78">[79]</ref>. It is implemented as a simple wrapper for most functions in the VFS layer. Therefore, its scalability behavior should be an ideal upper bound of the Linux file systems' performance that implements extra features on top of VFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We performed all of the experiments on our 80-core machine (8-socket, 10-core Intel Xeon E7-8870) equipped with 512 GB DDR3 DRAM. For storage, the machine has both a 1 TB SSD (540 MB/s for reads and 520 MB/s for writes) and a 7200 RPM HDD with a maximum transfer speed of 160 MB/s. We test with Linux kernel version 4.2-rc8. We mount file systems with the noatime option to avoid metadata update for read operations. RAMDISK is created using tmpfs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Profiling</head><p>While running each benchmark, FXMARK collects the CPU utilization for sys, user, idle, and iowait to see a microscopic view of a file system reaction to stressing its components. For example, a high idle time implies that the operation in a microbenchmark impedes the scalability behavior (e.g., waiting on locks); a high iowait time implies that a storage device is a scalability bottleneck. For further analysis, we use perf <ref type="bibr" target="#b6">[7]</ref> to profile the entire system and to dynamically probe a few interesting functions (e.g., mutex_lock(), schedule(), etc.) that likely induce such idle time during file system operations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Microbenchmark Analysis</head><p>In this section, we first describe the analysis results of each microbenchmark in buffered I/O mode starting from simple operations on file data ( §5.1) and going to more complicated file system metadata operations ( §5.2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Operation on File Data</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Block Read</head><p>We start from the simplest case: each test process reads a block in its respective private file (DRBL). As <ref type="figure" target="#fig_2">Figure 2</ref> shows, all test file systems scale nearly linearly. However, when each test process reads a private block in the shared file (DRBM), we observe that XFS's performance collapses near 10-core. Unlike other file systems, it spends 40% of execution and idle time at per-inode read/write semaphores (fs/xfs/xfs_file.c:329) when running on 80 cores. XFS relies heavily on per-inode read/write semaphores to orchestrate readers and writers in a fine-grained manner <ref type="bibr" target="#b84">[85]</ref>. However, the performance degradation does not come from unnecessary waiting in the semaphores. At every read(), XFS acquires and releases a read/write semaphore of a file being accessed in shared mode (down_read() and up_read()). A read/write semaphore internally maintains a reader counter, such that every operation in shared mode updates the reader counter atomically. Therefore, concurrent read operations on a shared file in XFS actually perform atomic operations on a shared reader counter. This explains the performance degradation after 10 cores. In fact, at XFS's peak performance, the cycles per instruction (CPI) is 20 at 10-core, but increases to 100 at 20-core due to increased cache line contention on updating a shared reader counter.</p><p>For reading the same block (DRBH), all file systems show a performance collapse after 10-core. Also, the performance at 80-core is 13.34× (for tmpfs) lower than that on a single core. We found that more than 50% of the time is being spent on reference counter operations for the page cache. The per-page reference counting is In DWOL, although they do not seem to have explicit contention, btrfs, ext4 and F2FS fail to scale due to their consistency mechanisms. Note that XFS is an exception among journaling file systems (see §5.1.2). In DWOM, all file systems fail to scale regardless of consistency mechanisms.</p><p>used for concurrency control of a page in a per-inode page cache. The per-page reference counter is also updated using atomic operations so that all test processes contend to update the same cache line. In the case of ext4, the CPI is 14.3 at 4-core but increases to 100 at 20-core due to increased cache-coherence delays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Block Overwrite</head><p>At first glance, overwriting a block in a private file (DWOL) should be an ideal, scalable operation; only the private file block and inode features such as last modified time need to be updated. However, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>, ext4, F2FS, and btrfs fail to scale. In ext4, starting and stopping journaling transactions (e.g., jbd2_journal_start()) in JBD2 impedes its scalability. In particular, acquiring a read lock (journal-&gt;j_state_lock) and atomically increasing a counter value (t_handle_count) take 17.2% and 9.4% of CPU time, respectively. Unlike ext4, XFS scales well due to delayed logging <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, which uses logical logging and re-logging to minimize the amount of journal write. In F2FS, write operations eventually trigger segment cleaning (or garbage collection) to reclaim invalid blocks. Segment cleaning freezes all file system operations for checkpointing of the file system status by holding the exclusive lock of a file system-wide read/write semaphore (sbi-&gt;cp_rwsem). btrfs is a CoW-based file system that never overwrites a physical block. It allo- cates a new block for every write operation so that its disk block allocation (i.e., updating its extent tree) becomes the sequential bottleneck. When every test process overwrites each private block in the shared file (DWOM), none of the file systems scale. The common bottleneck is an inode mutex (inode-&gt;i_mutex). The root cause of this sequential bottleneck stems from file system-specific implementations, not VFS. None of the tested file systems are implementing a range-based locking mechanism, which is common in parallel file systems <ref type="bibr" target="#b80">[81]</ref>. In fact, this is a serious limitation in scaling I/O-intensive applications like DBMS. The common practice of running multiple I/O threads is not effective when applications are accessing a shared file, regardless of the underlying file systems. Furthermore, it may incur a priority inversion between I/O and latencysensitive client threads <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, potentially disrupting the application's scalability behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">File Growing and Shrinking</head><p>For file growing and shirking, all disk-based file systems fail to scale after 10-core (DWAL in <ref type="figure" target="#fig_4">Figure 4</ref>). In F2FS, allocating or freeing disk blocks entails updating two data structures: SIT (segment information table) tracking block validity, and NAT (node address table) tracking inode and block mapping tables. Contention in updating SIT and NAT limits F2FS's scalability. When allocating blocks, checking disk free-space and updating the NAT consumes 78% of the execution time because of lock contentions (nmı-&gt;nat_tree_lock). Similarly, there is another lock contention for freeing blocks to invalidate free blocks. This exhausts 82% of the execution time (sit_i-&gt;sentry_lock). In btrfs, when growing a file, the sequential bottleneck is checking and reserving free space (data_info-&gt;lock and delalloc_block_rsv-&gt;lock). When shrinking a file, btrfs suffers from updating an extent tree, which keeps track of the reference count of disk blocks: A change in reference counts requires updates all the way up to the root node.</p><p>In ext4 and XFS, the delayed allocation technique, which defers block allocation until writeback to reduce fragmentation of a file, is also effective in improving manycore scalability by reducing the number of block allocation operations. Because of this, the scalability bottleneck of ext4 and XFS is not the block allocation but rather their journaling mechanisms. About 76-96% of the execution time was spent on journaling. ext4 spends most of its execution time on manipulating JBD2 shared data structures (e.g., journal_j_flags) protected by spinlocks and atomic instructions. XFS spends most of its execution time waiting on flushing its log buffers. Upon truncating files, these file systems face the same performance bottleneck. In tmpfs, checking the capacity limit (__vm_enough_memory()) becomes a scalability problem. As the used space approaches the capacity limit (at 50-core in this case), the checking takes a slow path for precise comparison of the remaining disk space. Tracking the used space by using a per-CPU counter scales up to 50-core, but fails to scale for more cores because of a contending spinlock in a per-CPU counter on the slow path to get a true value. When freeing blocks, updating per-cgroup page usage information using atomic decrements causes a performance collapse after 10-core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">File Sync Operation</head><p>When using fsync(), a file system synchronously flushes dirty pages of a file and disk caches. In this regard, all file systems can scale up to the limitation of the storage medium. Although we use memory as a storage backend, none of the file systems scale, except tmpfs, which ignores fsync() operations. Notice that fsync() on btrfs is significantly slower than other file systems (see <ref type="figure" target="#fig_5">Figure 5</ref>). Similar to §5.1.2, btrfs propagates a block update to its root node so a large number of metadata pages need to be written 2 . All file systems (except for tmpfs and btrfs) start to degrade after 10-core. That is due to locks protecting flush operations (e.g., journal-&gt;j_state_lock), which start contending after roughly 10-core. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Operation on File System Metadata</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Path Name Resolution</head><p>The Linux kernel maintains a directory cache, or dcache, which caches dentry structures. Only when a dcache miss happens, the kernel calls the underlying file system to fill up the dcache. Since dcache hits are dominant in our microbenchmark, MRPL, MRPM, and MRPH stress the dcache operations implemented in the VFS layer. Thus there is little performance difference among file systems, as shown in <ref type="figure">Figure 6</ref>. Our experiment shows that (1) dcache is scalable up to 10-core if multiple processes attempt to resolve the occasionally shared path names (MRPM), and (2) contention on a shared path is so serious that resolving a single common path in applications becomes a scalability bottleneck. In MRPM and MRPH, a lockref in a dentry (i.e., dentry-&gt;d_lockref), which combines a spinlock and a reference count into a single locking primitive for better scalability <ref type="bibr" target="#b37">[38]</ref>, becomes a scalability bottleneck. Specifically, the CPI of ext4 in MRPH increases from 14.2 at 10-core to 20 at 20-core due to increased cache-coherence delays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Directory Read</head><p>When listing a private directory (i.e., MRDL in <ref type="figure">Figure 7</ref>), all file systems scale linearly, except for btrfs. Ironically, the performance bottleneck of btrfs is the fine-grained locking. To read a file system buffer (i.e., extent_buffer) storing directory entries, btrfs first acquires read locks from a leaf, containing the buffer, to the root node of its B-tree (btrfs_set_lock_blocking_rw()); moreover, to acquire a read lock of a file system buffer, btrfs per- forms two read/write spinlock operations and six atomic operations for reference counting. Because such excessive synchronization operations increase cache-coherence delays, CPI in btrfs is 20.4× higher than that of ext4 at 80-core (20 and 0.98, respectively). XFS shows better scalability than btrfs even though its directory is represented as a B+-tree due to coarser-grained locking, i.e., per-directory locking. Unexpectedly, listing the shared directory (i.e., MRDM in <ref type="figure">Figure 7)</ref> is not scalable in any of the file systems; the VFS holds an inode mutex before calling a file systemspecific directory iteration function (iterate_dir()).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">File Creation and Deletion</head><p>File creation and deletion performance are critical to the performance of email servers and file servers, which frequently create and delete small files. However, none of the file systems scale, as shown in <ref type="figure" target="#fig_7">Figure 8</ref>.</p><p>In tmpfs, a scalability bottleneck is adding and deleting a new inode in an inode list in a super block (i.e., sb-&gt;s_inodes). An inode list in a super block is protected by a system-wide (not file system-wide) spinlock (i.e., inode_sb_list_lock), so the spinlock becomes a performance bottleneck and incurs a performance col- lapse after 10-core. In fact, CPI at 10-core increases from 20 to 50 at 20-core due to shared cache line contention on the spinlock. In ext4, inode allocation is a per-block group operation, so the maximum level of concurrency is the number of block groups (256 in our evaluation). But ext4's policy to preserve spatial locality (e.g., putting files under the same directory to the same block group) limits the maximum concurrency. Upon file deletion, ext4 first puts a deleted inode onto an orphan inode list in a super block (i.e., sbi-&gt;s_orphan), which is protected by a per-filesystem spinlock. This list ensures that inodes and related resources (e.g., disk blocks) are freed even if the kernel crashes in the middle of the delete. Adding an inode to an orphan list is a sequential bottleneck.</p><p>Like ext4, XFS also maintains inodes per-block group (or allocation group). But, unlike ext4, a B+-tree is used to track which inode numbers are allocated and freed. Inode allocation and free incurs changes in the B+-tree and such changes need to be logged for consistency. So, journaling overhead waiting for flushing log buffers is the major source of bottlenecks (90% of time).</p><p>In btrfs, files and inodes are stored in the file system Btree. Therefore, file creation and deletion incur changes in the file system B-tree, and such changes eventually need to be propagated to the root node. Similar to other write operations, updating the root node is again a sequential bottleneck. In fact, between 40% and 60% of execution time is spent contending to update the root node.</p><p>In F2FS, performance characteristics of file creation and deletion are similar to those of appending and truncating data blocks in §5.1.3. The reason for this, in the case of deletion, is the contention in updating the SIT (segment info table), which keeps track of blocks in active use. In fact, up to 85% of execution time is spent on contending for updating the SIT. During create, contention within the NAT (node address table) is the main reason for the performance collapse.</p><p>When creating and deleting files in a shared directory, additional contention updating the shared directory is noticeable (see MWCM and MWUM in <ref type="figure" target="#fig_7">Figure 8</ref>). Like MRDM, Linux VFS holds a per-directory mutex while creating and deleting files. Performance of all tested file systems are saturated or declining after 10-core, at which a single RAMDISK becomes a bottleneck. For write operations, btrfs suffers from its heavy b-tree operations the same as in the buffered mode. For DWOM:O_DIRECT, only XFS scales because it holds a shared lock for an inode while the others hold an inode mutex (inode-&gt;i_mutex).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">File Rename</head><p>As <ref type="figure" target="#fig_8">Figure 9</ref> shows, rename is a system-wide sequential operation regardless of the sharing level. In VFS, multiple readers optimistically access the dcache through rename_lock, concurrently with multiple writers, and then later, each reader checks if the sequence number is the same as the one at the beginning of the operation. If sequence numbers do not match (i.e., there were changes in dentries), the reader simply retries the operation. Therefore, a rename operation needs to hold a write lock (i.e., write_seqlock() on rename_lock), which turns out to be the bottleneck in our benchmark. In MWRL, on average, 84.7% of execution time is spent waiting to acquire the rename_lock. This scalability bottleneck is a serious limitation for applications that concurrently perform renaming of multiple files, like Exim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scalability in a Direct I/O Mode</head><p>Performance in direct I/O mode is critical for many I/Ointensive applications. To understand the scalability behavior, we ran microbenchmarks on file data operations from §5.1 in direct I/O mode (O_DIRECT). When each test process reads a block in its respective private file (DRBL:O_DIRECT), there is no apparent contention at file system so the storage device (i.e., a single RAMDISK) becomes the bottleneck. When more than 10 cores are used (i.e., more than two sockets are involved in our experimental setup), performance gradually degrades due to the NUMA effect. When reading a private block of the shared file (DRBM:O_DIRECT), XFS shows around 20-50% higher performance than the other file systems. The performance difference comes from the different locking mechanism of the shared file for writing. As discussed in §5.1.2, the file systems should lock the shared file before reading the disk blocks as the dirty pages of a file should be consistently written to that shared file. While writing dirty pages of a file in a direct I/O mode, XFS holds the shared lock of a file but others holds the inode mutex (inode-&gt;i_mutex). Thus, read operations of the other file systems are serialized by the inode mutex.</p><p>For write operations, btrfs suffers from its heavy btree operations regardless of the contention level. When writing private files (DWOL:O_DIRECT), the storage device is the bottleneck as same as DRBL:O_DIRECT. When writing a private block of the shared file (DWOM:O_DIRECT), only XFS scales up to 10-core. File systems other than XFS serialize concurrent write operations by holding the inode mutex. In contrast, since XFS holds the shared lock while writing disk blocks, write operations for the shared file can be concurrently issued.</p><p>The scalability bottleneck in accessing the shared file is a serious limitation for applications such as database systems and virtual machines, where large files (e.g., database table or virtual disk) are accessed in a direct I/O mode by multiple I/O threads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Impact of Storage Medium</head><p>To understand how the performance of the storage medium affects the scalability behavior, we ran FXMARK on SSD and HDD, and compared their performance at 80-core in <ref type="figure" target="#fig_10">Figure 11</ref>. For synchronous write operations (e.g., DWSL) or operations incurring frequent page cache misses (e.g., DWAL), the bandwidth of the storage medium is a dominant factor. However, for buffered reads (e.g., DRBL) or contending operations (e.g., DWOM), the impact of the storage medium is not dominant. With larger memory devices, faster storage mediums (e.g., NVMe), and increasing core counts in modern computers, it is important to understand, measure, and thus improve the scalability behavior of file systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Application Benchmarks Analysis</head><p>In this section, we explain the scalability behavior of three applications on various file systems backed by memory. Exim.</p><p>After removing the scalability bottleneck in Exim (see §3.2), it linearly scales up to 80-core (tmpfs in <ref type="figure" target="#fig_2">Figure 12(a)</ref>). With the optimized Exim, ext4 scales the most, followed by ext4 NJ , but it is still 10× slower than tmpfs. Since Exim creates and deletes small files in partitioned spool directories, performance bottlenecks in each file system are equivalent to both MWCL and MWUL (see §5.2.3). RocksDB. As <ref type="figure" target="#fig_2">Figure 12(b)</ref> illustrates, RocksDB scales fairly well for all file systems up to 10 cores but either flattens out or collapses after that. The main bottleneck can be found in RocksDB itself, synchronizing compactor threads among each other. Since multiple compactor threads concurrently write new merged files to disk, the behavior and performance bottleneck in each file system is analogous to DWAL (see §5.1.2). DBENCH. <ref type="figure" target="#fig_2">Figure 12</ref>(c) illustrates the DBENCH results, which do not scale linearly with increasing core count for any of the file systems. This happens because DBENCH reads, writes, and deletes a large number of files in a shared directory. This is similar to our microbenchmarks MWCM and MWUM ( §5.1.3). tmpfs suffers for two reasons: look-ups and insertions in the page cache and reference counting for the dentry of the directory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Summary of Benchmarks</head><p>We found 25 scalability bottlenecks in five widely-used file systems, as summarized in <ref type="figure" target="#fig_3">Figure 13</ref> and <ref type="table" target="#tab_3">Table 3</ref>. Some of the bottlenecks (e.g., inode list lock) are also found in recent literature <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. In our opinion, this is the most important first step to scale file systems. We draw the following observations, to which I/O-intensive application developers must pay close attention. High locality can cause performance collapse. Maintaining high locality is believed to be the golden rule to improve performance in I/O-intensive applications because it increases the cache hit ratio. But when the cache hit is dominant, the scalability of cache hits does matter. We found such performance collapses in the page cache and dentry cache in Linux file systems. [ §5.1.1, §5.2.1] Renaming is system-wide sequential.</p><p>rename() is commonly used in many applications for transactional updates <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b74">75]</ref>. However, we found that rename() operations are completely serialized at a system level in Linux for consistent updates of the dentry cache.   A file cannot be concurrently updated. All of the tested file systems hold an exclusive lock for a file (inode-&gt;i_mutex) during a write operation. This is a critical bottleneck for high-performance database systems allocating a large file but not maintaining the page cache by themselves (e.g., PostgreSQL  <ref type="table" target="#tab_0">80  80  80  80  80  80  80  80  80  80  80  10  4  4  1 0  4  2  10  10  10  80  10  80  80  1  1  2  1  1  2  10  10  80  10  50  30  2  10  10  10  10  10  30  10  10  10  80  10  80  80  80  80  80  80  20  20  20  30  20  20  10  10  10  10  10  10  4  80  80</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion</head><p>The next question is whether traditional file system designs can be used and implemented in a scalable way. It is difficult to answer conclusively. At a high level, the root causes of the scalability problems of file systems are not different from those of OS scalability in previous studies <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>: shared cache line contention, reference counting, coarse-grained locking, etc. But the devil is in the details; some are difficult to fix with known techniques and lead us to revisit the core design of file systems.</p><p>Consistency mechanisms need to be scalable. All three consistency mechanisms, journaling (ext4 and XFS), log-structured writing (F2FS), and copy-on-write (btrfs), are scalability bottlenecks. We think these are caused by their inherent designs so that our community needs to revisit consistency mechanisms from a scalability perspective.</p><p>In journaling file systems, committing a journal transaction guarantees a filesystem-wide consistency. To this end, ext4, for example, maintains a single running journal transaction, so accessing the journal transaction becomes a scalability bottleneck. Scalable journaling is still an unexplored area in the context of file systems, though there are some studies in the database field <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91]</ref>.</p><p>In the case when copy-on-write techniques are combined with self-balancing index structures (e.g., B-tree) like btrfs, such file systems are very fragile to scalability; a leaf node update triggers updates of all interim nodes to the root so that write locks of all nodes should be acquired.</p><p>Moreover, two independent updates should contend for acquiring locks of common ancestors. Besides locking overhead, this could result in a deadlock if two updates should be coordinated by other locks. We suspect this is the reason why btrfs uses the retry-based locking protocol to acquire a node lock (btrfs_tree_lock()). Parallelizing a CoW file system by extending the current B-tree scheme (e.g., LSM tree) or using non-self-balancing index structures (e.g., radix tree or hash table) is worth further research.</p><p>To our best knowledge, all log-structured file systems, including F2FS, NILFS2 <ref type="bibr" target="#b2">[3]</ref>, and UBIFS <ref type="bibr" target="#b3">[4]</ref>, adopt singlethreaded writing. By nature, log-structured file systems are designed to create a large sequential write stream, while metadata updates should be issued after writing file data for consistency guarantee. Multi-headed logstructured writing schemes are an unexplored area in the context of file systems, while some techniques are proposed at the storage device level <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>Spatial locality still needs to be considered. One potential solution to parallelizing file systems is partitioning. To see its feasibility, we modified Exim and RocksDB to run on multiple file system partitions for spool directories and database files, respectively. We set up 60 file system partitions, the maximum allowed on a disk, to spread files in 60 ways. Our results on RAMDISK and HDD show its potential and limitations at the same time. We see a significant performance improvement on RAMDISK <ref type="figure" target="#fig_4">(Figure 14)</ref>. It confirms that the reduced contentions in a file system can improve the scalability. However, the RocksDB results on HDD also show its limitation <ref type="figure" target="#fig_5">(Fig- ure 15)</ref>. In all file systems except for F2FS, the partitioned case performs worse, as partitioning ruins spatial locality. But F2FS performs equally well in both cases; because the log-structured writing scheme of F2FS always issues bulk sequential write for file data, the impact of partitioning is negligible. The above example shows the unique challenges in designing scalable file systems. Optimizing for storage devices based on their performance characteristics and achieving consistency guarantees in a scalable fashion will be critical in file systems.</p><p>File system-specific locking schemes in VFS. The scalable performance of locking strategies, such as granularity and types of lock, is dependent on data organization and management. The current locking schemes enforced in VFS will become obsolete as storage devices and file systems change. For example, the inode mutex for directory accesses, currently enforced by the VFS, should be flexible enough for each file system to choose proper, finer-grained locking.</p><p>Reference counting still does matter. We found scalability problems in the reference counters of various file system objects (e.g., page, dentry, and XFS inode struc- For clarity, results up to 20-core are presented because the performance of all file systems is saturated after that. Except for F2FS, all other file systems fail to scale after partitioning and perform around 50% of the original performance.</p><p>The impact of partitioning in F2FS is negligible because the log-structured writing in F2FS always generates large sequential write for file data.</p><p>tures). Many previous studies proposed scalable reference counting mechanisms, including clustered object <ref type="bibr" target="#b60">[61]</ref>, SNIZ <ref type="bibr" target="#b39">[40]</ref>, sloppy counter <ref type="bibr" target="#b20">[21]</ref>, Refcache <ref type="bibr" target="#b32">[33]</ref>, and Linux per-CPU counter <ref type="bibr" target="#b38">[39]</ref>, but we identified that they are not adequate for file system objects for two reasons. First, their space overhead is proportional to the number of cores since they speed up by separating cache lines per-core. Such space overhead is especially problematic to file system objects, which are many in number and small in size (typically a few tens or hundreds of bytes). For example, in our 80-core machine, the space required for per-core counters per page is 5× larger than the page structure itself (320 bytes vs. 64 bytes). Second, getting the true value is not scalable but immediate. Recall that we found the reader counter problem at the R/W semaphore used in the XFS inode ( §5.1.1). If getting the number of reader is not immediate, writers waiting for readers can starve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Related Work</head><p>Benchmarking file systems. Due to the complexity of file systems and interactions among multiple factors (e.g., page cache, on-disk fragmentation, and device characteristics), file system benchmarks have been criticized for decades <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref><ref type="bibr" target="#b88">[89]</ref>. Popular file system benchmarks, such as FIO <ref type="bibr" target="#b0">[1]</ref> and iozone <ref type="bibr" target="#b1">[2]</ref>, mostly focus on measuring bandwidth and IOPS of file system operations varying I/O patterns. In contrast, recently developed benchmarks focus on a specific system (e.g., smartphones <ref type="bibr" target="#b58">[59]</ref>) or a particular component in file systems (e.g., block allocation <ref type="bibr" target="#b46">[47]</ref>). Along this line, FXMARK focuses only on manycore scalability of file systems. Scaling operating systems. To improve the scalability of OS, researchers have been optimizing existing OSes <ref type="bibr">[20-22, 32, 33]</ref> or have been building new OSes based on new design principles <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b33">34]</ref>. However, previous studies used memory-based file systems to opt out of the effect of I/O operations. In Arrakis <ref type="bibr" target="#b73">[74]</ref>, since file system service is a part of applications, its manycore scalability solely depends on each application. Scaling file systems. The Linux kernel community has made a steady effort to improve the scalability of the file system by mostly reducing lock contentions <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>Hare <ref type="bibr" target="#b44">[45]</ref> is a scalable file system for non-cache-coherent systems. But it does not provide durability and crash consistency, which were significant performance bottlenecks in our evaluation. ScaleFS <ref type="bibr" target="#b40">[41]</ref> extends a scalable in-memory file system to support consistency by using operation log on an on-disk file system. SpanFS <ref type="bibr" target="#b56">[57]</ref> adopts partitioning techniques to reduce lock contentions. But how partitioning affects performance in a physical storage medium such as SSD and HDD is not explored.</p><p>Optimizing the storage stack for fast storage. As storage devices become dramatically faster, there are research efforts to make storage stacks more scalable. Many researchers made efforts to reduce the overhead and latency of interrupt handling in the storage device driver layer <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b92">93]</ref>. At the block layer, Bjørling et al. <ref type="bibr" target="#b17">[18]</ref> address the scalability of the Linux block layer and propose a new Linux block layer, which maintains a per-core request queue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Conclusion</head><p>We performed a comprehensive analysis of the manycore scalability of five widely-deployed file systems using our FXMARK benchmark suite. We observed many unexpected scalability behaviors of file systems. Some of them lead us to revisit the core design of traditional file systems; in addition to well-known scalability techniques, scalable consistency guarantee mechanisms and optimizing for storage devices based on their performance characteristics will be critical. We believe that our analysis results and insights can be a starting point toward designing scalable file systems for manycore systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Read a block in a private file None DRBM pread("share",b,4K,$ID*4K) Read a private block in a shared file Shared file accesses DRBH pread("share",b,4K,0) Read a shared block in a shared file Shared block accesses WRITE DWOL pwrite("$ID/data",b,4K,0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance of reading a data block in three settings; reading a block in a private file (DRBL), reading a private block in a shared file (DRBM), and reading a shared block in a shared file (DRBH). All file systems scale linearly in DRBL. XFS fails to scale in DRBM because of the per-inode read/write semaphore. In DRBH, all file systems show their peak performance around 10-core because of contending per-page reference counters in VFS ( §5.1.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of overwriting a data block in a private file (DWOL) and overwriting a private block in a shared file (DWOM). In DWOL, although they do not seem to have explicit contention, btrfs, ext4 and F2FS fail to scale due to their consistency mechanisms. Note that XFS is an exception among journaling file systems (see §5.1.2). In DWOM, all file systems fail to scale regardless of consistency mechanisms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of growing files (DWAL) and shrinking files (DWTL). None of the tested file systems scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Performance of synchronous overwrites of a private file (DWSL). Only tmpfs ignoring fsync() scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Performance of resolving path names; a private file path (MRPL), an arbitrary path in a shared directory (MRPM), and a single, shared path (MRPH). Surprisingly, resolving a single, common path name is the slowest operation (MRPH).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance of creating and deleting files in a private directory (i.e., MWCL and MWUL) and a shared directory (i.e., MWCM and MWUM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of renaming files in a private directory (MWRL) and shared directory (MWRM). None of file systems scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance of file data operations in a direct I/O mode on a single RAMDISK. Performance of all tested file systems are saturated or declining after 10-core, at which a single RAMDISK becomes a bottleneck. For write operations, btrfs suffers from its heavy b-tree operations the same as in the buffered mode. For DWOM:O_DIRECT, only XFS scales because it holds a shared lock for an inode while the others hold an inode mutex (inode-&gt;i_mutex).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Relative performance of SSD and HDD to RAMDISK at 80-core. For buffered reads (e.g., DRBL) and contending operations (e.g., DWOM), the performance of the storage medium is not the dominant factor of applications' I/O performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Performance of three applications-an email sever (Exim), a NoSQL key/value store (RocksDB), and a file server (DBENCH)-on various file systems backed by memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :Figure 15 :</head><label>1415</label><figDesc>Figure 14: Performance of Exim and RocksDB after dividing each file system to 60 partitions on RAMDISK.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Coverage of the FXMARK microbenchmarks. FXMARK 
consists of 19 microbenchmarks that we categorize based on 
four criteria: data types, modes, operations, and sharing levels 
that are accordingly represented in each column on the table. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : The identified scalability bottlenecks in tested file systems with FXMARK.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> To minimize fsync() latency, btrfs maintains a special log-tree to defer checkpointing the entire file system until the log is full. In the case of fsync()-heavy workloads, like DWSL, the log quickly becomes full; therefore, checkpointing or updating the root node becomes a sequential bottleneck.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Acknowledgment</head><p>We thank the anonymous reviewers, and our shepherd, Angela Demke Brown, for their helpful feedback. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I/O</forename><surname>Flexible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tester</surname></persName>
		</author>
		<ptr target="https://github.com/axboe/fio" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iozone Filesystem Benchmark</surname></persName>
		</author>
		<ptr target="http://www.iozone.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://nilfs.sourceforge.net/en/" />
		<title level="m">NILFS -Continuous Snapshotting Filesystem</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ubifs -Ubi File-System</forename></persName>
		</author>
		<ptr target="http://www.linux-mtd.infradead.org/doc/ubifs.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://dbench.samba.org/" />
	</analytic>
	<monogr>
		<title level="j">DBENCH</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mongodb</surname></persName>
		</author>
		<ptr target="https://www.mongodb.org/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Linux profiling with performance counters</title>
		<ptr target="https://perf.wiki.kernel.org/index.php/Main_Page" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Exim Internet</forename><surname>Mailer</surname></persName>
		</author>
		<ptr target="http://www.exim.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sap</forename><surname>Hana</surname></persName>
		</author>
		<ptr target="http://hana.sap.com/abouthana.html/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mariadb</surname></persName>
		</author>
		<ptr target="https://mariadb.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voltdb</surname></persName>
		</author>
		<ptr target="https://voltdb.com/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating Realistic Impressions for File-system Benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<idno>16:1-16:30</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2009-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">vIC: Interrupt coalescing for virtual machine storage device IO</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mashtizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ATC Annual Technical Conference (ATC)</title>
		<meeting>the 2011 ATC Annual Technical Conference (ATC)<address><addrLine>Portland, OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alcorn</surname></persName>
		</author>
		<ptr target="http://www.tomsitpro.com/articles/samsung-sm953-pm1725-pm1633-pm1633a,1-2805.html" />
		<title level="m">Samsung Releases New 12 Gb/s SAS, M.2, AIC And 2.5&quot; NVMe SSDs: 1 Million IOPS, Up To 15.63 TB</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Artem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bityutskiy</surname></persName>
		</author>
		<ptr target="http://linux-mtd.infradead.org/tech/JFFS3design.pdf" />
		<title level="m">JFFS3 design issues</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Multikernel: A New OS Architecture for Scalable Multicore Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Dagand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schüpbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singhania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 8th Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asynchronous I/O support in Linux 2.5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pulavarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Linux Block IO: Introducing Multi-queue SSD Access on Multi-core Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bjørling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Systems and Storage Conference (SYSTOR)</title>
		<meeting>the 6th International Systems and Storage Conference (SYSTOR)</meeting>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Thousand Core Chips: A Technology Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual Design Automation Conference (DAC)</title>
		<meeting>the 44th Annual Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Corey: An Operating System for Many Cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd-Wickizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pesterev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 8th Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An Analysis of Linux Scalability to Many Cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd-Wickizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pesterev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 9th Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Non-scalable locks are dangerous</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd-Wickizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An f2fs teardown</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Brown</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/518988/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bug #55004: async fuzzy checkpoint constraint isn&apos;t really async</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Callaghan</surname></persName>
		</author>
		<ptr target="http://bugs.mysql.com/bug.php?id=55004" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">InnoDB fuzzy checkpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Callaghan</surname></persName>
		</author>
		<ptr target="https://www.facebook.com/notes/mysqlfacebook/innodb-fuzzy-checkpoints/408059000932" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">XFS, ext and per-inode mutexes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Callaghan</surname></persName>
		</author>
		<ptr target="https://www.facebook.com/notes/mysql-at-facebook/xfs-ext-and-per-inode-mutexes/10150210901610933" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ext4 block and inode allocator improvements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dilger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using data clustering to improve cleaning performance for flash memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-L</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SoftwarePractice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="290" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Consistency Without Ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 10th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">XFS Delayed Logging Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chinner</surname></persName>
		</author>
		<ptr target="http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/xfs-delayed-logging-design.txt" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving Metadata Performance By Reducing Journal Overhead</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chinner</surname></persName>
		</author>
		<ptr target="http://xfs.org/index" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable Address Spaces Using RCU Balanced Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 17th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">RadixVM: Scalable Address Spaces for Multithreaded Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM EuroSys Conference</title>
		<meeting>the ACM EuroSys Conference<address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Scalable Commutativity Rule: Designing Scalable Software for Multicore Processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>Farmington, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">JLS: Increasing VFS scalability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/360199/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dcache scalability and RCU-walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/419811/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">XFS: the filesystem of the future?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/476263/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Introducing lockrefs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/565734/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Per-CPU reference counts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corbet</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/557478/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">SNZI: Scalable NonZero Indicators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ellen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Luchangco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing</title>
		<meeting>the 26th ACM SIGACT-SIGOPS Symposium on Principles of Distributed Computing<address><addrLine>OR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
	<note>Portland</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ScaleFS: A Multicore-Scalable File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ext4 Disk Layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ext4</forename><surname>Wiki</surname></persName>
		</author>
		<ptr target="https://ext4.wiki.kernel.org/index.php/Ext4_Disk_Layout" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rocksdb</surname></persName>
		</author>
		<ptr target="http://rocksdb.org/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Microsoft SQL Server 2014 released to manufacturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Foley</surname></persName>
		</author>
		<ptr target="http://www.zdnet.com/article/microsoft-sql-server-2014-released-to-manufacturing/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hare: a file system for non-cache-coherent multicores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gruenwald</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeldovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM EuroSys Conference</title>
		<meeting>the ACM EuroSys Conference<address><addrLine>Bordeaux, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A File is Not a File: Understanding the I/O Behavior of Apple Desktop Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dragga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vaughn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 23rd ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>Cascais, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reducing file system tail latencies with Chopper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpacidusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 13th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Performance improvements using Concurrent I/O on HP-UX 11i v3 with OnlineJFS 5.0.1 and the HP-UX 11i Logical Volume Manager</title>
		<ptr target="http://www.filibeto.org/unix/hp-ux/lib/os/volume-manager/perf-hpux-11.31-cio-onlinejfs-4AA1-5719ENW.pdf" />
	</analytic>
	<monogr>
		<title level="j">HP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Converged System for SAP HANA Scale-out Configurations</title>
		<ptr target="http://www8.hp.com/h20195/v2/GetPDF.aspx%2F4AA5-1488ENN.pdf" />
	</analytic>
	<monogr>
		<title level="j">HP</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Use concurrent I/O to improve DB2 database performance</title>
		<ptr target="http://www.ibm.com/developerworks/data/library/techarticle/dm-1204concurrent/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Performance Benchmarking for PCIe and NVMe Enterprise Solid-State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel</surname></persName>
		</author>
		<ptr target="http://www.intel" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Boosting quasi-asynchronous I/O for better responsiveness in mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 13th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shore-MT: A Scalable Storage Manager for the Multicore Era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Extending Database Technology: Advances in Database Technology, EDBT &apos;09</title>
		<meeting>the 12th International Conference on Extending Database Technology: Advances in Database Technology, EDBT &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="24" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pandis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Athanassoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<title level="m">Aether: A Scalable Approach to Logging. Proc. VLDB Endow</title>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="681" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An Efficient Buffer Replacement Algorithm for NAND Flash Storage Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Eom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Symposium on Modelling, Analysis &amp; Simulation of Computer and Telecommunication Systems (MAS-COTS)</title>
		<meeting>the 22nd International Symposium on Modelling, Analysis &amp; Simulation of Computer and Telecommunication Systems (MAS-COTS)<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">MultiLanes: providing virtualized storage for OS-level virtualization on many cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 12th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huai</surname></persName>
		</author>
		<title level="m">Proceedings of the 2015 ATC Annual Technical Conference (ATC)</title>
		<meeting>the 2015 ATC Annual Technical Conference (ATC)<address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
	<note>SpanFS: a scalable file system on fast storage devices</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The multi-streamed solid-state drive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX conference on Hot Topics in Storage and File Systems</title>
		<meeting>the 6th USENIX conference on Hot Topics in Storage and File Systems</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="13" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Revisiting Storage for Smartphones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ungureanu</surname></persName>
		</author>
		<idno>14:1-14:25</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">f2fs: introduce flash-friendly file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/518718/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">K42: Building a Complete Operating System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auslander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Wisniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xenidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ostrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Appavoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Butrico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Waterland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Uhlig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM EuroSys Conference</title>
		<meeting>the ACM EuroSys Conference<address><addrLine>Leuven, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ATAC: A 1000-core Cachecoherent Processor with On-chip Optical Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Psota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eastep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Kimerling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)<address><addrLine>Vienna, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">F2FS: A new file system for flash storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 13th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>Santa Clara, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wal-Dio</surname></persName>
		</author>
		<title level="m">Eliminating the Filesystem Journaling in Resolving the Journaling of Journal Anomaly</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2015 ATC Annual Technical Conference (ATC)</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">PATCH] dcache: Translating dentry into pathname without taking rename_lock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://lkml.org/lkml/2013/9/4/471" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rethinking main memory oltp recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Malviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weisberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th IEEE International Conference on Data Engineering Workshop</title>
		<meeting>the 30th IEEE International Conference on Data Engineering Workshop<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The new ext4 filesystem: current status and future plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dilger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Solaris Internals and Performance FAQ: Direct I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdougall</surname></persName>
		</author>
		<ptr target="http://www.solarisinternals.com/wiki/index.php/Direct_I/O" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A Fast File System for UNIX</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Mckusick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>Joy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Leffler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Fabry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="734" to="2071" />
			<date type="published" when="1984-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">SFS: Random Write Considered Harmful in Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Eom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 10th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Lightweight Application-Level Crash Consistency on Transactional Flash Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Eom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ATC Annual Technical Conference (ATC)</title>
		<meeting>the 2015 ATC Annual Technical Conference (ATC)<address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Morgan</surname></persName>
		</author>
		<ptr target="http://www.enterprisetech.com/2014/08/06/flashtec-nvram-15-million-iops-sub-microsecond-latency/" />
		<title level="m">Flashtec NVRAM Does 15 Million IOPS At Sub-Microsecond Latency</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The logstructured merge-tree (LSM-tree)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="385" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Arrakis: The Operating System is the Control Plane</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 11th Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>Broomfield, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">All File Systems Are Not Created Equal: On the Complexity of Crafting CrashConsistent Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alagappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Kiswany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 11th Symposium on Operating Systems Design and Implementation (OSDI)<address><addrLine>Broomfield, Colorado</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">CPU hotplug Support in Linux(tm) Kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raj</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/cpu-hotplug.txt" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">B-trees, Shadowing, and Clones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rodeh</surname></persName>
		</author>
		<idno>1553-3077</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">BTRFS: The Linux B-Tree Filesystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rodeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bacik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2013-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Tmpfs is a file system which keeps all files in virtual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rohland</surname></persName>
		</author>
		<ptr target="git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/tree/Documentation/filesystems/tmpfs.txt" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The Design and Implementation of a Log-structured File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Ousterhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="734" to="2071" />
			<date type="published" when="1992-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">GPFS: A Shared-Disk File System for Large Computing Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schmuck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Haskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 1st Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>Monterey, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Dynamic interval polling and pipelined post i/o processing for low-latency storage class memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Yeom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX conference on Hot Topics in Storage and File Systems</title>
		<meeting>the 5th USENIX conference on Hot Topics in Storage and File Systems</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="5" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title/>
		<ptr target="http://xfs.org/docs/xfsdocs-xml-dev/XFS_" />
	</analytic>
	<monogr>
		<title level="j">Silicon Graphics Inc. XFS Filesystem Structure</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">An Introduction to InnoDB Internals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Swanhart</surname></persName>
		</author>
		<ptr target="https://www.percona.com/files/percona-live/justin-innodb-internals.pdf" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Scalability in the XFS File System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doucette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 ATC Annual Technical Conference (ATC)</title>
		<meeting>the 1996 ATC Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="1996-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tallis</surname></persName>
		</author>
		<ptr target="http://www.anandtech.com/show/9646/intel-announces-ssd-dc-p3608-series" />
		<title level="m">Intel Announces SSD DC P3608 Series</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Lies, damned lies, and file system benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<idno>TR-34-94</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tarasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhanage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
		<title level="m">Benchmarking file system benchmarking: It* is* rocket science. HotOS XIII</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">A Nine Year Study of File System and Storage Benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Traeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Wright</surname></persName>
		</author>
		<idno>5:1-5:56</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Speedy Transactions in Multicore In-memory Databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)<address><addrLine>Farmington, PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Scalable Logging Through Emerging Non-volatile Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="865" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">When poll is better than interrupt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Minturn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Usenix Conference on File and Storage Technologies (FAST)</title>
		<meeting>the 10th Usenix Conference on File and Storage Technologies (FAST)<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Exploiting peak device throughput from random access workload</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Yeom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX conference on Hot Topics in Storage and File Systems. USENIX Association</title>
		<meeting>the 4th USENIX conference on Hot Topics in Storage and File Systems. USENIX Association</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
