<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Prefetching in Hybrid Main Memory Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subisha</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology Gandhinagar</orgName>
								<orgName type="institution" key="instit2">Ashoka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Gohil</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology Gandhinagar</orgName>
								<orgName type="institution" key="instit2">Ashoka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><surname>Ujjainkar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology Gandhinagar</orgName>
								<orgName type="institution" key="instit2">Ashoka University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manu</forename><surname>Awasthi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Indian Institute of Technology Gandhinagar</orgName>
								<orgName type="institution" key="instit2">Ashoka University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Prefetching in Hybrid Main Memory Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The architecture of main memory has experienced a paradigm shift in recent years, with non volatile memory technologies (NVM) like Phase Change Memory (PCM) being incorporated into the hierarchy at the same level as DRAM. This transformation is being carried out by either splitting the memory address across two or more memory technologies, or using a faster technology with higher lifetimes, typically the DRAM, as a cache for the higher capacity, albeit slower main memory made up of a NVM. Design of such hybrid architectures remains an active area of research from the perspective of DRAMas a cache design, since DRAM could quickly become the bottleneck, as cache lookups require multiple accesses for reading tag and data. In this paper, we augment the DRAMas a cache model with a novel DRAM cache prefetcher that builds on state of the art Alloy Cache. The new DRAM cache architecture allows for prefetching data at both cacheline and page granularities from the NVM, and as a result, provides upto a maximum of 2× performance improvement over a state of the art baseline.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been an increasing demand from applications for increased memory capacity. However, DRAM stands at the edge of its scaling capabilities and it is unclear if it can be scaled beyond 10 (or so) nano meters <ref type="bibr">[1]</ref>.</p><p>Emerging, non-volatile memory (NVM) technologies like PCM, STT-RAM etc. are touted to provide high capacity alternatives to DRAM as main memory technologies. The capacity benefits of all such technologies are apparent; most technologies have higher areal density <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b5">7]</ref> than DRAM. However, simply replacing DRAM with a NVM causes additional challenges. Owing to the increased access latencies of these technologies, the average memory access time (AMAT) increases significantly, resulting in unacceptable slowdown in application performance <ref type="bibr" target="#b3">[5]</ref>. Naive replacement of DRAM with any other NVM can lead to significant degradation in performance of memory intensive applications <ref type="bibr" target="#b3">[5]</ref>. Not only that, for a main memory comprised entirely of a NVM, a number of other issues like wear leveling and reducing the detrimental performance effects caused by asymmetric read and write latencies need to be addressed. As a result, unless any of these emerging technologies are able to achieve DRAM-compatible latencies, they will have to be used in conjunction with DRAM as main memories.</p><p>In the absence of simple solutions, architects have devised hybrid memory hierarchies, utilizing both DRAM and NVMs, in order to provide the best of both worlds by providing lowlatency data access via DRAM, and higher capacity via NVM. The two most popular hybrid architectures are split-address space <ref type="bibr" target="#b0">[2]</ref> and DRAM-as-a-cache <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b7">9]</ref>. In the split-address space variant, the physical address space is divided, in some proportion, across DRAM and NVM. The system software, typically the operating system, has to explicitly perform data placement between the two disparate memory technologies. However, this requires changes at multiple levels. Multiple types of memory controllers, each pertaining to a memory technology, have to be incorporated onto the CPU. The system software also has to be changed substantially <ref type="bibr" target="#b10">[12]</ref>.</p><p>To make the adoption of hybrid memory hierarchies less intrusive, an alternate architecture which uses DRAM as a cache to the NVM comprised of main memory was proposed <ref type="bibr" target="#b4">[6]</ref>. In this design, hardware seamlessly manages data between the DRAM and NVM. The AMAT experienced by the application is closer to that of DRAM, but visible memory capacity is that of NVM.</p><p>Multiple architectures for DRAM-as-a-cache have been proposed <ref type="bibr" target="#b6">[8]</ref>. Since a cache requires storage of both tag and data, overall access latency could increase if accesses to both are serialized. This becomes especially problematic in the case of DRAM caches, since serializing these accesses requires multiple round trips to DRAM, increasing overall access time, reducing effectiveness of the cache. Architectures like Alloy Cache <ref type="bibr" target="#b7">[9]</ref> have been proposed to alleviate these shortcomings of DRAM caches. Alloy Cache allows for reduction of average latency to access DRAM cache by storing We posit that one can further improve the AMAT by intelligently prefetching data from NVM to DRAM Cache. To the best of our knowledge, there exists no work for designing prefetchers for DRAM caches. If If prefetchers for DRAM caches are to be designed, we need to address some basic questions regarding those. For example, what should be the granularity of prefetch? Where should the prefetched data be placed? We try to answer some of these questions in this paper.</p><p>To gain insights into the workload behavior under DRAM Cache based hybrid memories, and their suitability for prefetchers, we conduct multiple experiments. First, we analyze per-page cacheline access characteristics of different applications <ref type="bibr">1</ref> . Assuming a 64 B cacheline and 4 KB pages, each page contains 64 cachelines. <ref type="figure" target="#fig_0">Figure 1</ref> shows that on average, every cacheline of 73% of the pages is accessed during program's execution. In addition, 84% of the pages have at least a third of their cachelines accessed. These experiments provide support for the presence of spatial locality in the workloads under consideration, and the fact that fetching data into the DRAM cache at larger than cacheline granularity will be helpful for application performance.</p><p>However, fetching at larger granularities can lead to cache pollution. In order to assess such detrimental effects, we tracked space allocation in Alloy Cache, the results of which are presented in <ref type="figure" target="#fig_1">Figure 2</ref>. We observe that large amount of DRAM cache capacity remains unutilized -across all workloads, 92% of all DRAM cache pages are unallocated.</p><p>These results provide us two insights. First, data in DRAM cache should be managed at multiple granularities, both large and small. Second, managing data at cacheline granularity only leads to under-utilization of DRAM Cache capacity. We combine these two insights to design a novel prefetcher that fetches data at page granularity into regions of DRAM cache that are not utilized by the Alloy Cache. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prefetcher Design</head><p>In the baseline Alloy Cache design, all pages in DRAM are used for storing cachelines, brought from NVM due to demand misses. Each such cacheline maps to a unique DRAM page, and each such page might contain cachelines from multiple physical pages in NVM. However, as described before, many physical pages in NVM exhibit significant spatial locality, and the application will benefit if the entire page, and hence all its cachelines, were prefetched into the DRAM. If this was enabled, the DRAM Cache will now have two kinds of pages:</p><p>Alloy Cache Page: These are the DRAM Cache pages that consist of Alloy Cachelines. Alloy Cachelines have data (64B) and tag (8 B) integrated together as a single unit (72 B). Hence, one 4KB Alloy Cache page can accommodate 56 cachelines.</p><p>Prefetched Page: These pages are prefetched from NVM to the DRAM Cache. Tags for cachelines in these pages are saved separately in the prefetcher component, which is described later in Section 2.4. A 4 KB Prefetched Page can accommodate 64 cachelines.</p><p>Prefetching physical pages from NVM into DRAM, while keeping the baseline design of Alloy Cache requires us to provide additional functionality to various memory controllers in the hybrid memory architecture, which are described next.</p><p>We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NVM Page Classifier (NPC)</head><p>The NVM Page Classifier keeps track of the access history of recently accessed NVM pages, which is used to identify pages suitable for prefetching. Ideally, we should prefetch a page with high number of accesses and with accesses to a number of unique cachelines. To identify such pages, we keep track of the number of cachelines touched in individual NVM pages. An NVM page is deemed a candidate for prefetch when the number of accesses to the page crosses an Access Threshold (AT), and the number of accesses to unique cachelines crosses Unique Access Threshold (UAT). Here, AT ≥ UAT. <ref type="figure" target="#fig_3">Figure 5a</ref> illustrates the composition of a single NPC entry. Considering there are N total NVM pages, each NPC entry has log 2 N bits to identify the page number. An Access Counter keeps track of the number of accesses to this page. We provide a log 2 AT bit saturating counter to keep track of Access Threshold. A 64 bit Cache Access Vector (CAV), with one bit for each cacheline in the page, is also provided. A set bit in CAV indicates that the cacheline corresponding to the bit has been accessed. Finally, we provide a log 2 AT bit saturating counter for keeping track of unique cachelines accessed. Whenever an NVM page is accessed, the access counter is incremented. The corresponding bit in the CAV is checked. If the bit is 0, it is set and the UAT is incremented. The NPC will contain multiple such entries. For every request a fullyassociative search is performed on the NPC whose entries are evicted using LRU.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Empty Page Classifier (EPC)</head><p>The Empty Page Classifier keeps track of empty pages in DRAM Cache. The EPC is used to identify an empty page in DRAM Cache and place a prefetched page at that location, improving DRAM Cache utilization. <ref type="figure" target="#fig_4">Figure 6</ref> represents the organization of the multi-level  The EPC is updated when a DRAM Cache page becomes empty or is occupied. This is done by first updating the last level and then progressing towards upper levels, updating the structure at each step. We perform a lookup on EPC for the last empty page by finding the last set bit in the first level of EPC and recursively traversing through the lower levels till we reach a page. For implementing this, we use the hardware find-first-set (ffs) function, which finds the index of the first set bit starting from the least-significant bit. <ref type="table">Table (</ref>PRT)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Page Redirection</head><p>The Page Redirection Table maintains a mapping of prefetched pages to their locations in DRAM Cache, and has a functionality similar to that of a page table. Every miss in the last level cache first accesses the PRT to identify whether the requested address exists as a cacheline in a prefetched page.</p><p>The PRT is a hash-table. We empirically find the optimal size and associativity for the PRT which minimizes collisions and improves look-up time. <ref type="figure" target="#fig_3">Figure 5c</ref> depicts an entry in PRT. Each PRT entry stores the tag of the cacheline to verify that the the cacheline in the prefetched page is indeed the requested cacheline. The number of tag bits depend of the number of offset bits, index bits and data bits in the address. To specify the location in DRAM Cache where the page is mapped to, we have log 2 D bits in each PRT entry. We also have a valid bit which when set indicates that the mapping is valid. A PRT entry is invalidated when an Alloy Cacheline is brought to the location which is occupied by a prefetched page due to some request. As demand fetches have higher priority than prefetches, we evict prefetched page and invalidate the PRT entry. We use LRU policy for evicting entries from PRT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Memory Request Service Routines</head><p>Algorithm 1 and 2 provide the service routines for write and read requests, respectively. Next, we cover the salient points of the read/write routines, including some interesting cases.</p><p>Given our design, there is a chance that a requested cacheline might be present at two locations in the DRAM: as a part of a Prefetched Page, which contains all the cachelines for the page. Or, it can be present in an Alloy Cache page, which might contain multiple other cachelines. The case for writing data when it is present in either of the two locations is easy. However, care has to be taken to ensure data consistency when a cacheline is present at multiple locations.</p><p>To ascertain the cacheline's presence, we first read the PRT and TC in parallel. A PRT hit indicates that the cacheline is present in a Prefetched Page. If there is a hit in the Alloy Cache as well, it indicates that the cacheline is also present as an Alloy Cacheline.</p><p>When requested cacheline is present in both, a prefetched page and an Alloy Cacheline, the read request is serviced from the Alloy Cacheline. In parallel, if the cacheline is dirty, the data is written to the corresponding region in the Prefetched Page and the cacheline is invalidated. In the case of a write request, the data is written to the cacheline in the Prefetched Page. If the Alloy Cacheline was dirty, the Prefetched Page is updated with the dirty cacheline before the current write is carried out. The Alloy Cacheline is also invalidated.</p><p>In cases of read requests where the requested cacheline is present in a Prefetched Page, and the corresponding Alloy Cache page doesn't have the requested cacheline, the data is returned from the Prefetched Page. The writes are also done at the corresponding location in the Prefetched Page.</p><p>Finally, there can be write cases where the cacheline needs to be written into the DRAM Cache, and is not present in any Prefetched Page. If the cacheline is present at its (fixed) location as an Alloy Cacheline, it is written in place. In case this location is currently being occupied by a Prefetched Page, the page will be evicted, and it's contents written back to NVM, if any cachelines are dirty. The location is now marked as an Alloy Cache page, and the write to the cacheline completes.</p><p>In cases where data is not present in either in a Prefetched Page, or as a part of a Alloy Cache page, the request for data is sent to the NVM. The data is then brought in at cacheline granularity, unless the criteria for prefetching the page are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We use ZSIM <ref type="bibr" target="#b8">[10]</ref> and NVMain <ref type="bibr" target="#b2">[4]</ref> for simulating hybrid main memory hierarchy with 1 GB 8-channel DRAM Cache and 16 GB Phase Change Memory (PCM). We chose PCM as a representative NVM technology, but the same set of experiments can be repeated by changing the NVMain parameters to match those of the memory technology that should be modeled. We modify the source code to incorporate our proposed prefetcher. We simulate an 8 core, 2.6 GHz processor with private L1 instruction, L1 data and L2 caches for each core. The L3 cache is shared across all cores. <ref type="table" target="#tab_3">Table 1</ref> shows the cache hierarchy and main memory configuration used for our experiments. We assume a 4 KB page size for a total of 4,194,304 pages in the NVM and 2,62,144 pages in DRAM Cache. As a result, we need 22 bits to represent NVM page number and 18 bits to represent DRAM Cache page number.</p><p>We observe that 82% of pages with accesses to at least onethird of their cachelines, have all their cachelines accessed <ref type="figure" target="#fig_0">(Figure 1)</ref>. Hence, we set AT to 22 ( ≈ 64/3). We empirically determine that it is best to set UAT as two-thirds of AT. Hence we set UAT to 15 (≈ AT*2/3). Each NPC entry is 12 bytes with 22 pagenumber bits, 5 bits each for Access Counter and Unique Access Counter, and 64 bits for Cacheline Access Vector. Further, we find that the optimal size of NPC is 16 entries, bringing the total NPC size to 192 bytes. We assume a 1 clock cycle overhead for accessing the NPC.</p><p>To cover all pages in the DRAM Cache we require a 2MB storage space for the TC table, shown in <ref type="figure" target="#fig_3">Figure 5b</ref> . Using CACTI <ref type="bibr" target="#b9">[11]</ref>, the access latency of TC table is estimated to be 4 clock cycles.</p><p>From design space exploration, we find that it is best to have a 4 way set associative table with 1024 sets for the PRT.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate the prefetcher on the PARSEC <ref type="bibr" target="#b1">[3]</ref> benchmarks with simlarge input set, for 2 cases -one being single-program workloads in which we run a single thread of each application for 5 billion instructions. The other is for multi-programed workloads where two instances of a single threaded applica-  <ref type="figure">Figure 7a</ref> shows the cache hit rate improvement for singleprogram workloads. We observe a 1.5-4× improvement in DRAM Cache hit rate across all PARSEC workloads. Streamcluster experiences a 12× improvement in cache hit rate. We believe that this is due to streamcluster's memory intensive behavior -no other application in the PARSEC suite generates more than 50% of streamcluster's DRAM Cache requests.</p><p>The improvement in cache hit rate also translates to lowered AMAT, shown in <ref type="figure">Figure 7b</ref>. Average memory access time for streamcluster reduces by 80%. For other applications we observe a 10-30% decrease in average memory latency. However, for bodytrack and fluidanimate, average memory access latency increases slightly despite increase in DRAM Cache hit-rate. We believe that this is due to the overheads introduced by prefetcher components and the lack of memory level parallelism of these applications.</p><p>Further, <ref type="figure">Figure 7c</ref> depicts the speedup of the prefetcher working in conjunction with Alloy Cache over the baseline Alloy Cache design. We use the ratio of instructions-per-cycle (IPC) as a measure of speedup. For streamcluster we observe a 7× speedup, while for fluidanimate and vips we do not see any performance impact. The rest of the applications observe 16-40% improvements in IPC. threaded instances of the same program. Two types of runs were done, one for 5 billion instructions, and the other for 10 billion. We observe a 2× increase in DRAM Cache hit rate, on average, across all workloads except streamcluster, for both 5 and 10 billion instructions. For streamcluster we observe a 16× improvement in the hit rate. Streamcluster, owing to a 50% reduction in AMAT, experiences a 2× speedup over the baseline Alloy Cache. The rest of the workloads experience a 10-40% reduction in AMAT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we provide the design and initial set of results for a novel prefetcher for the DRAM Cache in hybrid memory architectures. The prefetcher brings data at the granularity of pages from NVM in conjunction with Alloy Cache, which performs demand fetching at cacheline granularity. We show that it is possible to co-locate cached data with both cacheline and page granularities by augmenting the DRAM memory controller with a few low-latency and low overhead structures. Finally, we demonstrate that the novel prefetcher design has the potential to outperform the state of the art Alloy Cache baseline by up to 2× for memory intensive workloads.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cacheline accesses of NVM pages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Page Utilization in DRAM Cache</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 illustrates</head><label>3</label><figDesc>Figure 3: Proposed structures in Memory Controllers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Entries of Prefetcher components. Values are the number of bits required 2.2 Type Classifier (TC) As we prefetch pages directly to DRAM Cache, it now houses two types of pages: Alloy Cache Page and Prefetched Page. To use the system without errors, we need to identify the type of page that exists at a DRAM Cache location. The Type Classifier identifies whether a DRAM Cache location houses an Alloy Cache page or a Prefetched page. It also maintains the the occupancy details of all the cachelines in the Alloy Cache page. Figure 5b represents an entry in the TC table. Each TC entry has 2 bits to identify the type of page stored at the location. Using 2 bits, we can represent the following four states: State 0 : 00 ← Empty/Un-allocated Location State 1 : 01 ← Valid Prefetched Page at Location State 2 : 10 ← Valid Alloy Cache Page at Location State 3 : 11 ← Dirty Prefetched Page at Location When a page is prefetched from NVM to DRAM, the page is in state 1. When we write to a page in state 1, it transitions to state 3, indicating that the page is dirty. All Alloy Cache pages are in state 2, irrespective of cachelines being dirty. The dirtiness of an Alloy Cacheline is indicated by the tag stored along with the data. Figure 4 illustrates the transitions between all four states. Each TC entry contains a Cacheline Usage Vector (CUV), having one bit for each cacheline. These bits are valid only if the Type bits indicate State 2 (Alloy Cache page at location). A 4KB Alloy Cache page has 56 cachelines, implying a 56 bit CUV. A set bit signifies that the corresponding cacheline is occupied. An entry in Type Classifier is maintained for all locations in the DRAM Cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Structure of Empty Page Classifier</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc>Figure 7: Results for Single-Program workloads tion are simultaneously executed for both 5 and 10 billion instructions. Figure 7a shows the cache hit rate improvement for singleprogram workloads. We observe a 1.5-4× improvement in DRAM Cache hit rate across all PARSEC workloads. Streamcluster experiences a 12× improvement in cache hit rate. We believe that this is due to streamcluster's memory intensive behavior -no other application in the PARSEC suite generates more than 50% of streamcluster's DRAM Cache requests. The improvement in cache hit rate also translates to lowered AMAT, shown in Figure 7b. Average memory access time for streamcluster reduces by 80%. For other applications we observe a 10-30% decrease in average memory latency. However, for bodytrack and fluidanimate, average memory access latency increases slightly despite increase in DRAM Cache hit-rate. We believe that this is due to the overheads introduced by prefetcher components and the lack of memory level parallelism of these applications. Further, Figure 7c depicts the speedup of the prefetcher working in conjunction with Alloy Cache over the baseline Alloy Cache design. We use the ratio of instructions-per-cycle (IPC) as a measure of speedup. For streamcluster we observe a 7× speedup, while for fluidanimate and vips we do not see any performance impact. The rest of the applications observe 16-40% improvements in IPC. Figure 8 shows the results for multi-program workloads. Results are obtained by concurrently running 2 single-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 shows</head><label>8</label><figDesc>Figure 8: Results for Multi-Program workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>table . For</head><label>.</label><figDesc></figDesc><table>implementing the EPC, we group consecutive 
DRAM pages in groups of 64, each represented by a 64 bit 
vector. The 64 bit vectors for all groups together form the last 
Level L of the EPC table. A set bit in this vector indicates 
that the corresponding page is empty. We further group these 
vectors in groups of 64, leading to a 64 bit vector in Level 
(L − 1) of the EPC table. We recursively keep grouping the 
bit vectors in groups of 64 or less, till we have a single vec-
tor left i.e. we reach Level 1 of the EPC table. A set bit in 
the bit vectors from Level 1 to (L − 1) indicates that there 
exists at least one set bit in the corresponding bit vector in the 
next level. Such a multi-level design improves the worst-case 
lookup latency when compared to a single level design. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Algorithm 1 : Write Request Service Routine</head><label>1</label><figDesc></figDesc><table>input :Address and Data 
if PRT hits for Address then 
if TC is 2 then 
if Cacheline hits then 
Invalidate Cacheline 
end 
end 
Write Data to prefetched page 
else 
if TC is 1 then 
Evict prefetched page at DRAM Cache location 
end 
if TC is 3 then 
Evict prefetched page at DRAM Cache location and write evicted 
page to the NVM. 
end 
if TC is 2 then 
if Cacheline does not hit then 
Evict the Cacheline at the DRAM Cache location. 
end 
end 
write Data to Cacheline at DRAM Cache location. 
end 

fulfilled by this request. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Algorithm 2 : Read Request Service Routine</head><label>2</label><figDesc></figDesc><table>input :Address to be read 
if PRT hit occours for Address then 
if TC is 0 or 1 or 3 then 
return data from prefetched page 
else 
if Cacheline hits and Cacheline is dirty then 
Write DRAM Cacheline to prefetched page. 
Invalidate the DRAM Cacheline. 
end 
return data from prefetched page 
end 
else 
if TC is 0 then 
Send read request to NVM 
else if TC is 1 then 
Evict prefetched page at DRAM Cache location 
Send read request to NVM 
else if TC is 2 then 
if Cacheline hits then 
return data from DRAM Cacheline 
else 
Evict cacheline at DRAM Cache location. 
If evicted cacheline is dirty and belongs to a prefetched 
page, write the evicted cacheline to that prefetched page. 
Send read request to NVM 
end 
else 
Evict prefetched page at DRAM Cache location and write evicted 
page to the NVM. 
Send read request to NVM 
end 
end 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 1 : Experimental Configuration</head><label>1</label><figDesc></figDesc><table>Cache Hierarchy Configuration 
Size (KB) 
Latency (Cycles) 
L1 Insn 
32 
4 
L1 Data 
32 
4 
L2/L3 Unified 
256/4096 
8/16 
All caches are 8 way set associative 
Main Memory Configuration 
DRAM Cache 
PCM 
Frequency (MHz) 
1600 
400 
tRCD/tCCD (cycles) 
23/4 
312/13 
tCAS/tRP (cycles) 
23/23 
7/390 

This table is 20KBs, with a single entry of 5 bytes. While 
accessing the PRT, we use 10 bits (Bits 22-13) to index into 
the set and last 12 bits for offset. For our configuration, each 
PRT entry has 21 tag bits, 18 bits for DRAM Cache page 
number and 1 valid bit. We determine the access latency of 
PRT to be 2 clock cycles using CACTI [11]. 
We implement the EPC on a per-channel basis. Our con-
figuration, has 32,768 pages per channel, leading to a 3 level 
EPC table described in Section 2.3. EPC's Level 3 has 512 
64-bit vectors, Level 2 of EPC has 8 64-bit vectors and Level 
1 is a 8-bit vector. Overall, the EPC is approximately 33 KBs 
and has a 3 cycle latency, as per CACTI [11]. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Biswabandan Panda, IIT Kanpur for initial discussions and our shepherd Deepavali Bhagwat, IBM Research for her help in improving the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When physical is not real enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Bellosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Workshop on ACM SIGOPS European Workshop</title>
		<meeting>the 11th Workshop on ACM SIGOPS European Workshop<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The parsec benchmark suite: Characterization and architectural implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaswinder</forename><forename type="middle">Pal</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th international conference on Parallel architectures and compilation techniques</title>
		<meeting>the 17th international conference on Parallel architectures and compilation techniques</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nvmain extension for multi-level cache systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fazal</forename><surname>Asif Ali Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeronimo</forename><surname>Hameed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Castrillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Rapido&apos;18 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools</title>
		<meeting>the Rapido&apos;18 Workshop on Rapid Simulation and Performance Evaluation: Methods and Tools</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Architecting phase change memory as a scalable dram alternative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual International Symposium on Computer Architecture, ISCA &apos;09</title>
		<meeting>the 36th Annual International Symposium on Computer Architecture, ISCA &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficiently enabling conventional block sizes for very large die-stacked dram caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-44</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-44<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Tutorial on emerging memory devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darsen</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Available at people. oregonstate. edu/˜ sllu/Micro_MT/presentations/micro16_emerging_ mem_tutorial_darsen. pdf</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of techniques for architecting dram caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sparsh</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey S</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1852" to="1863" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fundamental latency trade-offs in architecturing dram caches: Outperforming impractical sram-tags with a simple and practical design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 45th Intl. Symp. on Microarchitecture</title>
		<meeting>of the 45th Intl. Symp. on Microarchitecture<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zsim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer architecture news</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="486" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyamkumar</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Cacti 6.5. hpl. hp. com</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hardware translation coherence for virtualized systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ján</forename><surname>Veselý</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilherme</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
