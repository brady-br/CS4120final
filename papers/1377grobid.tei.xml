<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">G 2 : A Graph Processing System for Diagnosing Distributed Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoqiang</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asia</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University § Harbin Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">G 2 : A Graph Processing System for Diagnosing Distributed Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>G 2 is a graph processing system for diagnosing distributed systems. It works on execution graphs that model runtime events and their correlations in distributed systems. In G 2 , a diagnosis process involves a series of queries, expressed in a high-level declarative language that supports both relational and graph-based operators. Each query is compiled into a distributed execution. G 2 &apos;s execution engine supports both parallel relational data processing and iterative graph traversal. Execution graphs in G 2 tend to have long paths and are in structure distinctly different from other large-scale graphs, such as social or web graphs. Tailored for execution graphs and graph traversal operations on those graphs, G 2 &apos;s graph engine distinguishes itself by embracing batched asynchronous iterations that allows for better parallelism without barriers, and by enabling partition-level states and aggregation. We have applied G 2 to diagnosis of distributed systems such as Berkeley DB, SCOPE/Dryad, and G 2 itself to validate its effectiveness. When co-deployed on a 60-machine cluster, G 2 &apos;s execution engine can handle execution graphs with millions of vertices and edges; for instance , using a query in G2, we traverse, filter, and summarize a 130 million-vertex graph into a 12 thousand-vertex graph within 268 seconds on 60 machines. The use of an asynchronous model and a partition-level interface delivered a 66% reduction in response time when applied to queries in our diagnosis tasks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Distributed applications in data centers are increasingly important as they power large-scale web and cloud services. Often, the execution of such an application involves a large number of cooperating processes running on different machines, spanning multiple software modules and layers, tolerating and recovering from various machine failures and network disruptions. Increases in both the scale and complexity of such systems have made it difficult to understand and diagnose their runtime (mis-)behavior.</p><p>Typical diagnosis tasks start with observing misbehavior or anomaly, navigating through runtime information such as logs to find relevant information, and processing the information to infer root causes. For example, starting with a log entry with an error message, diagnosis could find all relevant log entries to infer the root cause for the error. As another example, given two similar jobs that noticeably perform differently, diagnosis could extract related runtime information to identify major differences. Also, it might be difficult to spot problems from a large number of low-level runtime events. A useful practice is to aggregate performance information at an appropriate layer, identify which aggregated component in that layer is problematic, and then drill down into the next layer of details in an iterative process.</p><p>Effective diagnosis depends heavily on the ability to correlate runtime events and to leverage these correlations. Previous work, especially those on path-based analysis <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b24">27]</ref>, has largely addressed the important problem of generating and correlating runtime information from executions of a distributed system. Often the difficulty for diagnosis is not due to lack of information, but due to the inability to navigate through and process a sea of information to find out what is relevant.</p><p>In this paper, we propose G 2 , a distributed graph processing system for storing runtime information of distributed systems and for processing queries on such information. Runtime information is organized as a graph, where vertices correspond to events and edges correspond to correlations between events. Diagnosis then involves an iterative process of writing queries against the graph and analyzing the results of those queries. G 2 provides a declarative language that supports relational and graph operators that operate on the graph structure. For example, given an error log entry e, a G 2 query can be issued to find all events (vertices) that vertex e is causally dependent on, where causal dependencies are captured by certain types of edges. This query uses a slicing operator that G 2 provides. From a starting vertex v, forward slicing finds all vertices that causally and transitively depend on v, while backward slicing finds all vertices that v is dependent on.</p><p>Graph aggregation and summarization are another effective way of reducing the amount of information to be examined during diagnosis. In an execution graph, each vertex is associated with a context that indicates the aggregation units that the event belongs to. Examples of aggregation units include static ones such as components, classes, and functions, as well as dynamic ones such as machines, processes, and threads. A G 2 query can aggregate information at an appropriate level. For example, to compare executions of two jobs, a query can compute the forward slices from the starting points of two jobs. To make comparison easier, the query can continue to compute a machine-level aggregation from the two slices. This requires a hierarchical aggregation graph operator that transforms an input graph into a smaller one: it condenses each continuous segment of events with the same aggregation unit (e.g., machine) to create a single supernode and applies an aggregation function on those events to compute the an associated aggregated value.</p><p>Distributed query execution in G 2 is supported by a distributed storage and execution system that addresses the challenges of storing and processing large execution graphs with millions or even billions of vertices efficiently. In G 2 , events and correlations are captured on local machines as they occur during system execution, leading to a natural partitioning of an execution graph. G 2 's execution engine is tailored for execution graphs that exhibit significantly different characteristics from other large graphs, such as social and web graphs. Execution graphs tend to have long paths corresponding to events along a logically related progression of execution, where social and web graphs have relatively small diameters. Graph operations on execution graphs are often in the form of graph traversal, which is again different from iterative graph operations that must proceed in globally synchronized rounds, such as in page-rank computation for example. Consequently, G 2 embraces batched asynchronous iterations, where processing on each partition is batched, but does not have to proceed synchronously in lock steps. Both slicing and hierarchical aggregation fall into this model that allows for improved parallelism and efficiency than the bulk synchronous computation model in previous work, such as in Pregel <ref type="bibr" target="#b21">[24]</ref>. Barriers are used only at the end of graph traversal or to create global consistent checkpoints for failure recovery. Furthermore, partitions tend to contain long local paths before those paths connect to vertices on other partitions due to cross-machine communication. Graph traversal within each partition is therefore significant to the overall graph traversal performance. Instead of a vertex-oriented interface, G 2 exposes a partition-oriented interface that allows partition-level aggregation states to be maintained in an appropriate data structure. This is particularly valuable for hierarchical aggregation, where the choice of partition-level data structure significantly influences performance.</p><p>We have built a prototype and applied it to a set of distributed systems, including Berkeley DB <ref type="bibr" target="#b0">[2]</ref>, SCOPE/Dryad <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b19">22]</ref>, and G 2 itself. Berkeley DB is a replicated distributed key-value database that can be easily linked with applications. SCOPE/Dryad is a production data intensive computation system, which includes a distributed file system, a distributed execution engine (Dryad), and a declarative query language (SCOPE). G 2 is shown to be effective in diagnosis: for instance, using a query in G2, we traverse, filter, and summarize a 130 million-vertex graph into a 12 thousand-vertex graph within 268 seconds on 60 machines. The optimizations we introduce into G 2 's execution engine are effective: the use of asynchronous model and partition-level interface delivered up to a factor of 3 performance improvement when applied to graph operators in our diagnosis tasks. We have also studied scalability of G 2 and the checkpointing overhead introduced to enable failure recovery.</p><p>The contribution of G 2 is two-fold. First, as a tool, G 2 enables efficient distributed-system diagnosis by allowing users to write declarative queries with both relational and graph operators, and by providing a distributed engine that executes those queries efficiently. Second, as a distributed system, G 2 's execution engine targets a different type of graphs with different structural characteristics and with different type of graph operations. It allows a batched asynchronous graph computation model and a partition-level interface, which have contributed significantly to its efficiency.</p><p>The rest of the paper is organized as follows. Section 2 introduces the system execution graph data model, and the diagnosis primitives applied to the graph. Section 3 presents the operators, and the language that G 2 supports, as well as several examples expressed in those constructs. The design and optimization of the distributed graph engine is the focus of Section 4, followed by implementation details in Section 5. We evaluate G 2 and share experience in Section 6. Section 7 discusses the related work. Finally, we conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODEL</head><p>Distributed-system diagnosis in G 2 centers on the data model and the operations defined on the model, which are the topic of this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text, Paths, and Graphs</head><p>Traditionally, system diagnosis treats runtime information (e.g., logs) as unstructured text and involves a tedious and ineffective process of going through logs using primitive text-processing tools such as grep. Using grep on a special tag (such as a request id) captures all entries that are explicitly related to that request, but is likely to miss information that has implicit dependencies.</p><p>Previous work <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b24">27]</ref> on correlating runtime information has effectively addressed this shortcoming by capturing common causal relationship in distributed systems. A path-like abstraction is often used to track how a request flows through a distributed system. This relatively simple structure is effective for requestcentric analysis and modeling, and reflects a good balance between what an abstraction enables, the simplicity of an abstraction, and the complexity involved in supporting operations on an abstraction.</p><p>Yet, the effectiveness of a path-based model is constrained by its simplifying assumptions: by embracing paths based on requests, the model cuts off interactions between requests that occur in distributed systems. For example, <ref type="figure">Figure 1</ref> shows a piece of code for a replicated file system. The system receives client requests and appends them in a local cache (line 2-4). When there are enough accumulated requests (line 6), the system batches requests, writes them to local disks (line 11), and forwards them to secondaries for data replication (line 12). The OnPersistRequests call in <ref type="figure">Figure 1</ref> is in fact a batch operation of multiple write requests from clients to the distributed file system. In such a case, it is difficult to assign a path id to the events inside the call (e.g., event h can only share with the path id from e or f , but not both). Two paths might also be correlated when they access the same shared variables. In fact, a more general graph is already used to some extent in previous work such as Pip <ref type="bibr" target="#b23">[26]</ref>. G 2 instead explores a different point in the design space. Rather than constraining users to a path-based model a priori, G 2 preserves and presents the full structure captured during the execution of a distributed system as a graph. During diagnosis, users can choose to construct paths from such a graph if paths are appropriate for the diagnosis task at hand, or they can choose to process information in a different way that is more appropriate for that particular task. G 2 does not make that decision for users during the modeling phase. This design choice effectively shifts the burden to the underlying distributed engine, as it must enable efficient operations on a more complicated graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Execution Graph</head><p>G 2 's execution graph model embraces two key concepts: causality and aggregation. This is based on our observation of common system diagnosis practices: users tend to (i) follow cause-effect relations to find relevant information and (ii) to summarize runtime information at an appropriate aggregation level in a hierarchy in order to find trouble spots for further in-depth analysis.</p><p>In an execution graph of G 2 , each runtime event from a target system is represented as a vertex. In <ref type="figure">Figure 1</ref>, events are shown in small rectangles; examples are printf log event b (line 3), asynchronous request define events c and d, and request use event e. A context is associated with an event, indicating the aggregation units that the event belongs to. Multiple levels of aggregation units can be defined. Examples include static constructs, such as modules, classes, and functions, as well as runtime constructs, such as machines, processes, and threads.</p><p>Runtime events are correlated, where directed edges in an execution graph are used to represent such correlations. Different types of edges can be defined for different types of correlations. For example, an use edge connects a source event that defines/forwards an object with a destination event that consumes that object. Network messages or cross-thread requests are examples of such objects. c, e and d, e in <ref type="figure">Figure 1</ref> are use edges. A sync edge indicates synchronization of two events from two different threads in order to ensure exclusive access to a shared object or ensure ordered inter-thread execution. A fall-through edge connects two consecutive events in the same thread (e.g., b, c). G 2 provides primitives to define and customize graph traversal for diagnosis. Two are built-in: Slicing finds all causally related events in a graph and HierarchicalAggregate summarizes information at an appropriate aggregation level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Filter with Slicing</head><p>Instead of simply "grepping" runtime information with a special tag, Slicing filters information using graph structure: it starts from a root event and transitively collects causally dependent events. Forward and backward traversal yield a forward slice and a backward slice, respectively.</p><p>Computing precise and complete causal dependencies for slicing is usually too costly if not infeasible, where a reasonable approximation is often sufficient in practice. A naive way is to consider all use and fallthrough edges as causal edges. Our practical experience has shown that fall-through edges often do not imply causal relations. For example, in a typical implementation of message processing subsystem, a thread will continuously accept new incoming messages and call corresponding message handlers. Fall-through edges between two message handler invocations do not represent any meaningful causal dependencies. Such false causal dependencies could render slicing ineffective. All events in the corresponding message handler should however be considered causally dependent on the message-send event. G 2 introduces causal scope to specify, for each use edge, the set of events that are causally dependent on the source event of that edge. A causal scope consists of a continuous region from the destination event of each use edge: all events within that region are causally dependent on the source event; all fall-though edges within that region are considered causal edges. In <ref type="figure">Figure 1</ref>, large rectangle boxes define causal scopes. The shaded area outlines the forward slice from event a.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Summarize with HierarchicalAggregate</head><p>Aggregation is another effective way of managing a large amount of data, especially with a hierarchy. There are natural hierarchies in distributed systems: a program is  <ref type="figure">Figure 1</ref>: System execution graph, causal scope, and slice.</p><p>often made of modules, each module is comprised of classes, and each class contains a set of functions. A distributed-system execution can be aggregated at thread level, then at process level, and further at machine level. A distributed system often consists of multiple logical layers that are application-specific: for example, a system behavior can be analyzed at an RPC layer or at a lower OS layer with a socket interface.</p><p>G 2 supports an important notion called hierarchical aggregation. The key idea is to construct a condensed graph at an appropriate layer of a hierarchy to summarize system behavior. A continuous segment of events with the same aggregation unit in an execution graph is summarized and condensed into a single higher-level vertex in the resulting graph. G 2 by default attaches signatures of code and runtime location to all events for aggregation. Aggregation in G 2 is customizable: a user can leverage her domain knowledge to specify how to aggregate events and summarize high-level information (e.g., aggregated performance counters) from low-level events. <ref type="figure" target="#fig_1">Figure 2</ref> shows an example of event aggregation when debugging replication in the distributed storage for SCOPE. Numbers inside rectangles are total eventcounts within corresponding vertices in the aggregated graphs. An error occurs during a replicated write operation. The upper part of <ref type="figure" target="#fig_1">Figure 2</ref> performs event aggregation at machine level: it clearly shows whether the write operation was propagated to all replicas. Once a suspected machine is identified, a user selects that machine and zooms in to see how the write request was processed by each component in this machine, shown in the lower part of <ref type="figure" target="#fig_1">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROGRAMMING IN G 2</head><p>Programming in G 2 consists of two parts. One is to "program" distributed systems so as to to make them diagnosable by G 2 . We defer this to Section 5. The other is for "programming" queries to be executed on G 2 , which is the focus of this section. ReplicateWrite <ref type="formula">(149)</ref> SerializedIOWrite <ref type="formula">(17)</ref> WriteRequestFailed <ref type="formula">(24)</ref> Primary <ref type="formula">(</ref>    <ref type="figure" target="#fig_2">Figure 3</ref> shows the basic graph operators. Each Vertex contains its incoming and outgoing edge lists, and it is a generic type that can be instantiated with TV, TE. Type TV describes the data associated with the vertex, such as logs, code locations, and runtime locations, while type TE describes the data associated with each edge, such as timestamps for the source and the destination events. A generic Graph type can be further defined as a collection of vertices. As shown in the figure, operator Slicing takes srcVertex as the root event and type as the direction (forward or backward) for slicing. Operator HierarchicalAggregate condenses a graph to a higherlevel graph (line 5). Given a vertex in the original graph,   the labelCb callback returns its aggregation-unit label. The AggreFunc callback aggregates a continuous sequence of vertices with the same label (line 9) into a new vertex at the high-level graph (line 10) (Note the structure is determined by G 2 , and the associated value(THighV) is defined by the callback).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Operators</head><p>All those operators are built on top of two distributed primitives: GraphTraversal and MapReduce. GraphTraversal starts with a set of vertices in a graph and traverses the graph by following edges forward, backward, or bi-directionally. A user can customize graph traversal by deriving a graph traversal class, which defines computations on vertices, messages passed along edges, as well as final output during graph traversal. MapReduce is standard with a map function and a reduce function for aggregation. Details of these distributed primitives and how they are used to build graph operators are left to Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Composing Graph Operators</head><p>Extensibility and composability are two key features of G 2 design for programming. Slicing and HierarchicalAggregate both consume a graph and produce another, so they can be composed. We further leverage the extensibility of the LINQ framework <ref type="bibr" target="#b1">[3]</ref> in .Net, so that developers can write diagnosis queries using our new operators, LINQ's relational operators, and even customized local analysis modules, such as finding critical path (CriticalPath) and comparing two aggregated graphs (Diff ). <ref type="figure" target="#fig_4">Figure 4</ref> shows a set of examples; all from real diagnosis practice.</p><p>The first query returns the logs in a backward slice rooted from an error log event. The query first uses Where in LINQ to locate the error event and then invokes Slicing. The second query aims to find straggler machines during processing of a request. The query first calculates the forward slice from the point of request submission, aggregates the slice into a machine-level graph via HierarchicalAggregate, and computes the critical path for request processing. Each vertex in the returned critical path summarizes a continuous execution on a machine with the start and stop times of the execution, from which stragglers can be easily identified. The last query intends to find components responsible for an instance of slower-than-normal request processing. It extracts forward slices rooted from the slow request and normal ones, aggregates at the component level, and outputs differences. If needed, users could drill down into problematic components and investigate further at the function level or lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISTRIBUTED ENGINE</head><p>A distributed engine is responsible for transforming diagnosis queries into distributed jobs to be executed on the set of machines storing the execution graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>In G 2 , events and correlations between events are captured and recorded locally, and transformed into appropriate graph representations. G 2 therefore naturally partitions original system execution graphs based on where events occur. Such a partitioning method tends to exhibit good locality as distributed systems are usually designed to minimize cross-machine traffic.</p><p>A job manager initiates a job when a query is submitted. Each machine storing graph partitions runs a daemon. The job manager coordinates executions of phases by communicating with these daemons. A job involves multiple phases that can be represented as a data flow graph. <ref type="figure" target="#fig_5">Figure 5</ref> shows the data flow graph for the machine-level critical-path analysis query in <ref type="bibr">Fig- ure 4 (b)</ref>. It consists of 5 phases: Where, Slicing, HierarchicalAggregate, Merge, and CriticalPath. The distributed engine takes care of the first 4 phases and sends the aggregated results to clients for local critical-path analysis. The Merge phase does not appear in the original query and is added automatically during compilation. Both Slicing and HierarchicalAggregate involve graph traversal, where the latter consumes the graph created by the former and outputs an aggregated graph for client analysis. In particular, the mappers during HierarchicalAggregate shuffle the vertices according to which high level vertex they belong to, and the reducers aggregate the vertices inside one high level vertex using the AggreFunc callback provided by the queries.</p><p>The part of the data flow graph without graph traversal is similar to directed acyclic graphs (DAG) in previous data-parallel computation engines, such as Map/Reduce and Dryad. Graph traversal however requires a different type of coordination to support loops and barriers. G 2 's graph traversal support distinguishes itself from previous graph engines (e.g., Pregel <ref type="bibr" target="#b21">[24]</ref>) in several noticeable ways. First, for operations such as slicing and hierarchical aggregation, G 2 supports batched asynchronous iterations, where partitions batch operations locally, but do not have to be synchronized using a barrier in each iteration. Second, G 2 exposes a partitionlevel interface, rather than a vertex-level interface, to allow better batching and aggregation for graph computation. This is particularly important for enabling efficient implementation of hierarchical aggregation. These optimizations can be applied not only to G 2 but also to other distributed graph traversal problems such as shortest path computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Batched Asynchronous Iterations</head><p>A typical graph engine implements synchronous iterations through loops and barriers. For graph computation such as page-rank computation and belief propagation, all participants must synchronize with each other in each iteration via a barrier, and in each iteration the participants can only traverse one hop. Such synchronization is easily done with the help of a job manager.</p><p>In G 2 , we observe that graph traversal for slicing and hierarchical aggregation is inherently asynchronous. Take forward slicing for example, each partition has a set of vertices to start with in each iteration (except the first one where only one partition has the root vertex). For one local iteration, a partition starts graph exploration from those vertices following causal edges until it reaches cross-partition edges without synchronization   G 2 does support global barriers for two cases. In the first case, completion of a graph-traversal stage is through a global barrier: all participants must have completed their last iteration locally. The job manager initiates the next phase of computation only after that global barrier is established. In the second case, the job manager can periodically introduce a barrier to an ongoing graphtraversal stage for failure recovery; the barrier is used to perform a globally consistent snapshot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Partition vs. Vertex</head><p>G 2 provides a GraphTraversal interface so that users can implement their own custom graph-traversal algorithms. Previous graph processing systems such as Pregel allow users to specify actions on each vertex, which is natural for a large number of graph computation algorithms. However, we have found a partition-level interface offers additional opportunities for better performance.</p><p>Graph Traversal Interface. <ref type="figure" target="#fig_7">Figure 6 (a)</ref> shows the signature of GraphTraversal. It starts from a set of initial vertices (line 3), with traversal polices designated by TWorker derived from GPartitionWorker (line 4). When a graph traversal phase starts, G 2 creates an instance of GPartitionWorker on every graph partition of g (line 2), and the job manager coordinates the workers to perform multiple iterations of computation: a first round for Initialize (line 9), followed by multiple rounds of graph traversal via message exchanges among vertices (line 10) until all workers reach the completion barrier, and a last round for Finalize (line 11). In each round, a worker creates remote messages for other partitions. Those remote messages are eventually transported to appropriate partitions and serve as the input for next-round computation on those partitions. Forward Slicing. <ref type="figure" target="#fig_7">Figure 6</ref> (b) shows a sample that implements forward slicing. During Initialize, the worker sends a message to the initial vertices of the graph traversal via SendMessage (lines 5,6). After initialization, each worker invokes OnMessage (line 8) on each message, inside which a worker can read/write partitionlocal states (lines 9,10), produces partial outputs via WriteOutput (line 11), and send messages to other vertices via SendMessage (line 14) by following the edges of the current vertex (line 12). OnMessage does not cause a real network message to be sent: for a local destination, the worker again applies OnMessage on the destination vertex in the current round. Only messages destined to a remote partition are gathered and made available to other partitions at the end of this iteration. After a worker completes the current round, it fetches the available remote messages for its partition from other partitions, and starts a new round. This process ends when all workers have completed the current rounds with no new remote messages. In the final round, the workers invoke Finalize (line 16), which usually produces outputs of this traversal phase from final local states. It is empty in this case because output is generated during traversal (line 11). Hierarchical Aggregation. The value of exposing a partition-oriented interface is more evident in the implementation of HierarchicalAggregate. <ref type="figure" target="#fig_9">Figure 7</ref> (a) shows a simple vertex-oriented implementation from a vertex's perspective. Each vertex has a label based on its context; for example, the label is its process id for process-level aggregation. We use AggId to identify a set of vertices that have already been aggregated together: those vertices will have the same value v.AggId. Every vertex uses its own ID as the initial AggId (line 3), and broadcasts both its label and AggId to its neighbors (lines 4,5   its own AggId and propagates the change to its neighbors (lines 9-12). The traversal ends when all vertices are assigned the smallest label of the vertices to be aggregated together. <ref type="figure" target="#fig_9">Figure 7</ref> (b) shows a partition-oriented implementation, where partition-level aggregated states are maintained (lines 1-3) and initialized (lines 5-6). These data structures essentially aggregate local continuous segments with the same labels and update them as a single unit during traversal, rather than going through each vertex repeatedly: each local continuous segment with the same label is assigned a leader. Rather than having each vertex maintaining an aggId, the partition maintains a mapping from leaders to aggIds in LeaderAggIds. Because vertices with the same leader always have the same aggId, a partition can simply update one entry in LeaderAggIds for all those vertices when the aggId changes for any of the vertices. Similarly, destination vertices of cross-partition edges from this segment of vertices are recorded in RemoteVertexGroup and can be identified without following the edges within this segment repeatedly. Those data structures are populated during initialization. When a message arrives at a partition with a boundary vertex as the destination vertex, the worker checks its label (line 9), and updates the AggId of the corresponding leader vertex if it receives a smaller AggId (lines <ref type="bibr" target="#b8">[10]</ref><ref type="bibr" target="#b9">[11]</ref><ref type="bibr" target="#b10">[12]</ref><ref type="bibr" target="#b11">[13]</ref> to its cross-partition neighbors (lines 14-15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Failure Handling</head><p>G 2 supports iterative graph computation, which renders inapplicable the map/reduce type of failure recovery using re-computation. In a synchronous graph computation model, where a global barrier is established at each round, a globally consistent checkpoint can be taken at each barrier. When failures happen, computation can be rolled back to the most recent checkpoint. G 2 allows each partition to maintain states. A checkpoint therefore covers such per-partition state, as well as the remote messages that each partition generates at the end of a round for other partitions. Appropriate levels of redundancies might be needed for checkpoints in order to recover from permanent machine failures. With an asynchronous graph computation model, G 2 can choose to insert a global barrier at an appropriate interval for consistent checkpointing. We can also resort to the standard Chandy-Lamport algorithm for taking a consistent snapshot, where a global barrier is a special simple (yet less efficient) implementation for this algorithm. In the worst case, G 2 can always roll back to reexecute a graph-traversal phase (assuming that failures do not lead to data loss in the original graph information). Our current implementation uses global barriers for consistent checkpointing, but do not replicate the checkpoint to tolerate permanent failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">IMPLEMENTATION</head><p>G 2 provides a complete tool set to help developers diagnose systems. <ref type="table">Table 1</ref> shows the programming language and lines of code for components of G 2 : annotation library and binary rewriter are used to capture system execution graphs. Transformer is used to store a graph. Engine, Front-End, and Visual Studio AddIn are for processing and visualizing a graph. Capture Graph. G 2 can use existing traces from previous work, e.g., those on path-based analysis, to build execution graphs. It also provides its own tool chain for developers to instrument target systems for gathering information of interest. Whether instrumentation requires manual code change depends on the types of edges to be captured. We have developed a Phoenix <ref type="bibr" target="#b3">[5]</ref> based binary rewriter tool to annotate synchronous use edges (i.e., call) and their corresponding causal scopes (i.e., the call boundary) automatically. A user can choose what to instrument with a configuration file, reflecting her choice to balance between cost and coverage. G 2 also captures sync edges automatically at the Win32 layer by instrumenting Windows synchronization APIs. For asynchronous use edges, G 2 provides an annotation library; the following code illustrates how to track network messages and their corresponding handlers using this library. Users first annotate a context(NetworkMsg) by making it inherit a G2::CausalCtx object. Users then add a call to LogUseEdgeBegin and LogCausalScope respectively, when this context is about to be delivered and used. The macro call of LogCausalScope is a C++ object, whose lifetime defines a causal scope. Store Graph. We developed a Transformer that converts raw runtime-event streams to database tables. G 2 stores a system execution graph in four relational tables. Each object has a unique key, which we use as reference keys across tables and partitions. The CodeLocation provides the context for each event and in particular the component, class, function, file, and line number of the statement that generates the event. The ProcessInfo table covers the runtime process information, such as process id, machine name, process start time, and so on. The Event table contains information about each event, including its type, a reference to an edge if it is an endpoint of that edge, a physical timestamp, references to CodeLocation and ProcessInfo, and payload (e.g., printf log content). The Edge table contains the edge type, a unique edge ID, and references to the source and destination events. The Edge table defines the structure plane of a system execution graph, while the other three form the data plane. A slicing operation can be done purely on the Edge table, but for event aggregation it is often necessary to query the CodeLocation and ProcessInfo tables. The Edge table is frequently accessed during graph traversal and is therefore cached in memory for fast access. Process Graph. Queries submitted to G 2 are compiled into a distributed query plan, with appropriate resource files dispatched to workers running on machines managing partitions of a graph. Execution of a query plan is done through coordination between the job manager and the workers, as described in Section 4. The job manager  monitors progress of graph traversal and assists in message exchanges between workers. After a round of local processing ends on a partition A, the worker for A groups messages based on their destinations and notifies the job manager of the list of partitions with data from A. The job manager piggybacks the list partitions with data ready for A. The worker for A will then fetch those from the corresponding workers. To make message exchange efficient, workers cache generated message groups in memory and discard them after they are fetched. The job manager is also responsible for enforcing global barriers upon completion of graph traversal, as well as to create consistent checkpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS AND EXPERIENCE</head><p>We have applied G 2 to SCOPE/Dryad, G 2 itself, and BerkeleyDB. Our evaluation attempts to answer the following questions: a) what is the cost of applying G 2 ? b) how does the G 2 engine perform on real execution graphs? c) does G 2 help developers diagnose complicated distributed system problems? Target systems are co-deployed with G 2 on a cluster of 60 machines; each has a dual 2GHz Intel Xeon CPU, 8 GB memory, two 1TB SATA disks, and are connected with 1 Gb Ethernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Cost of Applying G 2</head><p>Human effort. <ref type="table" target="#tab_7">Table 2</ref> reports the statistics about the annotation effort to apply G 2 on these systems. For instrumenting functions, users write only a configuration file for the binary rewriter to specify names of functions they are interested in. For all three cases, the configuration files are less than 25 lines, as shown in <ref type="table" target="#tab_7">Table 2</ref>.</p><p>The asynchronous use edges and correspondent causal scopes require manual annotation on source code. Our experiences show that most asynchronous messages and events are handled by a small number of components or by a middleware library, making annotation easy. We annotated fewer than 20 places for each benchmark. Annotations on G 2 took us less than one hour. Interestingly, in our SCOPE/Dryad experiment, we did forget to annotate a place where the code directly uses the CreateProcess function to create a new process, bypassing the middleware component that we annotated. Such cases are rare and can often be discovered during a diagnosis process. Developers can optionally capture other dependencies: we do not model those. Runtime overhead. Runtime overhead for emitting events and edges is comparable to those in previous work on capturing causal dependencies <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b23">26,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b24">27]</ref>. There are several categories of events/edges. The first category are asynchronous use edges and the corresponding causal-scope events (e.g., message send/receive), which are always captured. The second are legacy printf logs. These two parts do not introduce noticeable system slowdown compared to previous systems (with the same printf logs). The third are events from instrumented functions, and the cost is proportional to the numbers of function invocations that are captured. Usually invocations of interface functions of each component that are associated with error and failure handling are enough for diagnosis. For our experiments on the three distributed systems, only less than 0.1% of overall function invocations are captured in system execution graphs, and no noticeable overhead was observed. If more functions need to be instrumented, and we cannot afford to instrument all, we can turn to dynamic instrumentation techniques as done in our previous work <ref type="bibr" target="#b20">[23]</ref>. <ref type="table" target="#tab_9">Table 3</ref> reports the statistics on sample execution graphs from our target systems. The G 2 data include events from executing tens of diagnosis queries against a SCOPE/Dryad snapshot. The SCOPE/Dryad data came from ten SCOPE queries for calculating different statistics of web data. The BerkeleyDB data was collected during approximately one hundred instances of system initialization guided by a model checker; the goal is to use G 2 to assist model checking research in another project. All numbers reported are per-machine averages. For example, for SCOPE/Dryad, a 120-minute trace generates about 1.2GB of G 2 data on each of the 60 machines. On average, the imposed I/O bandwidth ranges from 85.3 KB/s (G 2 ) to 174 KB/s (SCOPE/Dryad) on average, which did not cause noticeable runtime interference to the host systems. Not reported in this figure, in our sample SCOPE/Dryad execution graph, about 28% of the events recorded are legacy logs (category 2), which account for 64% of total sizes. Category 1 accounts for 33% by count and 16% by size, while category 3 takes the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance Evaluation</head><p>This section evaluates the performance of G 2 engine. Graph statistics. <ref type="table" target="#tab_9">Table 3</ref> shows the numbers of edges and vertices for the sample execution graphs from our target systems. The number of edges is far fewer than the number of events because all fall-through edges are implicit. The func# is the number of invocations for instru-  mented functions as specified in <ref type="table" target="#tab_7">Table 2</ref>. System execution graphs can be large: the SCOPE/Dryad snapshot has on average more than 20 million events on each machine. The database size (DB) is approximately three times the raw event stream size (Raw), due to verbose DB data format (factor of 1.5) and associated indices. The edge table (including its indices) counts for only 30% of the total database size. Because it is frequently accessed, caching it in memory makes sense.</p><p>End to end performance. We evaluate the end to end performance of G 2 with 5770 random queries on the SCOPE/Dryad graph. Each query calculates a forward slice from a randomly selected root event and then computes a process-level aggregation on the slice. <ref type="figure">Figure 8</ref> shows the overall running time of these queries with all optimizations turned-on. The G 2 engine is generally fast on these queries: 94.5% queries finished within 5 seconds, with only three queries (in the upper-right circle) taking more than 100 seconds. Our investigation shows that the randomly chosen root events for those three queries are close to the entry point of the Dryad job, yielding huge slices with up to 130 millions of events. Running time depends not only on the sizes of resulting slices, but also on properties of the graph (and its partitions), those properties dictating concurrency of query processing. For example, some queries (in the circle on the left) take more than 50 seconds, even though the corresponding slices are relatively small. This is because they experience a period of time with low concurrency.</p><p>We also inspected the result of hierarchical aggregation. The result shows that it is effective in simplifying graphs. All resulting process-level graphs contain less than 85 vertices, except the three queries in the upper right circle: the aggregation yields graphs with only 0.01% of vertices.</p><p>Graph and graph computation characteristics. We recorded and examined a large forward-slice computation on the SCOPE/Dryad graph. <ref type="figure" target="#fig_12">Figure 9</ref> shows how the numbers of events, local edges, and remote edges vary over time. The SCOPE/Dryad job has a bootstrap phase, during which a job scheduler copies resource files between machines. In the actual execution, this phase takes little time. However, because this phase involves a series of communication, slicing at this segment of the execution graph has little concurrency. It takes a relatively long time to process even though the number of events and the  total I/O are small. This is reflected in the flat start in the figure. Graph traversal experiences respectable concurrency in the middle range when traversing the portion of the graph for the real Dryad execution. Then after around time 00:40 it starts to process the portion of the graph corresponding to the final phase, in which a job manager again has to talk to many machines to fetch statistics and to write them into a distributed file. The overall concurrency level on 60 machines is 7.23. Effectiveness of graph engine optimizations. To evaluate the effectiveness of batched asynchronous iteration and partition-oriented interface design, we measured both Slicing and HierarchicalAggregate performance with nine different configurations, which are the combinations of two dimensions of configurations. The first is whether to enable Barriers among workers and Checkpointing after each round (None, B, or B/C). The second is to choose which local graph traversal policy <ref type="bibr">(OneHop, Batched, or Partition)</ref>. OneHop is to allow one hop traversal only in each round, a typical setting for other graph engines such as Pregel; Batched is to tra-  verse until no further local vertices to be visited during this round; and Partition is similar to the second, but with the partition-state optimization discussed in Section 4.3, enabled by G 2 's partition-oriented interface. <ref type="table" target="#tab_11">Ta- ble 4</ref> shows the average relative execution time for the component level aggregation analysis for a Dryad job with the nine configurations; each runs ten times. To be fair, with OneHop, B/C, we checkpoint every 7 and 18 rounds for slicing and aggregation, respectively, so that they take approximately the same number of checkpoints as in other configurations. Overall, batched asynchronous iterations and partition-oriented interface are effective: without checkpointing, we see a 62-63% reduction in latency for both slicing (2.70 vs. 1) and hierarchical aggregation (2.67 vs. 1). The data also reveal the following: (i) Batched asynchronous iterations bring benefits in two ways: First, it allows local traversal to proceed (as in Batched) and significantly reduces the number of global rounds (from 208 rounds to 28 rounds for Slicing and from 111 rounds to 6 rounds for hierarchical aggregation). Second, it removes the need for global barriers. This is particularly effective when there are many rounds and significant variations across machines in each round. It is noticeably ineffective for Aggregation (Partition, B, Partition, None) because our partition-oriented optimization makes process-time variations between partitions negligible. (ii) Partition-oriented interface and data structures are effective for Aggregation (with 32% reduction) because we are seeing large local islands (e.g., one island with 7.7 million internal edges and only 2,895 remote edges): those local islands do not have to be visited repeatedly with our optimization. (iii) Overhead of checkpointing depends on how frequent we checkpoint and how much data we checkpoint. OneHop introduces lower overhead (5.11 MB/s) because it checkpoints the same amount of state in a longer time period compared to Batched (7.18 MB/s), and Batched has higher overhead compared to Partition because its state size is larger than that under Partition (5.96 MB/s).</p><p>Scaling performance. We evaluated scaling performance from two perspectives. The first is to measure scaling in terms of the number of machines. We do not show figures due to space constraints. We observe that the job latency decreases almost linearly initially when more machines are used. But after we have more than 16 machines, the speedup slows down due to inherent limit on concurrency. With 60 machines, the average latency is reduced to under 3 minutes from over 14 minutes on 8 machines.</p><p>The second is to measure scaling in terms of the number of concurrent queries. We use two different sets of slices for those queries. The first set has several large slices, which involves 5 to 8 graph partitions and contains approximately 0.4 to 1 million events. The second set has a set of randomly selected slices, which typically involves 1 to 2 machines and contains several thousands of events. For large slices, latency increases dramatically when we reach 100 queries. For small slices, the system can support almost 500 queries simultaneously without affecting query latencies and can handle 5,000 queries with an average latency under 3 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Experience, Limitations and Future Work</head><p>To make G 2 accessible to developers, we have built a set of templates to guide the use of the system and integrated the tool into Visual Studio for a seamless debugging experience. The Visual Studio AddIn includes a set of common diagnosis tools based on G 2 , including event navigation along edges in all levels of graphs (called gwalker), as well as a set of wizards focusing on specific visualized diagnosis tasks, such as error log analysis, critical path analysis, and performance regression analysis, as discussed in Section 3.2. Following are two showcases from our experiences. Slower job. When developing G 2 , we found that query processing time became 60 times slower after minor code changes. To investigate, we applied G 2 for a machinelevel aggregation with critical-path analysis on a forward slice of a query job. The result showed that most of the processing time was spent on machine srgsi-10. We then zoomed into a thread-level execution graph, and performed a graph diff with its counterpart from the same run in the previous day (before code changes were applied). <ref type="figure" target="#fig_13">Figure 10</ref> depicts the diff result, which shows that thread 8772 has the largest deviation between the two jobs in terms of execution time. We further zoomed into the component level and then function level execution graphs in that thread, and found that the largest deviation happened in the AcquireRowById function which reads a row from a local table using the primary key. Our further investigation revealed that the table did not have a proper index, the result of a bug we introduced in Transformer that caused index creation to fail. This kind of perfor- mance regression problem is common in our experiences and a hierarchical diff analysis between two similar tasks is often effective in identifying root causes. Failed write. We have used G 2 to investigate root causes of error logs in SCOPE/Dryad. One case is shown in <ref type="figure">Fig- ure 11</ref>. The error log in the underlying distributed storage system reported that a write request to a chunk server c1 at time 21:27 failed. Using G 2 , we managed to find its root cause through several steps, marked in <ref type="figure">Figure 11</ref>.</p><p>1. We computed a backward slice starting from the error log entry and listed the warning and error entries ordered by time (OrderBy). A warning log entry showed up in the disk IO module, indicating that a chunk x was marked as deleted. However, we were not able to figure out why this happened based on the information in this backward slice.</p><p>2. We wrote a Where query for the most recent logs on the same machine who contain keyword "chunk x", trying to connect the missing edges which may tell why chunk x was marked as deleted. The query returned an event B indicating that at time 21:17 a background thread marked this chunk as deleted.</p><p>3. We use gwalker to navigate the logs on the graph from event B, and found an event C indicating that the meta server m1 sent a delete request to c1 because it found that chunk x no longer belonged to any file stream.</p><p>4. To locate the origin of the write request to c1 on presumably deleted chunk x, we aggregated the backward slice rooted at event A at process level and found the write request came from c3, and propagated by c2, where c1, c2, and c3 formed a replication group for x. A further drill-down of the logs on c3 showed that c3 restarted at 21:27. During its replication log replay, it found an incomplete local chunk x and issued an empty write request to sync data from other replicas (c1 and c2).</p><p>5. Trying to understand why c3 did not receive the delete request from meta server for chunk x, we ran a process level aggregation on the forward slice from event C, and found that m1 sent a delete request to c3 at time 21:25, but c3 was not online at that time. This revealed the root cause.</p><p>This interactive diagnosis process involved gwalker, slicing (at a specific layer), aggregation, and relational queries in G 2 , and is guided with human expert knowledge. It is worth pointing out that G 2 , as any diagnosis tool, is not intended to replace human completely. Rather, its value lies in its ability to allow users to find the right information efficiently.</p><p>Implicit dependencies. The backward slice in step 1 of the Failed Write diagnosis did not contain all the interesting events for us to find the root cause, due to implicit dependencies (through chunk x), which are not captured by G 2 . This is a common limitation in causality-based approaches. We managed to connect the dots through a relational query in this case. In our performance diagnosis experience, we also found a lot of problems caused by resource contention or interference, and again such causal relations are not modeled by G 2 . In the future, we plan to incorporate some interference analysis techniques (e.g., <ref type="bibr" target="#b22">[25]</ref>) to introduce interference edges into our model.</p><p>Customized slicing computation. We found some slices were fairly large and took a long time to compute. In many cases, users do not need all the information in a slice. To better control the cost, G 2 provides three additional parameters to the Slicing operator: maximum slice radius (hops starting from the root event), maximum network hops, and a customizable edge filter which decides whether the computation should continue following this edge for a bigger slice. Our experience shows these parameters can greatly improve the productivity, especially when people are familiar with their target systems.</p><p>Deployment and interference. The data placement has three reasonable choices as we see: data on each machine, on one dedicated machine per pod (in which the machines share a same uplink), and on a single machine. We did not run the second option (in a real data center) yet, and our scalability study above touched on diagnosis performance vs. number of machines. Our belief is that the second option is more suitable for a real deployment to minimize interference, while the other two can be used in testing environments depending on the size of the setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta Server: m1</head><p>Chunk Server: c1 Chunk Server: c2 Chunk Server: c3 Cloud9 <ref type="bibr" target="#b8">[10]</ref> has pioneered the concept of testing as a service. In particular, it shows a symbolic execution testing engine that can be parallelized in a cloud. We share the same vision and believe G 2 can enable diagnosis as a service. Wei et al. <ref type="bibr" target="#b26">[29]</ref> parallelized their algorithm for learning legacy logs on Amazon EC2 with Hadoop <ref type="bibr">[21]</ref>.</p><p>Dapper <ref type="bibr" target="#b24">[27]</ref> is a tracing framework designed for low overhead, application transparency, and ubiquitous deployment. Trace data are organized in a Dapper trace tree, where each node represents a basic unit of work called a span. Each trace is stored in BigTable. It also offers a programmatic API, as well as an annotation API. Due to its more restricted trace-tree model, it does not support graph-traversal or any of the operators in G 2 . DTrace <ref type="bibr" target="#b7">[9]</ref> is another tracing framework that supports on-demand instrumentation of distributed systems. It allows customized predicates and aggregation functions via a scripting language. The aggregation is applied to a set of flat trace records, which is different from G 2 as the later applies aggregation to a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUDING REMARKS</head><p>Execution graphs capture runtime behavior of distributed system executions. These graphs are unique in their value for distributed-system diagnosis and in their distinctly different characteristics compared to well-known social and web graphs. G 2 makes those graphs useful with new graph operators and with query support, and makes graph processing efficient with a distributed engine. By doing so, G 2 becomes an effective tool for distributed-system diagnosis and at the same time advances the state of art in distributed large-scale graph processing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical aggregation for a replication implementation. Numbers in rectangles show numbers of events within vertices in the aggregated graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Graph operators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>c) Component level performance regression analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sample diagnosis queries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Data flow for the machine-level critical-path analysis query in Figure 4 (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>8</head><label></label><figDesc>void WriteOutput(T val); 9 virtual void Initialize(VertexIterator&lt;TV, TE&gt;)=0; 10 virtual void OnMessage(Vertex&lt;TV, TE&gt;, TMsg) = 0; 11 virtual void Finalize() = 0; 12 }; (a) GraphTraversal interface. 1 class GPartitionSlicingWorker&lt;TV, TE&gt; 2 : GPartitionWorker&lt;TV, TE, bool, Vertex&lt;TV, TE&gt;&gt; { 3 HashSet&lt;ID&gt; VisitedVertices; 4 void Initialize(VertexIterator&lt;TV, TE&gt; inits) { 5 foreach (var v in inits) 6 SendMessage(v.ID, true); 7 } 8 void OnMessage(Vertex&lt;TV, TE&gt; v, bool msg) { 9 if (VisitedVertices.Contains(v.ID)) return; 10 VisitedVertices.Add(v.ID); 11 WriteOutput(v); 12 foreach(var e in v.OutEdgeIterator) 13 if (e.IsCausal()) 14 SendMessage(e.DstVertexID, true); 15 } 16 void Finalize() {} 17 } (b) GPartitionSlicingWorker for forward slicing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: GraphTraversal interface and example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Optimization for HierarchicalAggregate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head># of Remote Edges (Thousands) # of Local Edges /</head><label></label><figDesc>Figure 8: Process level aggregation performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: How the # of events, local edges, and remote edges vary along the execution time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Thread level performance regression diff.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>b c PersistMainThread OnReceiveClientRequest OnPersistRequests Fall-through edge</head><label></label><figDesc></figDesc><table>Event 

Use edge 

Causal scope 

Slice 
time 

thread 1 

thread 2 

thread 3 

OnReceiveClientRequest 

e 

1 class ClientReq { 
2 void OnRecieveClientRequest(...) { 
3 
Log(LOG_INFO, "..."); 
4 
IssuePersistRequest(...); } 
5 int PersistMainThread() { 
6 
while (IsEnoughRequest(reqs)) 
7 
OnPersistRequests(reqs); 
8 
…} 
9 int OnPersistRequests(list&lt;ClientReq*&gt; reqs) { 
10 
MemBuf* buf = CreateBuf(reqs); 
11 
WriteToLocalDisk(buf); 
12 
ForwardToSecondary(buf, ...); } 

. . . 
. . . 

a 
d 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>where TWorker : GPartitionWorker&lt;TV, TE, _, T&gt;; 5 class GPartitionWorker&lt;TV, TE, TMsg, T&gt; { 6 Vertex&lt;TV, TE&gt; GetLocalVertex(ID VertexID); 7 void SendMessage(ID VertexID, TMsg msg);</head><label></label><figDesc></figDesc><table>1 IQueryable&lt;T&gt; GraphTraversal&lt;TWorker&gt; ( 

2 

this Graph&lt;TV, TE&gt; 
g, 

3 

IQueryable&lt;Vertex&lt;TV, TE&gt;&gt; startVertices 
4 ) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>). A message is ignored when a re- ceiving vertex has a different label (line 8), indicating a boundary of aggregation. Otherwise, if an incoming AggId is smaller than the current one, a vertex changes</figDesc><table>1 void Initialize(VertexIterator inits) { 

2 

foreach (Vertex v in inits) { 

3 

v.AggId = v.ID; 

4 

foreach (Vertex iv in neighbour vertices) 

5 

SendMessage(iv, {v.ID, v.Label}); 

6 

} ... 
7 void OnMessage(Vertex v, MSG msg) { 

8 

if (msg.Label != v.Label) return; 

9 

if (msg.AggId &lt; v.AggId) { 

10 

v.AggId = msg.AggId; 

11 

foreach (var e in connected edges) 

12 

SendMessage(e.DstVertexID, msg); 

13 

} ... 

(a) Vertex oriented implementation. 

1 Map&lt;ID, ID&gt; 
VertexLeader; // vertex-&gt;leader 
2 Map&lt;ID, ID&gt; 
LeaderAggIds; // leader -&gt; aggId 
3 Map&lt;ID, ID[]&gt; RemoteVertexGroup;//leader-&gt;rvertices 
4 void Initialize(VertexIterator inits) { 

5 

... local aggregation to initialize the maps ... 

6 

... send messages to remote vertices ... 
7 } 
8 void OnMessage(Vertex v, MSG msg) { 

9 

if (msg.Label != v.Label) return; 

10 

ID leaderId = VertexLeader[v.ID]; 

11 

int oldAggId = LeaderAggIds[leaderId]; 

12 

if (msg.AggId &lt; oldAggId) { 

13 

... update aggId for the group ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>). Finally, it broadcasts the new AggId Component Name</head><label></label><figDesc></figDesc><table>Language LOC(K) 
Annotation Library 
C++, C 
3.4 
Binary Rewriter 
C++/CLI 
1.6 
Transformer 
C++ 
1.5 
Engine(JobMgr,Daemon,MetaSvr) C++ 
27.5 
FrontEnd(Compiler, JobClient) 
C# 
17.3 
VS AddIn(Wizards, UI) 
C# 
57.8 
Total 
-
109.1 

Table 1: Components in G 2 . 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 2 : Instrumentation statistics. Acs#, Ace#, Func#, and Rule# refer to the number of</head><label>2</label><figDesc></figDesc><table>manually annotated 
causal scopes, manually annotated edges, instrumented 
functions, and rules in the configuration files for the bi-
nary rewriter, respectively. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Execution graph statistics about a snapshot for the target systems. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Relative execution time for the component level 
aggregation analysis with nine different configurations. 
Abbreviations: B -barriers are enabled among workers; 
B/C -barriers are enabled among workers, and partition 
state is checkpointed. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12" validated="false"><head>is mark as deleted Mark chunk x as deleted C1.chunk x has no reference Stream y was deleted</head><label></label><figDesc></figDesc><table>21:25 

21:17 

21:27 

error: Write request failed 

warning: chunk x Server start 
Server start 

Replay log and sync chunk x 

Send an empty write request 

Forward write request 

Send message 
IO 
Thread 

Protocol 
Thread 

Network 
Thread 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berkeley</forename><surname>Db</surname></persName>
		</author>
		<ptr target="http://www.oracle.com/database/berkeley-db/db/index.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linq</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Project</surname></persName>
		</author>
		<ptr target="http://msdn.microsoft.com/netframework/future/linq/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pagerank</surname></persName>
		</author>
		<ptr target="http://en.wikipedia.org/wiki/PageRank" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phoenix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Framework</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/phoenix/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Program slicing</title>
		<ptr target="http://en.wikipedia.org/wiki/Program_slicing" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Performance debugging for distributed systems of black boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wiener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muthitacharoen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP. ACM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Using magpie for request extraction and workload modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mortier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamic instrumentation of production systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W S</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cantrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automated Software Testing as a Service (TaaS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Candea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zamfir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SoCC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scope: easy and efficient parallel processing of massive data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1265" to="1276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bigtable: A distributed storage system for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Path-based failure and evolution management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Accardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pinpoint: Problem determination in large, dynamic internet services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fratkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSN</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Condie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alvaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sears</surname></persName>
		</author>
		<title level="m">Mapreduce online. In NSDI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamo: amazon&apos;s highly available key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experiences with tracing causality in networked services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INM/WREN</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">X-Trace: A pervasive network tracing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">X-Trace: A pervasive network tracing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dryad: distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="72" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">D 3 S: Debugging deployed distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Using correlated surprise to infer shared influence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Oliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSN</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pip: Detecting the unexpected in distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Killian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wiener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dapper, a large-scale distributed systems tracing infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Sigelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shanbhag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical dynamic slicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychoudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSTA. ACM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Detecting large-scale system problems by mining console logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DryadLINQ: A system for general-purpose distributed data-parallel computing using a high-level language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI. USENIX Association</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sherlog: error diagnosis by connecting clues from run-time logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pasupathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS. ACM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
