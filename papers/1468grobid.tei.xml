<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Data Furnace: Heating Up with Cloud Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Microsoft Research One Microsoft Way Redmond</orgName>
								<orgName type="institution" key="instit2">University of Virginia Charlottesville</orgName>
								<address>
									<postCode>98052, 22904</postCode>
									<region>WA, VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Goraczko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Microsoft Research One Microsoft Way Redmond</orgName>
								<orgName type="institution" key="instit2">University of Virginia Charlottesville</orgName>
								<address>
									<postCode>98052, 22904</postCode>
									<region>WA, VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>James</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Microsoft Research One Microsoft Way Redmond</orgName>
								<orgName type="institution" key="instit2">University of Virginia Charlottesville</orgName>
								<address>
									<postCode>98052, 22904</postCode>
									<region>WA, VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Belady</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Microsoft Research One Microsoft Way Redmond</orgName>
								<orgName type="institution" key="instit2">University of Virginia Charlottesville</orgName>
								<address>
									<postCode>98052, 22904</postCode>
									<region>WA, VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiakang</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Microsoft Research One Microsoft Way Redmond</orgName>
								<orgName type="institution" key="instit2">University of Virginia Charlottesville</orgName>
								<address>
									<postCode>98052, 22904</postCode>
									<region>WA, VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamin</forename><surname>Whitehouse</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution" key="instit1">Microsoft Research One Microsoft Way Redmond</orgName>
								<orgName type="institution" key="instit2">University of Virginia Charlottesville</orgName>
								<address>
									<postCode>98052, 22904</postCode>
									<region>WA, VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The Data Furnace: Heating Up with Cloud Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this paper, we argue that servers can be sent to homes and office buildings and used as a primary heat source. We call this approach the Data Furnace or DF. Data Fu-rances have three advantages over traditional data centers: 1) a smaller carbon footprint 2) reduced total cost of ownership per server 3) closer proximity to the users. From the home owner&apos;s perspective, a DF is equivalent to a typical heating system: a metal cabinet is shipped to the home and added to the ductwork or hot water pipes. From a technical perspective, DFs create new opportunities for both lower cost and improved quality of service, if cloud computing applications can exploit the differences in the cost structure and resource profile between Data Furances and conventional data centers.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Cloud computing is hot, literally. Electricity consumed by computers and other IT equipment has been skyrocketing in recent years, and has become a substantial part of the global energy market. In 2006, the IT industry used 61 Billion kWh electricity (or 3% of total energy consumption in the U.S.), and is the fastest growing industrial sector <ref type="bibr" target="#b7">[9]</ref>. Energy efficiency is not only important to reduce operational costs, but is also a matter of social responsiblity for the entire IT industry. The emergence of cloud computing, online services, and digital media distribution has lead to more computing tasks being offloaded to service providers and increasing demand on datacenters infrastructure. For this reason, it is not surprising that data center efficiency has been one of the focuses of cloud computing and data center design and operation.</p><p>In this paper, we argue that the problem of heat generation can be turned into an advantage: computers can be placed directly into buildings to provide low latency cloud computing for its offices or residents, and the heat that is generated can be used to heat the building. This approach improves quality of service by moving storage and computation closer to the consumer, and simultaneously improves energy efficiency and reduces costs by reusing the electricity and electrical infrastructure that would normally be used for space heating alone.</p><p>Physically, a computer server is a metal box that converts electricity into heat <ref type="bibr">1</ref> . The temperature of the exhaust air (usually around 40-50Â°C) is too low to regenerate electricity efficiently, but is perfect for heating purposes, including home/building space heating, cloth dryers, water heaters, and agriculture. We propose to replace electric resistive heating elements with silicon heating elements, thereby reducing societal energy footprint by using electricity for heating to also perform computation. The energy budget allocated for heating would provide an ample energy supply for computing. For example, home heating alone constitutes about 6% of the U.S. energy usage 2 . By piggy-backing on only half of this energy, the IT industry could double in size without increasing its carbon footprint or its load on the power grid and generation systems. Technological and economical trends also make energy reuse a promising direction. After years of development of cloud computing infrastructure, system management capabilities are getting mature. Servers can be remotely re-imaged, re-purposed, and rebooted. Virtual machine encapsulation ensures certain degree of isolation. Secure executions on untrusted devices are feasible. Sensor networks have make high physical security within reach. At the same time, computers are getting cheaper, network connectivity is getting faster, yet energy becomes a scarce resource and its price is on the trend of fast rise.</p><p>From a manageability and physical security point of view, the easiest adopters of this idea are office buildings and apartment complexes. A mid-sized data center (e.g. hundreds of killowatts) can be hosted inside the building and the heat it generates will be circulated to heat the building. Dedicated networking and physical security infrastructure can be built around it, and a dedicated operator can be hired to manage one or more of them. Their operation cost will be similar to operating other urban data centers, and can leverage the current trend toward sealed server containers that are replaced as a unit to save repair/replacement costs.</p><p>As a thought provoking exercise, we push this vision to the extreme in this paper. We investigate the feasibility of Data Furances or DFs, which are micro-datacenters, on the order of 40 to 400 CPUs, that serve as the primary heat source for a single-family home. These microdatacenters use the home broadband network to connect to the cloud, and can be used to host customer virtual machines or dedicated Internet services. They are integrated into the home heating system the same way as a conventional electrical furnace, using the same power system, ductwork, and circulation fan. Thus, DFs reduce the cost per server in comparison to conventional data centers by leveraging the home's existing infrastructure, and precluding the cost of real estate and construction of new brick and mortar structures. Furthermore, they naturally co-locate computational power and storage close to the user population.</p><p>DFs are managed remotely and do not require more physical maintenance than conventional furnaces. The cloud service operators may further incentivize the host families by providing free heat in exchange for occasion physical touches such as replacing air filters and, in extreme cases, turning on/off servers.</p><p>Previous studies also suggest bringing microdatacenters close to the users, including the idea of renting condos to host servers <ref type="bibr" target="#b1">[3]</ref> and to use home routers as a nano-datacenter for content caching <ref type="bibr" target="#b8">[10]</ref>. In this paper, we suggest that a quantum leap is achieved when this idea scaled to the size of a furnace; at this scale, the micro-datacenter cannot only leverage existing residential infrastructure for power, networking, and air circulation, but it can also reuse the energy that would otherwise be consumed for home heating.</p><p>In the rest of the paper, we first compare the cost of Data Furances versus building and operating conventional data centers, by analyzing the heating demand in various climate zones to understand the capacity and utilization requirements. Then, in Section 3 we discuss what kind of services can run on DFs given different investment levels. Finally, we discuss technical challenges including security, power management and system man- Houston, TX 46.51% 0.15% <ref type="table">Table 1</ref>: Representative locations used in simulations </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Home Heating with Data Furances</head><p>In this section, we evaluate the financial viability of Data Furances from the perspective of cloud service providers. Because DFs serve as a primary heat source in homes, we first perform a simulation study to understand the heating demands for a single family house across the climatic zones in the U.S.. Based on the results, we discuss the expected savings if DFs were used in each zone. We use ballpark figures and back-of-the-envelope calculations; the exact numbers depend on the specific households and data centers under consideration. DFs reduce the total cost of conventional datacenters in three main ways. First, much of the initial capital investment to build the infrastructure for a datacenter is avoided, including real estate, construction costs, and the cost of new power distribution, networking equipment, and other facilities. A second and related benefit is that operating costs are reduced. For example, cooling cost is significant in centralized data centers due to the power density <ref type="bibr" target="#b11">[13]</ref>, but DFs have essentially no additional cooling or air circulation costs since the heat distribution system in the house already circulates air. Thus, DFs increase the power usage efficiencies (PUE) over conventional datacenters. Finally, the money to buy and operate a furnace for home heating is avoided, and can be used instead to offset the cost of servers: the cloud service provider can sell DFs at the price of a furnace, and charge household owners for home heating. By doing this, the heating cost remains the same for the host family, while costs are reduced for the cloud service provider.</p><p>One disadvantage of DFs is that the retail price of electricity is usually higher in the residential areas by 10% to 50% than industrial areas <ref type="bibr" target="#b6">[8]</ref>. Another potential disadvantage is that the network bandwidth can cost more in homes if the home broadband link cannot satisfy the service and a high bandwidth link must be purchased. Finally, maintenance costs will increase because the machines will be geographically distributed.</p><p>To weigh these advantages and disadvantages, we perform a total cost of ownership (TCO) analysis for both DFs and conventional data centers. The initial and operating cost can change based on climate zone, so we first measure the actual heating demand for homes using the U.S. Department of Energy's EnergyPlus simulator <ref type="bibr" target="#b0">[2]</ref>. This simulator calculates the heating load (BTU) required each minute to keep a home warm, using actual weather traces recorded at airports. We simulate a 1700 square foot residential house that is moderately insulated and sealed with a heating setpoint of 21Â°C (70Â°F). We use weather data of Typical Meteorological Year 3 (TMY3) <ref type="bibr" target="#b10">[12]</ref>, and replay the entire year for cities in each of the five climate zones in the U.S., as listed in <ref type="table">Table 1</ref>. The last two columns show the percentage of time (in minutes granularity) that the outside temperature is less than 21Â°C, (thus heating is useful) and that the outside temperature is greater than 35Â°C (thus the server may have to be shut down for thermo protection since we do not expect cooling the furnace.). The percentage of time in between is when the servers can be run but the heat must be pumped outside. <ref type="table">Table 2</ref> summarizes the results of our TCO analysis for a DF in each climate zone. Row 1 indicates the maximum heating rate required for the coldest day of the year in each zone, based on our simulations. Row 2 indicates the number of servers required in a DF to satisfy these requirements, assuming a typical server such as the Dell PowerEdge 1850 consumes 350 W generating 1,194.55 BTU/h heat dissipation <ref type="bibr">[1]</ref>. The third row shows the estimated home heating cost for each zone, assuming the consumer pays an amount equal to the cost of operating either an appropriately sized oil furnace in Zones 1 and 2, or a heat pump in Zones 3, 4 and 5. This number is calculated based on the data in <ref type="figure" target="#fig_0">Figure 1</ref>, which shows the average heating rate required for each month in each climate zone, as determined by our simulations. The actual home heating cost certainly varies based on the heating technology and thermostat setting. This is the incentive to the host family that the cloud operator can claim.</p><p>The fourth row shows the income per year from selling the DF to the consumer, assuming the consumer pays $4000 for a comparably sized heat pump or furnace. Based on the electricity price data, the fifth row shows the increased cost of electricity for the data center owner, due to residential electricity rates being higher than commercial electricity rates; we assume that the residential electricity rate in the U.S. is $0.10/kWh while the industrial rate is $0.05/kWh. The sixth and seventh rows indicate the cost per DF for each zone, and the cost per server based on the number of servers in each zone. These numbers represent overhead costs and do not include the costs of the servers themselves, since those costs must also be paid in a conventional data center. The eighth row indicates the optional cost of upgrading the network link at the home to a T1, which is assumed to cost $2,640 for the home per year. In all regions, the overhead cost is less than $90 per server per year.</p><p>In comparison, the overhead cost per server per year for a conventional datacenter is approximately $400. This cost is taken from James Hamilton's spreadsheet analysis <ref type="bibr" target="#b3">[5]</ref>, which estimates the cost of running a datacenter of over 45,000 servers, using a 3-year server amortization and a 15-year facilities amortization. These overhead costs include construction for a brick and mortar building, network equipment, cooling costs, and other equipment and operational costs that are not needed for DFs. Based on these numbers, the last row of <ref type="table">Table 2</ref> indicates the cost savings per server per year of running a DF, in comparison to a conventional datacenter. These savings would be higher for data centers that run applications that do not necessitate an upgrade to the network connection, or when comparing to urban data centers that pay more for electricity and real estate.</p><p>Designed correctly, Data Furances may not require frequent touch for maintenance. Studies have shown that on average 9% servers fail in 14-month of operation <ref type="bibr" target="#b9">[11]</ref>. In other industries, the cost to roll a truck is approximately $100 per maintenance visit, which is only necessary for a Data Furnace when a few physical failures occur that cannot be managed remotely. Even if these occur three times per year, it is an amortized cost of less than ten dollars per server per year and therefore does not substantially change the TCO analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Types of Service</head><p>The overhead costs for Data Furances are very small compared to centralized mega datacenters, but the true value proposition depends on the types of services it can provide. DFs will present both opportunities and challenges for cloud computing services. One one hand, DFs can improve performance for many applications because they are close to areas of high population density. On  <ref type="table">Table 2</ref>: Cost benefit to run a DF over a conventional datacenter (from the perspective of cloud service provider) the other hand, the new cost structure for bandwidth and energy may make DFs less attractive for other types of applications. We envision at least three classes of DFs, as described below:</p><p>Type A: Low-cost seasonal DFs The least expensive DFs will use the existing home broadband connection and will perform computation predominantly when the house needs heating (e.g. at nights or during winters), providing seasonal computing capacity with almost no operational cost. These low-cost DFs can also reduce initial costs by using recycled, older models of servers -hundreds of thousands of which are decommissioned from existing datacenters every year to make space for servers with latest hardware technology. These older servers are often less energy efficient and less cost effective in a conventional datacenter, but ideal for providing heat and maintaining basic disk and network operations year-round. Because of the population distribution on the planet, the computing capacity of this class of DFs will be skewed toward the northern hemisphere's winter months. Despite their bandwidth-limited and seasonal nature, Type A DFs can still support a wide range of services. For example, many delay-tolerant batch jobs can be performed opportunistically, such as non-real-time web crawling, content indexing, and the processing of large scientific data sets (e.g. astronomical data, genome sequencing operations, and SETI@Home). Type A DFs can also help developing communities, societal services, hobbiests, and other organizations looking for extremely low cost computing resources. Finally, governments currently subsidize home heating for developing communities <ref type="bibr" target="#b1">3</ref> . With the same fund, Type A DFs can have extended operation time for education, scientific, and social services, doubling the benefits.</p><p>Type B: Low-bandwidth neighborhood DFs A lower-cost DF can also operate year-round using the broadband channel of the home instead of upgrading the network link, thereby supporting typical computation loads but with a relatively slow egress link. The least expensive cable modem package from Comcast today provides 12 Mbps download and 2 Mbps upload speed, in comparison to a conventional data center, which may have a 10 Gbps link (shared among hundreds of thousands of machines). This new resource profile makes financial sense for DFs, particularly in warmer climates, where it will reduce overhead costs by up to 80% per server, as shown in <ref type="table">Table 2</ref>. Despite bandwidth limitations, Type B DF can nevertheless provide high quality computing services due to geographic proximity to the customers. For example, Internet television services and on-line video rental services could use pre-fetching based on local programming schedules or video queues of people in the local vicinity. Similarly, location-based services such as map serving, traffic estimation, local navigation, and advertisements for local stores are typically requested by customers from the local region. Network latency can be reduced by creating more data centers closer to customers <ref type="bibr" target="#b5">[7]</ref>, particularly for cloud applications such as E-mail, multimedia distribution <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b8">10]</ref>, and low-hit rate Web pages and services that are delay tolerant, embarrassingly distributable, and/or can benefit from large-scale replication and geo-distribution. Type C: Eco-friendly urban DFs . Closest to the current operation modes, high-end DFs will upgrade the communication link at the home to FiOS or a T1, and can operate year-round even when heat is not needed in the home by pumping excess heat directly outside. This is particular promising for business buildings and apartments. Having comparable resources per server, these DFs would give service providers the ability to expand into urban areas more quickly and easily without urban real estate and infrastructure expenses, as long as the application scale to the number of servers. On the other hand, Type C DFs also present new challenges. First, summer operation costs more because higher elec-tricity rates are not compensated by sale of heat. However, this additional cost would be marginal compared to the cost advantages of DFs, especially since many data centers only have about 20% server utilization anyway. Furthermore, urban areas present opportunities for larger DFs in commercial buildings, business parks, and apartment complexes where lower commercial energy rates are available and where the larger heating units have an energy footprint close to containerized datacenters that are used for edge services. A second challenge is that DFs may break large services into many small pieces, thereby introducing a different granularity to perform massively distributed computations, like MapReduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Technical Challenges</head><p>Despite the facts that server hardware is getting more robust and less sensitive to operation conditions, and that cluster-level management software is getting mature, DF present new unique challenges due to its distribution in residential households, including:</p><p>Isolation: DFs use the existing power and network infrastructure of a normal residence or business, and they must not affect normal building operations. A DF must monitor and react to the total power and broadband usage of its hosts in order not to exceed capacity (e.g. 24kW and 12Mbps for a typical home in Seattle), and cloud services must be designed to accommodate this low-priority operation.</p><p>Security: One difference between DFs and dedicated datacenters is the challenge of physical security, and DFs should be treated as being in the most insecure environment. For example, each server should have an individual tamper proofing device, such as a networked sensor. This will allow them to be individually swapped out by the hosting party only when necessary (e.g. due to unrecoverable device failure). Furthermore, all stored data and network traffic must be encrypted. Software running on the servers should be sandboxed and secured from the hosting party.</p><p>Zero-touch management: Automated solutions exist for monitoring server and network health, remotely re-purpose and re-image machines, and remotely diagnose faults and power cycle machines (e.g. <ref type="bibr" target="#b4">[6]</ref>). However, without dedicated system operators and in a residential environment, DFs require high heating reliability and zero-touch management. Even at the event of software failure, the system should continue to provide heat until receiving physical services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>With continuously increasing demand for computing, the community must explore novel ways to expand computing capacity without increasing financial burden and energy costs. Data Furances will reuse the facilities and energy already allocated for heating purposes to provide computing services with low cost and energy footprint. In this paper, we focus on homes as an illustrating example, but a similar approach could be used to heat water tanks, office buildings, apartment complexes, vegetable farms, and large campuses with central facilities.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Average monthly heating rate across the year</figDesc></figure>

			<note place="foot" n="1"> At the same time, some energy is used by the fans to move the air for cooling and a very small amount of energy is used to drive electrons or photons on the network interface 2 According to the Energy Information Administration (EIA), U.S. households used 154 billion kWh for home heating, 116 billion kWh of which goes to the heating element[4]</note>

			<note place="foot" n="3"> e.g. Wisconsin Home Energy Assistance Program (WHEAP) (http://homeenergyplus.wi.gov/category.asp? linkcatid=239&amp;linkid=118)</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://apps1.eere.energy.gov/buildings/energyplus/" />
	</analytic>
	<monogr>
		<title level="j">EnergyPlus Energy Simulation Software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On delivering embarrassingly distributed cloud services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Church</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamilton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-COMM Hotnets VII</title>
		<meeting><address><addrLine>Calgary Alberta, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-Use Consumption of Electricity Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Energy</forename><surname>Information Administration</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In The Us De-Partment Of</forename><surname>Energy</surname></persName>
		</author>
		<ptr target="http://www.eia.doe.gov/emeu/recs/recs2001/enduse2001/enduse2001.html" />
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamilton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<ptr target="http://perspectives.mvdirona.com/2010/09/18/OverallDataCenterCosts.aspx" />
		<imprint>
			<publisher>Overall Data Center Costs</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autopilot: automatic data center management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cloudcmp: Comparing public cloud providers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internet Measurement Conference</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Average Retail Price of Electricity to Ultimate Customers by End-Use Sector, by State</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Energy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Administration</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Environmental</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En-Ergy Star</forename><surname>Agency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Program</surname></persName>
		</author>
		<ptr target="http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf" />
		<imprint>
			<biblScope unit="page" from="109" to="431" />
		</imprint>
	</monogr>
<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Greening the internet with nano data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valancius</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Laoutaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Massoulie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodriguez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CoNEXT&apos;09</title>
		<meeting><address><addrLine>Rome, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizing cloud computing hardware reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishwanath</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagappan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM symposium on Cloud computing</title>
		<meeting>the 1st ACM symposium on Cloud computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="193" to="204" />
		</imprint>
	</monogr>
	<note>SoCC &apos;10, ACM</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Users Manual for TMY3 Data Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilcox</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">National Renewable En-Ergy</forename><surname>Laboratory</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>U.S.</pubPlace>
		</imprint>
		<respStmt>
			<orgName>National Renewable Energy Laboratory</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cooling the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woods</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Queue</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
