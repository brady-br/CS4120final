<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MatrixKV: Reducing Write Stalls and Write Amplification in LSM-tree Based KV Stores with Matrix Container in NVM MatrixKV: Reducing Write Stalls and Write Amplification in LSM-tree Based KV Stores with a Matrix Container in NVM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-17, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiguang</forename><surname>Wan</surname></persName>
							<email>jgwan@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PingCAP</roleName><forename type="first">Liu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">T</forename><surname>Arlington</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WNLO, Huazhong University of Science and Technology, China Key Laboratory of Information Storage System, Ministry of Education of China 2 PingCAP</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WNLO, Huazhong University of Science and Technology, China Key Laboratory of Information Storage System, Ministry of Education of China 2 PingCAP</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiguang</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WNLO, Huazhong University of Science and Technology, China Key Laboratory of Information Storage System, Ministry of Education of China 2 PingCAP</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Arlington</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">WNLO, Huazhong University of Science and Technology, China Key Laboratory of Information Storage System, Ministry of Education of China 2 PingCAP</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Introduction</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Temple University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Huazhong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Huazhong University of Science and Technology; Xubin He</orgName>
								<orgName type="institution" key="instit3">Temple University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MatrixKV: Reducing Write Stalls and Write Amplification in LSM-tree Based KV Stores with Matrix Container in NVM MatrixKV: Reducing Write Stalls and Write Amplification in LSM-tree Based KV Stores with a Matrix Container in NVM</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference</title>
						<meeting>the 2020 USENIX Annual Technical Conference						</meeting>
						<imprint>
							<date type="published">July 15-17, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-14-4 Open access to the Proceedings of the 2020 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc20/presentation/yao *Corresponding author.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Popular LSM-tree based key-value stores suffer from subopti-mal and unpredictable performance due to write amplification and write stalls that cause application performance to periodically drop to nearly zero. Our preliminary experimental studies reveal that (1) write stalls mainly stem from the significantly large amount of data involved in each compaction between L 0-L 1 (i.e., the first two levels of LSM-tree), and (2) write amplification increases with the depth of LSM-trees. Existing works mainly focus on reducing write amplification, while only a couple of them target mitigating write stalls. In this paper, we exploit non-volatile memory (NVM) to address these two limitations and propose MatrixKV, a new LSM-tree based KV store for systems with multi-tier DRAM-NVM-SSD storage. MatrixKV&apos;s design principles include performing smaller and cheaper L 0-L 1 compaction to reduce write stalls while reducing the depth of LSM-trees to mitigate write amplification. To this end, four novel techniques are proposed. First, we relocate and manage the L 0 level in NVM with our proposed matrix container. Second, the new column compaction is devised to compact L 0 to L 1 at fine-grained key ranges, thus substantially reducing the amount of compaction data. Third, MatrixKV increases the width of each level to decrease the depth of LSM-trees thus mitigating write amplification. Finally, the cross-row hint search is introduced for the matrix container to keep adequate read performance. We implement MatrixKV based on RocksDB and evaluate it on a hybrid DRAM/NVM/SSD system using Intel&apos;s latest 3D Xpoint NVM device Optane DC PMM. Evaluation results show that, with the same amount of NVM, MatrixKV achieves 5× and 1.9× lower 99 th percentile latencies, and 3.6× and 2.6× higher random write throughput than RocksDB and the state-of-art LSM-based KVS NoveLSM respectively.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Persistent key-value stores are increasingly critical in supporting a large variety of applications in modern data centers. In write-intensive scenarios, log-structured merge trees (LSMtrees) <ref type="bibr" target="#b48">[49]</ref> are the backbone index structures for persistent key-value (KV) stores, such as RocksDB <ref type="bibr" target="#b23">[24]</ref>, LevelDB <ref type="bibr" target="#b24">[25]</ref>, HBase <ref type="bibr" target="#b25">[26]</ref>, and Cassandra <ref type="bibr" target="#b34">[35]</ref>. Considering that random writes are common in popular OLTP workloads, the performance of random writes, especially sustained and/or bursty random writes, is a serious concern for users <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b51">51]</ref>. This paper takes random write performance of KV stores as a major concern. Popular KV stores are deployed on systems with DRAM-SSD storage, which intends to utilize fast DRAM and persistent SSDs to provide high-performance database accesses. However, limitations such as cell sizes, power consumption, cost, and DIMM slot availability prevent the system performance from being further improved via increasing DRAM size <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref>. Therefore, exploiting non-volatile memories (NVMs) in hybrid systems is widely considered as a promising mechanism to deliver higher system throughput and lower latencies.</p><p>LSM-trees <ref type="bibr" target="#b48">[49]</ref> store KV items with multiple exponentially increased levels, e.g., from L 0 to L 6 . To better understand LSM-tree based KV stores, we experimentally evaluated the popular RocksDB <ref type="bibr" target="#b23">[24]</ref> with a conventional system of DRAM-SSD storage, and made observations that point to two challenging issues and their root causes. First, write stalls lead to application throughput periodically dropping to nearly zero, resulting in dramatic fluctuations of performance and long-tail latencies, as shown in Figures 2 and 3. The troughs of system throughput indicate write stalls. Write stalls induce highly unpredictable performance and degrade the quality of user experiences, which goes against NoSQL systems' design goal of predictable and stable performance <ref type="bibr" target="#b53">[53,</ref><ref type="bibr" target="#b57">57]</ref>. Moreover, write stalls substantially lengthen the latency of request processing, exerting high tail latencies <ref type="bibr" target="#b5">[6]</ref>. Our experimental studies demonstrate that the main cause of write stalls is the large amount of data processed in each L 0 -L 1 compaction. The L 0 -L 1 compaction involves almost all data in both levels due to the unsorted L 0 (files in L 0 are overlapped with key ranges). The all-to-all compaction takes up CPU cycles and SSD bandwidth, which slows down the foreground requests and results in write stalls and long-tail latency. Second, write amplification (WA) degrades system performance and storage devices' endurance. WA is directly related to the depth of the LSMtree as a deeper tree resulting from a larger dataset increases the number of compactions. Although a large body of research aims at reducing LSM-trees' WA <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b51">51]</ref>, only a couple of published studies concern mitigating write stalls <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b53">53]</ref>. Our study aims to address both challenges simultaneously.</p><p>Targeting these two challenges and their root causes, this paper proposes MatrixKV, an LSM-tree based KV store for systems with DRAM-NVM-SSD storage. The design principle behind MatrixKV is leveraging NVM to (1) construct cheaper and finer granularity compaction for L 0 and L 1 , and (2) reduce LSM-trees' depth to mitigate WA. The key enabling technologies of MatrixKV are summarized as follows:</p><p>Matrix container. The matrix container manages the unsorted L 0 of LSM-trees in NVM with a receiver and a compactor. The receiver adopts and retains the MemTable flushed from DRAM, one MemTable per row. The compactor selects and merges a subset of data from L 0 (with the same key range) to L 1 , one column per compaction.</p><p>Column compaction. A column compaction is the finegrained compaction between L 0 and L 1 , which compacts a small key range a time. Column compaction reduces write stalls because it processes a limited amount of data and promptly frees up the column in NVM for the receiver to accept data flushed from DRAM.</p><p>Reducing LSM-tree depth. MatrixKV increases the size of each LSM-tree level to reduce the number of levels. As a result, MatrixKV reduces write amplification and delivers higher throughput.</p><p>Cross-row hint search. MatrixKV gives each key a pointer to logically sort all keys in the matrix container thus accelerating search processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>In this section, we present the necessary background on NVM, LSM-trees, LSM-based KV stores, and the challenges and motivations in optimizing LSM-based KV stores with NVMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-volatile Memory</head><p>Service providers have constantly pursued faster database accesses. They aim at providing users with a better quality of service and experience without a significant increase in the total cost of ownership (TCO). With the emergence and development of new storage media such as phase-change memory <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b52">52]</ref>, memristors <ref type="bibr" target="#b55">[55]</ref>, 3D XPoint <ref type="bibr" target="#b27">[28]</ref>, and STT-MRAM <ref type="bibr" target="#b20">[21]</ref>, enhancing storage systems with NVMs becomes a cost-efficient choice. NVM is byte-addressable, persistent, and fast. It is expected to provide DRAM-like performance, disk-like persistency, and higher capacity than DRAM at a much lower cost <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b61">61]</ref>. Compared to SSDs, NVM is expected to provide 100× lower read and write latencies and up to ten times higher bandwidth <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. NVM works either as a persistent block storage device accessed through PCIe interfaces or as main memory accessed via memory bus <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38]</ref>. Existing research <ref type="bibr" target="#b30">[31]</ref> shows that the former only achieve marginal performance improvements, wasting NVM's high media performance. For the latter, NVM can supplant or complement DRAM as a single-level memory system <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b65">65]</ref>, a system of NVM-SSD <ref type="bibr" target="#b29">[30]</ref>, or a hybrid system of DRAM-NVM-SSD <ref type="bibr" target="#b30">[31]</ref>. In particular, systems with DRAM-NVM-SSD storage are recognized as a promising way to utilize NVMs due to the following three reasons. First, NVM is expected to co-exist with large-capacity SSDs for the next few years <ref type="bibr" target="#b31">[32]</ref>. Second, compared to DRAM, NVM still has 5 times lower bandwidth and 3 times higher read latency <ref type="bibr" target="#b27">[28]</ref>. Third, a hybrid system balances the TCO and system performance. As a result, MatrixKV focuses on efficiently using NVMs as persistent memory in a hybrid system of DRAM, NVMs, and SSDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Log-structured Merge Trees</head><p>LSM-trees <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b48">49]</ref> defer and batch write requests in memory to exploit the high sequential write bandwidth of storage devices. Here we explain a popular implementation of LSMtrees, the widely deployed SSD-based RocksDB <ref type="bibr" target="#b23">[24]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a), RocksDB is composed of a DRAM component and an SSD component. It also has a write-ahead log in SSDs protecting data in DRAM from system failures.</p><p>To serve write requests, writes are first batched in DRAM by two skip-lists <ref type="table">(MemTable and Immutable MemTable)</ref>. Then, the immutable MemTable is flushed to L 0 on SSDs generating Sorted String <ref type="table">Tables (SSTables)</ref>. To deliver a fast flush, L 0 is unsorted where key ranges overlap among dif-ferent SSTables. SSTables are compacted from L 0 to deeper levels (L 1 , L 2 ...L n ) during the lifespan of LSM-trees. Compaction makes each level sorted (except L 0 ) thus bounding the overhead of reads and scans <ref type="bibr" target="#b53">[53]</ref>.</p><p>To conduct a compaction, (1) an SSTable in L i (called a victim SSTable) and multiple SSTables in L i+1 who has overlapping key ranges (called overlapped SSTables) are picked as the compaction data. (2) Other SSTables in L i that fall in this compaction key ranges are selected reversely. (3) Those SSTables identified in steps (1) and (2) are fetched into memory, to be merged and sorted. (4) The regenerated SSTables are written back to L i+1 . Since L 0 is unsorted and each SSTable in L 0 spans a wide key range, the L 0 -L 1 compaction performs step (1) and (2) back and forth involving almost all SSTables in both levels, leading to a large all-to-all compaction.</p><p>To serve read requests, RocksDB searches the MemTable first, immutable MemTable next, and then SSTables in L 0 through L n in order. Since SSTables in L 0 contain overlapping keys, a lookup may search multiple files at L 0 <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">LSM-tree based KV stores</head><p>Existing improvements on LSM-trees includes: reducing write amplification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b62">[62]</ref><ref type="bibr" target="#b63">[63]</ref><ref type="bibr" target="#b64">[64]</ref>, improving memory management <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b56">56]</ref>, supporting automatic tuning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40]</ref>, and using LSM-trees to target hybrid storage hierarchies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b50">50]</ref>. Among them, random write performance is a common concern since it is severely hampered by compactions. In the following, we discuss the most related studies of our work in three categories: those reducing write amplification, addressing write stalls, and utilizing NVMs.</p><p>Reducing WA: PebblesDB [51] mitigates WA by using guards to maintain partially sorted levels. Lwc-tree <ref type="bibr" target="#b62">[62]</ref> provides lightweight compaction by appending data to SSTables and only merging the metadata. WiscKey <ref type="bibr" target="#b35">[36]</ref> separates keys from values, which only merges keys during compactions thus reducing WA. The key-value separation solution brings the complexities of garbage collection and range scans and only benefits large values. LSM-trie <ref type="bibr" target="#b59">[59]</ref> de-amortizes compaction overhead with hash-range based compaction. VTtree <ref type="bibr" target="#b54">[54]</ref> uses an extra layer of indirection to avoid reprocessing sorted data at the cost of fragmentation. TRIAD <ref type="bibr" target="#b43">[44]</ref> reduces WA by creating synergy between memory, disk, and log. However, almost all these efforts overlook performance variances and write stalls.</p><p>Reducing write stalls: SILK <ref type="bibr" target="#b5">[6]</ref> introduces an I/O scheduler which mitigates the impact of write stalls to clients' writes by postponing flushes and compactions to low-load periods, prioritizing flushes and lower level compactions, and preempting compactions. These design choices make SILK exhibits ordinary write stalls on sustained write-intensive and long peak workloads. Blsm <ref type="bibr" target="#b53">[53]</ref> proposes a new merge scheduler, called "spring and gear", to coordinate compactions of multiple levels. However, it only bounds the maximum write processing latency while ignoring the large queuing latency <ref type="bibr" target="#b42">[43]</ref>. <ref type="bibr">KVell [37]</ref> makes KV items unsorted on disks to reduce CPU computation cost thus mitigating write stalls for NVMe SSD based KV stores, which is inapplicable to systems with general SSDs.</p><p>Improving LSM-trees with NVMs: SLM-DB <ref type="bibr" target="#b29">[30]</ref> proposes a single level LSM-tree for systems with NVM-SSD storage. It uses a B + -tree in NVM to provide fast read for the single level LSM-tree on SSDs. This solution comes with the overhead of maintaining the consistency between B + -trees and LSM-trees. MyNVM <ref type="bibr" target="#b22">[23]</ref> leverages NVM as a block device to reduce the DRAM usage in SSD based KV stores. NoveLSM <ref type="bibr" target="#b30">[31]</ref> is the state-of-art LSM-based KV store for systems with hybrid storage of DRAM, NVMs, and SSDs. NVMRocks <ref type="bibr" target="#b37">[38]</ref> aims for an NVM-aware RocksDB, similar to NoveLSM, which adopts persistent mutable MemTables on NVMs. However, as we verified in § 2.4.3, mutable NVM MemTables only reduce access latencies to some extent while generating a negative effect of more severe write stalls.</p><p>Since we build MatrixKV for systems with multi-tier DRAM-NVM-SSD storage and redesign LSM-trees to exploit the high performance NVM, NoveLSM <ref type="bibr" target="#b30">[31]</ref> is considered the most relevant to our work. We use NoveLSM as our main comparison in evaluations. In addition, we also evaluate PebblesDB and SILK on NVM-based systems since they are state-of-art solutions for reducing WA or write stalls but their original designs are not for the hybrid systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Challenges and Motivations</head><p>To explore the challenges in LSM-tree based KV stores, we conduct a preliminary study on the SSD-based RocksDB. In this experiment, an 80 GB dataset of 16bytes-4KB key-value items is written/loaded to RocksDB in uniformly random order. The evaluation environments and other parameters are described in § 5. We record random write throughput every ten seconds as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The experimental results expose two challenging issues. Challenge 1, Write stalls. System performance experiences peaks and troughs, and the troughs of throughput manifest as write stalls. The significant fluctuations indicate unpredictable and unstable performance. Challenge 2, Write amplification. WA causes performance degradation. System performance (i.e., the average throughput) shows a downward trend with the growth of the dataset size since the number of compactions increases with the depth of LSM-trees, bringing more WA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Write Stalls</head><p>In an LSM-based KV store, there are three types of possible stalls as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>  storage are blocked. (3) Compaction stalls: too many pending compaction bytes block foreground operations. All these stalls have a cascading impact on write performance and result in write stalls. Evaluating these three types of stalls individually by recording the period of flushes and compactions at different levels, we find that the period of L 0 -L 1 compaction approximately matches write stalls observed, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Each red line represents a L 0 -L 1 compaction, where the length along the x-axis represents the latency of the compaction and the right y-axis shows the amount of data processed in the compaction. The average amount of compaction data is 3.10 GB. As we elaborate in § 2.2, since L 0 allows overlapping key ranges between SSTables, almost all SSTables in both levels join the L 0 -L 1 compaction. A large amount of compaction data leads to heavy read-merge-writes, which takes up CPU cycles and the SSD bandwidth, thus blocking foreground requests and making L 0 -L 1 compaction the primary cause of write stalls.</p><p>Write stalls not only are responsible for the low system throughput, but also induce high write latency leading to the long-tail latency problem. <ref type="figure" target="#fig_2">Figure 3</ref> shows the cumulative distribution function (CDF) of the latency for each write request during the 80 GB random load process. Although the latency of 76% of the write requests is less than 48 us, the write latency of the 90 th , 99 th , and 99.9 th percentile reaches 1.15, 1.24, and 2.32 ms respectively, a two-order magnitude increase. The high latency significantly degrades the quality of user experiences, especially for latency-critical applications.  <ref type="figure" target="#fig_1">Figure 2</ref>, the average period of write stalls is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Write Amplification</head><p>Next, we analyze the second observation, i.e., system throughput degrades with the increase in dataset size. Write amplification (WA) is defined as the ratio between the amount of data written to storage devices and the amount of data written by users. LSM-tree based KV stores have long been criticized for their high WA due to frequent compactions. Since the sizes of adjacent levels from low to high increase exponentially by an amplification factor (AF = 10), compacting an SSTable from L i to L i+1 results in a WA factor of AF on average. The growing size of the dataset increases the depth of an LSM-tree as well as the overall WA. For example, the WA factor of compacting from L 1 to L 2 is AF, while the WA factor of compacting from L 1 to L 6 is over 5 × AF. The increased WA consumes more storage bandwidth, competes with flush operations, and ultimately slows down application throughput. Hence, system throughput decreases with higher write amplification caused by the increased depth of LSM-trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">NoveLSM</head><p>NoveLSM <ref type="bibr" target="#b30">[31]</ref> exploits NVMs to deliver high throughput for systems with DRAM-NVM-SSD storage, as shown in <ref type="bibr">Fig- ure 1(b)</ref>. The design choices of NoveLSM include: (1) adopting NVMs as an alternative DRAM to increase the size of MemTable and immutable MemTable; (2) making the NVM MemTable mutable to allow direct updates thus reducing compactions. However, these design choices merely postpone the write stalls. When the dataset size exceeds the capacity of NVM MemTables, flush stalls still happen, blocking foreground requests. Furthermore, the enlarged MemTables in NVM are flushed to L 0 and dramatically increase the amount of data in L 0 -L 1 compactions, resulting in even more severe write stalls. The worse write stalls magnify performance variances and hurt user experiences further.</p><p>We evaluate NoveLSM (with 8 GB NVM) by randomly writing the same 80 GB dataset. Test results in <ref type="figure" target="#fig_3">Figure 4</ref> show that NoveLSM reduces the overall loading time by 1.7× compared to RocksDB <ref type="figure" target="#fig_1">(Figure 2)</ref>. However, the period of write stalls is significantly longer. This is because the amount of data involved in each L 0 -L 1 compaction is over 15 GB, which is 4.86× larger than that of RocksDB. A write stall starts when compaction threads call for the L 0 -L 1 compaction. Then, the compaction waits and starts until other pending compactions with higher priorities complete (i.e., the grey dashed lines). Finally, performance rises again as the compaction completes. In general, NoveLSM exacerbates write stalls.</p><p>From the above analysis, we conclude that the main cause of write stalls is the large amount of data involved in L 0 -L 1 compactions, and the main cause of increased WA is the deepened depth of LSM-trees. The compounded impact of write stalls and WA deteriorates system throughput and lengthens tail latency. While NoveLSM attempts to alleviate these issues, it actually exacerbates the problem of write stalls. Motivated by these observed challenging issues, we propose MatrixKV that aims at providing a stable low-latency KV store via intelligent use of NVMs, as elaborated in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MatrixKV Design</head><p>In this section, we present MatrixKV, an LSM-tree based keyvalue store for systems with multi-tier DRAM-NVM-SSD storage. MatrixKV aims to provide predictable high performance through the efficient use of NVMs with the following four key techniques, i.e., the matrix container in NVMs to manage the L 0 of LSM-trees ( § 3.1), column compactions for L 0 and L 1 ( § 3.2), reducing LSM-tree levels ( § 3.3), and the cross-row hint search ( § 3.4). <ref type="figure" target="#fig_4">Figure 5</ref> shows the overall architecture of MatrixKV. From top to bottom, (1) DRAM batches writes with MemTables, (2) MemTables are flushed to L 0 that is stored and managed by the matrix container in NVMs, (3) data in L 0 are compacted to L 1 in SSDs through column compactions, and (4) SSDs store the remaining levels of a flattened LSM-tree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Matrix Container</head><p>LSM-tree renders all-to-all compactions for L 0 and L 1 because L 0 has overlapping key ranges among SSTables. The heavy L 0 -L 1 compactions are identified as the root cause of write stalls as demonstrated in § 2.4. NoveLSM <ref type="bibr" target="#b30">[31]</ref> exploits NVM to increase the number and size of MemTables. However, it actually exacerbates write stalls by having a larger L 0 and keeping the system bottleneck, L 0 -L 1 compactions, on lower-speed SSDs. Hence, the principle of building an LSM-tree based KV store without write stalls is to reduce the granularity of L 0 -L 1 compaction via high-speed NVMs. Based on this design principle, MatrixKV elevates L 0 from SSDs to NVMs and reorganizes L 0 into a matrix container to exploit the byte-addressability and fast random accesses of NVMs. Matrix container is a data management structure for the L 0 of LSM-trees. <ref type="figure" target="#fig_5">Figure 6</ref> shows the organization of a matrix container, which comprises one receiver and one compactor.</p><p>Receiver: In the matrix container, the receiver accepts and retains MemTables flushed from DRAM. Each such MemTable is serialized as a single row of the receiver and organized as a RowTable. RowTables are appended to the matrix container row by row with an increasing sequence number, i.e., from 0 to n. The size of the receiver starts with one RowTable. When the receiver size reaches its size limit (e.g., 60% of the matrix container) and the compactor is empty, the receiver stops receiving flushed MemTables and dynamically turns into the compactor. In the meantime, a new receiver is created for receiving flushed MemTables. There is no data migration for the logical role change of the receiver to the compactor.</p><p>RowTable: <ref type="figure" target="#fig_6">Figure 7</ref>(a) shows the RowTable structure consisting of data and metadata. To construct a RowTable, we first serialize KV items from the immutable MemTable in the order of keys (the same as SSTables) and store them to the data region. Then, we build the metadata for all KV items with a sorted array. Each array element maintains the key, the page number, the offset in the page, and a forward pointer (i.e., p n ). To locate a KV item in a RowTable, we binary search the sorted array to get the target key and find its value with the page number and the offset. The forward pointer in each array element is used for cross-row hint searches that contribute to improving the read efficiency within the matrix container. The cross-row hint search will be discussed in § 3.4. <ref type="figure" target="#fig_6">Figure 7</ref>(b) shows the structure of conventional SSTable in LSM-trees. SSTables are organized with the basic unit of blocks in accordance with the storage unit of devices such as SSDs and HDDs. Instead, RowTable takes an NVM page as its basic unit. Other than that, RowTables are only different from SSTables in the organization of metadata. As a result, the construction overhead of SSTables and RowTables is similar.</p><p>Compactor: The compactor is used for selecting and merging data from L 0 to L 1 in SSDs at a fine granularity. Leveraging the byte addressability of NVMs and our proposed RowTables, MatrixKV allows cheaper compactions that merge a specific key range from L 0 with a subset of SSTables at L 1 without needing to merge all of L 0 and all of L 1 . This new L 0 -L 1 compaction is referred to as column compaction (detailed in § 3.2). In the compactor, KV items are managed by logical columns. A column is a subset of key spaces with a limited amount of data, which is the basic unit of the compactor in column compactions. Specifically, KV items from different RowTables that fall in the key range of a column compaction logically constitute a column. The amount of these KV items is the size of a column, which is not strictly fixed but at a threshold determined by the size of column compactions.</p><p>Space management: After compacting a column, the NVM space occupied by the column is freed. To manage those freed spaces, we simply apply the paging algorithm <ref type="bibr" target="#b2">[3]</ref>. Since column compactions rotate the key ranges, at most one page per RowTable is partially fragmented. The NVM pages fully freed after column compactions are added to the free list as a group of page-sized units. To store incoming RowTables in the receiver, we apply free pages from the free list. The 8 GB matrix container contains 2 11 pages of 4 KB each. Each page is identified by the page number of an unsigned integer. Adding the 8 bytes pointer per list element, the metadata size for each page is 12 bytes. The metadata of the free list occupies a total space of 24 KB on NVMs at most.</p><p>It is worth noting that in the matrix container, while columns are being compacted in the compactor, the receiver can continue accepting flushed MemTables from DRAM simultaneously. By freeing the NVM space one column at a time, MatrixKV ends the write stalls forced by merging the entire L 0 with all of L 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Column Compaction</head><p>Column compaction is a fine-grained L 0 -L 1 compaction that each time compacts only a column, i.e., a small subset of the data in a specific key range. Thus, column compaction can significantly reduce write stalls. The main workflow of column compaction can be described in the following seven steps. (1) MatrixKV separates the key space of L 1 into multiple contiguous key ranges. Since SSTables in L 1 are sorted and each SSTable is bounded by its smallest key and largest key, the smallest keys and largest keys of all the SSTables in L 1 form a sorted key list. Every two adjacent keys represent a key range, i.e., the key range of an SSTable or the gap between two adjacent SSTables. As a result, we have multiple contiguous key ranges in L 1 . (2) Column compaction starts from the first key range in L 1 . It selects a key range in L 1 as the compaction key range. (3) In the compactor, victim KV items within the compaction key range are picked concurrently in multiple rows. Specifically, assuming N RowTables in the compactor, k threads work in parallel to fetch keys within the compaction key range. Each thread in charge of N/k RowTables. We maintain an adequate degree of concurrent accesses on NVMs with k = 8. (4) If the amount of data within this key range is under the lower bound of compaction, the next key range in L 1 joins. The k threads keep forward in N sorted arrays (i.e., the metadata of the RowTables) fetching KV items within the new key range. This key range expansion process continues until the amount of compaction data reaches a size between the lower bound and the upper bound (i.e., <ref type="bibr" target="#b0">1</ref> 2 AF × S sst and AF × S sst respectively). The two bounds guarantee the adequate overhead of a column compaction. (5) Then a column in the compactor is logically formed, i.e., KV items in N RowTables that fall in the compaction key range make up a logical column. (6) Data in the column are merged and sorted with the overlapped SSTables of L 1 in memory. <ref type="formula">(7)</ref> Finally, the regenerated SSTables are written back to L 1 on SSDs. Column compaction continues between the next key range of L 1 and the next column in the compactor. The key ranges of column compaction rotate in the whole key space to keep LSM-trees balanced.</p><p>We show an example of column compaction in <ref type="figure" target="#fig_7">Figure 8</ref>. First, MatrixKV picks the SSTable with key range 0-3 in L 1 as the candidate compaction SSTable. Then, we search the metadata arrays of the four RowTables. If the amount of compaction data within key range 0-3 is under the lower bound, the next key range (i.e., key range 3-5) joins to form a larger key range 0-5. If the amount of compaction data is still beneath the lower bound, the next key range 5-8 joins.</p><p>Once the compaction data is larger than the lower bound, a logical column is formed for the compaction. The first column compaction compacts the column at the key range of 0-8 with the first two SSTables in L 1 .</p><p>In general, column compaction first selects a specific key range from L 1 , and then compacts with the column in the compactor that shares the same key range. Comparing to the original all-to-all compaction between L 0 and L 1 , column compaction compacts at a much smaller key range with a limited amount of data. Consequently, the fine-grained column compaction shortens the compaction duration, resulting in reduced write stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reducing LSM-tree Depth</head><p>In conventional LSM-trees, the size limit of each level grows by an amplification factor of AF = 10. The number of levels in an LSM-tree increase with the amount of data in the database. Since compacting an SSTable to a higher level results in a write amplification factor of AF, the overall WA increases with the number of levels (n) in the LSM-tree, i.e., WA=n*AF <ref type="bibr" target="#b35">[36]</ref>. Hence, the other design principle of MatrixKV is to reduce the depth of LSM-trees to mitigate WA. MatrixKV reduces the number of LSM-tree levels by increasing the size limit of each level at a fixed ratio making the AF of adjacent levels unchanged. As a result, for compactions from L 1 and higher levels, the WA of compacting an SSTable to the next level remains the same AF but the overall WA is reduced with due to fewer levels.</p><p>Flattening conventional LSM-trees with wider levels brings two negative effects. First, since the enlarged L 0 has more SSTables that overlap with key ranges, the amount of data in each L 0 -L 1 compaction increases significantly, which not only adds the compaction overhead but also lengthens the duration of write stalls. Second, traversing the larger unsorted </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cross-row Hint Search</head><p>In this section, we discuss solutions for improving the read efficiency in the matrix container. In the L 0 of MatrixKV, each RowTable is sorted and different RowTables are overlapped with key ranges. Building Bloom filters for each table is a possible solution for reducing search overheads. However, it brings costs on the building process and exhibits no benefit to range scans. To provide adequate read and scan performances for MatrixKV, we build cross-row hint searches. Constructing cross-row hints: When we build a RowTable for the receiver of the matrix container, we add a forward pointer for each element in the sorted array of metadata ( <ref type="figure" target="#fig_6">Figure 7)</ref>. Specifically, for a key x in RowTable i, the forward pointer indexes the key y in the preceding RowTable i −1, where the key y is the first key not less than x (i.e., y ≥ x). These forward pointers provide hints to logically sort all keys in different rows, similar to the fractional cascading <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b53">53]</ref>. Since each forward pointer only records the array index of the preceding RowTable, the size of a forward pointer is only 4 bytes. Thus, the storage overhead is very small.</p><p>Search process in the matrix container: A search process starts from the latest arrived RowTable i. If the key range of RowTable i does not overlap the target key, we skip to its preceding RowTable i − 1. Else, we binary search RowTable i to find the key range (i.e., bounded by two adjacent keys) where the target key resides. With the forward pointers, we can narrow the search region in prior RowTables, i − 1, i − 2, . . . continually until the key is found. As a result, there is no need to traverse all tables entirely to get a key or scan a key range.</p><p>Cross-row hint search improves the read efficiency of L 0 by significantly reducing the number of tables and elements involved in a search process.</p><p>An example of cross-row hint search is shown in <ref type="figure" target="#fig_8">Figure 9</ref>. The blue arrows show the forward pointers providing crossrow hints. Suppose we want to fetch a target key k = 12 in the matrix container, we first binary search RowTable 3 to get a narrowed key range of key=10 to key=23. Then their hints lead us to the key 13 and 30 in RowTable 2 (the red arrows). The preceding key is added into the search region when the target key is not included in the key range of the two hint keys. Next, we binary search between key=8 and key=30. Failing to find the target key, we move to the prior RowTable 1, then RowTable 0, with the forward pointers. Finally, the target key 12 is obtained in RowTable 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>We implement MatrixKV based on the popular KV engine RocksDB <ref type="bibr" target="#b23">[24]</ref> from Facebook. The LOC on top of RocksDB is 4117 lines 1 . As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, MatrixKV accesses NVMs via the PMDK library and accesses SSDs via the POSIX API. The persistent memory development kit (PMDK) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b60">60]</ref> is a library based on the direct access feature (DAX). Next, we briefly introduce the write and read processes and the mechanism for consistency as follows.</p><p>Write: (1) Write requests from users are inserted into a write-ahead log on NVMs to prevent data loss from system failures. (2) Data are batched in DRAM, forming MemTable and immutable MemTable. (3) The immutable MemTable is flushed to NVM and stored as a RowTable in the receiver of the matrix container. (4) The receiver turns into the compactor logically if the number of RowTables reaches a size limit (e.g., 60% of the matrix container) and the compactor is empty. This role change has no real data migrations. (5) Data in the compactor is column compacted with SSTables in L 1 column by column. In the meantime, a new receiver receives flushed MemTables. (6) In SSDs, SSTables are merged to higher levels via conventional compactions as RocksDB does. Compared to RocksDB, MatrixKV is completely different from step 3 through step 5.</p><p>Read: MatrixKV processes read requests in the same way as RocksDB. The read thread searches with the priority of DRAM&gt;NVMs&gt;SSDs. In NVMs, the cross-row hint search contributes to faster searches among different RowTables of L 0 . The read performance can be further improved by concurrently searching in different storage devices <ref type="bibr" target="#b30">[31]</ref>.</p><p>Consistency: Data structures in NVM must avoid inconsistency caused by system failures <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b58">58]</ref>. For MatrixKV, writes/updates for NVM only happen in two processes, flush and column compaction. For flush, immutable MemTables flushed from DRAM are organized as RowTables and written to NVM in rows. If a failure occurs in the middle of writing a RowTable, MatrixKV can re-process all the transactions that were recorded in the write-ahead log. For column compaction, MatrixKV needs to update the state of RowTables after each column compaction. To achieve consistency and reliability with low overhead, MatrixKV adopts the versioning mechanism of RocksDB. RocksDB records the database state with a manifest file. The operations of compaction are persisted in the manifest file as version changes.</p><p>If the system crashes during compaction, the database goes back to its last consistent state with versioning. MatrixKV adds the state of RowTables into the manifest file, i.e., the offset of the first key, the number of keys, the file size, and the metadata size, etc. MatrixKV uses lazy deletion to guarantee that stale columns invalidated by column compactions are not deleted until a consistent new version is completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In this section, we run extensive experiments to demonstrate the key accomplishments of MatrixKV. (1) MatrixKV obtains better performance on various types of workloads and achieves lower tail latencies ( § 5.2). (2) The performance benefits of MatrixKV come from reducing write stalls and write amplification by its key enabling techniques ( § 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>All experiments are run on a test machine with two Genuine Intel(R) 2.20GHz 24-core processors and 32 GB of memory. The kernel version is 64-bit Linux 4.13.9 and the operating system in use is Fedora 27. The experiments use two storage devices, an 800 GB Intel SSDSC2BB800G7 SSD and 256 GB NVMs of two 128 GB Intel Optane DC PMM <ref type="bibr" target="#b27">[28]</ref>. <ref type="table" target="#tab_0">Table 1</ref> lists their maximum single-thread bandwidth, evaluated with the versatile storage benchmark tool FIO. We mainly compare MatrixKV with NoveLSM and RocksDB (including RocksDB-SSD and RocksDB-L0-NVM). RocksDB-SSD represents the conventional RocksDB on a DRAM-SSD hierarchy. The other three KV stores are for systems with DRAM-NVM-SSD storage. They use 8 GB NVM to be consistent with the setup in NoveLSM's paper and force the majority of the 80 GB test data to be flushed to SSDs. RocksDB-L0-NVM simply enlarges L 0 into 8 GB and stores it in NVM. MatrixKV reorganizes the 8 GB L 0 in NVM and enlarges the L 1 in SSDs into the same 8 GB.</p><p>NoveLSM employs NVM to store two MemTables (2*4 GB).</p><p>Test results from this configuration can also demonstrate that MatrixKV achieves system performance improvement with the economical use of NVMs. Finally, we evaluate PebblesDB and SILK for systems with DRAM-NVM storage since they are the representative studies on LSM-tree improvement but are not originally designed for systems with multi-tier storage. Unless specified otherwise, the evaluated KV stores assume the default configuration of RocksDB, i.e., 64 MB MemTables/SSTables, 256 MB L 1 size, and AF of 10. The default key-value sizes are 16 bytes and 4 KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overall performance evaluation</head><p>In this section, we first evaluate the overall performance of the four KV stores using db_bench, the micro-benchmark released with RocksDB. Then, we evaluate the performance of each KV store with the YCSB macro-benchmarks <ref type="bibr" target="#b14">[15]</ref>.</p><p>Write performance: We evaluate the random write performance by inserting KV items totaling 80 GB in a uniformly distributed random order. <ref type="figure" target="#fig_0">Figure 10(a)</ref> shows the random write throughput of four KV stores as a function of value size. The performance difference between RocksDB-SSD and RocksDB-L0-NVM suggests that simply placing L 0 in NVM brings about an average improvement of 65%. We use RocksDB-L0-NVM and NoveLSM as baselines of our evaluation. MatrixKV improves random write throughput over RocksDB-L0-NVM and NoveLSM in all value sizes. Specifically, MatrixKV's throughput improvement over RocksDB-L0-NVM ranges from 1.86× to 3.61×, and MatrixKV's throughput improvement over NoveLSM ranges from 1.72× to 2.61. Taking the commonly used value size of 4 KB as an example, MatrixKV outperforms RocksDB-L0-NVM and NoveLSM by 3.6× and 2.6× respectively. RocksDB-L0-NVM delivers relatively poor performance since putting L 0 in NVM only brings a marginal improvement. NoveLSM uses a large mutable MemTable in NVM to handle a portion of update requests thus slightly reducing WA. However, for both RocksDB and NoveLSM, the root causes of write stalls and WA remain unaddressed, i.e., the all-to-all L 0 -L 1 compaction and the deepened depth of LSM-trees.</p><p>We evaluate sequential write performance by inserting a total of 80 GB KV items in sequential order. From the test results in <ref type="figure" target="#fig_0">Figure 10(b)</ref>, we make three main observations. First, sequential write throughput is higher than random write throughput for the four KV stores as sequential writes incur no compaction. Second, RocksDB-SSD performs the best since the other three KV stores have an extra NVM tier, requiring data migration from NVMs to SSDs. Three, MatrixKV and RocksDB-L0-NVM have better sequential write throughput than NoveLSM since contracting RowTable/SSTables in NVMs is cheaper than updating the skip list of NoveLSM's large mutable MemTable.</p><p>Read performance: Random/sequential read performances are evaluated by reading one million KV items from the 80 GB randomly loaded database. To obtain the read performance free from the impact of compactions, we start the reading test after the tree becomes well-balanced. <ref type="figure" target="#fig_0">Figure 10</ref>(c) and (d) show the test results of random reads and sequential reads. Since NVM only accommodates 10% of the dataset, the read performance in SSDs dominates the overall read performance. Besides, since a balanced tree is well-sorted from L 1 to L n on SSDs, the four KV stores exhibit similar read throughputs. MatrixKV does not degrade read performance and even has a slight advantage in sequential reads for two reasons. First, the cross-row hint search reduces the search overhead of the enlarged L 0 . Second, MatrixKV has fewer LSM-tree levels, resulting in less search overhead on SSDs.</p><p>Macro-benchmarks: Now we evaluate four KV stores with YCSB <ref type="bibr" target="#b14">[15]</ref>, a widely used macro-benchmark suite delivered by Yahoo!. We first write an 80 GB dataset with 4KB values for loading, then evaluate workload A-F with one million KV items respectively. From the test results shown in <ref type="figure" target="#fig_0">Figure 11</ref>, we draw three main conclusions. First, MatrixKV gets the most advantage from write/load dominated workloads, i.e., load, and workload A and F. MatrixKV is 3.29× and 2.37× faster than RocksDB-L0-NVM and NoveLSM on the load workload (i.e., random write). Second, MatrixKV maintains adequate performance over read-dominated workloads, i.e., workloads B to E. Third, NoveLSM and MatrixKV behave better on workload D due to the latest distribution, where they both hit more in NVMs and thus MatrixKV can  <ref type="figure" target="#fig_0">Figure 11</ref>: Macro-benchmarks. The y-axis shows the throughput of each KV store normalized to RocksDB-SSD. The number on each bar indicates the throughput in ops/s. benefit more from cross-row hints.</p><p>Tail latency: Tail latency is especially important for LSMtree based KV stores, since they are widely deployed in production environments to provide services for write-heavy workloads and latency-critical applications. We evaluate the tail latency with the same methodology used in SILK <ref type="bibr" target="#b5">[6]</ref>, i.e., using the YCSB-A workload and setting request arrival rate at around 20K requests/s. <ref type="table" target="#tab_2">Table 2</ref> shows the average, 90 th , 99 th , and 99.9 th percentile latencies of four key-value stores. MatrixKV significantly reduces latencies in all cases. The 99 th percentile latency of MatrixKV is 27×, 5×, and 1.9× lower than RocksDB-SSD, NoveLSM, and RocksDB-L0-NVM respectively. The test results demonstrate that by reducing write stalls and WA, MatrixKV improves the quality of user experience with much lower tail latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Gain Analysis</head><p>To understand MatrixKV's performance improvement over random write workloads, we investigate the main challenges of LSM-trees ( § 5.3.1) and the key enabling techniques of MatrixKV ( § 5.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Main Challenges</head><p>In this section, we demonstrate that MatrixKV does address the main challenges of LSM-trees, i.e., write stalls and WA.</p><p>Write Stalls: We record the throughput of the four KV stores in every ten seconds during their 80 GB random write process (similar to <ref type="figure" target="#fig_1">Figures 2 and 4)</ref> to visualize write stalls. From the performance variances shown in <ref type="figure" target="#fig_0">Figure 12</ref>, we draw three observations. (1) MatrixKV takes a shorter time to process the same 80GB random write since it has higher random write throughput than other KV stores (as demonstrated in § 5.2). (2) Both RocksDB and NoveLSM suffer from write stalls due to the expensive L 0 -L 1 compaction. NoveLSM takes longer to process a L 0 -L 1 compaction because L 0 maintains large MemTables flushed from NVMs. Comparing to RocksDB-SSD, RocksDB-L0-NVM has lower throughput during write stalls, which means that it blocks foreground requests more severely because of the enlarged L 0 . (3) MatrixKV achieves the most stable performance. The reason is that we reduce write stalls by the fine-grained column compaction which guarantees a small amount of data processed in each L 0 -L 1 compaction. Write Amplification: We measure the WA of four systems on the same experiment of randomly writing 80 GB dataset. <ref type="figure" target="#fig_0">Figure 13</ref> shows the WA factor measured by the ratio of the amount of data written to SSDs and the amount of data coming from users. The WA of MatrixKV, NoveLSM, and RocksDB-L0-NVM are 2.56×, 1.83×, and 1.99× lower than RocksDB-SSD respectively. MatrixKV has the smallest WA since it reduces the number of compactions by lowering the depth of LSM-trees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">MatrixKV Enabling Techniques</head><p>Column compaction: To demonstrate the efficiency of column compaction, we record the amount of data involved, the duration of every L 0 -L 1 compaction for four KV stores in the same 80 GB random write experiment. As shown in <ref type="figure" target="#fig_0">Figure 14</ref>  written a total of 153 GB data. RocksDB-SSD processes 52 compactions, each 3.1 GB on average, written a total of 157 GB data. MatrixKV processes more fine-grained L 0 -L 1 compactions, where each has the least amount of data and the shortest compaction duration. As a result, column compactions have only a negligible influence on foreground requests and finally significantly reduce write stalls. NoveLSM actually exacerbates write stalls since the enlarged MemTables flushed from NVM significantly increase the amount of data processed in each L 0 -L 1 compaction. Overall compaction efficiency: We further record the overall compaction behaviors of four KV stores by recording the amount of data for every compaction during the random write experiment. From the test results shown in <ref type="figure" target="#fig_0">Figure 15</ref>, we draw four observations. First, MatrixKV has the smallest number of compactions, attributed to the reduced LSM-tree depth. Second, all compactions in MatrixKV process similar amount of data since we reduce the amount of compaction data on L 0 -L 1 and does not increase that on other levels. Third, NoveLSM and RocksDB-L0-NVM have fewer compactions than RocksDB-SSD. The reasons are: (1) NoveLSM uses large mutable MemTables to serve more write requests and absorb a portion of update requests, and (2) RocksDB-L0-NVM has an 8 GB L 0 in NVM to store more data. Fourth, the substantial amount of compaction data in NoveLSM and RocksDB stems from the L 0 -L 1 compaction.</p><p>Reducing LSM-tree depth: To evaluate the technique of flattening LSM-trees, we change level sizes for both RocksDB Figure 16: Reducing LSM-tree depth. The y-axis shows random write throughputs of RocksDB and MatrixKV when L 1 is 256 MB/8 GB. The table below shows the data distribution among levels (in GB).</p><p>and MatrixKV. The first configuration is L 1 = 256MB (the default L 1 size of RocksDB). The second configuration is L 1 = 8GB. The following levels exponentially increased at the ratio of AF=10. <ref type="figure" target="#fig_0">Figure 16</ref> shows the throughput of randomly writing an 80 GB dataset. The table under the figure shows the data distribution on different levels after balancing LSM-trees. The test results demonstrate that both RocksDB and MatrixKV reduce the number of levels by enlarging level sizes, i.e., from 5 to 3. However, they exert opposite influences on system performance. RocksDB-SSD and RocksDB-L0-NVM reduce their random write throughputs by 3× and 1.5× respectively as level sizes increase. The reason is that the enlarged L 1 significantly increases the amount of compaction data between L 0 and L 1 . RocksDB-L0-NVM is slightly better than RocksDB-SSD since it puts L 0 in NVMs. For MatrixKV, the throughput increases 25% since the fine granularity column compaction is independent of level sizes. Furthermore, the MatrixKV with 256 MB L 1 shows the performance improvement of only addressing write stalls. Cross-row hint search: To evaluate the technique of crossrow hint search, we first randomly write an 8 GB dataset with 4 KB value size to fill the L 0 in NVMs for MatrixKV and RocksDB-L0-NVM. Then we search for one million KV items from NVMs in uniformly random order. This experiment makes NVMs accommodate 100% of the dataset to fully reflect the efficiency of cross-row hint searches. The random read throughput of RocksDB-L0-NVM and MatrixKV are 9 MB/s and 157.9 MB/s respectively. Hence, compared to simply placing L 0 in NVMs, the cross-row hint search improves the read efficiency by 17.5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Extended Comparisons on NVMs</head><p>To further verify that MatrixKV's benefits are not solely due to the use of fast NVMs, we evaluate more KV stores on the DRAM-NVM hierarchy, i.e., RocksDB, NoveLSM, PebblesDB, SILK, and MatrixKV, where DRAM stores MemTables, and all other components are stored on NVMs.   Throughput: <ref type="figure" target="#fig_0">Figure 17</ref> shows the performance for randomly writing an 80 GB dataset. MatrixKV achieves the best performance among all KV stores. It demonstrates that the enabling techniques of MatrixKV are appropriate for NVM devices. Using NVM as a fast block device, PebblesDB does not show much improvement over RocksDB. SILK is slightly worse than RocksDB since its design strategies have limited advantages over intensive writes.</p><p>Tail latency: Tail latencies are evaluated with YCSB-A workload as in § 5.2. Since NVM has a significantly better performance than SSDs, we speed up the requests from clients (60K requests/s). Test results in <ref type="table" target="#tab_5">Table 3</ref> show that with the persistent storage of NVMs most KV stores provide adequate tail latencies. However, MatrixKV still achieves the shortest tail latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present MatrixKV, a stable low-latency keyvalue store based on LSM-trees. MatrixKV is designed for systems with multi-tier DRAM-NVM-SSD storage. By lifting the L 0 to NVM, managing it with the matrix container, and compacting L 0 and L 1 with the fine granularity column compaction, MatrixKV reduces write stalls. By flattening the LSM-trees, MatrixKV mitigates write amplification. MatrixKV also guarantees adequate read performance with crossrow hint searches. MatrixKV is implemented on a real system based on RocksDB. Evaluation results demonstrate that MatrixKV significantly reduces write stalls and achieves much better system performance than RocksDB and NoveLSM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The structure of RocksDB and NoveLSM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RocksDB's random write performance and L 0 -L 1 compactions. The blue line shows the random write throughput measured in every 10 seconds. The green line shows the average throughput. Each red line represents the duration and amount of data processed in a L 0 -L 1 compaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The CDF of latencies of the 80 GB write requests.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: NoveLSM's random write performance and L 0 -L 1 compactions. Comparing to RocksDB in Figure 2, the average period of write stalls is increased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MatrixKV's architectural overview. MatrixKV is a KV store for systems consisting of DRAM, NVMs, and SSDs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Structure of matrix container. The receiver absorbs flushed MemTables, one per row. Each row is reorganized as a RowTable. The compactor merges L 0 with L 1 in fine-grained key ranges, one range at a time, referred to as column compaction. In Process A, the receiver becomes the compactor once RowTables fill its capacity. In Process B, each column compaction frees a column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: RowTable and conventional SSTable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Column compaction: an example. There are 4 RowTables in the compactor. Each circle represents an SSTable on L 1 . Columns are logically divided (red dashed lines) according to the key range of compaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Cross-row hint search. This figure shows an example of searching the target key (k = 12) with forward pointers of each array element.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance on Micro-benchmarks with different value sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: Throughput fluctuation as a function of time. The random write performance fluctuates where the troughs on curves signify the occurrences of possible write stalls.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: The L 0 -L 1 compaction. Each line segment indicates an L 0 -L 1 compaction. The y-axis shows the amount of data involved in the compaction and the length along x-axis shows the duration of the compaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Compaction analysis. This figure shows the amount of data of every individual compaction during the 80 GB random write.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Throughput on NVM based KV stores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : FIO 4 KB read and write bandwidth</head><label>1</label><figDesc></figDesc><table>SSDSC2BB800G7 Optane DC PMM 
Rnd write 68 MB/s 
1363 MB/s 
Rnd read 
250 MB/s 
2346 MB/s 
Seq write 
354 MB/s 
1444 MB/s 
Seq read 
445 MB/s 
2567 MB/s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Tail Latency</head><label>2</label><figDesc></figDesc><table>Latency (us) 
avg. 90% 
99% 99.9% 
RocksDB-SSD 
974 
566 
11055 
17983 
NoveLSM 
450 
317 
2080 
2169 
RocksDB-L0-NVM 
477 
528 
786 
1112 
MatrixKV 
263 
247 
405 
663 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Tail latency on NVM-based KV stores 

Latency (us) avg. 
90% 
99% 99.9% 
RocksDB 
385 
523 
701 
864 
NoveLSM 
377 
250 
808 
917 
SILK 
351 
445 
575 
747 
PebblesDB 
335 1103 1406 
1643 
MatrixKV 
209 
310 
412 
547 

</table></figure>

			<note place="foot" n="1"> MatrixKV source code is publicly available at https://github.com/ PDS-Lab/MatrixKV.</note>

			<note place="foot" n="26"> 2020 USENIX Annual Technical Conference USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>We thank our shepherd Patrick Stuedi, the anonymous reviewers, and Damien Le Moal for their insightful comments and guidance. We appreciate Yun Liu, Hua Jiang, and <ref type="table">Tao</ref>  </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Persistent memory development kit</title>
		<ptr target="https://github.com/pmem/pmdk" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fawn: A fast array of wimpy nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles (SOSP 09)</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles (SOSP 09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Operating systems: Three easy pieces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Remzi H Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arpacidusseau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Arpaci-Dusseau Books Wisconsin</publisher>
			<biblScope unit="volume">151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How to build a nonvolatile memory database management system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Arulraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1753" to="1758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid hbase: Leveraging flash ssds to improve cost per throughput of hbase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avani</forename><surname>Nandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Sehgal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Management of Data</title>
		<meeting>the 18th International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Silk: Preventing latency spikes in log-structured merge key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Balmau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravishankar</forename><surname>Chandhiramoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Didona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ATC 19</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flodb: Unlocking memory in persistent key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Balmau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachid</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Trigonakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Zablotchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth European Conference on Computer Systems</title>
		<meeting>the Twelfth European Conference on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="80" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bankshot: Caching slow storage in fast nonvolatile memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meenakshi</forename><surname>Sundaram Bhaskaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Interactions of NVM/FLASH with Operating Systems and Workloads</title>
		<meeting>the 1st Workshop on Interactions of NVM/FLASH with Operating Systems and Workloads</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overview of candidate device technologies for storage-class memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bülent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kurdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><forename type="middle">Hon</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailash</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="449" to="464" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>4.5</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Moneta: A high-performance storage array architecture for nextgeneration, non-volatile memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arup</forename><surname>Adrian M Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Todor I Mollow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fractional cascading: I. a data structuring technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Chazelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="133" to="162" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nv-heaps: making persistent objects fast and safe with next-generation, non-volatile memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Coburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameen</forename><surname>Akel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Grupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Jhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 11)</title>
		<meeting>the Sixteenth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="105" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fine-grain checkpointing with in-cacheline logging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nachshon</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hillel</forename><surname>David T Aksun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James R</forename><surname>Avni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 19)</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 19)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="441" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Better i/o through byte-addressable, persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Condit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derrick</forename><surname>Coetzee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="133" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with ycsb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing (SOCC 10)</title>
		<meeting>the ACM Symposium on Cloud Computing (SOCC 10)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The case for safe ram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Copeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc G</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="327" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Monkey: Optimal navigable key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manos</forename><surname>Athanassoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="79" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dostoevsky: Better spacetime trade-offs for lsm-tree based key-value stores via adaptive removal of superfluous merging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Management of Data</title>
		<meeting>the 2018 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="505" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The log-structured mergebush &amp; the wacky continuum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data</title>
		<meeting>the 2019 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="449" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Skimpystash: Ram space skimpy key-value store on flash-based storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biplob</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGMOD International Conference on Management of data</title>
		<meeting>the 2011 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Latest advances and future prospects of stt-ram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Driskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Non-Volatile Memories Workshop</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">System software for persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Subramanya R Dulloor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Keshavamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jackson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems</title>
		<meeting>the Ninth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reducing dram footprint with nvm in facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darryl</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Islam</forename><surname>Abdelrahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Axboe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siying</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth EuroSys Conference</title>
		<meeting>the Thirteenth EuroSys Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rocksdb, a persistent key-value store for fast storage enviroments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Facebook</surname></persName>
		</author>
		<ptr target="http://rocksdb.org/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leveldb</surname></persName>
		</author>
		<ptr target="https://github.com/google/leveldb" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Analysis of hdfs under hbase: a facebook messages case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruba</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siying</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amitanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyin</forename><surname>Aiyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi H Arpaci-Dusseau</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>FAST 14</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Endurable transient inconsistency in byte-addressable persistent b+-tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deukyeon</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wook-Hee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th USENIX Conference on File and Storage Technologies (FAST 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
	<note>Youjip Won, and Beomseok Nam</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Basic performance measurements of the intel optane dc persistent memory module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirsaman</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><forename type="middle">Joon</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Subramanya R Dulloor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05714</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Incremental organization for data recording and warehousing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hv Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Sudarshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanneganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Slm-db: Singlelevel key-value store with persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olzhas</forename><surname>Kaiyrakhmet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyi</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Conference on File and Storage Technologies (FAST 19)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="191" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau. Redesigning lsms for nonvolatile memory with novelsm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Sudarsun Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavrilovska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ATC 18</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phase change memory in enterprise storage systems: silver bullet or snake oil?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangeetha</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Dickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="89" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Resolving journaling of journal anomaly in android i/o: multi-version b-tree with lazy split</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wook-Hee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomseok</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongil</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjip</forename><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="273" to="285" />
		</imprint>
	</monogr>
	<note>FAST 14</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-performance transactions for persistent memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aasheesh</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Pelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas F</forename><surname>Peter M Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentyFirst International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 16)</title>
		<meeting>the TwentyFirst International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 16)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="399" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cassandra: A decentralized structured storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">WiscKey: separating keys from values in ssdconscious storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Lanyue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pillai Thanumalayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename><surname>Sankaranarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andrea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename><surname>Remzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>FAST 16</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kvell: the design and implementation of a fast persistent key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Baptiste Lepers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Balmau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Nvmrocks: Rocksdb on non-volatile memory systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siying</forename><surname>Dong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Elasticbf: elastic bloom filter with hotness awareness for boosting read performance in large key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinlong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 USENIXAnnual Technical Conference (USENIX ATC 19)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="739" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Towards accurate and fast evaluation of multi-stage log-structured designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX Conference on File and Storage Technologies (FAST 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="149" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Silt: A memory-efficient, highperformance key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>David G Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23th ACM Symposium on Operating Systems Principles (SOSP 11)</title>
		<meeting>the 23th ACM Symposium on Operating Systems Principles (SOSP 11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dudetm: Building durable transactions with decoupling for persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the TwentySecond International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 17)</title>
		<meeting>the TwentySecond International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 17)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="329" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">On performance stability in lsm-based storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09667</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>extended version</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Triad: creating synergies between memory, disk and log in log structured key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didona</forename><surname>Balmau Oana Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guerraoui</forename><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zwaenepoel</forename><surname>Rachid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Willy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arora</forename><surname>Huapeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gupta</forename><surname>Aashray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konka</forename><surname>Karan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Nvmkv: A scalable, lightweight, ftl-aware key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Marmol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Sundararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisha</forename><surname>Talagala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Rangaswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 USENIX Annual Technical Conference (ATC 15)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sifrdb: A unified solution for write-optimized key-value stores in large datacenter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="477" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Optimizing key-value stores for hybrid storage architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilmann</forename><surname>Rabl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sadoghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Arno</forename><surname>Jacobsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 24th Annual International Conference on Computer Science and Software Engineering</title>
		<meeting>24th Annual International Conference on Computer Science and Software Engineering</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="355" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Consistent, durable, and safe memory management for byte-addressable non volatile main memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulian</forename><surname>Moraru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niraj</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Parthasarathy Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First ACM SIGOPS Conference on Timely Results in Operating Systems</title>
		<meeting>the First ACM SIGOPS Conference on Timely Results in Operating Systems</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The log-structured merge-tree (lsm-tree)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Oneil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Oneil</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="385" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A log-structured history data access method (lham)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPTS, page 0. Citeseer</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pebblesdb: Building key-value stores using fragmented log-structured merge trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pandian</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Kadekodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chidambaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ittai</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles</title>
		<meeting>the 26th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="497" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Phase-change random access memory: A scalable technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Raoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">W</forename><surname>Burr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">T</forename><surname>Breitwisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-C</forename><surname>Rettner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Shelby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Salinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S-H</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>4.5</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">blsm: A general purpose log structured merge tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2012 ACM SIGMOD International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>SIGMOD 12</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Building workload-independent storage with vt-trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravikant</forename><surname>Spillane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binesh</forename><surname>Malpani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Seyster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Conference on File and Storage Technologies (FAST 13)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The missing memristor found</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dmitri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Strukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Stanley</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="issue">7191</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Lsbm-tree: Re-enabling buffer caching in data management for mixed reads and writes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejun</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Terry</surname></persName>
		</author>
		<title level="m">Transactions and Scalability in Cloud Databases-Can&apos;t We Have Both? USENIX Association</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">envy: a nonvolatile, main memory storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 94)</title>
		<meeting>the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 94)</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="86" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Lsmtrie: An lsm-tree-based ultra-large key-value store for small data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingbo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference</title>
		<meeting>the USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ATC 15</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Finding and fixing performance pathologies in persistent memory software stacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirsaman</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Swanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 19)</title>
		<meeting>the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 19)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="427" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nv-tree: Reducing consistency cost for nvm-based single level systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingsong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chundong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khai Leong</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Conference on File and Storage Technologies (FAST 15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A light-weight compaction tree to reduce i/o amplification toward efficient key-value stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiguang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Massive Storage Systems and Technology (MSST 2017</title>
		<meeting>the 33rd International Conference on Massive Storage Systems and Technology (MSST 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Geardb: A gc-free key-value store on hm-smr drives with gear compaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiguang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Conference on File and Storage Technologies (FAST 19)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="159" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Building an efficient put-intensive key-value store with skip-tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinliang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="961" to="973" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Write-optimized and high-performance hashing index scheme for persistent memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="461" to="476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
