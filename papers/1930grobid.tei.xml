<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">deTector: a Topology-aware Monitoring System for Data Center Networks deTector: a Topology-aware Monitoring System for Data Center Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 12-14, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanxiong</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengchen</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongpeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Jiaotong University</orgName>
								<address>
									<addrLine>Chuan Wu</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">Microsoft Research</orgName>
								<address>
									<addrLine>Chengchen Hu, Xi&apos;an Jiaotong University; Zongpeng Li</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Calgary</orgName>
								<orgName type="institution" key="instit2">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Jiaotong University</orgName>
								<orgName type="institution" key="instit2">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Jiaotong University</orgName>
								<orgName type="institution" key="instit3">University of Calgary</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">deTector: a Topology-aware Monitoring System for Data Center Networks deTector: a Topology-aware Monitoring System for Data Center Networks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2017 USENIX Annual Technical Conference (USENIX ATC &apos;17)</title>
						<meeting>the 2017 USENIX Annual Technical Conference (USENIX ATC &apos;17) <address><addrLine>Santa Clara, CA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 12-14, 2017</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 2017 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc17/technical-sessions/presentation/peng</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Troubleshooting network performance issues is a challenging task especially in large-scale data center networks. This paper presents deTector, a network monitoring system that is able to detect and localize network failures (manifested mainly by packet losses) accurately in near real time while minimizing the monitoring overhead. deTector achieves this goal by tightly coupling detection and localization and carefully selecting probe paths so that packet losses can be localized only according to end-to-end observations without the help of additional tools (e.g., tracert). In particular, we quantify the desirable properties of the matrix of probe paths, i.e., coverage and identifiability, and leverage an efficient greedy algorithm with a good approximation ratio and fast speed to select probe paths. We also propose a loss localization method according to loss patterns in a data center network. Our algorithm analysis, experimental evaluation on a Fattree testbed and supplementary large-scale simulation validate the scalability, feasibility and effectiveness of deTector.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A variety of services are hosted in large-scale data centers today, e.g., search engines, social networks and file sharing. To support these services with high quality, data center networks (DCNs) are carefully designed to efficiently connect thousands of network devices together, e.g., a 64-ary Fattree <ref type="bibr" target="#b4">[9]</ref> DCN has more than 60,000 servers and 5,000 switches. However, due to the large network scale, frequent upgrades and management complexity, failures in DCNs are the norm rather than the exception <ref type="bibr" target="#b16">[21]</ref>, such as routing misconfigurations, link flaps, etc. Among these failures, those leading to userperceived performance issues (e.g., packet losses, latency spikes) are among the first priority to be detected and eliminated promptly <ref type="bibr" target="#b22">[27,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b16">21]</ref>, in order to maintain high quality of service (QoS) for users (e.g., no more than a few minutes of downtime per month <ref type="bibr" target="#b16">[21]</ref>) and to increase revenue for operators. Rapid failure recovery is not possible without a good network monitoring system. There have been a number of systems proposed in the past few years <ref type="bibr" target="#b31">[36,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b32">37,</ref><ref type="bibr" target="#b44">48]</ref>. Several limitations still exist in these systems that prohibit fast failure detection and localization.</p><p>First, existing monitoring systems may fail to detect one type of failures or another. Traditional passive monitoring approaches, such as querying the device counter via SNMP or retrieving information via device CLI when users have perceived some issues, can detect clean failures such as link down, line card malfunctions. However, gray failures may occur, i.e., faults not detected or ignored by the device, or malfunctioning not properly reported by the device due to some bugs <ref type="bibr" target="#b32">[37]</ref>. Active monitoring systems (e.g., Pingmesh <ref type="bibr" target="#b21">[26]</ref>, NetNORAD <ref type="bibr" target="#b32">[37]</ref>) can detect such failures by sending end-to-end probes, but they may fail to capture failures that cause low rate losses, due to ECMP in data center ( §2).</p><p>Second, probe systems such as Pingmesh and NetNO-RAD inject probes between each pair of servers without selection, which may introduce too much bandwidth overhead. In addition, they typically treat the whole DCN as a black box, and hence require many probes to cover all parallel paths between any server pair with high probability.</p><p>Third, failures in the network can be reported in these active monitoring systems, but the exact failure locations cannot be pinpointed automatically. The network operator typically learns a suspected source-destination server pair once packet loss happens. Then she/he needs to resort to additional tools such as tracert to verify the issue and locate the faulty spot. However, it may be difficult to play back the issues due to transient failures. Hence this diagnosis approach (i.e., separation of detection and localization) may take several hours or even days to pinpoint the fault spot <ref type="bibr" target="#b16">[21]</ref>, yet ideally the failures should be repaired as fast as possible before users complain.</p><p>A desirable monitoring system in a DCN should meet three objectives: exhaustive failure detection (i.e., detecting as many types of losses as possible), low overhead and real-time failure localization. In this paper, we seek to investigate the following question: if we are aware of the network topology of a DCN, can we design a much better network monitoring system that achieves all these goals? Our answer is deTector, a topology-aware network monitoring system that we design, implement and evaluate following the three design objectives. The secret weapon of deTector is a carefully designed probe matrix ( §4), which achieves good link coverage, identifiability and evenness. deTector is designed to detect and localize network failures manifested by user-perceptible performance problems such as packet losses and latency spikes in large-scale data centers. We mainly focus on packet loss in this paper, but deTector can also handle latency issues by treating a round trip time (RTT) larger than a threshold as a packet loss. Throughout the paper, we use "failure localization", "fault localization" and "loss localization" interchangeably. Specifically, we make the following contributions in developing deTector.</p><p>񮽙 As compared to the existing active monitoring systems adopting end-to-end probes (e.g., Pingmesh <ref type="bibr" target="#b21">[26]</ref>, NetNORAD <ref type="bibr" target="#b32">[37]</ref>), we treat each switch instead of the whole network as a blackbox, i.e., our system requires the knowledge of the network topology and routing protocols in a DCN (i.e., topology-aware) and we use source routing to control the probing path. In order to achieve real-time failure localization, we couple detection and localization closely and only rely on end-to-end measurements to localize failures without the help of other tools (e.g., fbtracert <ref type="bibr" target="#b0">[3]</ref>). To make it possible, we quantify several desirable properties of probe matrix (e.g., identifiability) and propose a greedy algorithm to minimize probe cost. To address the scalability issue in DCNs, we apply several optimization heuristics and exploit characteristics of the DCN topology to accelerate path computation ( §4).</p><p>񮽙 We modify a failure localization algorithm based on packet loss characteristics in large-scale data centers. Compared to the existing algorithms, our algorithm runs within seconds and achieves higher accuracy and lower false positive rate ( §5).</p><p>񮽙 We implement and evaluate our system on a 4-ary Fattree testbed built with 20 switches. The experiments show that deTector is practically deployable and can accurately localize failures in near real time with less probe overhead, e.g., for 98% accuracy, deTector requires 3.9x and 1.9x times fewer probes than Pingmesh and NetNORAD while localizing failures 30 seconds in advance without the use of other loss localization tools. Our supplementary simulation further shows that deTector achieves greater than 98% accuracy in failure localization with a less than 1% false positive ratio for most failures in large-scale <ref type="bibr">DCNs ( §6)</ref>. We have open sourced deTector <ref type="bibr">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>DCNs are usually multi-stage Clos networks with multiple paths between commodity servers for load balancing and fault tolerance <ref type="bibr" target="#b4">[9,</ref><ref type="bibr" target="#b17">22,</ref><ref type="bibr" target="#b21">26,</ref><ref type="bibr" target="#b40">45]</ref>. Each DCN has its favorable routing protocols for path selection. For example, in a Fattree topology <ref type="bibr" target="#b4">[9]</ref> and a VL2 topology <ref type="bibr" target="#b17">[22]</ref>, the shortest paths between any two ToRs are typically used in practice <ref type="bibr" target="#b25">[30]</ref>. We describe how existing monitoring systems fall short in achieving the three design objectives. <ref type="table" target="#tab_0">Table 1</ref> shows detailed comparison among deTector and the existing systems.</p><p>The passive approach stores packet statistics on switch counters, which are polled from SNMP or CLI periodically. In <ref type="figure">Fig. 1</ref>, if link AB is down, the switch counters will show a lot of packet losses. However, if the failure is a gray failure rather than link down, it may go undetected. For example, when silent packet drops occur, the switch do not show any packet drop hints (e.g., syslog errors) due to various reasons (e.g., ASIC deficit), and hence SNMP data may not be fully trustworthy <ref type="bibr" target="#b21">[26]</ref>. Furthermore, switches counters can be noisy, such that problems identified by this approach may or may not lead to end-to-end delay or loss perceived by users.</p><p>Pingmesh and NetNORAD adopt an end-to-end probing approach to measure network latency and packet loss. Pingmesh selects probe paths by constructing two complete graphs within a DCN: one includes all servers under the same ToR switch (i.e., the switch in the edge layer in <ref type="figure">Fig. 1</ref>) and the other spans all ToR switches. NetNORAD is similar to Pingmesh but places pingers in a few pods instead of all servers. Their approaches simplify the design but bring quite significant overhead ( §6). Although gray failures can be captured, it is difficult to detect failures causing low rate losses (e.g., 1%) of a link, when ECMP is adopted in the DCN: there are many paths be- <ref type="figure">Figure 1</ref>: A 4-radix Fattree topology: failure on link AB can be detected by sending probes from s1 to s3. tween a pair of servers, low-rate losses on a particular link may not affect much the overall end-to-end loss rate between the two servers. The exact location of losses cannot be pinpointed using Pingmesh or NetNORAD, since they do not know which paths the probes take (e.g., due to ECMP). Therefore, other tools such as Netbouncer <ref type="bibr" target="#b1">[4]</ref> and fbtracert <ref type="bibr" target="#b0">[3]</ref> are needed, which send additional probes to play back the losses. These post-alarm tools may fail to pinpoint transient failures, those caused by transient bit errors, nonatomic rule updates or network upgrade (e.g., a transient inconsistency between the link configuration and routing information <ref type="bibr" target="#b16">[21]</ref>). To pinpoint such failures, close coupling of detection and localization is required, so that losses are localized only according to detection data, instead of additional probes after detection alarms. Such coupling further enables near real-time fault localization.  Controller. The logical controller periodically constructs the probe matrix indicating the paths for sending probes (see §4 for details). We mainly focus on failure localization on links inter-connecting switches, as the fault on a link connecting a server with a ToR switch can be easily identified as discussed in the next paragraph. The probe matrix indicates paths between ToRs. Since we do not rely on ToRs with ping capability, probes are sent by 2-4 selected servers (pingers) under each ToR.</p><p>Pinger. Each pinger receives the pinglist from the controller, which contains server targets, probe format and ping configuration ( §6.1). The probe paths from a ToR switch to different destinations are distributed among pinglists of pingers under the ToR switch, with each path distributed to at least 2 pingers for fault tolerance. In this way, in case that one pinger is down, other pingers in the same rack can still probe the paths, avoiding any large drop in link coverage. To detect failure on links connecting servers and the respective ToRs, pingers are also responsible for probing other servers under the same ToR. The number of probe paths for each pinger is no more than a hundred even for a large DCN ( §4.4). The probe packets are sent over UDP. Though TCP is used to carry most traffic in a DCN, the DCN does not differentiate TCP and UDP traffic (e.g., the forwarding behavior) in the vast majority of cases <ref type="bibr" target="#b32">[37,</ref><ref type="bibr" target="#b21">26]</ref>, and hence UDP probes can also manifest network performance. When a pinger detects a probe loss, it confirms the loss pattern by sending two probe packets of the same content additionally.</p><p>Responder. The responder is a lightweight module running on all servers. Upon receiving a probe packet, the responder echoes it back. A responder does not retain any states and all probing results are logged by pingers.</p><p>Diagnoser. Each pinger records packet loss information and sends it to the diagnoser for loss localization. These logs are saved into a database for real-time analysis and later queries. The diagnoser runs the PLL algorithm ( §5) to pinpoint packet losses and estimates the loss rates of suspected links.</p><p>For the controller and the diagnoser to be fault-tolerant and scalable, we can use existing solutions (e.g., Software Load-Balancer <ref type="bibr" target="#b36">[41,</ref><ref type="bibr" target="#b21">26]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Workflow Overview</head><p>deTector works in three steps in cycles: path computation, network probing and loss localization.</p><p>Path computation. At the beginning of each cycle, the controller reads the data center topology and server health from data center management service (e.g., <ref type="bibr" target="#b26">[31]</ref>), and selects the minimal number of probe paths ( §4). The controller then selects pingers in each ToR, constructs and dispatches the pinglists to them.</p><p>Network probing. Next, probe packets are sent along the specified paths across the DCN. Since data center usually adopts ECMP for load balancing, we have to use source routing to control the path traveled by each probe packet, which can be implemented using various methods. <ref type="bibr">1</ref> A general and feasible solution is to employ packet encapsulation and decapsulation to create end-toend tunnels, though it may cause encapsulating packets twice in virtualized networks created by VXLAN <ref type="bibr">[1]</ref> or NVGRE <ref type="bibr">[2]</ref>. Take the Fattree network in <ref type="figure">Fig. 1</ref> as an example: fixing a core switch, there is only one path between two inter-pod servers; we can use IP-in-IP encapsulation to wrap the probe on a server; after the packet arrives at the core switch, the outer header is removed and the packet is routed to the real destination. Such a source routing mechanism incurs little overhead on servers and core switches.</p><p>Loss localization. The probe loss measurements are aggregated and analyzed by our loss localization algorithm ( §5) on the diagnoser. We pinpoint the faulty links, estimate the loss rates, and send alerts to the network operator for further action (e.g., examining switch logs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Probe Matrix Design</head><p>The main limitation of existing monitoring systems is that the probe path selection is far from optimum, such that not enough useful information can be collected and additional probes are needed to reproduce losses for localization. In this section, we elaborate how we carefully select probe paths to overcome such a limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem</head><p>Consider a data center network graph G = (V, E), where V is the set of switches and E is the set of links. R is the m × n routing matrix defined by</p><formula xml:id="formula_0">R i, j = 񮽙 1 if link j is on path i 0 otherwise</formula><p>where m is the number of paths and n = |E| is the number of links. The possible paths and the routing matrix are decided by the routing protocols employed in the data center, e.g., ECMP is typically used to exploit k 2 /4 parallel paths between any two ToRs in a k-ary Fattree. <ref type="figure">Fig. 3</ref> gives a routing matrix R with 3 paths and 3 links. Note that each link in a DCN is typically bi-directional. Once we select a path from server s1 to server s2 and send a probe, the reverse path from s2 to s1 is automatically selected, since the response packet can probe faults along the reverse direction. When we identify that link AB has failed, it implies that the failure may lie in either direction of the link, switch A, or switch B.</p><formula xml:id="formula_1">R = ⎛ ⎝ l 1 l 2 l 3 p 1 1 1 0 p 2 1 0 1 p 3 0 0 1 ⎞ ⎠ → R' = ⎛ ⎝ l 1 l 2 l 3 l 12 l 13 l 23 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 ⎞ ⎠</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Extend routing matrix with virtual links</head><p>Problem 1 Given a DCN routing matrix R, select a set of paths to construct a probe matrix P, such that P simultaneously (1) minimizes the number of paths, and achieves (2) α-coverage and <ref type="formula">(3)</ref> β -identifiability.</p><p>Minimizing the number of probe paths is desirable for minimizing network bandwidth consumption and analysis overhead, such that we may finish probing and diagnosing the entire DCN in merely a few minutes. Under the same probing bandwidth budget, it allows each pinger to probe the same set of paths more frequently.</p><p>α-coverage requires that each link is covered by at least α paths in the probe matrix. Covering a link multiple times brings higher statistical accuracy for loss detection, as well as better resilience to pinger failures (since a link is more likely to be covered by probes from multiple pingers).</p><p>β -identifiability states that the simultaneous failures of any (no more than) β links in the DCN can be localized correctly. For the routing matrix in <ref type="figure">Fig. 3</ref>, suppose we select p 1 and p 2 to constitute the probe matrix, i.e., the probe matrix contains the first two rows of R. If 2 or more links fail simultaneously, the faulty links cannot be correctly identified, as the observation from the end is the same, i.e., packet losses are observed on both paths. On the other hand, if only one link is faulty, the bad link can be identified effectively: losses are observed on both paths, p 1 , or p 2 if link 1, 2, or 3 is faulty, respectively. Therefore, the probe matrix achieves 1-identifiability, but not 2 or higher identifiability. Better identifiability contributes to higher accuracy of loss localization.</p><p>We find that Problem 1 is NP-hard for general DCNs as the Minimum Set Cover Problem is a special case of the problem. We hence resort to an approximation algorithm to compute the probing path, which is at the heart of deTector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">PMC Algorithm</head><p>We extend a well-known greedy algorithm <ref type="bibr" target="#b8">[13]</ref> for constructing a probe matrix achieving 1-identifiability to one achieving β -identifiability, as well as α-coverage using a minimal number of probe paths.</p><p>In a probe matrix, a link belongs to a set of paths. To achieve 1-identifiability, the path sets of different links should all be different, so that losses can be observed on a particular set of paths to identify the faulty link. Recall that the set of links in our DCN is E. Once we select a path from the set of all feasible paths decided by the routing matrix based on some criterion, it splits E into two subsets E 1 and E 2 , containing links on the selected path and the other links, respectively. If we do not observe any packet loss on this path, it implies that all links in E 1 are good; otherwise, there must be at least one bad link in E 1 . Similarly, we select another path to further split E 1 and E 2 into smaller subsets, and repeat this procedure. Eventually if we can obtain subsets each containing only one link, then the probe matrix constructed using the selected paths achieves 1-identifiability (since the set of paths traversing each link is unique); otherwise, there does not exist a 1-identifiable probe matrix in the DCN. Throughout the process, if we always select a path whose links are present in the largest number of link sets to further split the link sets as much as possible, we will end up with the minimal number of paths needed.</p><p>To achieve β -identifiability, we expand the DCN graph G with "virtual links". A virtual link is a combination of multiple physical links, and the set of paths a virtual link belongs to can be computed by "OR"-ing together the paths including the individual links <ref type="bibr" target="#b8">[13]</ref>. For the example in <ref type="figure">Fig. 3</ref>, the original routing matrix R is extended to R' with three additional virtual links l 12 , l 13 and l 23 added; the column corresponding to the virtual link l 12 can be computed by "OR"-ing the two columns corresponding to links l 1 and l 2 . For β -identifiability, The probe matrix does not achieve even path coverage among the links yet. For example, for a 1-identifiable probe matrix constructed on a 64-ary Fattree, the gap between the maximal and minimal numbers of probing paths passing through any two links can be as large as 188. To achieve better evenness (i.e., spreading paths and thus probe overhead evenly among the physical links), we introduce a link weight w <ref type="bibr">[link]</ref>, denoting the number of paths that the link resides on, and ensure that it is no smaller than α for any physical link. We also define a score for each (extended) path, i.e., the path includes virtual links from the extended routing matrix R':</p><formula xml:id="formula_2">score(path) = ∑ link∈path w[link] − # of link sets on path (1)</formula><p>Here the link sets are the split link sets produced by the procedure above. We say that a link set is on a path if the link set contains at least one (physical or virtual) link of the path. Thus, a lower score indicates that the links on the path are not covered much by paths already selected and/or more link sets can be split if the path is selected update setnum as the total number of link sets after split by path 15: return probe matrix constructed by paths in sel paths (retaining only physical links on the paths) in the above procedure. We strive to achieve better evenness among the links while guaranteeing α-coverage, by always selecting a path with the lowest score. Our Probe Matrix Construction algorithm, PMC, is summarized in Alg. 1. We first reduce the problem of constructing a β -identifiable matrix to one constructing 1-identifiable matrix, by adding virtual links to the original routing matrix of the DCN graph (line 2, where LINKOR denotes the method for extending routing matrix discussed above). Then in each iteration we update the score of each (extended) path (lines 5-6) and select a path which has the minimal score among all candidate paths (lines 7-8). We remove the selected path from the candidate path set (line 9), and update the weight of physical links (w <ref type="bibr">[physlink]</ref>) on the selected path (lines 10-11) and the total number of link sets that the already selected paths can split into (line 14, which corresponds to the procedure discussed in the second paragraph of this subsection). If the number of paths that cover one (physical) link exceeds α, we remove the link from the set of all links (line 12-13). The loop stops when the probe matrix achieves α-coverage (i.e., the set physlinks is empty) and β -identifiability (i.e., the number of link sets split equals the number of links), or there are no more candidate paths (i.e., the set paths is empty).</p><p>Theorem 1 The PMC algorithm achieves (1 − 1 e ) approximation of the optimum in terms of the total number of probe paths selected, where e is natural logarithm.</p><p>We can prove Theorem 1 by showing that the score of a path set is monotone, submodular and non-negative.</p><p>The detailed proof is in the technical report <ref type="bibr" target="#b3">[7]</ref>. In practice, the PMC algorithm performs much better than the (1 − 1 e ) ≈ 0.63 approximation ratio ( §4.4). The issue of this algorithm, however, is the computation time. The time complexity of the algorithm is O(m 2 ), where m is the number of paths, since in the worst case we may update the scores of all paths in each iteration and end up with selecting all paths. In a 64-radix Fattree, there are about 4.3 × 10 9 desirable paths among ToRs. As we will see in §4.4, the algorithm is still too slow for any data center at a reasonable scale, and we adopt a number of optimizations to further speed it up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Algorithm Speedup</head><p>To speed up the PMC algorithm, we apply several optimizations based on the following three observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 1 Problem 1 can be divided into a series of subproblems.</head><p>We can construct a bipartite graph according to the routing matrix: one partition corresponds to paths and the other consists of links; an edge exists between a path node and a link node if the link is on the path. We observe that if the routing matrix can be partitioned into sets of paths with no links in common, then the problem can be divided into independent subproblems. For example, in <ref type="figure">Fig. 1</ref>, paths traversing the red link have no link overlapping with paths traversing the blue link. Therefore, the bipartite graph can typically be divided into connected subgraphs and each subgraph represents a smaller routing matrix and hence a subproblem. Finding connected subgraphs can be done in linear time by traversing the bipartite graph once. Then the PMC algorithm can be applied to the subproblems in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 2</head><p>The score of each path is non-decreasing over all iterations.</p><p>It can be proved that the score of a path is non-decreasing (Appendix A in <ref type="bibr" target="#b3">[7]</ref>). Inspired by the CELF algorithm for outbreak detection in networks <ref type="bibr" target="#b33">[38]</ref>, we adopt a strategy called lazy update which defers the update of a path score as much as possible even though we know the score is outdated. Specifically, we maintain a min-heap for all paths with scores as the keys and only update the score of a path when the path is at the top of the heap. After score update, if the path still stays at the top of the heap, i.e., the path has the minimal score among all available paths, we will select the path as a probe path, even though some path scores have yet to be updated. The correctness of this heuristic is guaranteed by submodularity of the score of a path set: the marginal gain provided by a path selected in the current iteration can not be larger than that provided by the path in the previous iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation 3 The DCN topology is typically symmetric.</head><p>Due to symmetry, when a path is selected, all its topologically isomorphic paths can be selected. For example in <ref type="figure">Fig. 1</ref>, if the dashed green path spanning Pod 1 and Pod 2 is selected, then the dashed purple path spanning Pod 3 and Pod 4 may be a good choice too. This helps us reduce the scale of the problem since the routing matrix R can be reduced to a smaller matrix by excluding paths that are topologically isomorphic to other paths. For example, if the green path is in the matrix, we do not need to include the purple path. For this purpose, we first need to compute the symmetric components in a DCN graph. There are many fast algorithms available for symmetry discovery <ref type="bibr" target="#b12">[17,</ref><ref type="bibr" target="#b10">15]</ref>, e.g., O 2 <ref type="bibr" target="#b10">[15]</ref> can finish computation within 5 seconds for a Fattree(100) DCN, and we only need to precompute it once for a DCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance</head><p>We run our PMC algorithm on a Dell PowerEdge R430 rack server with 10 Intel Xeon E5-2650 CPUs and 48GB memory, to test its running time and number of paths selected. We compare results on three well-known DCNs, Fattree <ref type="bibr" target="#b4">[9]</ref>, VL2 <ref type="bibr" target="#b17">[22]</ref> and BCube <ref type="bibr" target="#b19">[24]</ref>. <ref type="bibr">2</ref> Running time. <ref type="table" target="#tab_2">Table 2</ref> shows the algorithm running time for constructing a probe matrix achieving 2-coverage and 1-identifiability. The strawman approach is our PMC algorithm without any optimizations. The last three columns contain results when the respective optimization is in place (in addition to the previous one(s)). The results show that PMC can efficiently select probe paths for very large DCNs. Specifically, without algorithm speedup, the computation time of PMC can be larger than 24 hours; after each optimization, the time decreases significantly and we can compute the probe matrix for Fattree(72), VL2(140,120,100) and BCube(8,4) within 18 seconds, 86 seconds and 70 seconds, respectively. We note that the running time in case of problem decomposition for VL2 and BCube is a bit longer than that of strawman. This is because decomposition does not apply to the two DCN topologies, but we need extra time to decide whether the matrix is decomposable.</p><p>Path number. <ref type="table" target="#tab_3">Table 3</ref> shows the number of selected paths with different α and β in different DCNs. Compared with the number of original paths in DCNs, PMC only selects a small percentage of paths. We can prove that the least number of paths for achieving 1-coverage and 1-identifiability is k 3 /5 for any k-ary Fattree (Appendix B in <ref type="bibr" target="#b3">[7]</ref>). Thus, a Fattree(64) DCN needs at least 52428 paths and our algorithm selects slightly more, i.e.,  61440 paths. This implies that pingers under each selected ToR in the Fattree are only responsible for probing about 60 paths, much fewer than that of Pingmesh (about 2000-5000 paths). We also find that VL2 requires much fewer paths than Fattree and BCube. This is because VL2 has a much smaller number of links between switches (12288 links in VL2(128, 96,80)), as compared to Fattree (131072 links in Fattree(64)) and BCube (163840 links in BCube <ref type="figure">(8,4)</ref>).</p><p>Note that the number of selected path may change when the third optimization, based on topology symmetry, is in place. Our evaluation shows that the number of selected paths with symmetry reduction is very similar to that without symmetry reduction. This is consistent with the result in <ref type="bibr" target="#b25">[30]</ref>, and we hence omit the analysis.</p><p>Results for β ≥ 3. The probe matrices we constructed above achieve at most 2-identifiability. For β ≥ 3, the computation of PMC is not efficient in large DCNs. For the example of a 48-ary Fattree, computing a probe matrix achieving 3-identifiability requires at least 24 hours, even when we apply all speedup optimizations in §4.3. The fundamental reason is that the routing matrix R becomes much larger when the number of column increases from n to ∑ 1≤i≤β C(n, i), by adding virtual links. However, surprisingly, we find that 2-identifiability is enough for loss localization in DCNs, as we will see in §6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Loss Localization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Pre-processing</head><p>After collecting the probe data, the first step is to preprocess the data, removing outliers and normal cases. Severe packet losses could be caused by bad pingers and responders (e.g., the server is down or was rebooting during probing, thus causing many false alarms <ref type="bibr" target="#b32">[37]</ref>). Such outliers can be identified by keeping track of the status of servers using a watchdog service. In addition, a link normally has a regular low loss rate, e.g., 10 −4 -10 −5 , due to transient congestion, bit errors, which should not be considered as failures <ref type="bibr" target="#b21">[26]</ref>. To exclude such normal cases, we filter out paths with extremely low packet loss rates by setting a threshold on the number of packet losses in a period of time or on packet loss ratio (e.g., 10 −3 <ref type="bibr" target="#b21">[26,</ref><ref type="bibr" target="#b16">21]</ref>). 3 After pre-processing, the loss data that remain (in the form of (path, number o f losses)) are likely manifest of network failures rather than noises.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Problem</head><p>Our fault localization problem is: given end-to-end packet loss observations, find the smallest set of faulty links that best explains the observations. This problem is NP-hard as it can be reduced to the NP-complete Minimum Hitting Set Problem <ref type="bibr" target="#b13">[18]</ref>. Besides, we face two challenges not existed in previous work:</p><p>Much larger problem scale. Our study focuses on large-scale DCN networks, different from smaller networks investigated in the existing loss localization work <ref type="bibr" target="#b5">[10,</ref><ref type="bibr" target="#b13">18,</ref><ref type="bibr" target="#b37">42]</ref>. At our problem scale, the existing algorithms are not fast enough (taking tens of seconds or even minutes) for real-time loss localization.</p><p>Different loss patterns. Network failures are mainly exhibited as two kinds of packet losses: full packet loss and partial packet loss, meaning that all or part of the packets traversing a link are dropped. Existing tomography techniques assume that if all links on a path are good, then the path is good <ref type="bibr" target="#b14">[19]</ref>. This is not true in case of partial packet loss in data centers, e.g., packet blackhole may lead to losses on a link only for a subset of paths using that link.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">PLL Algorithm</head><p>Based on the Tomo algorithm in <ref type="bibr" target="#b13">[18]</ref>, we design an efficient Packet Loss Localization algorithm, PLL, to local-ize packet losses in DCNs (see <ref type="bibr" target="#b3">[7]</ref> for more details). The basic idea of PLL is as follows.</p><p>Step 1: Divide the problem into a series of subproblems, by decomposing the probe matrix following the same steps discussed for decomposing the routing matrix in §4.3. For each subproblem, run the following steps.</p><p>Step 2: If all probe paths traversing a link experience no packet loss, we exclude the link. For the remaining links, we calculate a hit ratio for each link, i.e., the ratio of the number of observed lossy paths through the link over the number of all probe paths using the link <ref type="bibr" target="#b29">[34]</ref>.</p><p>Step 3: We compute a score for each link as the number of lost packets that the link can explain, i.e., if a link lies in the packet path, we say the link can explain the packet loss.</p><p>Step 4: Among those links whose hit ratios are larger than a preset threshold, we greedily select the link with the maximal score and remove those losses this link can explain.</p><p>Step 5: Repeat Step 3 and Step 4 until no loss remains unexplained.</p><p>PLL differs from Tomo mainly in handling partial packet losses, i.e., we use a hit ratio threshold to filter suspected links. Setting the threshold requires network operator's experience and, if possible, by learning from real loss data. The analysis on setting this threshold is presented in <ref type="bibr" target="#b3">[7]</ref> due to space constraint and we set it to 0.6 by default in our experiments.</p><p>We have compared performance of PLL and other existing loss localization methods (e.g., Tomo, SCORE <ref type="bibr" target="#b29">[34]</ref> and OMP <ref type="bibr" target="#b37">[42]</ref>), and present the results in <ref type="bibr" target="#b3">[7]</ref>. The results show that given the same probe matrix, PLL achieves 2% higher accuracy (defined as true positive ratio, i.e., the percentage of bad links correctly identified as bad over all truly bad links), 2% lower false positive ratio (i.e., the percentage of good links incorrectly identified as bad over all correctly and incorrectly identified links), and is an order of magnitude faster (e.g., localizing failures within 1 second in a large DCN with 82944 links) than the other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation and Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation</head><p>We run the controller on one Dell server (or it can run in a distributed fashion over multiple servers for large-scale networks). A watchdog service also runs on the server for monitoring the health of other servers and removing bad ones. The controller runs the PMC algorithm to recompute the probe matrix every 10 minutes, based on the current network topology from the watchdog service. <ref type="bibr" target="#b1">4</ref> The computed probe matrix is divided into XML pinglist files for dispatching to pingers. A pinglist file contains file version, the pinger's IP address, IP addresses of responders, transport port numbers, the packet-sending interval and IP addresses of core switches. Our measurement shows that the controller can handle 4473 pinglist requests per second on average with maximal bandwidth consumption 688.56Mb/s using one core. Since pingers are deployed on a small number of servers (about 10% among all servers), the controller can support more than 100,000 pingers by slightly randomizing the time when pingers request for pinglists in each cycle.</p><p>Each pinger implements a communication module and a probing module. The communication module is responsible for connections with the controller and the diagnoser. It fetches the pinglist file from the controller by an HTTP GET request in every cycle (i.e., 10 minutes). The probing module generates probe packets according to the pinglist and encapsulates them by IP-in-IP ( §3.1). In our experiments, a pinger loops over a range of ports for each path, and emits several packets for every port. Each probe packet has an average size of 850 Bytes, carrying a specified DSCP value in the IP header to test different QoS classes <ref type="bibr" target="#b7">[12]</ref>. If there is no response for a probe within 100ms, we mark it as a loss. A pinger repeatedly sends packets by looping through the paths in the pinglist for multiple times (for statistical accuracy), at the rate of 10 packets per second. Every 30 seconds, the pinger aggregates the probing results (i.e., the number of packet losses and the number of packets sent on each probe path) into an XML file and sends it to the diagnoser by an HTTP POST request. The responder module runs in the userspace of all servers, which listens to a particular port, and upon packet arrival, it adds a timestamp and sends the packet back. The pinger and responder incur little overhead on servers, as we will see in §6.3.</p><p>The diagnoser is a Web server module running on the same server where the controller is in our experiments. It runs the PLL algorithm for fault localization once every half a minute, using collected probe results in the past 30 seconds. Given the limited number of servers in our testbed, we run a virtual machine to emulate a server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiment Setup</head><p>We build a 4-ary Fattree testbed with 20 ONetSwitch <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b24">29,</ref><ref type="bibr" target="#b23">28]</ref>, each equipped with FPGA-based hardware reconfigurable dataplane, four 1GbE ports and one dedicated management port. Though we do not require programmable switches in deTector, employing SDN switches facilitates our emulation of various failure cases that may happen in a real-world DCN. Specifically, we categorize all losses into three types: it does not affect symmetry computation which only pre-runs once on the original DCN topology.</p><p>Full packet loss. We install OpenFlow rules with high priority to drop all packets coming from a particular port, to emulate a faulty link with full packet loss. To emulate a switch down case, we install rules to drop all packets at the switch.</p><p>Deterministic partial loss. Packets with certain features (e.g., specific IPs, port numbers) may be dropped on a link deterministically, e.g., in case of packet blackhole or misconfigured routing rules. To emulate such failures, we install rules on the switches to match and drop packets with certain headers.</p><p>Random partial loss. Sometimes packets on a link are dropped randomly, as caused by bit flips, CRC errors, buffer overflow, etc. SDN switches do not support random packet dropping. To emulate such losses, we install rules on the switches to redirect all packets on an emulated bad link to the SDN controller, and the SDN controller drops the received packets with certain probability, following the pattern extracted from <ref type="bibr" target="#b7">[12]</ref>.</p><p>Due to no access to loss data in real-world data centers, we produce the above loss types according to the failure measurements in <ref type="bibr" target="#b15">[20]</ref> and traffic measurements in <ref type="bibr" target="#b7">[12]</ref>. Specifically, we set parameters such as link vs. switch failure percentage, link loss rates (ranging from 10 −4 to 1), failure probabilities for switches in different tiers, all based on the above measurements. The loss distribution for links in different tiers is extracted from <ref type="figure">Fig. 3</ref> in <ref type="bibr" target="#b7">[12]</ref>. Aside from deTector, we also implement the probing modules of Pingmesh and NetNORAD on our testbed for performance comparison, as well as their failure localization tools, Netbouncer and fbtracert. Since we do not know some of their implementation details (e.g., how data pre-processing is done), we implement those details in the same way across all three systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Performance</head><p>We first investigate how probing itself affects the whole DCN. We use realistic packet traces (including information such as packet header, timestamp) from a university data center <ref type="bibr" target="#b6">[11]</ref> (mostly HTTP flows) to generate workload traffic in our testbed, where each server continuously replays flows based on the packet traces and sends them to a random receiver. We evaluate how our probing frequency (i.e., the number of probes a pinger sends per second) affects the performance of the PLL algorithm, the overhead on the pinger, and RTT and jitter experienced by the workload traffic. In each minute of our experiment, we emulate a failure randomly picked among the three types of failures, with the failed switches or links randomly picked in the DCN. We run our experiment for 2000 minutes and obtain the average results. <ref type="figure">Fig. 4</ref> shows that a higher probe sending frequency leads to a higher accuracy and a lower false positive ra-  <ref type="figure">Fig. 4(a)</ref>), but causes higher CPU utilization and bandwidth consumption on pingers <ref type="figure">(Fig. 4(b)</ref>) as well as slightly larger fluctuation of the RTT <ref type="figure">(Fig. 4(c)</ref>) and jitter ( <ref type="figure">Fig. 4(d)</ref>) experienced by the workload. We find that 10-15 probes per second is good enough since we can still achieve higher than 95% accuracy and a lower than 3% false positive ratio, while only consuming about 100Kbps bandwidth, 0.4% CPU and 13MB memory on each pinger. Besides, it does not introduce apparent delay and jitter variations for workload traffic. Note that the overhead of a responder is much smaller than a pinger because it resumes fewer tasks (e.g., no communication with the controller and the diagnoser), and hence the results are omitted. <ref type="bibr" target="#b2">5</ref> In all our experiments, the pinger sends 10 packets per second by default (i.e., the red square in <ref type="figure">Fig. 4</ref>).</p><p>We then compare the accuracy, false positive ratio and overhead among deTector, Pingmesh and NetNORAD. Since Pingmesh can not localize failures by itself, once it detects a suspected source-destination server pair, we use Netbouncer <ref type="bibr" target="#b1">[4]</ref> to go through all possible paths between this server pair for loss localization. As for NetNORAD, similarly, we use fbtracert <ref type="bibr" target="#b0">[3]</ref> to probe all possible paths between the suspected server pair. The interval of loss data collection is 30 seconds for three systems. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the comparison when one failure is emulated in the testbed (the failure is randomly picked as in the previous experiment). The number of (ping and reply) probes in the figure includes probes sent for detection and probes for localization (if any) in each minute of the experiment. More probes indicate not only more bandwidth consumption, but also higher CPU and memory usage. For deTector, we use a probe matrix with 1-identifiability and 3-coverage (since it is impossible to achieve 2-identifiability in a 4-ary Fattree). As we can see, deTector achieves high accuracy and a low false positive ratio with a much smaller number of probes, because deTector covers more types of losses (e.g., low rate loss) and takes carefully planned paths. For instance, to achieve 98% accuracy and 1% false positives, deTector, NetNORAD and Pingmesh need to send 7200, 20700 and 35100 probes per minute, respectively. When the probe overhead is same (same number of probes sent per minute), the accuracy and false positive ratio achieved by deTector is better than those of NetNROAD; as compared to Pingmesh, the accuracy of deTector is much better, while the false positive ratio of Pingmesh is slightly smaller sometimes, since it possibly probes all paths. <ref type="figure">Fig. 6</ref> further shows the accuracy and false positive ratio with multiple failures, when the probe overhead is fixed to be the same, i.e., 5850 probes per minute. deTector always achieves much better performance than Pingmesh and NetNORAD. Note that deTector also detects and localizes failures much faster than NetNORAD and Pingmesh (30 seconds in advance in our experiments), because deTector does not need any other diagnosis tools to send an additional round of probes for loss localization, while others do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Simulation</head><p>We supplement our experimental evaluation with simulations, to investigate how identifiability of the probe ma- trix influences the accuracy of our failure localization, when running deTector in larger Fattree networks. We first vary α and β for probe matrix construction in an 18-radix Fattree. <ref type="table" target="#tab_4">Table 4</ref> shows that higher coverage and higher identifiability lead to higher accuracy, while the overhead (i.e., the number of selected paths) does not increase much. Also, we find that identifiability is more effective and desirable than coverage for failure localization, since a 1-identifiability matrix increases the accuracy a lot (from one with 0-identifiability guarantee), with much less overhead than a 3-coverage probe matrix.</p><p>Note that further increasing the level of identifiability for β &gt; 1 does not increase the accuracy much, and probe matrices achieving 1-identifiability can already lead to higher than 90% accuracy. According to the measurements in <ref type="bibr" target="#b7">[12]</ref>, less than 10% failure events (failures occurring concurrently) contain more than four failures and less than 1% failure events contain more than 20 failures. This implies that a probe matrix with 1-identifiability can guarantee higher than 93% accuracy for 90% failure events and 2-identifiability provides a 98% accuracy for 99% failure events.</p><p>The result is surprising but reasonable: Since we use a number of optimizations ( §4.3) to reduce the size of the routing matrix, the PMC algorithm in fact achieves β 񮽙 -identifiability (where β 񮽙 is larger than β used in the algorithm) for the whole probe matrix, rather than β -identifiability computed for each small probe matrix (corresponding to a small network topology). Therefore, deTector may fail to localize all failures only if more than β failures appear in a small topology, which occurs with relatively low probability. This shows that using a probe matrix with a low level of identifiability guarantee is good enough to identify a much larger number of concurrent failures.</p><p>In addition, by examining the failure events that deTector fails to localize with a low identifiability probe matrix but can identify using a high identifiability matrix, we find that higher identifiability achieves better results only when the number of simultaneously failed links is very large. Such a failure event with many concurrent link failures is usually triggered by a common bug in practice (e.g., 180 links fail simultaneously due to scheduled maintenance to multiple aggregation switches <ref type="bibr" target="#b15">[20]</ref>), and thus those faulty links are spatially clustered. In such cases, operators can locate the failure spot effectively according to the positions of most failed links. We further examine the fault localization accuracy, false positive and false negative (bad links incorrectly identified as good) ratios achieved using a probe matrix of 2-identifiability in a 48-ary Fattree. <ref type="table" target="#tab_5">Table 5</ref> shows that the false positive and false negative ratios remain in a very low level. In particular, the false positive rate is extremely low (&lt; 1%), which is desirable in practice <ref type="bibr" target="#b13">[18]</ref>.</p><p>The false negatives are mainly caused by losses of extremely low loss rate and intermittent losses which may happen at longer intervals (than 1 minute) <ref type="bibr" target="#b18">[23]</ref>. Since it takes longer time to expose these losses, we can further reduce false negatives by examining loss measurements in larger time windows, e.g., 10 minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussions</head><p>Packet entropy. deTector tries to increase packet entropy (i.e., different packet patterns) by varying IP addresses, port numbers and DSCP values, to cover as many failures as possible. However, our implementation uses IP-in-IP encapsulation for source routing, and hence the range of destination IP addresses is somewhat limited. In addition, since we use UDP for network probing, deTector may not be able to detect failures related to other protocols, e.g., misconfigured TCP parameters <ref type="bibr" target="#b21">[26]</ref>. Adopting other source routing solutions and adding more protocols to increase packet entropy are part of our future work. Loss diagnosis. While deTector can localize where packet drops occur, it does not know what causes the drops, e.g., software bugs, misconfigured rules or bursty traffic. This is a common deficiency of existing monitoring systems, since network diagnosis is rather complex. However, it is possible to distinguish full losses, deterministic partial losses, random partial losses and losses due to congestion, to narrow down the diagnosis scope (e.g., using machine learning approaches), since they exhibit different loss characteristics. We consider this as a promising future direction to explore. Beyond deTector. As opposed to probe-based solutions like deTector, there are some recent efforts on embedding metadata in the packet header to trace packet path for network debugging (e.g., <ref type="bibr">CherryPick [46]</ref>, PathDump <ref type="bibr" target="#b43">[47]</ref>). Our technique can be applied to reduce the overhead involved in these approaches, i.e., only packets traversing those paths computed by the PMC algorithm need to carry routing information in the packet headers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Probe design. Many existing work (e.g., <ref type="bibr" target="#b9">[14,</ref><ref type="bibr" target="#b13">18,</ref><ref type="bibr" target="#b38">43,</ref><ref type="bibr" target="#b28">33,</ref><ref type="bibr" target="#b22">27]</ref>) exploit logs on switches, or utilize multicast or network coding for network probing. Instead, we treat each switch as a blackbox, and adopt a topology-aware endto-end probing approach. Some studies <ref type="bibr" target="#b11">[16,</ref><ref type="bibr" target="#b35">40,</ref><ref type="bibr" target="#b18">23]</ref> estimate loss rates of all links, while we aim at identifying bad links (i.e., failure spots). Zeng et al. <ref type="bibr" target="#b44">[48]</ref> and Nicolas et al. <ref type="bibr" target="#b18">[23]</ref> propose monitoring solutions for backbone networks that do not apply in DCNs due to scalability, and the main difference lies in probe matrix design. Fault localization. Our goal of accurately identifying faulty links falls squarely in the area of binary network tomography. Tomography algorithms such as Sherlock <ref type="bibr" target="#b5">[10]</ref>, Tomo <ref type="bibr" target="#b13">[18]</ref>, GREEDY <ref type="bibr" target="#b30">[35]</ref>, SCORE <ref type="bibr" target="#b29">[34]</ref> and OMP <ref type="bibr" target="#b37">[42]</ref> do not work well for DCNs due to their problem scales and loss characteristics. Our PLL algorithm is built on these work and conquers their limitations. DCN monitoring. Our work mainly differs from existing monitoring systems such as Pingmesh <ref type="bibr" target="#b21">[26]</ref> and NetNORAD <ref type="bibr" target="#b32">[37]</ref> in the design of probe matrix. We argue that loss detection and localization must be coupled together to localize more failures (e.g., transient failures) in real time with low overhead. Carefully designed probe matrix is the key to achieve them. LossRadar <ref type="bibr" target="#b34">[39]</ref> is a switch-based solution but it requires programmable switches. <ref type="bibr">Dapper [44]</ref> and Zipkin <ref type="bibr" target="#b3">[8]</ref> are distributed tracing systems to gather timing data for root-cause analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>deTector is a real-time, low-overhead and high-accuracy monitoring system for large-scale data center networks. At its core is a carefully designed probe matrix, constructed by a scalable greedy path selection algorithm with minimized probe overhead. We also design an efficient failure localization algorithm according to different patterns of packet losses. Our analysis, testbed experiments and large-scale simulations show that deTector is highly scalable, practically deployable with low overhead, and can localize failures with high accuracy in near real time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3</head><label></label><figDesc>System Design 3.1 Architecture deTector includes four loosely coupled components: a controller, a diagnoser, pingers and responders, as de- picted in Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: System architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>∑ 2≤i≤β C(|E|, i) virtual links should be added in the DCN graph (routing matrix), corresponding to all combina- tions of 2 to β links in the original graph. Then we can run the above algorithm for constructing 1-identifiable matrix based on the new routing matrix, and the result- ing probe matrix achieves β -identifiability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(</head><label></label><figDesc>Figure 4: Sensitivity test of sending frequency tio (Fig. 4(a)), but causes higher CPU utilization and bandwidth consumption on pingers (Fig. 4(b)) as well as slightly larger fluctuation of the RTT (Fig. 4(c)) and jitter (Fig. 4(d)) experienced by the workload. We find that 10-15 probes per second is good enough since we can still achieve higher than 95% accuracy and a lower than 3% false positive ratio, while only consuming about 100Kbps bandwidth, 0.4% CPU and 13MB memory on each pinger. Besides, it does not introduce apparent delay and jitter variations for workload traffic. Note that the overhead of a responder is much smaller than a pinger because it resumes fewer tasks (e.g., no communication with the controller and the diagnoser), and hence the results are omitted. 5 In all our experiments, the pinger sends 10 packets per second by default (i.e., the red square in Fig. 4). We then compare the accuracy, false positive ratio and overhead among deTector, Pingmesh and NetNORAD. Since Pingmesh can not localize failures by itself, once it detects a suspected source-destination server pair, we use Netbouncer [4] to go through all possible paths between this server pair for loss localization. As for NetNORAD, similarly, we use fbtracert [3] to probe all possible paths between the suspected server pair. The interval of loss data collection is 30 seconds for three systems. Fig. 5 shows the comparison when one failure is emulated in the testbed (the failure is randomly picked as in the previous experiment). The number of (ping and reply) probes in the figure includes probes sent for detection and probes for localization (if any) in each minute of the experiment. More probes indicate not only more</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy and false positives of three monitoring systems with different number of probes per minute</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Comparison among deTector and existing representative monitoring systems</head><label>1</label><figDesc></figDesc><table>Gray failures Low rate loss 
Failure localization 
Transient failures 
Timeliness 
Overhead 
SNMP/CLI 
No 
No 
Yes 
Yes 
minutes 
switch resources 
Pingmesh [26] 
Yes 
No 
No, need Netbouncer 
No 
minutes 
many probes 
NetNORAD[3] 
Yes 
No 
No, need fbtracert 
No 
minutes 
many probes, switch CPU 
deTector 
Yes 
Yes 
Yes 
Yes 
near real-time 
minimal probes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Algorithm 1 Probe Matrix Construction (PMC) Algo- rithm Require: R, α, β 1: Initialize w, score to 0, setnum to 1, sel paths to / 0 2: R' ← LINKOR(R, β ) 3: paths ← all paths in R', physlinks ← E 4: while (setnum 񮽙 = |E| | physlinks 񮽙 =argmin path 񮽙 ∈paths score[path 񮽙 ] 8: sel paths ← sel paths ∪ {path} 9: paths ← paths/{path} 10: for physlink on path do</head><label>1</label><figDesc></figDesc><table>/ 
0) &amp;&amp; paths 񮽙 = / 
0 
do 

5: 

for path ∈ paths do 

6: 

update score[path] according to (1) 

7: 

path ← 11: 

w[physlink] ← w[physlink] + 1 

12: 

if w[physlink] &gt; α then 

13: 

physlinks ← physlinks/{physlink} 

14: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 : Algorithm running time (seconds) with α = 2, β = 1 in different DCNs DCNs # of nodes # of links # of original paths Strawman Decomposition Lazy update Symmetry reduction</head><label>2</label><figDesc></figDesc><table>Fattree(12) 
612 
1296 
184,032 
231.458 
5.216 
0.506 
0.126 
Fattree(24) 
4,176 
10,368 
11,902,464 
&gt; 24h 
1381.226 
23.254 
0.280 
Fattree(72) 
99,792 
279,936 
8,703,770,112 
&gt; 24h 
&gt; 24h 
&gt; 24h 
17.054 
VL2(20, 12, 20) 
1,282 
1,440 
70,800 
22.030 
23.126 
0.77 
0.253 
VL2(40, 24, 40) 
9,884 
10,560 
4,588,800 
7387.412 
7470.476 
39.028 
1.404 
VL2(140,120,100) 
424,390 
436,800 
4,938,024,000 
&gt; 24h 
&gt;24h 
&gt;24h 
85.567 
BCube(4, 2) 
112 
192 
12,096 
4.871 
4.936 
0.227 
0.117 
BCube(8, 2) 
704 
1,536 
784,896 
4050.776 
4390.168 
9.854 
0.220 
BCube(8, 4) 
53,248 
163,840 
5,368,545,280 
&gt; 24h 
&gt; 24h 
&gt; 24h 
69.778 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Number of selected paths with different (α,β ) 

DCNs 
Original paths 
Selected paths with (α, β ) 
(1, 0) 
(1, 1) 
(3, 2) 

Fattree(32) 
66,977,792 
4,096 
7,680 
12,288 
Fattree(64) 
4,292,870,144 32,768 61,440 
98,304 
VL2(72,48,40) 
107,371,008 
864 
1,440 
2,640 
VL2(128,96,80) 
2,415,132,672 
3,072 
5,760 
9,216 
BCube (8,2) 
784,896 
1,712 
2,016 
2,832 
BCube (8,4) 
5,368,545,280 
49,152 
70,572 
119,556 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 : Accuracy in a 18-radix Fattree, with probe ma- trices of different levels of coverage and identifiability (α, β ) # of paths</head><label>4</label><figDesc></figDesc><table>Accuracy (%) with # of failed links 
1 
5 
10 
20 
50 
(1, 0) 
729 
30.56 
30.87 30.30 
30.26 
29.19 
(2, 0) 
1485 
58.43 
57.43 
57.08 
56.81 
57.11 
(3, 0) 
2187 
68.22 
70.61 
69.89 
70.40 
70.14 
(1, 1) 
1269 
94.74 
93.37 
94.21 
93.43 
90.29 
(1, 2) 
1512 
99.26 
99.06 
99.02 
98.77 
95.92 
(1, 3) 
2349 
99.63 
99.63 
99.67 
99.62 
98.07 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 : Fault localization performance with probe ma- trix of 2-identifiability in a 48-ary Fattree</head><label>5</label><figDesc></figDesc><table># of failed links 
1 
5 
10 
20 
50 
Accuracy (%) 
98.95 98.99 98.98 98.93 98.87 
False positive (%) 
0.01 
0.02 
0.02 
0.02 
0.02 
False negative (%) 
1.05 
1.01 
1.02 
1.07 
1.13 

</table></figure>

			<note place="foot" n="1"> Source routing protocols have been designed in some DCNs like BCube [24] and DCell [25]; [30, 32] introduce other solutions for explicit path control.</note>

			<note place="foot" n="2"> BCube is a server centric architecture and we treat servers as switches to run our algorithm.</note>

			<note place="foot" n="3"> To avoid inaccuracy of the threshold approach, we can use statistical hypothesis testing to look at loss rates over time for noisy data filtering [27].</note>

			<note place="foot" n="4"> Once a link or a switch has failed, we remove related link(s) from the routing matrix to avoid selecting bad paths for probing. Note that</note>

			<note place="foot" n="5"> Even when we place the pinger and responder on the same server, the overhead is negligible.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fbtracert</surname></persName>
		</author>
		<ptr target="https://github.com/facebook/fbtracert" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microsoft</forename><surname>Netbouncer</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=nfEOEKlInK8" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Onetswitch</surname></persName>
		</author>
		<ptr target="http://www.onetswitch.org/index" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zipkin</surname></persName>
		</author>
		<ptr target="http://zipkin.io" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A scalable, commodity data center network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al-Fares</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loukissas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahdat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards highly reliable enterprise network services via inference of multi-level dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahl</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Data set for IMC 2010 data center measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<ptr target="http://pages.cs.wisc.edu/~tbenson/IMC10_Data.html" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding data center traffic characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimizing probe selection for fault localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brodie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th International Workshop on Distributed Systems: Operations and Management (DSOM)</title>
		<meeting>of the 12th International Workshop on Distributed Systems: Operations and Management (DSOM)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Network tomography: recent developments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Castro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="499" to="517" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generic and automatic address configuration for data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An algebraic approach to practical and scalable overlay network monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Faster symmetry discovery using sparsity of symmetries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darga</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Sakallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 45th Annual Design Automation Conference (DAC)</title>
		<meeting>of the 45th Annual Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Netdiagnoser: Troubleshooting network unreachabilities using end-to-end probes and routing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhamdhere</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dovrolis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diot</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd ACM International Conference on emerging Networking EXperiments and Technologies (CoNEXT</title>
		<meeting>of the 3rd ACM International Conference on emerging Networking EXperiments and Technologies (CoNEXT</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Network tomography of binary network performance characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duffield</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="5373" to="5388" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding network failures in data centers: measurement, analysis, and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gill</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagappan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evolve or die: High-availability design principles drawn from Google&apos;s network infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Govindan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Minei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kallahalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahdat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VL2: a scalable and flexible data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greenberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lahiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sengupta</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilbaud</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cartlidge</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<ptr target="https://www.nanog.org/meetings/nanog57/presentations/Tuesday/tues.general.GuilbaudCartlidge.Topology.7.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BCube: a high performance, server-centric network architecture for modular data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">DCell: a scalable and fault-tolerant network structure for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pingmesh: A large-scale system for data center network latency measurement and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIG-COMM</title>
		<meeting>of ACM SIG-COMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable near real-time failure localization of data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herodotou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Outhred</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fitter</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 20th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>of the 20th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DesktopDC: setting all programmable data center networking testbed on desk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIG-COMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="593" to="594" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Design of all programmable innovation platform for software defined networking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Open Networking Summit (ONS)</title>
		<meeting>of the 4th Open Networking Summit (ONS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Explicit path control in commodity data centers: Design and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Autopilot: automatic data center management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="2007-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards a flexible data center fabric with source routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyothi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Godfrey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research (SOSR)</title>
		<meeting>of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research (SOSR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Detailed diagnosis in enterprise networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandula</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verkaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Padhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahl</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">IP fault localization via risk modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kompella</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snoeren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd USENIX Symposium on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>of the 2nd USENIX Symposium on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detection and localization of network black holes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kompella</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snoeren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE INFO-COM</title>
		<meeting>of IEEE INFO-COM</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapukhov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Configuring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ipsla</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Network debugging at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lapukhov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<ptr target="https://www.nanog.org/sites/default/files/Lapukhov_Move_Fast_Unbreak.pdf" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cost-effective outbreak detection in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leskovec</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vanbriesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glance</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>of the 13th ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">LossRadar: Fast detection of lost packets in data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li´uli´</forename><surname>Li´u</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And Yu´uyu´ Yu´u</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th ACM International Conference on emerging Networking EXperiments and Technologies (CoNEXT</title>
		<meeting>of the 12th ACM International Conference on emerging Networking EXperiments and Technologies (CoNEXT</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient identification of additive link metrics via network tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd IEEE International Conference on Distributed Computing Systems (ICDCS)</title>
		<meeting>of the 33rd IEEE International Conference on Distributed Computing Systems (ICDCS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ananta: cloud scale load balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Kern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ku-Mar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zikos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIG-COMM</title>
		<meeting>of ACM SIG-COMM</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pati</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Rezaiifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krish-Naprasad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th Asilomar Conference on Signals, Systems and Computers (ACSSC)</title>
		<meeting>of the 27th Asilomar Conference on Signals, Systems and Computers (ACSSC)</meeting>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Network tomography via network coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Information Theory and Applications Workshop (ITA)</title>
		<meeting>of the 3rd Information Theory and Applications Workshop (ITA)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dapper, a large-scale distributed systems tracing infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigelman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jaspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanbhag</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Google, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Armistead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bannon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Boving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Felderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Germano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Jupiter rising: A decade of Clos topologies and centralized control in Google&apos;s datacenter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SIGCOMM</title>
		<meeting>of ACM SIGCOMM</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Cherrypick: Tracing packet trajectory in softwaredefined datacenter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tammana</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research (SOSR)</title>
		<meeting>of the 1st ACM SIGCOMM Symposium on Software Defined Networking Research (SOSR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Simplifying datacenter network debugging with pathdump</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tammana</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Measuring and troubleshooting large operational multipath networks with gray box testing, msr-tr-2015-55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Varghese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. rep</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
