<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:53+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HiTune: Dataflow-Based Performance Analysis for Big Data Cloud</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinquan</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Intel Asia-Pacific Research and Development Ltd Shanghai</orgName>
								<address>
									<postCode>200241</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Huang</surname></persName>
							<email>jie.huang@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Asia-Pacific Research and Development Ltd Shanghai</orgName>
								<address>
									<postCode>200241</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengsheng</forename><surname>Huang</surname></persName>
							<email>shengsheng.huang@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Asia-Pacific Research and Development Ltd Shanghai</orgName>
								<address>
									<postCode>200241</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Huang</surname></persName>
							<email>bo.huang@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Asia-Pacific Research and Development Ltd Shanghai</orgName>
								<address>
									<postCode>200241</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<email>yan.b.liu@intel.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Intel Asia-Pacific Research and Development Ltd Shanghai</orgName>
								<address>
									<postCode>200241</postCode>
									<country key="CN">P.R.China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HiTune: Dataflow-Based Performance Analysis for Big Data Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although Big Data Cloud (e.g., MapReduce, Hadoop and Dryad) makes it easy to develop and run highly scalable applications, efficient provisioning and fine-tuning of these massively distributed systems remain a major challenge. In this paper, we describe a general approach to help address this challenge, based on distributed instrumentations and dataflow-driven performance analysis. Based on this approach, we have implemented HiTune, a scalable, lightweight and extensible performance analyzer for Hadoop. We report our experience on how HiTune helps users to efficiently conduct Hadoop performance analysis and tuning, demonstrating the benefits of dataflow-based analysis and the limitations of existing approaches (e.g., system statistics, Hadoop logs and metrics, and traditional profiling).</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are dramatic differences between delivering software as a service in the cloud for millions to use, versus distributing software as bits for millions to run on their PCs. First and foremost, services must be highly scalable, storing and processing an enormous amount of data. For instance, in June 2010, Facebook reported 21PB raw storage capacity in their internal data warehouse, with 12TB compressed new data added every day and 800TB compressed data scanned daily <ref type="bibr" target="#b0">[1]</ref>. This type of "Big Data" phenomenon has led to the emergence of several new cloud infrastructures (e.g., MapReduce <ref type="bibr" target="#b1">[2]</ref>, Hadoop <ref type="bibr" target="#b1">[2]</ref>, Dryad <ref type="bibr" target="#b3">[4]</ref>, Pig <ref type="bibr" target="#b4">[5]</ref> and Hive <ref type="bibr" target="#b5">[6]</ref>), characterized by the ability to scale to thousands of nodes, fault tolerance and relaxed consistency. In these systems, the users can develop their applications according to a dataflow graph (either implicitly dictated by the programming/query model or explicitly specified by the users). Once an application is cast into the system, the cloud runtime is responsible for dynamically mapping the logical dataflow graph to the underlying cluster for distributed executions.</p><p>With these Big Data cloud infrastructures, the users are required to exploit the inherent data parallelism exposed by the dataflow graph when developing the applications; on the other hand, they are abstracted away from the messy details of data partitioning, task distribution, load balancing, fault tolerance and node communications. Unfortunately, this abstraction makes it very difficult, if not impossible, for the users to understand the cloud runtime behaviors. Consequently, although Big Data Cloud makes it easy to develop and run highly scalable applications, efficient provisioning and fine-tuning of these massively distributed systems remain a major challenge. To help address this challenge, we attempt to design tools that allow users to understand the runtime behaviors of Big Data Cloud, so that they can make educated decisions regarding how to improve the efficiency of these massively distributed systems -just as what traditional performance analyzers do for a single execution of a single program.</p><p>Unfortunately, performance analysis for Big Data Cloud is particularly challenging, because these applications can potentially comprise several thousands of programs running on thousands of machines, and the low level performance details are hidden from the users by using a high level dataflow model. In this paper, we describe a specific solution to this problem based on distributed instrumentations and dataflow-driven performance analysis, which correlates concurrent performance activities across different programs and machines, reconstructs the dataflow-based, distributed execution process of the Big Data application, and relates the low level performance activities to the high level dataflow model.</p><p>Based on this approach, we have implemented HiTune, a scalable, lightweight and extensible performance analyzer for Hadoop. We report our experience on how HiTune helps users to efficiently conduct Hadoop performance analysis and tuning, demonstrating the benefits of dataflow-based analysis and the limitations of existing approaches (e.g., system statistics, Hadoop logs and metrics, and traditional profiling). For instance, reconstructing the dataflow execution process of a Hadoop job allows users to understand the dynamic interactions between different tasks and stages (e.g., task scheduling and data shuffle; see sections 7.1 and 7.2). In addition, relating performance activities to the dataflow model allows users to conduct fine-grained, dataflow-based hotspot breakdown (e.g., for identifying application hotspots and hardware problems; see sections 7.2 and 7.3).</p><p>The rest of the paper is organized as follows. In section 2, we introduce the motivations and objectives of our work. We give an overview of our approach in section 3, and present the dataflow-based performance analysis in section 4. In section 5, we describe the implementation of HiTune, a performance analyzer for Hadoop. We experimentally evaluate HiTune in section 6, and report our experience in section 7. We discuss the related work in section 8, and finally conclude the paper in section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Statement</head><p>In this section, we describe the motivations, challenges, goals and non-goals of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Big Data Cloud</head><p>In Big Data Cloud, the input applications are modeled as directed acyclic dataflow graphs to the users, where graph vertices represent processing stages and graph edges represent communication channels. All the data parallelisms of the computation and the data dependencies between processing stages are explicitly encoded in the dataflow graph. The users can develop their applications by simply supplying programs that run on the vertices to these systems; on the other hand, they are abstracted away from the low level details of the distributed executions of their applications. The cloud runtime is responsible for dynamically mapping the logical dataflow graph to the underlying cluster, including generating the optimized dataflow graph of execution plans, assigning the vertices and edges to physical resources, scheduling and executing each vertex (usually using multiple instances and possibly multiple times due to failures).</p><p>For instance, the MapReduce model dictates a twostage group-by-aggregation dataflow graph to the users, as shown in <ref type="figure">Figure 1</ref>. A MapReduce application has one input that can be trivially partitioned. In the first stage a Map function, which specifies how the grouping is performed, is applied to each partition of input data. In the second stage a Reduce function, which performs the aggregation, is applied to each group produced by the first stage. The MapReduce framework is then responsible for mapping this logical dataflow graph to the physical resources. For instance, the Hadoop framework automatically executes the input MapReduce application using an internal dataflow graph of execution plan, as shown in <ref type="figure">Figure 2</ref>. The input data is divided into splits, and a distinct Map task is launched for each split. Inside each Map task, the map stage applies the Map function to the input data, and the spill stage stores the map output on local disks. In addition, a distinct Reduce task is launched for each partition of the map outputs. Inside each Reduce task, the copier and merge stages run in a pipelined fashion, fetching the relevant partition over the network and merging the fetched data respectively; after that, the sort and reduce stages merge the reduce inputs and apply the Reduce function respectively. In addition, the Pig and Hive systems allow the users to perform ad-hoc analysis of Big Data on top of Hadoop, using dataflow-style scripts and SQL-like queries respectively. For instance, <ref type="figure">Figure 3</ref> shows the Pig program (an example in the original Pig paper <ref type="bibr" target="#b4">[5]</ref>) and Hive query for the same operation (i.e., finding, for each sufficiently large category, the average pagerank of high-pagerank urls in that category). In these two systems, the logical dataflow graph of the operation is implicitly dictated by the language or query model, and is automatically compiled into the physical execution plan (another dataflow graph) that is executed on the underlying Hadoop system.</p><p>Unlike the aforementioned systems that restrict their applications' dataflow graph, the Dryad system allows the users to specify an arbitrary directed acyclic graph to describe the application, as illustrated in <ref type="figure" target="#fig_1">Figure 4</ref> (an example in the Dryad website <ref type="bibr" target="#b6">[7]</ref>). The cloud runtime then refines the input dataflow graph and executes the optimized execution plan on the underlying cluster. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivations and Challenges</head><p>By exposing data parallelisms through the dataflow model and hiding the low level details of the underlying cluster, Big Data Cloud allows users to work at the appropriate level of abstraction, which makes it easy to develop and run highly scalable applications. Unfortunately, this abstraction makes it very difficult, if not impossible, for users to understand the cloud runtime behaviors. Consequently, it remains as a big challenge to efficiently provision and tune these massively distributed systems, which entails requesting and allocating the optimal number of (physical or virtual) resources, and optimizing the system and applications for better resource utilizations.</p><p>As Big Data Cloud grows in pervasiveness and scale, addressing this challenge becomes critically important (for instance, tuning Hadoop jobs is considered as a very difficult problem and requires a lot of efforts on understanding Hadoop internals in Hadoop community <ref type="bibr" target="#b7">[8]</ref>; in addition, lack of tuning tools for Hadoop often forces users to resort to trial and error tuning <ref type="bibr" target="#b8">[9]</ref>). This motivates our work to design tools that allows users to understand the runtime behaviors of Big Data applications, so that they can make educated decisions regarding how to improve the efficiency of these massively distributed systems -just as what traditional performance analyzers (e.g., gprof <ref type="bibr" target="#b9">[10]</ref> and Intel VTune <ref type="bibr" target="#b10">[11]</ref>) do for a single execution of a single program. Unfortunately, performance analysis for Big Data Cloud is particularly challenging due to its unique properties.</p><p>• Massively distributed systems: Each Big Data application is a complex distributed application, which may comprise tens of thousands of processes and threads running on thousands of machines. Understanding system behaviors in this context would require correlating concurrent performance activities (e.g., CPU cycles, retired instructions, lock contentions, etc.) across many programs and machines with each other.</p><p>• High level abstractions: Big Data Cloud allows users to work at an appropriately high level of abstraction, by hiding the messy details of parallelisms behind the dataflow model and dynamically instantiating the dataflow graph (including resource allocations, task scheduling, fault tolerance, etc.). Consequently, it is very difficult, if not impossible, for users to understand how the low level performance activities can be related to the high level abstraction (which they have used to develop and run their applications).</p><p>In this paper, we address these technical challenges through distributed instrumentations and dataflowdriven performance analysis. Our approach allows users to easily associate different low level performance activities with the high level dataflow model, and provide valuable insights into the runtime behaviors of Big Data Cloud and applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Goals and Non-Goals</head><p>Our goal is to design tools that help users to efficiently conduct performance analysis for Big Data Cloud. In particular, we want our tools to be broadly applicable to many different applications and systems, and to be applicable to even production systems (because it is often impossible to reproduce the cloud behaviors given the scale of Big Data Cloud). Several concrete design goals result from these requirements.</p><p>• Low overhead: It is critical that our tools have negligible (e.g., less than a few percent) performance impacts on the running applications.</p><p>• No source code modifications: Our tools should not require any modifications to the cloud runtime, middleware, messages, or applications.</p><p>• Scalability: Our tools need to handle applications that potentially comprise tens of thousands of processes/threads running on thousands of servers.</p><p>• Extensibility: We would like our tools to be flexible enough so that it can be easily extended to support different cloud systems.</p><p>We also have several non-goals.</p><p>• We are not developing tools that can replace the need for developers (e.g., by automatically allocating the right amount of resources). Performance analysis for distributed systems is hard, and our goal is to make it easier for users, not to automate it.</p><p>• Our tools are not meant to verify the correct system behaviors, or diagnose the cause of faulty behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview of Our Approach</head><p>Our approach relies on distributed instrumentations on each node in the cloud, and then aggregating all the instrumentation results for dataflow-based analysis. The performance analysis framework consists of three major components, namely tracker, aggregation engine and analysis engine, as illustrated in <ref type="figure">Figure 5</ref>. The tracker is a lightweight agent running on every node. Each tracker has several samplers, which inspect the runtime information of the programs and system running on the local node (either periodically or based on specific events), and sends the sampling records to the aggregation engine. Each sampling record is of the format shown in <ref type="figure">figure 6</ref>.</p><p>• Timestamp is the sampling time for each record.</p><p>Since the nodes in the cloud are usually in the same administrative domain and highly connected, it is easy to have all the nodes time-synchronized (e.g., in Hadoop all the slaves exchange heartbeat messages with the master periodically and the time synchronization information can be easily piggybacked); consequently the sampler can directly record its sampling time. Alternatively, the sampler can send the sampling record to the aggregation engine in real-time, which can then record the receiving time.</p><p>• Type specifies the type of the sampling record (e.g., CPU cycles, disk bandwidth, log files, etc.).</p><p>• Target specifies the source of the sampling record.</p><p>It contains the name of the local node, as well as other sampler-specific information (e.g., CPUID, network interface name or log file name).</p><p>• Value contains the detailed sampling information of this record (e.g., CPU load, network bandwidth utilization, or a line/record in the log file).</p><p>The aggregation engine is responsible for collecting the sampling information from all the trackers in a distributed fashion, and storing the sampling information in a separate monitoring cluster for analysis. Any distributed log collection tools (e.g., Chukwa <ref type="bibr" target="#b11">[12]</ref>, Scribe <ref type="bibr">[13]</ref> and Flume <ref type="bibr" target="#b12">[14]</ref>) can be used as the aggregation engine. In addition, the analysis engine runs on the monitoring cluster, and is responsible for conducting the performance analysis and generating the analysis report, using the collected sampling information and a specification file describing the logical dataflow model of the specific Big Data cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataflow-Based Performance Analysis</head><p>In order to help users to understand the runtime behaviors of Big Data Cloud, our framework presents the performance analysis results in the same dataflow model that is used in developing and running the applications. The key technical challenge is to reconstruct the high level, dataflow-based, distributed and dynamic execution process for each Big Data application, based on the low level sampling records collected across different programs and machines. We address this challenge by: 1) Running a task execution sampler on every node to collect the execution information of each task in the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Describing the high level dataflow model of Big</head><p>Data Cloud in a specification file provided to the analysis engine. 3) Constructing the dataflow execution process for the application based the dataflow specification and the program execution information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Execution Sampler</head><p>To collect the program execution information, the task execution sampler instruments the cloud runtime and tasks running on the local node, and stores associated information into its sampling records at fixed time intervals as follows.</p><p>• The Target field of the sampling record needs to store the identifier (e.g., application name, task name, process ID and/or thread ID) of the program that is instrumented to collect this piece of sampling information.</p><p>• The Value field of the sampling record must contain the execution position of the program (e.g., thread name, stack trace, basic-block ID and/or instruction address) at which the program is running when it is instrumented to collect this piece of sampling information.</p><p>In practice, the task execution sampler can be implemented using any traditional instrumentation tool (which runs on a single machine), such as Intel VTune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataflow Specification</head><p>In order for the analysis engine to efficiently conduct the dataflow-based analysis, the dataflow specification needs to describe not only the dataflow graph (e.g., vertices and edges), but also the high level resource mappings of the dataflow model (e.g., physical implementations of vertices/edges, parallelisms between different phases/stages, and communication patterns between different stages). Consequently, the dataflow specification does require a priori knowledge of the cloud system. On the other hand, the users are not required to write the specification; instead, the dataflow specification is provided by the cloud system or the performance analyzer, and is either written by the developers of the cloud system (and/or the performance analyzer), or dynamically generated by the cloud runtime (e.g., by the Hive query compiler). The format of the dataflow specification is illustrated in <ref type="figure">Figure 7</ref> and described in detail below.</p><p>• The Input (Output) section contains a list of &lt;inputId: storage location&gt; (&lt;outputId: storage location&gt;), where the storage location specifics which storage system (e.g., HDFS or MySQL) is used to store the input (output) data.</p><p>• The Vertices section contains a list of &lt;vertexId: program location&gt;, where the program location specifies the portion of program (e.g., the corresponding thread or function) running on this graph vertex (i.e., processing stage). It is required that each execution position collected by the task execution sampler can be mapped to a unique program location in the specification, so that the analysis engine can determine which vertex each task execution sampling record belongs to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 7. Dataflow specification of Big Data Cloud</head><p>• The Edges section contains a list of &lt;edgeId: inputId/vertexId񮽙vertexId/outputId&gt;, which defines all the graph edges (i.e., communication channels).</p><p>• The Vertex Mapping section describes the high level resource mappings and parallelisms of the graph vertices. This section contains a list of Task Pool subsections; for each Task Pool subsection, the cloud runtime will launch several tasks (or processes) that can potentially run on the different nodes in parallel. The Task Pool subsection contains an ordered list of Phase subsections, and each task belonging to this task pool will sequentially execute these phases in the specified order.</p><p>• The Phase subsection contains a list of Thread Pool or Thread Group Pool subsections; for each of these subsections, the associated task will spawn several threads or thread groups in parallel. The Thread Pool subsection contains an ordered list of vertexId, and each thread belonging to this thread pool will sequentially execute these vertices in the specified order. On the other hand, a number of threads (as determined by group size) in the thread group will run in concert with each other, executing the vertex specified in the Thread Group Pool subsection.</p><p>• The cardinality of the Task Pool, Thread Pool or Thread Group Pool subsections determines the numbers of instances (i.e., processes, threads or thread groups) to be launched. It can have several values as follows.</p><p>(1) N -exactly N instances will be launched.</p><p>(2) 1~N -up to N instances will be launched. (3) 1~∞ -the number of instances to be launched is dynamically determined by the cloud runtime.</p><p>• The Edge Mapping section contains a list of &lt;edgeId: edge type, endpoint location&gt;. The edge type specifies the physical implementation of the edge, such as network connection, local file or memory buffer. The endpoint location specifies the communication patterns between the vertices, which can be intra-thread/intra-task/intra-node (i.e., data transfer exists only between vertex instances running in the same thread, the same task and the same node respectively), or unconstrained.</p><p>It is possible to extend the specification to support even more complex dataflow model and resource mappings (e.g., Process Group Pool); however, the current model is sufficient for all the Big Data cloud infrastructures that we have considered. For instance, the dataflow specification for our Hadoop cluster is shown in <ref type="figure" target="#fig_3">Figure  8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataflow-Based Analysis</head><p>As described in the previous sections, the program execution information collected by task execution samplers is generic in nature, and the dataflow model of the specific Big Data cloud is defined in a specification file. Based on these data, the analysis engine can reconstruct the dataflow execution process for the Big Data applications, and associate different performance activities with the high level dataflow model. In this way, our framework can be easily applied to different cloud systems by simply changing the specification file. We defer the detailed description of the dataflow construction algorithm to section 5.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">HiTune: A Dataflow-Based Hadoop Performance Analyzer</head><p>Based on our general performance analysis framework, we have implemented HiTune, a scalable, lightweight and extensible performance analyzer for Hadoop. In this section, we describe the implementation of HiTune, and in particular, how it is carefully engineered to meet our design goals that are described in section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation of Tracker</head><p>In our current implementation, all the nodes in the  Hadoop cluster are time synchronized (e.g., using a time service), and a tracker runs on each node in the cluster. Currently, the tracker consists of three samplers (i.e., task execution sampler, system sampler and log file sampler), and each sampler directly stores the sampling time in the Timestamp field of its sampling record.</p><note type="other">map: MapTask.run spill: SpillThread.run copier: MapOutputCopier.run merge: InMemFSMergeThread.run or LocalFSMerger.run sort: ReduceCopier.createKVIterator#ReduceCopier.access reduce: runNewReducer or runOldReducer } Edges { //list of &lt;edgeId: inputId/vertexId񮽙vertexId/outputId&gt; E1: Input񮽙map E2: map񮽙spill E3: spill񮽙copier E4: copier񮽙merge E5: merge񮽙sort E6: sort񮽙reduce E7: reduce񮽙Output } Vertex Mapping { /</note><p>We have chosen to implement the task execution sampler using binary instrumentation techniques, so that it can instrument and collect the program execution information without any source code modifications. Specifically, the task execution sampler runs as a Java programming language agent <ref type="bibr" target="#b13">[15]</ref>; whenever the Hadoop framework launches a JVM to be instrumented, it dynamically attaches to the JVM the sampler agent, which samples the Java thread stack trace and state for all the threads in the JVM at specified intervals (during the entire lifetime of the JVM).</p><p>For each sampling record generated by the task execution sampler, its Target field contains the node name, the task name and the Java thread ID; and its value field contains the current execution position (i.e., the Java thread name and thread stack trace) as well the as the Java thread state. That is, the Target field is specified using the identifier of the runtime program instance (i.e., the thread ID), which allows the analysis engine to construct the entire sampling trace of each thread; in addition, the execution position is specified using the static program names (i.e., the Java thread name and method names), which allows the dataflow model and resource mappings to be easily described in the specification file.</p><p>In addition, the system sampler simply reports the system statistics (e.g., CPU load, disk I/O, network bandwidth, etc.) periodically using the sysstat package, and the log sampler reports the Hadoop log information (including Hadoop metrics and job history files) whenever there is new log information.</p><p>Since the tracker (especially the task execution sampler) needs to instrument the Hadoop tasks running on each node, it is the major source of performance overheads in HiTune. We have carefully designed and implemented the tracker (e.g., the task execution sampler caches the stack traces in memory and batches the write-out to the aggregation engine), so that it has very low (less than 2% according to our measurement) performance impacts on Hadoop applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation of Aggregation Engine</head><p>To ensure its scalability, the aggregation engine is implemented as a distributed data collection system, which can collect the sampling information from potentially thousands of nodes in the Hadoop cluster. In the current implementation, we have chosen to use Chukwa (a distributed log collection framework) as our aggregation engine. Every sampler in HiTune directly sends its sampling records to the Chukwa agent running on the local node, which in turn sends data to the Chukwa collectors over the network; the collector is responsible for storing the sampling data in a (separate) monitoring Hadoop/HDFS cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation of Analysis Engine</head><p>The sampling data for a Hadoop job can be potentially very large in size (e.g., about 100GB for TeraSort <ref type="bibr">[16]</ref>[17] in our cluster). We address the scalability challenge by first storing the sampling data in HDFS (as described in section 5.2), and then running the analysis engine as a Hadoop application on the monitoring Hadoop cluster in an offline fashion.</p><p>Based on the Target field (i.e., the node name, the task name and the Java thread ID) of every task execution sampling record, the analysis engine first constructs a sampling trace for each thread (i.e., the sequence of all task execution sampling records belonging to that thread, ordered by the record timestamps) in the Hadoop job.</p><p>The program location (used in the dataflow specification) can therefore be defined as a range of consecutive sampling records in one thread trace (or, in the case of thread group, multiple ranges each in a different thread). Each record range is identified by the starting and ending records, which are specified using their execution positions (i.e., partial stack traces). For instance, all the records between the first appearance and the last appearance of MapTask.run (or simply the MapTask.run method) constitute one instance of the map vertex. See <ref type="figure" target="#fig_3">Figure 8</ref> for the detailed dataflow specification of our Hadoop cluster.</p><p>Based on the Target and Timestamp fields of the two boundary records of corresponding program locations, the analysis engine then determines which machine each instance of a vertex runs on, when it starts and when it ends. Finally, it associates all the system and log file sampling records to each instance of the dataflow graph vertex (i.e., the processing stage), again using the Target and Timestamp information of the records.</p><p>With the algorithm and dataflow specification described above, the analysis engine can easily reconstruct the dataflow execution for the Hadoop job and associates different sampling records with the dataflow graph. In addition, the performance analysis algorithm is itself implemented as a Hadoop application, which processes the sampling records for each JVM simultaneously.</p><p>Consequently, we can generate various analysis reports that provide valuable insights into the Hadoop runtime behaviors (presented in the same dataflow model used in developing and running the Hadoop job). For instance, a timeline based execution chart for all task instances, similar to the pipeline space-time diagram <ref type="bibr" target="#b15">[18]</ref>, can be generated so that users can get a complete picture about the dataflow-based execution process of the Hadoop job. It is also possible to generate the hotspot breakdown (e.g., disk I/O vs. network transfer vs. computations) for each vertex in the dataflow, so that users can identify the possible bottlenecks in the Hadoop cluster. We show some analysis reports and how they are used to help our Hadoop performance analysis and tuning in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluations</head><p>In this section, we experimentally evaluate the runtime overheads and scalability of HiTune, using three benchmarks (namely, Sort, WordCount and Nutch indexing) in the HiBench Hadoop benchmark suite <ref type="bibr" target="#b14">[17]</ref>, as shown in <ref type="table" target="#tab_4">Table 1</ref>. The Hadoop cluster used in our experiment consists of one master (running JobTracker and NameNode) and up to 20 slaves (running TaskTracker and DataNode); the detailed server configurations are shown in <ref type="table" target="#tab_5">Table 2</ref>. Every server has two GbE NICs, each of which is connected to a different gigabit Ethernet switch, forming two different networks; one network is used for the Hadoop jobs, and the other is used for administration and monitoring tasks (e.g., the Chukwa aggregation engine in HiTune).  We evaluate the runtime overheads of HiTune by measuring the Hadoop performance (speed and throughput) as well as the system resource (e.g., CPU</p><p>and memory) utilizations of the Hadoop cluster. The speed is measured using the job running time, and the throughput is defined as the number of tasks completed per minute when the Hadoop cluster is at full utilization (by continuously submitting multiple jobs to the cluster). In addition, we evaluate the scalability of HiTune by analyzing that, when there are more nodes in the Hadoop cluster, whether the runtime overheads increase and whether it becomes more complex for HiTune to conduct the dataflow-based performance analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Runtime Overheads</head><p>As mentioned in section 5.1, the tracker (especially the task execution sampler) is the major source of runtime overheads in HiTune. This is because the task execution sampler needs to instrument the Hadoop tasks running on each node, while the aggregation and analysis of sampling data are performed on a separate monitoring cluster in an offline fashion.</p><p>We first compare the instrumented Hadoop performance (measured when the tracker is running) and the baseline performance (measured when the tracker is completely turned off). In our experiment, when the tracker is running, the task execution sampler dumps the Java thread stack trace every 20 milliseconds, the system sampler reports the system statistics every 5 seconds, and the Hadoop cluster is configured to output its metrics to the log file every 10 seconds. <ref type="figure">Figures 9  and 10</ref> show the ratio of the instrumented performance over the baseline performance for job running time (lower is better) and throughput (higher is better) respectively. It is clear that the overhead of running the tracker is very low in terms of performance -the instrumented job running time is longer than the baseline by less than 2%, and the instrumented throughput is lower than the baseline by less than 2%.</p><p>In addition, we also compare the instrumented system resource utilizations (measured when the tracker is running) and the baseline utilizations (measured when only the system sampler is running, which is needed to report the system resource utilizations periodically).</p><p>Since the sampling records are aggregated using a separate network, we only present the CPU and memory utilization results of the Hadoop cluster in this paper. <ref type="figure">Figures 11 and 12</ref> show the ratio of the instrumented CPU and memory utilizations over the baseline utilizations respectively. It is clear that the overhead of running the tracker is also very low in terms of resource utilizations -either the instrumented CPU or memory utilization is higher than the baseline by less than 2%.  In summary, HiTune is a very lightweight performance analyzer for Hadoop, with very low (less than 2%) runtime overheads in terms of speed, throughput and system resource utilizations. In addition, HiTune scales very well in terms of the runtime overheads, because it instruments each node in the cluster independently and consequently the runtime overheads remain the same even when there are more nodes in the cluster (as confirmed by the experimental results).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Complexity of Performance Analysis</head><p>Since the analysis engine needs to re-construct the dataflow execution of a Hadoop job and associate the sampling records to each vertex instance in the dataflow, the complexity of analysis can be evaluated by comparing the sizes of sampling data and the numbers of vertex instances between different sized clusters. <ref type="figure" target="#fig_7">Figure 13</ref> shows the sampling data sizes for the 5-, 10-and 20-slave clusters. It is clear that the sampling data sizes remain about the same (or increase very slowly) for different sized clusters (e.g., only less than 18% increase in the sample data size even when the cluster size is increased by 4x). Intuitively, since HiTune samples each instance of the processing stages at fixed time intervals, the sampling data size is proportional to the sum of the running time of all vertex instances. As long as the underlying Hadoop framework scales well with the cluster sizes, the sum of the vertex instance running time will remain about the same (or increase very slowly), and so does the sampling data size. In practice, even with very large (1000s of nodes) clusters, a MapReduce job usually runs on about 100s of worker machines <ref type="bibr" target="#b16">[19]</ref>, and the Hadoop framework scales reasonably well with that number (100s) of machines.  In addition, assume M and R are the total numbers of the map and reduce tasks of a Hadoop job respectively. Since in the Hadoop dataflow model (as shown in <ref type="figure" target="#fig_3">Figure 8</ref>) each map task contains two stages and each reduce task contains four stages, the total number of vertex instances can be computed as 2*M+4*R. In practice, the number of map tasks is about 26x of that of reduce tasks in average for each MapReduce job <ref type="bibr" target="#b17">[20]</ref>, and therefore the vertex instance count is about 2.15*M.</p><p>Since the number of map tasks (M) of a Hadoop job is typically determined by its input data size (e.g., by the number of HDFS file blocks), the number of vertex instances will also remain about the same for different sized clusters in practice.</p><p>In summary, the complexity for HiTune to conduct the dataflow-based performance analysis will remain about the same even when there are more nodes in the cluster (or, more precisely, the dataflow-based performance analysis in HiTune scales as well as Hadoop does with the cluster sizes), because the sampling data sizes and the vertex instance counts will remain about the same even when there are more nodes in the cluster. In addition, we have implemented the analysis engine as a Hadoop application, so that the dataflow-based performance analysis can be parallelized using another monitoring Hadoop cluster. For instance, to process the 100GB sampling data generated when running TeraSort in our cluster, it takes about 16 minutes on a singleslave monitoring cluster, and about 5 minutes on a 4-slave monitoring cluster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experience</head><p>HiTune has been used intensively inside Intel for Hadoop performance analysis and tuning (e.g., see <ref type="bibr" target="#b14">[17]</ref>).</p><p>In this section, we share our experience on how we use HiTune to efficiently conduct performance analysis and tuning for Hadoop, demonstrating the benefits of dataflow-based analysis and the limitations of existing approaches (e.g., system statistics, Hadoop logs and metrics, and traditional profiling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Tuning Hadoop Framework</head><p>One performance issue we encountered is extremely low system utilizations when sorting many small files (3200 500KB-sized files) using Hadoop 0.20.1 -system statistics collected by the cluster monitoring tools (e.g., Ganglia <ref type="bibr">[21]</ref>) show that the CPU, disk I/O and network bandwidth utilizations are all below 5%. That is, there are no obvious bottlenecks or hotspots in our cluster; consequently, traditional tools (e.g., system monitors and program profilers) fail to reveal the root cause. To address this performance issue, we used HiTune to reconstruct the dataflow execution process of this Hadoop job, as illustrated in <ref type="figure" target="#fig_1">Figure 14</ref>. The x-axis represents the elapse of wall clock time, and each horizontal line in the chart represents a map or reduce task. Within each line, bootstrap represents the period before the task is launched, idle represents the period after the task is complete, map represents the period when the map task is running, and shuffle, sort and reduce represent the periods when (the instances of) the corresponding stages are running respectively.</p><p>As is obvious in the dataflow execution, there are few parallelisms between the Map tasks, or between the Map tasks and Reduce tasks in this job. Clearly, the task scheduler in Hadoop 0.20.1 <ref type="bibr">(Fair Scheduler [22]</ref> is used in our cluster) fails to launch all the tasks as soon as possible in this case. Once the problem is isolated, we quickly identified the root cause -by default, the Fair Scheduler in Hadoop 0.20.1 only assigns one task to a slave at each heartbeat (i.e., the periodical keepalive message between the master and slaves), and it schedules map tasks first whenever possible; in our job, each map task processes a small file and completes very fast (faster than the heartbeat interval), and consequently each slave runs the map tasks sequentially and the reduce tasks are scheduled after all the map tasks are done.</p><p>To fix this performance issue, we upgraded the cluster to Fair Scheduler 2.0 [23] <ref type="bibr" target="#b19">[24]</ref>, which by default schedules multiple tasks (including reduce tasks) in each heartbeat; consequently the job runs about 6x faster (as shown in <ref type="figure" target="#fig_9">Figure 15</ref>) and the cluster utilization is greatly improved. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Analyzing Application Hotspots</head><p>In the previous section, we demonstrate that the high level dataflow execution process of a Hadoop job helps users to understand the dynamic task scheduling and assignment of the Hadoop framework. In this section, we show that the dataflow execution process helps users to identify the data shuffle gaps between map and reduce, and that relating the low level performance activities to the high level dataflow model allows users to conduct fine-grained, dataflow-based hotspot breakdown (so as to understand the hotspots of the massively distributed applications).  However, the dataflow execution process of TeraSort shows that there is a large gap (about 15% of the total job running time) between the end of map tasks and the end of shuffle phases. According to the communication patterns specified in the Hadoop dataflow model (see <ref type="figure">Figure 2</ref> and <ref type="figure" target="#fig_3">Figure 8</ref>), shuffle phases need to fetch the output from all the map tasks in the copier stages, and ideally should complete as soon as all the map tasks complete. Unfortunately, traditional tools or Hadoop logs fail to reveal the root cause of the large gap, because during that period, none of the CPU, disk I/O and network bandwidth are bottlenecked, the "Shuffle Fetchers Busy Percent" metric reported by the Hadoop framework is always 100%, while increasing the number of copier threads does not improve the utilization or performance.</p><p>To address this issue, we used HiTune to conduct hotspot breakdown of the shuffle phases, which is possible because HiTune has associated all the low level sampling records with the high level dataflow execution of the Hadoop job. The dataflow-based hotspot breakdown (see <ref type="figure">Figure 17)</ref> shows that, in the shuffle stages, the copier threads are actually idle 80% of the time, waiting (in the ShuffleRamManager. reserve method) for the occupied memory buffers to be freed by the memory merge threads. (The idle vs. busy breakdown and the method hotspot are determined using the Java thread state and stack trace in the task execution sampling records respectively). On the other hand, most of the busy time of the memory merge thread is due to the compression, which is the root cause of the large gap between map and shuffle. To fix this issue and reduce the compression hotspots, we changed the compression codec to LZO <ref type="bibr" target="#b20">[25]</ref>, which improves the TeraSort performance by more than 2x and completely eliminates the gap (see <ref type="figure" target="#fig_3">Figure 18</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Diagnosing Hardware Problems</head><p>By examining <ref type="figure" target="#fig_3">Figure 18</ref> in more detail, we also found that the reduce stage running time is significantly skewed among different reduce tasks -a small number of stages are much slower than the others, as shown in <ref type="figure" target="#fig_13">Figure 19</ref>. Based on the association of the low level sampling records and the high level dataflow model, we use HiTune to generate the normalized average running time and the idle vs. busy breakdown of the reduce stages (grouped by the Tasktrackers that the stages run on) in <ref type="figure" target="#fig_14">Figure 20</ref>. It is clear that reduce stages running on the 3 rd and 7 th TaskTrackers are much slower (about 20% and 14% slower than the average respectively). In addition, while all the reduce stages have about the same busy time, the reduce stages running on these two TaskTrackers have more idle time, waiting in the DFSOutputStream.writeChunk method (i.e., writing data to HDFS). Since the data replication factor in TeraSort is set to 1 (as required by the benchmark specs), the HDFS write operations in the reduce stage only writes to the local disks. By examining the average write bandwidth of the disks on these two TaskTrackers, we finally identified the root cause of this problemthere is one disk on each of these two nodes that is much slower than other disks in the cluster (about 44% and 30% slower than the average respectively), which is later confirmed to have bad sectors through a very expensive fsck process. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Extending HiTune to Other Systems</head><p>Since the initial release of HiTune inside Intel, it has been extended by the users in different ways to meet their requirements. For instance, new samplers are added so that processor microarchitecture events and power state behaviors of Hadoop jobs can be analyzed using the dataflow model.</p><p>In addition, HiTune has also been applied to Hive (an open source data warehouse built on top of Hadoop), by extending the original Hadoop dataflow model to include additional phases and stages, as illustrated in <ref type="figure">Figure 21</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related Work</head><p>There are several distributed system tracing tools (e.g., Magpie <ref type="bibr" target="#b21">[27]</ref>, X-Trace <ref type="bibr" target="#b22">[28]</ref> and Dapper <ref type="bibr" target="#b23">[29]</ref>), which associates and propagates the tracing metadata as the request passes through the system. With this type of path information, the tracing tools can easily construct an event graph capturing the causality of events across the system, which can be then queried for various analyses <ref type="bibr" target="#b24">[30]</ref>. Unfortunately, these tools would require changes not only to source codes but also to message schemas, and are usually restricted to a small portion of the system in practice (e.g., Dapper only instruments the threading, callback and RPC libraries in Google <ref type="bibr" target="#b23">[29]</ref>). In contrast, our approach uses binary instrumentations to sample the tasks in a distributed and independent fashion at each node, and reconstructs the dataflow execution process of the application based on a priori knowledge of Big Data Cloud. Consequently, it requires no modifications to the system, and therefore can be applied more extensively to obtain richer information (e.g., the hottest function) than these tracing tools.</p><p>Our distributed instrumentations are similar to GoogleWide Profiling (GWP) <ref type="bibr" target="#b25">[31]</ref>, which samples across machines in multiple data centers for production applications. In addition, the current Hadoop framework can profile specific map/reduce tasks using traditional Java profilers (e.g., HPROF <ref type="bibr" target="#b26">[32]</ref>), which however have very high overheads and are usually applied to a small (2 or 3) number of tasks. More importantly, both GWP and the existing profiling support in Hadoop focus on providing traditional performance analysis to the distributed systems (e.g., by allowing the users to directly query the low level sampling data). In contrast, our key technical challenge is to reconstruct the high level dataflow execution of the application based on the low level sampling data, so that users can work on the same dataflow model used in developing and running their Big Data applications.</p><p>In the industry, traditional cluster monitoring tools (e.g., Ganglia <ref type="bibr">[21]</ref>, Nagios <ref type="bibr" target="#b27">[33]</ref> and Cacti <ref type="bibr">[34]</ref>) have been widely used to collect system statistics (e.g., CPU load) from all the machines in the cluster; in addition, several large-scale log collection systems (e.g., Chukwa <ref type="bibr" target="#b11">[12]</ref>, Scribe <ref type="bibr">[13]</ref> and Flume <ref type="bibr" target="#b12">[14]</ref>) have been recently developed to aggregate log data from a large number of servers. All of these tools focus on providing a distributed framework to collect statistics and logs, and are orthogonal to our work (e.g., we have actually used</p><p>Chukwa as the aggregation engine in the current HiTune implementation).</p><p>Existing diagnostic tools for Hadoop and Dryad (e.g., <ref type="bibr">Vaidya [35]</ref>, <ref type="bibr">Kahuna [36]</ref> and Artemis <ref type="bibr" target="#b30">[37]</ref>) focus on mining the system logs to detect performance problems. For instance, it is possible to construct the task execution chart (as shown in section 7.1) using the Hadoop job history files. Compared to these tools, our approach (based on distributed instrumentation and dataflow-driven performance analysis) has many advantages. First, it can provide much more insights, such as dataflow-based hotspot breakdown (see sections 7.2 and 7.3), into the cloud runtime behaviors. More importantly, performance problems of massively distributed systems are very complex, and are often due to issues that the developers are completely unaware of (and therefore are not exposed by the existing codes or logs). For instance, in section 7.2, the Hadoop framework shows that the shuffle fetchers are always busy, while detailed breakdown provided by HiTune reveals that copiers are actually idle most of the time. Finally, our approach is much more general, and consequently can be easily extended to support other systems such as Hive (see section 7.4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 1. Dataflow graph of a MapReduce application</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Dataflow graph of a Dryad application [7]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Performance analysis framework Timestamp Type Target Value Figure 6. Format of the sampling record</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Hadoop dataflow specification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Figure 9. Ratio of instrumented job running time over baseline job running time</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. Ratio of instrumented cluster CPU utilization over baseline cluster CPU utilization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Comparison of sampling data sizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Dataflow execution for sorting many small files with Hadoop 0.20.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Dataflow execution for sorting many small files with Fair Scheduler 2.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 16</head><label>16</label><figDesc>Figure 16 shows the dataflow execution, as well as the timeline based CPU, disk and network bandwidth utilizations of TeraSort [16][17] (sorting 10 billion 100-byte records). It has high CPU utilizations during the map tasks, because the map output data are compressed (using the default codec in Hadoop) to reduce the disk and network I/O. (Compressing the input or output of TeraSort is not allowed in the benchmark specs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. TeraSort (using default compression codec)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 .Figure 18 .</head><label>1718</label><figDesc>Figure 17. Copier and Memory Merge threads breakdown (using default compression codec)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 19 .</head><label>19</label><figDesc>Figure 19. Reduce tasks of TeraSort (using LZO compression)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 20 .</head><label>20</label><figDesc>Figure 20. Normalized average running time and busy vs. idle breakdown of reduce stages</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 24 .</head><label>24</label><figDesc>Figure 24. Reduce stage breakdown Figure 22 shows the dataflow execution process for the aggregation query in Hive performance benchmarks [26][9]. In addition, Figures 23 and 24 show the dataflow-based breakdown of the map/reduce stages for the aggregation query (both the map and reduce Hive active stages are further broken into 3 portions: Hive Input, Hive Operation and Hive Output based on the Java methods). As shown in Figures 23 and 24, the query spends only about 32% of its time performing the Hive Operations; on the other hand, it spends about 68% of its time on the data input/output, as well as the initialization and cleanup of the Hadoop/Hive frameworks. Therefore, to optimize this Hive query, it is more critical to reduce the size of intermediate results,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>/ /dataflow graph Input { //list of &lt;inputId: storage location&gt; In1: storage location … } Output { //list of &lt;outputId: storage location&gt; Out1</head><label>/</label><figDesc></figDesc><table>: storage location 
… 
} 
Vertices { //list of &lt;vertexId: program location&gt; 
V1: program location 
… 
} 
Edges { //list of &lt;edgeId: inputId/vertexId񮽙vertexId/outputId&gt; 
E1: In1񮽙V1 
E2: V1񮽙V2 
… 
} 
//resource mapping 
Vertex Mapping { //list of Task Pool 
Task Pool [(name)] &lt;(cardinality)&gt; { //ordered list of Phase 
Phase [(name)] { //list of Thread Pool or Thread Group Pool 
Thread Pool [(name)] &lt;(cardinality)&gt; { 
//ordered list of vertexId 
V1, V2, … 
} //end of Thread Pool 
Thread Group Pool [(name)] &lt;(cardinality, group size)&gt; { 
//a single vertexId 
V3 
} //end of Thread Group Pool 
.. 
} //end of Phase 
Phase [(name)] { … } 
… 
} //end of Task Pool 
Task Pool [(name)] &lt;(cardinality)&gt; { … } 
… 
} 
Edge Mapping { //list of &lt;edgeId: edge type, endpoint location&gt; 
E1: edge type, endpoint location 
… 
} 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>/ /Hadoop dataflow graph Input { //list of &lt;inputId: storage location&gt; Input:HDFS } Output { //list of &lt;outputId: storage location&gt; Output:</head><label>/</label><figDesc></figDesc><table>HDFS 
} 
Vertices { //list of &lt;vertexId: program location&gt; 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 1 . Hadoop benchmarks</head><label>1</label><figDesc></figDesc><table>Benchmark 
Input Data 
Sort 
60GB generated by RandomWriter example. 
WordCount 60GB generated by RandomTextWriter example 
Nutch 
indexing 

19GB data generated by crawling an in-house 
Wikipedia mirror 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 2 . Server configurations</head><label>2</label><figDesc></figDesc><table>Processor 
Dual-socket quad-core Intel Xeon processor 
Disk 
4 SATA 7200RPM HDDs 
Memory 
24 GB ECC DRAM 
Network 
2 Gigabit Ethernet NICs 
OS 
Redhat Enterprise Linux 5.4 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusions</head><p>In this paper, we propose a general approach of performance analysis for Big Data Cloud, based on distributed instrumentations and dataflow-driven performance analysis. Based on this approach, we have implemented HiTune (a Hadoop performance analyzer), which provide valuable insights into the Hadoop runtime behaviors with every low overhead, no source code changes, very good scalability and extensibility. We also report our experience on how to use HiTune to efficiently conduct performance analysis and tuning for Hadoop, demonstrating the benefits of dataflow-based analysis and the limitations of existing approaches.</p></div>
			</div>

			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data warehousing and analytics infrastructure at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 36th ACM SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 6th Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2nd European Conference on Computer Systems</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pig latin: a not-so-foreign language for data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 34th ACM SIGMOD international conference on Management of data</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hive -A Petabyte Scale Data Warehousing Using Hadoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 26th IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dryad</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/en-us/projects/Dryad/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Hadoop and HBase at RIPE NCC</title>
		<ptr target="http://www.cloudera.com/blog/2010/11/hadoop-and-hbase-at-ripe-ncc/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A comparison of approaches to large-scale data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 35th SIGMOD international conference on Management of data</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gprof: A call graph execution profiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Kessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Mckusick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 1982 ACM SIGPLAN Symposium on Compiler Construction</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Intel Vtune Performance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Analyzer</surname></persName>
		</author>
		<ptr target="http://software.intel.com/en-us/intel-vtune/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Chukwa: A system for reliable large-scale log collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<idno>UCB/EECS-2010-25</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flume</surname></persName>
		</author>
		<ptr target="https://github.com/cloudera/flume" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Java</forename><surname>Instrumentation</surname></persName>
		</author>
		<ptr target="http://download.oracle.com/javase/6/docs/api/java/lang/instrument/package-summary.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The HiBench Benchmark suite: Characterization of the MapReduce-Based Data Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th International Conference on Data Engineering Workshops</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>Morgan Kaufmann, 4 th edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Designs, Lessons and Advice from Building Large Distributed Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and Middleware</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Experiences with MapReduce, an abstraction for large-scale computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://ganglia.sourceforge.net/[22]Afairsharingjobscheduler.https://issues.apache.org/jira/browse/HADOOP-3746" />
	</analytic>
	<monogr>
		<title level="m">The 15th International Conference on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Job Scheduling for Multi-User MapReduce Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Elmeleegy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<idno>UCB/EECS-2009-55</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hadoop Fair Scheduler Design Document</title>
		<ptr target="https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk/src/contrib/fairscheduler/designdoc/fair_scheduler_design_doc.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lzo</forename><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://github.com/kevinweil/hadoop-lzo" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Magpie: online modelling and performance-aware systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 9th conference on Hot Topics in Operating Systems</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">X-trace: A pervasive network tracing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 4th USENIX Symposium on Networked Systems Design &amp; Implementation</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dapper, a Large-Scale Distributed Systems Tracing Infrastructure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Sigelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaspan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shanbhag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<pubPlace>Google Research</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<title level="m">D3: Declarative Distributed Debugging</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Google-Wide Profiling: A Continuous Profiling Infrastructure for Data Centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="page" from="65" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">HPROF: a Heap/CPU Profiling Tool in J2SE 5</title>
		<ptr target="http://java.sun.com/developer/technicalArticles/Programming/HPROF.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagios</surname></persName>
		</author>
		<ptr target="http://www.nagios.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hadoop Vaidya</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gogate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhandarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hadoop World</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kahuna: Problem Diagnosis for MapReduce-Based Cloud Computing Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kavulya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/IFIP Network Operations and Management Symposium (NOMS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hunting for problems with Artemis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cretu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First USENIX conference on Analysis of system logs</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
