<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:52+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Edge Computing Resource Management System: a Critical Building Block! Initiating the debate via OpenStack</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan-Alexandre</forename><surname>Cherrueau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">STACK Research Group -IMT-Atlantique</orgName>
								<address>
									<postCode>LS2N</postCode>
									<settlement>Inria</settlement>
									<country>France, Canada &amp; Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Lebre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">STACK Research Group -IMT-Atlantique</orgName>
								<address>
									<postCode>LS2N</postCode>
									<settlement>Inria</settlement>
									<country>France, Canada &amp; Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Pertin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">STACK Research Group -IMT-Atlantique</orgName>
								<address>
									<postCode>LS2N</postCode>
									<settlement>Inria</settlement>
									<country>France, Canada &amp; Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fetahi</forename><surname>Wuhib</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">STACK Research Group -IMT-Atlantique</orgName>
								<address>
									<postCode>LS2N</postCode>
									<settlement>Inria</settlement>
									<country>France, Canada &amp; Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><forename type="middle">Monteiro</forename><surname>Soares</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">STACK Research Group -IMT-Atlantique</orgName>
								<address>
									<postCode>LS2N</postCode>
									<settlement>Inria</settlement>
									<country>France, Canada &amp; Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Edge Computing Resource Management System: a Critical Building Block! Initiating the debate via OpenStack</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>This paper aims at initi-ating the discussion in our community.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>While it is clear that edge infrastructures are required for emerging use-cases related to IoT, VR or NFV, there is currently no resource management system able to deliver all features for the edge that made cloud computing successful (e.g., an OpenStack for the edge). Since building a system from scratch is seen by many as impractical, this paper provides reflections regarding how existing solutions can be leveraged. To that end, we provide a list of the features required to operate and use edge computing resources, and investigate how an existing IaaS manager (i.e., OpenStack) satisfies these requirements. Finally, we identify from this study two approaches to design an edge infrastructure manager that fulfils our requirements, and discuss their pros and cons.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While several academic studies have highlighted the advantages of the edge computing paradigm in various domains <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, progress on how to operate and use infrastructures that serve it are marginal. Solutions like Akamai Cloudlets <ref type="bibr" target="#b0">[1]</ref> or AWS Lambda <ref type="bibr" target="#b1">[2]</ref> are close to the initial fog proposal that allows to run domainspecific applications on NFV-enabled infrastructures (at the edge) and centralized clouds <ref type="bibr" target="#b6">[8]</ref>. Since these solutions do not allow to run stateful workloads in isolated environments (e.g. containers, virtual machines (VM)), they do not fully cover the requirements from developers and operators (DevOps) who expect to find most features that made current cloud solutions successful also at the edge. Therefore, such solutions are not considered in this (paper's) initial debate.</p><p>The ETSI Mobile Edge Computing Industry Specification Group defined in 2016 an architecture to orchestrate distinct independent cloud systems, a.k.a. Virtual Infrastructure Managers (VIM) <ref type="bibr" target="#b13">[14]</ref>. The idea consists in federating VIMs of the different Data Centers (DCs) that compose the edge infrastructure. By reusing VIMs, ETSI targets edge computing resource managers that behave in the same fashion as traditional ones, while mitigating development requirements. Although there is no implementation available, the idea of federating VIMs seems promising as several projects have been built on similar concepts. ONAP <ref type="bibr">[3]</ref>, an industry-driven solution, enables the orchestration and automation of virtual network functions across distinct VIMs. From the academic side, FogBow <ref type="bibr" target="#b7">[9]</ref> aims to support federations of Infrastructure-as-a-Service (IaaS) providers. More recently, NIST initiated a collaborative effort with IEEE to advance Federated cloud through the development of a conceptual architecture and a vocabulary <ref type="bibr" target="#b0">1</ref> .</p><p>Although all these projects provide valuable contributions, they all have been designed by only considering the DevOps' perspective. They provide abstractions to manage the life cycle of geo-distributed applications, but do not address administrative requirements. However, edge computing infrastructures differ from federated cloud systems in various aspects <ref type="bibr" target="#b17">[18]</ref>, for instance: edge sites are potentially unmanned and therefore must be administered remotely; management systems should be designed to cope with intermittent network access to sites; distinct operators might be interested in interconnecting their infrastructures (like network peering).</p><p>To capitalize on the advent of edge computing, our community should take part in current discussions and actions in order to deliver a well-suited resource management system. A system that will (i) let an operator aggregate, supervise and expose the massively distributed resources of the infrastructure, and (ii) let DevOps implement new kinds of services on top of an infrastructure that may be deployed and managed on-demand.</p><p>In this paper, we present reflections to initiate discussions through our community.</p><p>• First, we introduce a classification of the features expected by both administrators and DevOps. This classification is valuable to identify missing mecha-  <ref type="figure">Figure 1</ref>: Edge Computing Infrastructure <ref type="bibr" target="#b9">[11]</ref>. The red dashed lines depict a split-brain situation that isolates Site 1 from other sites.</p><p>nisms in resource management systems.</p><p>• Second, based on the identified requirements, we discuss how an edge resource management system should be designed. In particular, we study pros and cons of top-down and bottom-up approaches. The former consists in interacting with each site only through exposed VIM APIs, such as in federated approaches. The latter aims at revising internal mechanisms of VIMs to enable native collaborations. Since there are many possible edge infrastructure designs, we highlight that the infrastructure considered in this study is composed of several individually-managed and geo-distributed micro DCs (up to thousands) composed of up to one hundred servers (nearly two racks). <ref type="figure">Figure 1</ref> depicts such an infrastructure. The expected latency and bandwidth between elements may fluctuate, in particular because networks can be wired or wireless. Moreover, disconnections between sites may occur, leading to network split-brain situations <ref type="bibr" target="#b10">[12]</ref>. Finally, it is possible to consider additional DCs at the extreme edge, within private institutions or public transports. From the software viewpoint, since we investigate how existing cloud managers can be extended to the edge, we consider as our default VIM OpenStack <ref type="bibr" target="#b3">[5]</ref>: the de-facto opensource solution for cloud computing infrastructures. <ref type="bibr" target="#b1">2</ref> The remainder of the paper is organized as follows. Section 2 provides a list of features expected by administrators and DevOps. Section 3 studies how OpenStack satisfies these features, highlighting the need for collaboration across the entire edge infrastructure. Section 4 discusses two approaches to design a resource management system for the edge. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Admin/DevOps' Requirements</head><p>Herein we classify features administrators and DevOps expect to find in the context of edge computing infrastructures. The classification is based on 5 levels, starting from the easiest aspects, i.e., interacting with a single site (considered in level 1, L1), to more complex aspects like managing multiple sites (L2), up to taking into account that sites can be owned by different operators (L5). Table 1 summarizes the classification we detail below.</p><p>As previously mentioned, a large part of these features are common to the ones offered by current IaaS resource management systems. They are implemented by various services, each of which is in charge of the management of a particular aspect of the infrastructure <ref type="bibr" target="#b12">[13]</ref>.</p><p>L1. Operate/use any site This level considers the actions both administrators and DevOps expect to perform on a single, reachable site. Most operations are elementary from the edge viewpoint because they correspond to the ones already provided by a VIM such as OpenStack. In other words, each edge site can be considered as an independent cloud at this level. The unmanned aspect only impacts this level by requiring to perform all operations remotely if needed. Furthermore, the resource management system should provide means to ensure the integrity of the hardware resources taking part to the edge infrastructure. Strategies such as enabling/disabling physical interactions with the equipment should be considered.</p><p>L2. Operate/use several sites In L2, L1 features are considered but over several sites. This includes operations such as provisioning/managing multiple resources or gathering information from various sites simultaneously. Operations can be either intra-service (same service from different sites) or inter-service (different services from different sites). Examples of such operations include configuring users' access on a per-site basis; listing available VM images or pushing new ones on multiple sites. From the DevOps viewpoint, a user should be able to boot a VM on Site 1 using an image defined in Site 2 (inter-service operation). Similarly to L1, DevOps also expect metrics to be collected from several sites and collaboration mechanisms regarding the security (e.g., secret key sharing, network encryption).</p><p>Because collaboration between sites can be either explicit (i.e., the targeted sites are explicitly specified in the operation), or implicit (i.e., the resource management system is in charge of selecting resources), we have defined two sub-levels: L2.1 and L2.2, as depicted in <ref type="table">Table 1</ref>. The implicit manner suggests that policies (e.g., performance objectives, energy consumption) and constraints (e.g., affinity rules, hardware requirements) are provided by admins/DevOps so that the resource management system takes the right decisions regarding the defined desiderata and the state of the infrastructure (e.g., auto-scaling, relocating workloads between sites, re-scheduling faulty resources across sites).  <ref type="table">Table 1</ref>: Classification of the requirements to operate and use edge computing infrastructures in 5 levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Levels</head><p>L3. Robustness w.r.t. network split-brains L3 includes L2 operations but with the possibility to face situations where the infrastructure is partitioned due to network disconnections. <ref type="figure">Figure 1</ref> depicts such a case where Site 1 is isolated from the other sites. In this scenario, administrators/DevOps that can reach Site 1 (i.e., located in the same geographical area) should be able to perform L1 operations on Site 1 despite the split-brain. Such a requirement makes sense as L1 is limited to one site and does not impact other ones. For the remaining sites, L1 and L2 must be guaranteed. Since split-brains might impact differently alreadyprovisioned resources and management services, we refine L3 into two sub-features. L3.1 is related to features that allow already-deployed applications to continue to serve local requests without being impacted. For instance, an apache server or a storage backend should be able to satisfy requests coming from the same geographical area, even if management services cannot be reached. L3.2 features are related to the provisioning of new resources and other management operations as described in the previous paragraph. Note that guaranteeing L2 features will not be always possible at the L3.2 level because information cannot be gathered in case of a disconnection. For instance, guaranteeing quotas across the infrastructure might be a challenge without first relaxing the consistency requirement of the information. New approaches will have to be proposed for such operations.</p><p>Finally, the possibly intermittent network connectivity between edge resources requires the ability for sites to join and leave the infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L4. Multiple cloud environments</head><p>Delivering a resource management system at large scale in a unified manner poses a great challenge from the software viewpoint. Since different versions and types of infrastructure managers might co-exist at the same time across the whole infrastructure, L4 gathers the related requirements. More precisely, L4.1 considers L3 features when different versions of the same software stack co-exist across the infrastructure. L4.2 increases the complexity by considering the collaboration of mutiple systems, including possibly different concepts (e.g., OpenStack for VMs and Kubernetes for Containers). Such a requirement implies the ability to get sites' capabilities in order to only allow meaningful collaborations.</p><p>L5. Multiple operators L5 corresponds to the holy grail in terms of expected features. In addition to L4, it includes the possibility to use sites owned by different operators. We do not specify any requirement for administrators at this level as an operator is unlikely to let other operators administer its own site. However, operators should be able to collaborate to offer their sites' resources to any DevOps like it has been done for a while for cellular networks. The requirements here are more related to the collection and sharing of relevant metrics enabling each operator to perform charging/billing.</p><p>Summary As previously mentioned, each level of our classification increases the complexity in terms of design and development constraints. Note however that since L4 and L5 both extend L3, they can be considered at the same level and can be swapped as a consequence.</p><p>Although we tried to be exhaustive, this list of features could probably be extended, e.g., by considering different edge infrastructure scenarios (e.g., including smaller and limited devices). However, we believe it is already valuable as it delivers significant insights on the design and implementation of an edge resource management system. In the next section, we study whether OpenStack can fulfill the L1, L2, and L3 levels. The discussion of L4 and L5 is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OpenStack at the Edge</head><p>This section studies the use of OpenStack to control an edge infrastructure. Such an analysis is meaningful as the OpenStack code base has been evolving to deal with large scale and multi-site objectives for the two last years. Particularly, the section evaluates OpenStack, including its latest improvements, with respect to the requirements defined in the previous section.</p><p>At a high level, OpenStack has two types of nodes: data nodes delivering XaaS capabilities (compute/storage/network, i.e., data plane); control nodes executing OpenStack services (i.e., control plane). Whenever users issue a request to OpenStack, the control plane processes the request which may potentially also affect the data plane in some manner. Key control plane services include keystone, nova, glance, and neutron, respectively responsible for authentication/authorization, VM life cycle management, VM image management and associated network management.</p><p>Because OpenStack comes with several deployment alternatives and because the edge sites considered in our study can host data and control nodes, we elected to discuss two scenarios: a centralized management scenario and a multiple regions scenario described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Centralized (Remote) Management</head><p>In this scenario, OpenStack operates an edge infrastructure as a traditional single DC environment, the key difference being the wide-area networking found between the control and compute nodes <ref type="bibr" target="#b5">[7]</ref>. The distinction between the different edge sites can be realized by leveraging the concept of host aggregates within OpenStack.</p><p>From the requirements' viewpoint, L1 and most L2 requirements can be fulfilled in a straightforward way (because the infrastructure can be spread over several network domains, some L2 operations including specific network actions cannot be satisfied). For L3, only L3.1 requirements can be satisfied while L3.2 cannot be met due to the centralized control plane. For instance, most OpenStack services collaborate through a message system and through the manipulation of logical objects that are persisted in shared databases (DBs). While this enables services to easily collaborate, it imposes a requirement of permanent connectivity between the services located at the compute nodes and the control services like the databases and message buses located in the DC. In other words, while this scenario provides a "single pane of glass" for administrators and DevOps, it has the drawback of being a "single point of failure" preventing DevOps to use edge resources in case of network split-brains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multiple Regions</head><p>In this scenario, each edge site corresponds to a region in the OpenStack terminology, which is a complete deployment of OpenStack with all control services and a "shared" Keystone. The main advantage of this deployment is related to the independency of each site in case of network disconnections. The downside relates to the fact that the current codebase does not provide any mechanism to allow the collaboration between several regions and thus L2 requirements cannot be met. <ref type="bibr">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Effective Collaboration is Needed</head><p>Despite the fallibility of the network, and frequent isolation risks of an edge site from the rest of the infrastructure, an edge infrastructure may be able to meet L3 requirements. This can be achieved by supposing a collaboration a la peer-to-peer, that is, an edge site always serves local resources and collaborates with other edge sites if need be. To develop such a resource management system, two fundamental design options exist: top-down or bottom-up. Both designs differently impact the way this required collaboration can be handled.</p><p>A top-down collaboration design implements the collaboration by federating VIMs' API. As a consequence the existing VIM codebases are used without introducing modifications/extensions. Examples of approaches following this design are: ONAP <ref type="bibr">[3]</ref>, Kingbird <ref type="bibr" target="#b2">[4]</ref>, FogBow <ref type="bibr" target="#b7">[9]</ref> and p2p-OpenStack <ref type="bibr" target="#b16">[17]</ref>.</p><p>A bottom-up collaboration design lays on making VIMs mechanisms/services directly collaborative <ref type="bibr" target="#b9">[11]</ref>. For example, having two OpenStack Nova services able to cooperate and communicate directly would be a realization of a bottom-up design. Such design implies either the modification/extension of existing VIMs or the creation of a completely new system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design Discussion</head><p>In this section, we discuss the pros and cons of the top-down and bottom-up designs targeted at the end of the previous section. More precisely, our discussion is driven by the following questions. (i) On the one hand, while the top-down design is the most common approach, can it fulfill all the expected requirements listed in our classification without requiring changes in the VIM codebase? (ii) On the other hand, while the bottom-up design seems to disrupt the design principles by requiring a la peer-to-peer strategies in VIM's internal mechanisms, should it be discarded?</p><p>Top-Down Design The top-down approach consists in designing a set of overlay components that interact with existing VIM APIs to avoid modifying VIM codebases. Avoiding codebase modifications is of particular importance in fast-growing software stacks. For example, a new version of OpenStack is released every six months with a lot of changes in the codebase, whereas changes rarely impact the APIs. Therefore, a top-down design makes VIM development and the overlay system components independent. If designed to do so, the system can easily allow L4, i.e., the support of different versions and types of VIMs -as demonstrated by FogBow <ref type="bibr" target="#b7">[9]</ref>.</p><p>Unfortunately, a top-down design cannot satisfy all L2 requirements without extending or revising the existing VIM codebase. For example, OpenStack Tricircle <ref type="bibr" target="#b4">[6]</ref>, a top-down project to allow virtual networking across different sites, ended up "breaking" the core of OpenStack by introducing specific L2 mechanisms. Such intrusive modifications negate the aforementioned independence. Moreover, L2 features in general require reimplementing many low-level VIM functionalities at the overlay level. For instance, the OpenStack "boot a VM" process looks as follows from a bird's-eye view: (1) Get the URL of the image by looking up in the database, (2) Schedule and boot the VM. Thus, booting a VM on Site 1 using an image defined in Site 2 would require implementing a dedicated workflow at the overlay level in order to interact with both sites and copy the image from the image manager (i.e., Glance) of Site 2 to the one of Site 1 before booting the VM. This is valid for most L2 features such as a fine-grained authorization management with different rights in different edge sites (L2.1) or a cross-site scheduling functionality when sites are specified implicitly (L2.2). Such a mechanism will be similar to the placement workflow already available in Nova.</p><p>Bottom-Up Design In a bottom-up design, the system is not limited by what is available through VIM APIs. Specifically, there are two possibilities: i) rearchitecting the services/components of existing cloud platforms, in our case OpenStack; ii) through a clean-slate approach.</p><p>By rearchitecting OpenStack to allow native collaboration, several 'local' features can be supported across the entire edge infrastructure for free. For instance, the aforementioned OpenStack "boot a VM on Site 1 with image on Site 2" process would be feasible without modifying the VIM codebase if Site 1 can either directly contact the image manager of Site 2, or share the database backend with Site 2. However, since OpenStack has not been designed to be collaborative, most mechanisms must be revised to consider side-effects related to collaboration operations. For instance, a VM boot process initiated on Site 1 can be completed on Site 2. The question is then to define where the states related to this VM should be stored, keeping in mind the split-brain challenge. It is noteworthy that identifying all these aspects requires the understanding of existing code, which is a daunting task in respect to the OpenStack ecosystem size.</p><p>Therefore, the complexity of rearchitecting existing services/components might make a clean-slate approach as the most likely approach for a bottom-up design.</p><p>Finally, L4 requirements cannot be intrinsically satisfied by the bottom-up approach while L5 implies strong limitations regarding how collaborations should be implemented (for instance sharing internal states of different edge sites between operators looks unlikely).</p><p>Summary There are pros and cons for both approaches, and none individually seems to meet all requirements. From the technical perspective, the bottomup design seems to be the most appropriate to cope with L1, L2 and L3 requirements, while the top-down is the only one to satisfy L4 and L5. From the business perspective, a top-down approach is likely to fulfill the short and medium-term Time-To-Market (TTM) requirements while a bottom-up approach is required to fulfill all requirements. From the codebase perspective, top-down approach has limited impact on existing VIM codebase while the bottom-up, despite the impact on existing codebase, allows implementing functionalities efficiently, without code duplication. All in all, our community should investigate a long-term solution that builds on a bottom-up approach. This will require first diving into OpenStack and understanding its intricate internal mechanisms leveraging tools like EnOS <ref type="bibr" target="#b8">[10]</ref> to mitigate the efforts, and second, to address scientific challenges such as the definition of a reference architecture for a resource management system for edge infrastructures, how to share internal system states in an edge context etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>The emergence of Network Function Virtualization (NFV) technologies as well as Internet of Things (IoT) and Augmented/Virtual Reality (AR/VR) applications herald a new era of cloud computing where infrastructures need to leverage resources at the edge of the network in order to cope with latency requirements. Despite this growing need, there is no cloud management system that is designed with such an infrastructure in mind.</p><p>In this paper, we presented reflections to initiate discussions on this hot topic. We outlined a systematic grouping of the features that administrators/DevOps expect and the requirements an edge infrastructure puts on a cloud management system. We briefly studied the use of existing IaaS managers (i.e., OpenStack) to control an edge infrastructure highlighting the need for an effective collaboration between the edge sites. We then discussed pros and cons of two possible strategies to follow when designing a solution: (1) a top-down approach which designs overlay components that interact with underlying IaaS managers without modifying their codebase; (2) a bottom-up approach that extensively improves existing software to fulfill the requirements. We concluded that in the short-term, a top-down approach is required to have a timely working solution available. In the long-term, both approaches should be considered simultaneously: bottom-up to realize a native and efficient system for the edge; top-down to enable collaboration between different cloud stacks (e.g., OpenStack, Kubernetes).</p><p>Finally, it is noteworthy that our study considers one kind of edge infrastructure. Other variants can be envisioned where, for example, third-party hardware resources can dynamically join and leave the infrastructure. For such cases, this study needs to be extended to integrate additional features not discussed in this paper.</p></div>
			<note place="foot" n="2"> Note that our study applies to any other resource manager built on the similar building blocks as OpenStack (e.g., Kubernetes). Such building blocks are conceptually identified in [13]. 3 Due to space limitation, we did not discuss a third scenario leveraging the OpenStack cells concept. However, we underline that such an approach has the drawback of the two discussed scenarios.</note>
		</body>
		<back>
			<div type="annex">
			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akamai</forename><surname>Cloudlets</surname></persName>
		</author>
		<idno>Ac- cessed: 2018-03-08</idno>
		<ptr target="http://cloudlets.akamai.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amazon</forename><surname>Lambda@edge</surname></persName>
		</author>
		<ptr target="https://aws.amazon.com/lambda/edge/.(Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openstack</forename><surname>Kingbird</surname></persName>
		</author>
		<ptr target="https://wiki.openstack.org/wiki/Kingbird.(Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">OpenStack: Open source software for creating private and public Clouds</title>
		<ptr target="https://www.openstack.org" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Openstack</forename><surname>Tricircle</surname></persName>
		</author>
		<ptr target="https://wiki.openstack.org/wiki/Tricircle.(Accessed" />
		<imprint>
			<biblScope unit="page" from="2018" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Evaluating OpenStack WANwide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toward</forename><surname>Fog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nfv</forename><surname>Edge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deployments</surname></persName>
		</author>
		<imprint>
			<pubPlace>Boston OpenStack Summit</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fog Computing and its role in the Internet of Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonomi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Addepalli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the first edition of the MCC workshop on Mobile Cloud Computing</title>
		<meeting>the first edition of the MCC workshop on Mobile Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fogbow: A middleware for the federation of IaaS Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brasileiro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Araújo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nóbrega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rocha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGrid)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="531" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toward a holistic framework for conducting scientific evaluations of OpenStack</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherrueau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-A</forename><surname>Pertin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lebre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simonin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</title>
		<meeting>the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="544" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Revising OpenStack to operate Fog/Edge Computing infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lebre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simonet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desprez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Cloud Engineering (IC2E)</title>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="138" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Characterization of failures in an operational ip backbone network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markopoulou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chuah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Ganjali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diot</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="749" to="762" />
			<date type="published" when="2008-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">IaaS Cloud architecture: From virtualized datacenters to federated Cloud infrastructures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moreno-Vozmediano</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llorente</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mobile-Edge Computing architecture: The role of MEC in the Internet of Things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabella</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vaillant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rauschenbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giust</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Consumer Electronics Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="84" to="91" />
			<date type="published" when="2016-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The emergence of Edge Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyanarayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edge Computing: Vision and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="637" to="646" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Re-designing Cloud platforms for massive scale using a P2P architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wuhib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Yadhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Cloud Computing Technology and Science (CloudCom)</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cloud Edge Computing: Beyond the Data Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The</forename><surname>Openstack Foundation</surname></persName>
		</author>
		<ptr target="https" />
		<imprint>
			<publisher>White Paper</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fog Computing: Platform and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Third IEEE Workshop on Hot Topics in Web Systems and Technologies (HotWeb)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Cloud is not enough: Saving IoT from the Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Lutz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wawrzynek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kubiatow-Icz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>In HotStorage</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
