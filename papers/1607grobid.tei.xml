<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive Memory System over Ethernet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
							<email>j-suzuki@ax</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruyuki</forename><surname>Baba</surname></persName>
							<email>t-baba@ax</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Hidaka</surname></persName>
							<email>y-hidaka@bq</email>
							<affiliation key="aff1">
								<orgName type="laboratory">† 2nd Computers Software Division</orgName>
								<orgName type="institution">NEC Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Higuchi</surname></persName>
							<email>j-higuchi@ax</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nobuharu</forename><surname>Kami</surname></persName>
							<email>n-kami@ak</email>
							<affiliation key="aff1">
								<orgName type="laboratory">† 2nd Computers Software Division</orgName>
								<orgName type="institution">NEC Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Uchida</surname></persName>
							<email>s-uchida@ap</email>
							<affiliation key="aff1">
								<orgName type="laboratory">† 2nd Computers Software Division</orgName>
								<orgName type="institution">NEC Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiko</forename><surname>Takahashi</surname></persName>
							<email>m-takahashi@ex</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoyoshi</forename><surname>Sugawara</surname></persName>
							<email>tom-sugawara@ap</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Yoshikawa</surname></persName>
							<email>yoshikawa@cd.jp.nec.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">System Platforms Research Laboratories</orgName>
								<orgName type="institution" key="instit1">NEC Corporation † IP Network Division</orgName>
								<orgName type="institution" key="instit2">NEC Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive Memory System over Ethernet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>For cloud computing, computer infrastructures need to be scaled up adaptively. However, their local memories cannot be expanded beyond the amount loaded to each computer. We present a method for scaling up of memory system beyond its local memory&apos;s capacity by high-speed page swapping using an adaptively attached solid-state disk (SSD) to a computer. Our PCI Express (PCIe) technology, &quot;ExpEther&quot; (Express Ether), interconnects a computer and a PCIe-based SSD via a standard Ethernet. The data transfer between the local memory of the computer and the SSD is performed without slow TCP/IP but with PCIe-standard direct memory access (DMA). It achieves IOPS of 33-K read and 36-K write for an access of 4-KB page size, which is twice as good as that for iSCSI with TCP-offloading. With the proposed method, a computer which only has 2-GB local physical memory can sustain its performance even when a 10-GB in-memory database is loaded.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>For cloud computing, computer infrastructures need to provide computing resources adaptively, in accord with resource utilization. However, one of the most important resources, local memory, cannot be shared among individual computers. This leads to a situation in which allocations to individual computers will be made on the basis of their respective needs at peak utilization times. This results in over-provisioning for most of their operational times.</p><p>This problem can be overcome by adaptively attaching external memory resources to a computer and expand its total memories. Some studies have reported sharing the local memory resources of individual computers using a technique of remote paging <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> or global management <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. With these methods, a bottleneck of the performance exists in memory management performed in software, and also in communication performed in such slow protocols as TCP/IP. Other studies have reported assigning external memory resources to a computer from special memory modules, for use as a second memory <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. These methods have a potential to realize adaptive memory attachment in a relatively small performance-overhead. However, it needs to overcome implementation hardship in software and hardware. For hardware, a memory module is hard to place in a network. And also for software, special treatment is necessary for such memories to hide the overhead of access latency over the network.</p><p>Today, flash memories appear to hold great promise for complementing local memories from the access speed point of view <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, and it is a good candidate for use in memory expansion. However, it usually provides only block access as a solid-state disk (SSD) with standardized local bus interfaces such as PCI Express (PCIe) and serial ATA, which are for use only inside a computer.</p><p>In this paper, we follow the external memory modules approach, but realize high-performance adaptive memory expansion with less implementation hardship and evaluate its performance. In our proposed method, we adaptively attach a PCIe-based SSD on Ethernet to a computer by page swapping. The performance test reveals it can sustain 10GB-in-memory database for a computer equipped with only 2-GB local memory.</p><p>The remainder of this paper is organized as follows. In Section 2, we present our method for adaptively attaching an SSD to a computer for memory expansion. In Section 3, we discuss our experimental results. We discuss future works in Section 4 and summarize our study in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attaching SSD using Ethernet</head><p>We propose an adaptive memory system where we attach a PCIe-based SSD to computers and use it for page swapping. This allows us to dynamically scale up computer systems beyond their local memories. We adopt existing PCIe over Ethernet technology, "ExpEther" <ref type="bibr" target="#b10">[11]</ref> which interconnects a computer and a PCIe-based SSD via an Ethernet. It provides two important features for the memory system:</p><p>• DMA over Ethernet: We perform the data transfer between the local memory of a computer and an SSD without TCP/IP but with PCIe-based direct memory access (DMA) over an Ethernet. It enables us to perform high-speed IOPS in a small access size sutable for use in page swapping which is performed in 4 KB in many platforms.</p><p>• Hot-Plug and Remove over Ethernet: By emulating PCIe-compliant hot-plug and remove functions over an Ethernet, we are able to adaptively attach SSD resources without stopping computer operations.</p><p>These technologies make it possible to perform highspeed page swapping and adaptive attachment of an SSD using commercially available Ethernet switches and PCIe-based SSDs. Although other interconnection technologies such as InfiniBand can perform the similar I/O attachment function <ref type="bibr" target="#b11">[12]</ref>, the special implementation to a host bus adaptor and an SSD is needed to adapt them to their high-speed interconnection specifications.</p><p>In the following subsections, we discuss our overnetwork DMA and hot-plug in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">DMA over Ethernet</head><p>Our method performs PCIe-based DMA between the local memory of a computer and an SSD over an Ethernet. Because it skips intermediate protocol processing and memory copying of the transferred data, it provides higher-speed data transfer over network than those of conventional protocols.</p><p>To perform PCIe-based DMA over an Ethernet, ExpEther provides two functions shown in <ref type="figure">Figure 1</ref>: (1) it extends the PCIe tree of each computer to an Ethernet, functioning as a PCIe switch; (2) it performs lossless, low-latency transmission between ExpEther bridges located in a computer-side and SSD-side <ref type="bibr" target="#b12">[13]</ref>.</p><p>Extending PCIe Tree to Ethernet: A PCIe switch is a device which splits a PCIe bus in order to connect a computer to multiple I/O devices. A pair of ExpEther bridges and Ethernet performs a comparable function of a single PCIe switch between the computer and I/O devices, by the emulation of response of a PCIe switch in ExpEther bridges. Since Ethernet transport part is transparent to OS, ExpEther is able to extend the PCIe tree of the computer to an Ethernet-attached I/O device, which here is an SSD, without the need for any additional device driver or OS modification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lossless Low-Latency PCIe Packet Transmission:</head><p>PCIe system assumes packet loss during transmission not to occur, and its performance depends on the latency of PCIe paths. For these reasons, we make ExpEther bridges perform lossless transmission and congestion control of encapsulated PCIe packets in an endto-end manner. The lossless transmission function resends frames lost during transmission, guaranteeing the transmission of frames encapsulating PCIe packets. The congestion control function minimizes queuing delay at Ethernet switches. It employs delay-based sending-rate control. It monitors round-trip times (RTTs) between ExpEther bridges and adjusts sending rates to minimize RTTs. The result is suppression of queuing at Ethernet switches, and minimizing transmission delay between the ExpEther bridges. The simulation result we reported in <ref type="bibr" target="#b12">[13]</ref> showed the transmission latency of our mechanism was less than 8.5% of that of TCP which adopts an algorithm based on packet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Hot-Plug and Remove</head><p>Since the standard PCIe protocol mainly focuses on I/O buses inside a computer and it is incapable of clusterwide I/O resource management where several computers are included in a cluster, with ExpEther, we perform the separate management for the allocation of I/O resources and their attachment to computers. The individual computer to which a given endpoint is to be connected is determined by the VLAN. As may be seen in <ref type="figure">Figure 2</ref>, ExpEther bridges for endpoints connecting to the same computer are assigned a VLAN number corresponding to that computer. To change a connection for a given endpoint, a system manager alters the VLAN number assignment.</p><p>For triggering the attachment and detachment of I/O resources, ExpEther bridges periodically broadcast keep-alive frames within individual VLAN groupings. When a frame broadcast by the ExpEther bridge of an endpoint not currently attached to a computer is received, the receiving computer-side bridge interrupts the computer and begins a PCIe-compliant hot-plug process. By way of contrast, when no information frame has been received for a certain period of time, the computer-side ExpEther bridge interrupts the computer for a hot-remove process. When an interconnected SSD is used for a swap device, the hot-remove of an SSD should be performed when data is not swapped to the device. In an uncoordinated event, a process whose data is placed on the removed SSD needs to be killed and restarted afterward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Results and Discussion</head><p>We performed two evaluation experiments. In the first experiment, we measured block I/O performance. In the  <ref type="figure">Figure 1</ref>. DMA data transfer using ExpEther. second experiment, we were able to show that a computer maintains its application performance even when it is consuming more memory than that available at its local memory slots.</p><p>The experiments were performed for the four setups shown in <ref type="figure">Figure 3</ref>: (a) an PCIe SSD used as a swap device is directly inserted to the I/O slot of a computer; (b) an SSD is connected to a computer by ExpEther (EE) bridge through DMA over Ethernet; (c) iSCSI is used as the interconnection protocol between a computer and an SSD (at the target, the SSD is inserted into an I/O slot, and the initiator and the target use a network interface card (NIC) with a TCP-offload engine (TOE)); and (d) iSCSI is used with an initiator and a target, employing an ordinary NIC.</p><p>Experimental setup (a) was used to determine a baseline for DMA data transfer to and from an I/O inserted into the I/O slot of the computer. Experimental setups (c) and (d) were measured as a reference of the performance of conventional protocols. <ref type="table" target="#tab_2">Table 1</ref> summarizes the specifications of the computers used in the experiments. The sum of the latency in the packet forwarding of a computer-side and an SSD-side ExpEther bridge was 1.45μs, while the latency in the inserted 10GbE switch was 0.92μs. Our prototype ExpEther bridge was implemented in a Field Programmable Gate Array (FPGA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Block I/O Performance</head><p>We measured the block I/O performance of the SSDs using Iometer <ref type="bibr">[14]</ref>. <ref type="figure" target="#fig_0">Figure 4</ref> shows the IOPS performance for the four setups shown in <ref type="figure">Figure 3</ref>. <ref type="table" target="#tab_1">Table 2</ref> summarizes IOPS performance for the 4-KB access used for data transfer in swap operations in the proposed method. It also shows the results when a standard Ethernet switch was inserted between a computer and an SSD. The values are normalized to that of the direct insertion. Read performance with our method is nearly   the same as that with direct insertion (less than a 10% difference) and almost twice as good as that for iSCSI with TOE. While write performance was roughly 30% worse than that with direct insertion, it was still roughly twice as good as that for iSCSI with TOE. The performance was almost the same when an Ethernet switch was inserted.</p><p>The experimental results for block I/O performance show that our method successfully provides the benefits of high-speed SSD access by employing DMA data transfer. It also provides better performance than does a conventional protocol for sharing storage resources in a network. This is because our proposed method can skip protocol processing and memory copying which are performed by conventional protocols.</p><p>We analyzed 30% degradation in write performance by monitoring the PCIe traffic between the computer and the SSD. We found that, in the write access, the DMA controller in an SSD sends up to four memory read requests and waits to send the next one until it receives the completion. With the increase of the number of requests sent by the DMA controller at one time, we would be able to decrease the interval and fill the link bandwidth with the transferred data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Application Performance</head><p>In this subsection, we show how a computer is able to maintain its application performance even when it is consuming more memory than that available at its local memory slots. As a benchmark application, we use Relational Database (RDB), with the RDB files stored in a ramdisk which resides in the local memory of a computer to demonstrate large memory consumption.</p><p>We used postgresql 8.1 for an RDB platform and a TPC-B-like pgbench <ref type="bibr">[15]</ref> for a benchmark tool. We gradually increased the total size of RDB files which were placed in a ramdisk. When the total size of database files exceeded the size of the local memory, portions of the files began to be swapped out to a swap device. We performed the experiment using the SSD as a swap device in the previously mentioned 4 setups. With the setup (c) (iSCSI with TOE), unfortunately, we were unable to measure performance due to TOE software failure.</p><p>We first measured how much our method is capable of expanding memory. <ref type="figure">Figure 5</ref> shows the performance of our method (setup (b) in <ref type="figure">Figure 3</ref>) and the HDD base lines for varying size of RDB files. We tested our method for local memory sizes of 4 and 2 GB, and the Ethernet-attached SSD was used as a swap device. For the baselines, the memory sizes were 8, 4, and 2 GB, and a local HDD was used as a swap device. With respect to the baselines, when RDB files started to be swapped out to the local HDD, performance decreased steeply because of the low access speed of the HDD. With our method, there was less performance degradation, for both 2 and 4-GB local memories, when RDB files started to be swapped out to the SSD. The average swap-in and swap-out for the 10-GB file size were 100 MB/s and 42 MB/s for the 2-GB-memory case, and 78 MB/s and 67 MB/s for the 4-GB-memory case. The performance when the SSD was directly inserted to the I/O slot of the computer (setup (a)) was same as our method. These results show application performance is sustained with our method even when the computer is consuming more memory than that which is available at its local memory slots. The results in <ref type="figure">Figure 5</ref> also indicate the effectiveness of local memory reduction. For 6-GB RDB files, performance results achieved with our method in the 4-GB local memory case (33% memory reduction) were only 13% worse than those for the 8-GB baseline, and our results for the 2-GB case (66% reduction) were only 27% worse.</p><p>We next measured how much our method outperformed a conventional protocol in the sharing of storage resources in a network. <ref type="figure">Figure 6</ref> shows the performance of our method and that of iSCSI (setup(d)). In the best cases, our method outperformed iSCSI by 113% with a 4-GB local memory and by 139% with a 2-GB local memory. These differences result from differences in data transfer methods. Our method uses DMA data transfer over an Ethernet, while iSCSI requires protocol processing and memory copying in order to transfer data. Although, as has previously been noted, we were unable to obtain performance results for iSCSI with TOE, the calculated results showed the performances for the case of iSCSI with TOE for the 10-GB file size were 1711 transactions per sec. for 4-GB local memory and 1541 transactions per sec. for 2-GB memory. In the calculation, we supposed that the expected number of accesses to the SSD per transaction was same among different setups and used the relative block I/O performances illustrated in <ref type="figure" target="#fig_0">Figure 4</ref> to estimate the pgbench performance of iSCSI with TOE by the results of ExpEther and iSCSI.</p><p>The merits of memory expansion with the proposed method are also apparent from the standpoint of CPU utilization. <ref type="figure" target="#fig_2">Figure 7</ref> shows CPU utilization levels with respect to cases when the local memory was set to 4 and 2 GB, and the Ethernet-attached SSD was used as a swap device. It also shows the case of the baseline with the memory size of 8 GB. The results shown here are for 10-GB RDB files and indicate that, with the proposed method, the CPU time was spent for application processing. On the other hand, in the 8-GB-memory baseline case, most of the CPU time is spent waiting for the completion of I/O requests for swap operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Future Works</head><p>In our work, we attach an SSD to a single computer and it is exclusively used by its computer. Ethernetconnected SSD resources should be shared among multiple computers for efficient hardware utilization. This would be enabled by using recently standardized PCIe I/O virtualization where an I/O device can process multiple contexts <ref type="bibr" target="#b13">[16]</ref>. Also, we use one SSD for a swap device. Multiple devices could be used to increase the capacity and reliability. The multiple devices can be coordinated at device driver level using a RAID method or at OS level to coordinate data placement and I/O device assignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented adaptive memory system. It attaches a PCIe-based SSD to a computer over Ethernet by page swapping to scale it up beyond its local memories. The proposed system with our ExpEther technology enables high-speed page swapping and adaptive attachment of an SSD using commercially available Ethernet switches and PCIe-based SSDs. The block I/O performance is twice as good as that for iSCSI with TOE, which indicates higher speed swapping than conventional protocols. Further, application benchmark of a database shows we can sustain the performance when 10-GB in-memory database is loaded to a computer equipped with only 2-GB memory. These evaluation results show we can realize both highperformance adaptive memory expansion and less implementation hardship with our proposed method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Block I/O performance. (a) Random read I/O per second (IOPS). (b) Random write IOPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .Figure 5 .</head><label>65</label><figDesc>Figure 6. pgbench performance for EE and iSCSI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. CPU utilization with pgbench.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>IOPS for 4 KB access. Performance values 
are normalized to Direct Insertion. 

(b) EE 
(c) iSCSI 

w/ TOE 
(d) iSCSI 

Ran. Read 
92 
50 
14 

Ran. Write 
74 
42 
14 

Ran. Read w/ Switch 
91 
46 
14 

Ran. Write w/ Switch 
68 
39 
14 

(a) Random Read IOPS 

0 

10000 

20000 

30000 

40000 

50000 

512 4K 16K 32K 64K 256K 1M 
4M 
Access Size 

IOPS 

Direct Insertion 
EE 
iSCSI with TOE 
iSCSI 

(b) Random Write IOPS 

0 

10000 

20000 

30000 

40000 

50000 

60000 

512 4K 16K 32K 64K 256K 1M 
4M 
Access Size 

IOPS 

Direct Insertion 
EE 
iSCSI with TOE 
iSCSI 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 . Experimental environments.</head><label>1</label><figDesc></figDesc><table>Experiment 
Block I/O 
Database 

CPU 
Intel 
Core 
2 

2.66GHz 

Intel Core 2 Quad 

2.83GHz 

OS 
Cent OS 5.3 kernel-2.6.18 

Ethernet 
10GbE 

SSD 
16-GB partition of 160-GB Fusion IO SSD 

(a) Direct Insertion 
into I/O Slot of Computer 

SSD 
SSD 

Computer 

(b) EE 

EE 
Bridge 

EE 
Bridge 

(c) iSCSI with TOE 

(d) iSCSI 

SSD 
SSD 

Initiator 

TOE 
NIC 

TOE 
NIC 

Target 

SSD 
SSD 

EE 
Bridge 

EE 
Bridge 

Computer 

TOE 
NIC 

TOE 
NIC 

Initiator 

NIC 
NIC 

Target 

SSD 
SSD 
NIC 
NIC 

(a) Direct Insertion 
into I/O Slot of Computer 

SSD 
SSD 

Computer 

(b) EE 

EE 
Bridge 

EE 
Bridge 

(c) iSCSI with TOE 

(d) iSCSI 

SSD 
SSD 

Initiator 

TOE 
NIC 

TOE 
NIC 

Target 

SSD 
SSD 

EE 
Bridge 

EE 
Bridge 

Computer 

TOE 
NIC 

TOE 
NIC 

Initiator 

NIC 
NIC 

Target 

SSD 
SSD 
NIC 
NIC 

Figure 3. Experimental setups. EE denotes Ex-
pEther. 

Computer 
A 

ExpEther 
Bridge 

ExpEther 
Bridge 

Ethernet 
Ethernet 

Computer 
B 

Computer 
C 

System 
Manager 

VLAN 1 
VLAN 2 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

ExpEther 
Bridge 

Endpoint 
A 

Endpoint 
B 

Endpoint 
C 

Computer 
A 

ExpEther 
Bridge 

ExpEther 
Bridge 

Ethernet 
Ethernet 

Computer 
B 

Computer 
C 

System 
Manager 

VLAN 1 
VLAN 2 </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Y. Hasebe, H. Shimamoto, and K. Egarashi for their helpful suggestions. We also thank K. M. Fujita for his cooperation to experiment. This work was partly supported by Ministry of Internal Affairs and Communications (MIC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Iftode</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compcon Spring &apos;93</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="538" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Implementation of a Reliable Remote Memory Pager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Markatos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nswap: A Network Swapping Module for Linux Clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Euro-Par 2003 Parallel Processing</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2790</biblScope>
			<biblScope unit="page" from="1160" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cooperative Caching: Using Remote Client Memory to Improve File System Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Dahlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USENIX OSDI</title>
		<imprint>
			<biblScope unit="issue">19</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Reducing Network Latency Using Subpages in a Global Memory Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Jamrozik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Disaggregated Memory for Expansion and Sharing in Blade Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
	<note>ISCA-36</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evaluation of Memory System Extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="84" to="93" />
		</imprint>
	</monogr>
	<note>ISCA-18</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Cost-Effective Main Memory Organization for Future Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IPDPS&apos;05</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">FlashCache: A NAND Flash Memory File Cache for Low Power Web Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<ptr target="http://www.intel.com/design/flash/nand/turbomemory/" />
	</analytic>
	<monogr>
		<title level="j">Intel Turbo Memory with User Pinning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ExpressEther -Ethernet-Based Virtualization Technology for Reconfigurable Hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE HOTI&apos;06</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">InfiniBand Architecture Specification</title>
		<imprint/>
	</monogr>
	<note>Release 1.1</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Congestion Control Algorithm for Data Center Area Communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimonishi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Int. CQR Workshop</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Single Root I/O Virtualization and Sharing Specification Revision 1.1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pci-Sig</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
