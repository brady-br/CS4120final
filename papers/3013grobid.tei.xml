<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Specialize in Moderation -Building Application-aware Storage Services using FPGAs in the Datacenter</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kuhring</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IMDEA Software Institute</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Garcia</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Universidad Autónoma de Madrid</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>István</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IMDEA Software Institute</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Specialize in Moderation -Building Application-aware Storage Services using FPGAs in the Datacenter</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In order to keep up with big data workloads, distributed storage needs to offer low latency, high bandwidth and energy efficient access to data. To achieve these properties, most state of the art solutions focus either exclusively on software or on hardware-based implementation. FPGAs are an example of the latter and a promising platform for building storage nodes but they are more cumbersome to program and less flexible than software, which limits their adoption. We make the case that, in order to be feasible in the cloud, solutions designed around programmable hardware, such as FPGAs, have to follow a service provider-centric methodology: the hardware should only provide functionality that is useful across all tenants and rarely changes. Conversely, application-specific functionality should be delivered through software that, in a cloud setting, is under the provider&apos;s control. Deploying FPGAs this way is less cumbersome, requires less hardware programming and flexibility increases overall. We demonstrate the benefits of this approach by building an application-aware storage for Parquet files, a columnar data format widely used in big data frameworks. Our prototype offers transparent 10Gbps deduplication in hardware without sacrificing low latency operation and specializes to Parquet files using a companion library. This work paves the way for in-storage filtering of columnar data without having to implement file-type and tenant-specific parsing in the FPGA.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the wide-spread deployment of big data workloads in the cloud, it is important to increase the efficiency of distributed storage. Most recent work in this area focuses on either highlytuned software solutions <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> to exploit fast networks, or pushes all functionality into specialized hardware nodes <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> that promise an order of magnitude improvement in energy efficiency. Hardware solutions, however, face two impediments: 1) deploying them in the cloud is uneconomical unless * Work done while employed at the IMDEA Software Institue. high utilization can be achieved and 2) they are limited in the operations they can efficiently support due to small on-chip memories and the fact that functionality is typically translated to pipelines or circuitry with an upper bound on size.</p><p>Field Programmable Gate Arrays (FPGAs) are overcoming the first impediment in part thanks to machine learning and similar compute-intensive workloads that are often bound by the performance of general-purpose CPUs. As a result, programmable accelerators are becoming common and can be rented in the public cloud of companies such as Amazon, Baidu and Huawei. Furthermore, they have been recently successfully used in production <ref type="bibr" target="#b6">[7]</ref> at Microsoft where they accelerate networking functions in the Azure Cloud. These uses demonstrate that FPGAs and similar programmable hardware can be economical if they benefit a large number of tenants. Beyond compute kernels and network functions, even complex stateful applications, such as low-latency key-value stores <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>, can be implemented on FPGAs, resulting in at least an order of magnitude improvement in energy efficiency.</p><p>Given these developments and the growing size and heterogeneity of FPGAs <ref type="bibr" target="#b8">[9]</ref>, new opportunities emerge in architectural exploration for data-intensive applications. The challenge of providing rich functionality with bounded hardware resources is, however, not solved yet. As an example, in the case of smart storage, even though the FPGA could implement different application-specific modules, it is unclear how to share the device efficiently across tenants. If the device is spatially partitioned, less functionality can be provided for each tenant, and this could lead to wasted resources if not all applications are used equally often. Temporal partitioning is not a perfect solution either, because reconfiguring parts of the FPGA can be done only at coarse granularity <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref> and takes tens to hundreds of milliseconds. Proposal. We make the case that, for specialized-hardwarebased storage to be practical, it has to cater to many tenants and use-cases. The way to achieve this is by splitting functionality: keeping the common data or compute-intensive parts of workloads on the hardware and providing additional tenantand application-specific operations in software libraries. Such a separation also reduces the hardware programming effort and allows faster adaptation to changing workloads, which is important in the context of today's quickly evolving dataintensive service offering <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref><ref type="bibr" target="#b13">[13]</ref>. Even though the underlying idea is not new, we believe it is time to revisit it from the perspective of FPGA-based storage and cloud service providers. Furthermore, the common blocks we can identify across applications can be used to guide the process of developing FPGAs tailored to the needs of datacenter workloads.</p><p>Applying the Methodology. To demonstrate our approach, we implement a storage solution with line-rate data management and deduplication in hardware and move applicationspecific operations to a client library without reducing performance. We chose to target Apache Parquet <ref type="bibr" target="#b14">[14]</ref> in our prototype, a commonly used columnar file format, but the methodology is generalizable to other formats as well. This is because overall it ensures that the FPGA can be in the role of both a general and an application-aware storage node and, in the future, the free chip space can be used to provide instorage processing to multiple tenants, even if only a subset of them store Parquet files.</p><p>The software library breaks files up based on their internal structure and stores them as multiple smaller entries in the storage node. This reduces the size of buffers on the FPGA and keeps the data management logic general purpose. Since the software library handles file-specific meta-data processing, it is possible to implement in-storage processing without requiring Parquet-specific parsers on the FPGA. This is important because the requirement to parse different file formats can reduce the usable space on the FPGA: we know from earlier work <ref type="bibr" target="#b15">[15]</ref> that meta-data parsing and data fetching logic for simple database pages can be comparable in size to a 10Gbps Regex processing module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>FPGA-based Storage. There is an increasing interest in offering FPGA-based or FPGA-accelerated distributed storage <ref type="bibr">[4-6, 8, 18]</ref>. This interest is in large motivated by energy efficiency concerns: when deploying TBs of storage, the energy footprint of individual nodes becomes an important cost factor and FPGA-based solutions offer orders of magnitude better energy efficiency than traditional CPU-based solutions. An other advantage of FPGAs (and other specialized hardware) is that they are well suited to network-facing operation because they can guarantee line-rate operation by design. A drawback of many FPGA-based KVS prototypes is that they are designed with small (16-512 B) key-value pairs in mind, motivated by caching use-cases -when targeting big data workloads, however, this design decision has to be revisited.</p><p>Deduplication Strategies. Deduplication has been studied in extensive related work and one important differentiating factor between solutions is the way that files are subdivided (chunked) for fingerprinting. Fixed chunk size solutions <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref> are computationally cheaper and offer good deduplication ratios for files that are modified through append operations. Conversely, for files that are changed through random insertions and deletions, variable chunk size hashing <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b21">[21]</ref><ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref>, that computes a running hash of the data to identify content-based "cut" points, is better suited.</p><p>Another differentiating factor is the location of deduplication: on the client-side <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b24">[24]</ref><ref type="bibr" target="#b25">[25]</ref><ref type="bibr" target="#b26">[26]</ref> or in the storage node/device <ref type="bibr" target="#b26">[26]</ref><ref type="bibr" target="#b27">[27]</ref><ref type="bibr" target="#b28">[28]</ref><ref type="bibr" target="#b29">[29]</ref>. In the case of the former, the bandwidth requirement when writing to storage is reduced by not sending duplicate data, but latency is increased because multiple round-trips are required to identify which parts of a file are unique. This approach also requires the hashing to happen on the client, which can interfere with the client workload. Performing deduplication inside in the storage node addresses these shortcomings but requires that the underlying hardware guarantees high throughput for deduplication operations.</p><p>As related work already explored <ref type="bibr" target="#b23">[23]</ref>, deduplication methods that take into account the internal structure of files are likely to perform better than general-purpose ones. In a similar vein, we propose a hybrid chunking scheme that first breaks up the files into relatively small pages (in the range of 1 KB -8 KB) along their internal structure without requiring hash computations on the clients, then stores pages as multiple keyvalue pairs, depending on the maximum value size supported by the KVS. This results in behavior similar to variable size chunking and in very similar, or better, deduplication ratios. Parquet Files. Emerging machine learning workloads have made column-based file formats, such as Apache Parquet <ref type="bibr" target="#b14">[14]</ref>, wide-spread. Depending on the computation pipeline, users often access only a subset of the columns in these files, and having a storage solution that can efficiently handle such partial reads reduces data movement bottlenecks. Furthermore, Parquet files are often transformed in ways that touch only specific columns or derive new ones in the feature engineering step of machine learning pipelines <ref type="bibr" target="#b30">[30]</ref>, which leads to partially duplicate data and therefore potential for deduplication. Starting Point. This work is based on Multes <ref type="bibr" target="#b31">[31]</ref>, an open source, replicated <ref type="bibr" target="#b32">[32]</ref>, multi-tenant key-value store (KVS) with performance and data isolation guarantees across tenants. The KVS supports basic set/get/delete operations executed as part of a single pipeline tailored to 10Gbps network speeds and parameterizable to match the performance characteristics of emerging NVM devices (for details see <ref type="bibr" target="#b7">[8]</ref>). Internally, a high throughput Cuckoo hash table <ref type="bibr" target="#b33">[33]</ref> is combined with a slab-based storage allocator. The hash table stores keys and value-pointers and the actual values are stored separately in a "value area" that is managed by the allocator. It has five slab classes, increasing by factor of two until 1KB, and ensures that variable sized values can be stored without too much internal fragmentation.</p><p>Multes stores data in DRAM, which is a placeholder for non-volatile media. The design allows for using durable storage that is fast enough for 10Gbps operation (e.g. to NVM DIMMs) by resizing on-chip buffers but without having to change the KVS design. Hardware Implementation. We achieved the goals stated at the beginning of this section by extending the internal pipeline of Multes with deduplication. For this, we had to modify the key management logic, but this does not slow down the system nor changes the way its multi-tenancy and replication mechanisms works.</p><p>We added a fingerprinting step that precedes the hash table operations <ref type="figure" target="#fig_0">(Figure 1</ref>), and each value that is part of a set request is hashed to determine its fingerprint. For this, we use open-source SHA256 cores 1 in a data-parallel way. Each core takes 66 clock cycles at 156.25 MHz to hash 64B of data, so we deploy nine parallel ones to achieve 10Gbps line-rate (8B/cycle at 156.25 MHz) fingerprinting.</p><p>Since the hash  structure of the hash table entries slightly compared to Multes so that each entry holds not only a key and a pointer (which is a combination of a value memory location and the length of the value) but also an additional "Meta-data" field (see bottom of <ref type="figure" target="#fig_0">Figure 1</ref>). In the case of regular keys, this field is used to store the fingerprint <ref type="bibr" target="#b1">2</ref> of the values and in the case of fingerprint entries, it is used to store the number of keys with that particular value (i.e., a reference counter). For reading the value of a key we can use the original hash table logic since we store the pointers to the values with the keys. This means that the read throughput of Multes++ is identical to that of Multes. As Algorithm 1 shows, write operations require an additional memory access per key-value pair, which could result in lower throughput for small values (&lt;128B) but targeting small values for deduplication does not pay off due to the meta-data overhead required. Delete operations behave similarly to writes because they need to update the fingerprint entry as well. Software Library.</p><p>We built the software library for Multes++ in Golang. Beyond the basic get/set operations, the library also provides the ability of storing very large values and arrays transparently <ref type="figure" target="#fig_1">(Figure 2</ref> shows its layers). The Parquet file functionality is layered on top of these operations.</p><p>One limitation of FPGA designs shows up when dealing Figure 3: The library can be called from Python and clients can access specific columns directly. In this snippet, the columns are loaded into a Pandas DataFrame for analysis.</p><p>with very large request sizes: To handle a multi-KB set request, for instance, the FPGA has to have dedicated buffer space for each tenant. Since on-chip memory is limited, these buffers have to be moved to DRAM, which increases latency and could jeopardize line-rate guarantees. In Multes++, we limit key-value sizes to the KB range, so that each logical request will fit in an MTU and fragmentation happens only seldom. This allows Multes++ to operate with small buffers. We handle larger values by slicing them on the client and storing multiple key-value pairs. To differentiate between user-provided keys and "slice" ones, the library appends index bytes to the key. The first key-value pair encodes how many additional ones follow. To improve throughput, the library batches the access to the slices of the same logical key. In addition to large value operations, we also implemented an array abstraction in the library. Arrays are accessed through a "directory value" that stores the internal keys pointing to the array entries. These can be single values, long values, or even arrays themselves.</p><p>In order to manipulate Parquet files, we embedded an open source Parquet library 3 in our solution to divide the Parquet file into pages. We store all pages as array entries, with filespecific meta-data stored at the first locations in the array. It is also feasible to plug the library into clients written in other languages. In Listing 3 we show, for instance, our experimental go-to-Python binding. Clients can access the Parquet files either as a whole or on a per-column basis. This reduces unnecessary data movement, and opens up opportunities for offloading computation to the storage node in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Setup. We use four dual-socket servers with Intel Xeon Silver 4114 CPUs and 10Gbps networking and a Xilinx VC709 Board with a XC7VX690T FPGA and 8GB of DRAM that is used both for the hash table and the value storage.</p><p>The client machines ran Multes++'s Go-based client library and used Parquet files generated from the datasets available at the San Francisco Open Data Website <ref type="bibr" target="#b3">4</ref> . We compare Multes++ with Memcached, that even though not the most optimized software solution, is representative of the performance of general purpose systems. For memcached we change the lowest layer in the library but the others remain the same.</p><p>The evaluation focuses on the perspective of individual clients storing and retrieving data from Multes++, simulating the behavior of cloud tenants running machine learning jobs, e.g., in Python. In this scenario, even though a tenant can have many parallel tasks, each one is heavily impacted by the data retrieval rate from the storage nodes and can not parallelize across multiple machines. Resource Consumption. Multes++ occupies 57% of the logic resources and 42% of BRAM on the chip with an 8 tenant setup (Multes needs 51% and 37% for the same), the remaining chip space can be used to implement processing on the reading path. It is important to highlight that this deployment has only a single copy of all data management modules, which are then shared by tenants -clearly, replicating these several times is not an option.</p><p>The estimated power consumption of the FPGA when running Multes++ is 12W, an order of magnitude lower than what a general purpose CPU would use to provide 10Gbps KVS functionality, without additional processing <ref type="bibr" target="#b7">[8]</ref>.</p><p>Latency. When compared to Multes, the only step that has been added in Multes++ that impacts latency is the deduplication logic for set (and delete) operations. The overhead is composed predominantly of the SHA256 hashing that adds in the range of 3µs of latency 5 per 512 B values <ref type="figure" target="#fig_3">(Figure 4</ref>). The overhead of the additional DDR memory access is negligible.</p><p>As for manipulating array data structures, since this operation is entirely implemented in software, it is possible to run it both on top of Multes++ and Memcached. <ref type="figure" target="#fig_4">Figure 5</ref> shows that when reading a single element of an array, two RTT to the server have to be paid: one to retrieve the array meta data and one to read the actual value. If the clients read multiple elements in a single operation, however, the cost of retrieving the meta data is quickly amortized. Memcached shows the same trend as the FPGA, but its base latency is significantly higher than two accesses to Multes++.  Throughput. In <ref type="figure" target="#fig_5">Figure 6</ref>, we focus on read/write bandwidth as a function of value size ranging from 128B to 64KB (in the Parquet use-case the divided parts of the Parquet file are typically in the 1-8 KB range). The bottom lines in <ref type="figure" target="#fig_5">Fig- ure 6</ref> show the read and write bandwidth that a single client experiences when interacting with Multes++ or Memcached. Not surprisingly, at lower bandwidths, the memcached-backed version can match the FPGA in terms of throughput. The figure also shows that with more clients (128) 10Gbps line-rate is successfully achieved for setting values that are at least 512 B (for smaller values the hash table and memory allocator become a bottleneck). Since Multes++ does not change the read logic of Multes and the latter supports line-rate operation, we omit these measurements for simplicity.</p><p>Deduplication. <ref type="figure" target="#fig_6">Figure 7</ref> shows storing in Multes++ the Police dataset in original and then a second time with a) half of its rows filtered out, then b) with one column removed (without re-encoding the rest of the pages). We use our Parquetaware storing method, as well as storing key-value pairs determined by the state of the art chunking methods. For this, we use a helper application that computes the running hash of the data in the Parquet file using Rabin fingerprinting <ref type="bibr" target="#b34">[34]</ref> to determine variable sized chunks (VCS). Based on a 48B sliding window the tool finds cut points, aiming for 512B and 1KB chunks on average (in practice the average size is around 600B, respectively 1KB).</p><p>As <ref type="figure" target="#fig_6">Figure 7</ref> shows, our proposal can store the modified file variants in negligible additional storage space. The overhead is mostly dominated by meta-data (the keys and fingerprints describing the new file) and the unique page data only represents a quarter of it. When storing the files using VCS-based chunking, the deduplication results are similar. It is important to point out that when using a chunking configuration that produces values smaller than the FPGA's internal maximum (1KB), there are more key-value pairs to be managed, which in turn increases the size of the meta-data (almost doubling it in our case). The effectiveness of data page deduplication, however, is very similar to the other two variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Given the increasing availability of programmable hardware in the datacenter, we believe that future distributed storage solutions should aim to utilize them as efficiently as possible. Furthermore, since the size and capabilities of such programmable chips are increasing, it is important to understand what works best for the domain of smart storage -possibly influencing this way the design of future programmable hardware tailored to the datacenter market.</p><p>To this end, we make the case that the design of cloud storage systems has to be driven by a service-centric methodology: implement stable multi-tenant functionality in hardware and tenant-and application-specific functionality on the client side in software. This way we can reduce programming complexity in hardware and ensure overall higher flexibility.</p><p>Our prototype, Multes++, is an FPGA-based key-value store that offers line-rate in-storage deduplication and, thanks to its software library, can store and manipulate Parquet files directly. The service provider could implement further library versions for different columnar formats with minimum effort even though the underlying service remains general purpose.</p><p>An important benefit of storing Parquet files in a divided way is that it leads to at least as high deduplication ratios as the state of the art, while it also enables future processing of columnar file formats inside the FPGA-based storage node, without having to dedicate resources to file-type specific parsing logic for each tenant (this could be even impossible due to on-chip resource constraints).</p><p>Even though it has been shown that it is possible to build complex systems entirely using specialized hardware, in production environments the presence of general purpose CPU cores would be welcome. In the case of Multes++, these could help with control and configuration of the FPGA-based service. For instance, reconfiguring replication groups after a failure requires branching logic that can't efficiently take advantage of FPGA resources. Therefore, our first question revolves around the choice of hardware for a Multes++-like system. Should we use a regular FPGA attached to a regular server machine that can handle management tasks on the side (e.g., similar to Catapult <ref type="bibr" target="#b6">[7]</ref>), or rather a stand alone FPGA with a small embedded CPU (e.g., Xilinx Zynq devices 6 )? Even though there is a clear benefit in pushing both networking and data management tasks into the FPGA (which suggests a large-FPGA-small-CPU configuration), is there a scenario, in the distributed storage context, where using a large-CPU-small-FPGA configuration would be more useful (e.g., Intel XEON+FPGA <ref type="bibr" target="#b35">[35]</ref>)?</p><p>Second, it is also a question what emerging operations could be offloaded into FPGA-based storage in a multi-tenant setting. For the reasons explained in the paper, choosing processing functionality that benefits only a small subset of tenants and workloads is not economical, so instead we have to devise operators that either benefit a wide cross-section of workloads (e.g. compression) or that are parameterizable at runtime to adjust their behavior. In our earlier work, we explored, for instance, how the same regular expression matching module could be used to filter strings, binary data, help with parsing or be used for decisions of set membership. As a trade-off, the resource requirements of the module were larger than that of a hard-coded regular expression matching unit, but smaller than deploying each of the previously listed modules individually.</p><p>Third, this work builds on the assumption that it is the service provider who controls the FPGA and the software library as well. This is motivated by the wide-spread usage of comprehensive frameworks such as Tensorflow <ref type="bibr" target="#b10">[10]</ref> and Microsoft ML.NET <ref type="bibr" target="#b11">[11]</ref>. In such frameworks it is realistic to assume that the service provider can specialize a part of the infrastructure and code they are executing and is already happening with the Google TPUs and Microsoft's Brainwave accelerators for machine learning. Are there compelling usecases for application-aware distributed storage outside of the obvious area of machine learning/big data analytics? Or are these the only candidates thanks to their data intensive nature?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Highlighted in green are the deduplication-specific operations and meta-data added to Multes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our library exposes several layers of abstraction, hiding low level communication and FPGA idiosyncrasies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3Figure 4 :</head><label>4</label><figDesc>Figure 4: When deduplicating SETs with 512B values, the added latency is visible but not significant for clients. Read operations are not impacted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Since array accesses require an additional level of indirection, accessing a single element will incur two RTT latencies. For multi-element access this overhead is amortized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The write throughput of the system reaches line-rate once the value size is &gt;512B. For smaller values it is bound by the hash table logic dealing with deduplication meta data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: After storing a Parquet file (156 MB), we show the overhead of storing a version with half of the rows (78 MB) and then a column removed (71MB). Our proposed method fairs as good as the state of the art (the overhead includes both data and meta-data).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>table stores all</head><label>stores</label><figDesc></figDesc><table>its entries in off-chip memory, 
it has a large enough capacity to hold both keys and fingerprint 
entries of values. To achieve this, we changed the internal 

1 https://github.com/secworks/sha256 

Algorithm 1 Writing to the KVS with deduplication 

1: function SET(Key,Value) 
2: 

F = SHA256Hash(Value) 
Compute fingerprint 

3: 

[Pointer, Count] = GetHashTable(F) Read #1 -NULL returned if not found 

4: 

[_, F Key ] = GetHashTable(Key) 
Read #2 -NULL returned if not found 

5: 

if F Key !=NULL and F!=F Key then 

6: 

DeleteHashTable(Key) If the key already has a different value, delete it 

7: 

end if 

8: 

if F Key !=NULL and F=F Key then 

9: 

SetHashTable(F, [Pointer,Count]) No change in counts (can be omitted) 

10: 

end if 

11: 

if F Key ==NULL and Pointer != NULL then 

12: 

SetHashTable(F, [Pointer,Count + 1]) New key with an existing value 

13: 

else 

14: 

if Pointer == NULL then 

15: 

Pointer = Malloc(Size(Value)) 
Allocate space for new value 

16: 

SetHashTable(F, [Pointer, 1]) 
First time we store this value 

17: 

end if 

18: 

end if 

19: 

SetHashTable(Key, [Pointer, Fingerprint]) 
Update the key 

20: 

WriteValueArea(Pointer, Value) 
Write to value area 

21: end function 

</table></figure>

			<note place="foot" n="3"> Design Overview Our prototype system, Multes++, shows that by combining software and specialized hardware using a service-oriented methodology, it is possible to deliver application-specific improvements without reducing the efficiency benefits of the hardware nodes. It brings the following properties: 1. Line-rate read and write behavior in an energy efficient package -enabled by the use of specialized hardware. 2. Structure-aware storage and access of Parquet files without having to dedicate hardware resources to file-type-specific parsing -enabled by the use of a software library. 3. Efficient deduplication using a hybrid chunking schemeenabled by combining software and hardware.</note>

			<note place="foot" n="2"> We reduce fingerprints to 64bits in order to fit the current hash table. While false positives should be still extremely unlikely, in a production-grade system with large capacity it would be preferred to store fingerprints in full.</note>

			<note place="foot" n="5"> By increasing the frequency of the hashing cores, it would be possible to improve this metric by a factor of 2, but likely not much more.</note>

			<note place="foot" n="6"> https://www.xilinx.com/products/silicon-devices/soc/ zynq-ultrascale-mpsoc.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank our shepherd, Tim Harris, as well as Mark Silberstein for their valuable feedback on the work. We also thank Xilinx for their generous donation of software tools used in this project and G. Sutter, S. Lopez-Buedo and their group at UAM Madrid for lending us the FPGA board.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Farm: Fast remote memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dragojevi´cdragojevi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Castro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Architecting to achieve a billion requests per second throughput on a single key-value store server platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Seongil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="476" to="488" />
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ramcloud storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kejriwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Montazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Scaling out to a single-node 80gbps memcached server with 40terabytes of memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Vissers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage&apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kv-direct: high-performance in-memory key-value store with programmable nic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</title>
		<meeting>the 26th Symposium on Operating Systems Principles (SOSP&apos;17)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="137" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bluecache: A scalable distributed flash-based key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hicks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A cloudscale acceleration architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Putnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual IEEE/ACM International Symposium on Microarchitecture (Micro&apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Caribou: intelligent distributed storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1202" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Versal: The xilinx adaptive compute acceleration platform (acap)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vissers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">ACM/SIGDA International Symposium on FieldProgrammable Gate Arrays (FPGA&apos;19)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="83" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Machine learning at microsoft with ml. net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Interlandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Matusevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zahirazami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weimer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Serving dnns in real time at datacenter scale with project brainwave</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haselman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual IEEE/ACM International Symposium on Microarchitecture (Micro&apos;18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Amazon redshift and the case for simpler data warehouses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stefani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGMOD international conference on Management of data (Sigmod&apos;15)</title>
		<meeting>the 2015 ACM SIGMOD international conference on Management of data (Sigmod&apos;15)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1917" to="1923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Introducing Parquet: Efficient columnar storage for Apache Hadoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kestelyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cloudera Blog</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Runtime parameterizable regular expression operators for databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM&apos;16</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Resource elastic virtualization for fpgas using opencl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaishnav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Garside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 28th International Conference on Field Programmable Logic and Applications (FPL)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="111" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sharing, protection, and compatibility for reconfigurable fabric with amorphos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khawaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Landgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schkufza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Rossbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="107" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An FPGA memcached appliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Chalamalasetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Auyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Margala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/SIGDA international symposium on Field programmable gate arrays</title>
		<meeting>the ACM/SIGDA international symposium on Field programmable gate arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving duplicate elimination in storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Bobbarjung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dubnicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="424" to="448" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Opportunistic use of content addressable storage for distributed file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tolia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Satyanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Bressoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perrig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC&apos;03)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="127" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A lowbandwidth network file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Muthitacharoen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mazieres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="174" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Peerdedupe: Insights into the peer-assisted sampling deduplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Tenth International Conference on Peer-to-Peer Computing (P2P)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Metadata considered harmful. . . to deduplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Douglis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smaldone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage&apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data deduplication and tivoli storage manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tivoli Storage, IBM Software Group</title>
		<imprint>
			<date type="published" when="2007-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Why storeonce federated deduplication matters to hp -and should to you, too</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buffington</surname></persName>
		</author>
		<ptr target="http://www.hp.com/hpinfo/newsroom/press_kits/2014/HPDiscover2014/ESG_WP_HP_StoreOnce_Federated_Deduplication_June_2014.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backup and recovery: Accelerating efficiency and driving down it costs using data deduplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amatruda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMC Corporation</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chunkstash: Speeding up inline storage deduplication using flash memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC&apos;10)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">idedup: latency-aware, inline data deduplication for primary storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bisson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Goodson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Voruganti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST&apos;12)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse indexing: Large scale, inline deduplication using sampling and locality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lillibridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Eshghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bhagwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Deolalikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trezis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Camble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Conference on File and Storage Technologies (FAST&apos;09)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="111" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A few useful things to know about machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="87" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Providing multitenant services with FPGAs: Case study on a key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The International Conference on Field-Programmable Logic and Applications (FPL 2018)</title>
		<meeting>the The International Conference on Field-Programmable Logic and Applications (FPL 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Consensus in a box: Inexpensive coordination in hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>István</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vukolic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Networked Systems Design and Implementation (NSDI&apos;16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="425" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cuckoo hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Rodler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="144" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fingerprinting by random polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Rabin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A reconfigurable computing system based on a cache-coherent fabric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on ReConFigurable Computing and FPGAs (ReConFig&apos;11)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
