<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Connections Between Active Learning and Model Extraction Exploring Connections Between Active Learning and Model Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Giacomelli</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Protocol Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbai</forename><surname>Yan</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Kamalika Chaudhuri</orgName>
								<orgName type="department" key="dep2">Songbai Yan</orgName>
								<orgName type="laboratory">Protocol Labs; Somesh Jha</orgName>
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of California San Diego</orgName>
								<orgName type="institution" key="instit3">Irene Giacomelli</orgName>
								<orgName type="institution" key="instit4">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit5">University of California San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Connections Between Active Learning and Model Extraction Exploring Connections Between Active Learning and Model Extraction</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine learning is being increasingly used by individuals , research institutions, and corporations. This has resulted in the surge of Machine Learning-as-a-Service (MLaaS)-cloud services that provide (a) tools and resources to learn the model, and (b) a user-friendly query interface to access the model. However, such MLaaS systems raise concerns such as model extraction. In model extraction attacks, adversaries maliciously exploit the query interface to steal the model. More precisely, in a model extraction attack, a good approximation of a sensitive or proprietary model held by the server is extracted (i.e. learned) by a dishonest user who interacts with the server only via the query interface. This attack was introduced by Tramèr et al. at the 2016 USENIX Security Symposium, where practical attacks for various models were shown. We believe that better understanding the efficacy of model extraction attacks is paramount to designing secure MLaaS systems. To that end, we take the first step by (a) formalizing model extraction and discussing possible defense strategies, and (b) drawing parallels between model extraction and established area of active learning. In particular, we show that recent advancements in the active learning domain can be used to implement powerful model extraction attacks, and investigate possible defense strategies.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Advancements in various facets of machine learning has made it an integral part of our daily life. However, most real-world machine learning tasks are resource intensive. To that end, several cloud providers, such as Amazon, Google, Microsoft, and BigML offset the storage and computational requirements by providing Machine Learning-as-a-Service (MLaaS). A MLaaS server offers support for both the training phase, and a query interface for accessing the trained model. The trained model is then queried by other users on chosen instances (refer <ref type="figure">Fig. 1</ref>). Often, this is implemented in a pay-per-query regime i.e. the server, or the model owner via the server, charges the the users for the queries to the model. Pricing for popular MLaaS APIs is given in <ref type="table">Table 1</ref>.</p><p>Current research is focused at improving the performance of training algorithms, while little emphasis is placed on the related security aspects. For example, in many real-world applications, the trained models are privacy-sensitive -a model can (a) leak sensitive information about training data <ref type="bibr" target="#b3">[5]</ref> during/after training, and <ref type="bibr">(b)</ref> can itself have commercial value or can be used in security applications that assume its secrecy (e.g., spam filters, fraud detection etc. <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b51">53]</ref>). To keep the models private, there has been a surge in the practice of oracle access, or black-box access. Here, the trained model is made available for prediction but is kept secret. MLaaS systems use oracle access to balance the trade-off between privacy and usability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Machine Learning Overview</head><p>In this section, we give a brief overview of machine learning, and terminology we use throughout the paper. In particular, we summarize the passive learning framework in § 2.1, and focus on active learning algorithms in § 2.2. A review of the state-of-the-art of active learning algorithms is needed to explicitly link model extraction to active learning and is presented in § 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Passive learning</head><p>In the standard, passive machine learning setting, the learner has access to a large labeled dataset and uses it in its entirety to learn a predictive model from a given class. Let X be an instance space, and Y be a set of labels. For example, in object recognition, X can be the space of all images, and Y can be a set of objects that we wish to detect in these images. We refer to a pair (x, y) ∈ X × Y as a data-point or labeled instance (x is the instance, y is the label). Finally, there is a class of functions F from X to Y called the hypothesis space that is known in advance. The learner's goal is to find a functionˆffunctionˆ functionˆf ∈ F that is a good predictor for the label y given the instance x, with (x, y) ∈ X × Y. To measure how welî f predicts the labels, a loss function ℓ is used. Given a data-point z = (x, y) ∈ X × Y, ℓ( ˆ f , z) measures the difference betweenˆf betweenˆ betweenˆf (x) and the true label y. When the label domain Y is finite (classification problem), the 0-1 loss function is frequently used:</p><formula xml:id="formula_0">ℓ( ˆ f , z) = 0, ifˆfifˆ ifˆf (x) = y 1, otherwise</formula><p>If the label domain Y is continuous, one can use the square loss: ℓ( ˆ f , z) = ( ˆ f (x) − y) 2 . In the passive setting, the PAC (probably approximately correct) learning <ref type="bibr" target="#b54">[56]</ref> framework is predominantly used. Here, we assume that there is an underlying distribution D on X × Y that describes the data; the learner has no direct knowledge of D but has access to a set of training data D drawn from it.</p><p>The main goal in passive PAC learning is to use the labeled instances from D to produce a hypothesisˆfhypothesisˆ hypothesisˆf such that its expected loss with respect to the probability distribution D is low. This is often measured through the generalization error of the hypothesisˆfhypothesisˆ hypothesisˆf , defined by</p><formula xml:id="formula_1">Err D ( ˆ f ) = E z∼D [ℓ( ˆ f , z)]<label>(1)</label></formula><p>More precisely, we have the following definition.</p><p>Definition 1 (PAC passive learning <ref type="bibr" target="#b54">[56]</ref>). An algorithm A is a PAC passive learning algorithm for the hypothesis class F if the following holds for any D on X × Y and any ε, δ ∈ (0, 1): If A is given s A (ε, δ) i.i.d. data-points generated by D, then A outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ min f ∈F Err D ( f ) + ε with probability at least 1 − δ. We refer to s A (ε, δ) as the sample complexity of algorithm A.</p><p>Remark 1 (Realizability assumption). In the general case, the labels are given together with the instances, and the factor min f ∈F Err D ( f ) depends on the hypothesis class. Machine learning literature refers to this as agnostic learning or the non-separable case of PAC learning. However, in some applications, the labels themselves can be described using a labeling function f * ∈ F . In this case (known as realizable learning), min f ∈F Err D ( f ) = 0 and the distribution D can be described by its marginal over X. A PAC passive learning algorithm A in the realizable case takes s A (ε, δ) i.i.d. instances generated by D and the corresponding labels generated using f * , and outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ ε with probability at least 1 − δ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Active learning</head><p>In the passive setting, learning an accurate model (i.e. learningˆf learningˆ learningˆf with low generalization error) requires a large number of data-points. Thus, the labeling effort required to produce an accurate predictive model may be prohibitive. In other words, the sample complexity of many learning algorithms grows rapidly as ε → 0 (refer Example 1). This has spurred interest in learning algorithms that can operate on a smaller set of labeled instances, leading to the emergence of active learning (AL). In active learning, the learning algorithm is allowed to select a subset of unlabeled instances, query their corresponding labels from an annotator (i.e. oracle) and then use it to construct or update a model. How the algorithm chooses the instances varies widely. However, the common underlying idea is that by actively choosing the data-points used for training, the learning algorithm can drastically reduce sample complexity.</p><p>Formally, an active learning algorithm is an interactive process between two parties -the oracle O and the learner L. The only interaction allowed is through queries -L chooses x ∈ X and sends it to O, who responds with y ∈ Y (i.e., the oracle returns the label for the chosen unlabeled instance). This value of (x, y) is then used by L to infer some information about the labeling procedure, and to choose the next instance to query.</p><p>Over many such interactions, L outputsˆfoutputsˆ outputsˆf as a predictor for labels. We can use the generalization error (1) to evaluate the accuracy of the outputˆfoutputˆ outputˆf . However, depending on the query strategy chosen by L, other types of error can be used.</p><p>There are two distinct scenarios for active learning: PAC active learning and Query Synthesis (QS) active learning. In literature, QS active learning is also known as Membership Query Learning, and we will use the two terms synonymously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">PAC active learning</head><p>This scenario was introduced by <ref type="bibr">Dasgupta in 2005 [20]</ref> in the realizable context and then subsequently developed in following works (e.g., <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b24">26]</ref>). In this scenario, the instances are sampled according to the marginal of D over X, and the learner, after seeing them, decides whether to query for their labels or not. Since the data-points seen by L come from the actual underlying distribution D, the accuracy of the output hypothesisˆfhypothesisˆ hypothesisˆf is measured using the generalization error (1), as in the classic (i.e., passive) PAC learning.</p><p>There are two options to consider for sampling data-points. In stream-based sampling (also called selective sampling) , the instances are sampled one at a time, and the learner decides whether to query for the label or not on a per-instance basis. Pool-based sampling assumes that all of the instances are collected in a static pool S ⊆ X and then the learner chooses specific instances in S and queries for their labels. Typically, instances are chosen by L in a greedy fashion using a metric to evaluate all instances in the pool. This is not possible in stream-based sampling, where L goes through the data sequentially, and has to therefore make decisions to query individually. Pool-based sampling is extensively studied since it has applications in many real-world problems, such as text classification, information extraction, image classification and retrieval, etc. <ref type="bibr" target="#b37">[39]</ref>. Stream-based sampling represents scenarios where obtaining unlabeled data-points is easy and cheap, but obtaining their labels is expensive (e.g., stream of data is collected by a sensor, but the labeling requires an expert).</p><p>Before describing query synthesis active learning, we wish to highlight the advantage of PAC active learning over passive PAC learning (i.e. the reduced sample complexity) for some hypothesis class through Example 1. Recall that this advantage comes from the fact that an active learner is allowed to adaptively choose the data from which it learns, while a passive learning algorithm learns from a static set of data-points.</p><p>Example 1 (PAC learning for halfspaces). Let F d,HS be the hypothesis class of d-dimensional halfspaces 1 , used for binary classification. A function in f w ∈ F d,HS is described by a normal vector w ∈ R d (i.e., ||w|| 2 = 1) and is defined by</p><formula xml:id="formula_2">f w (x) = sign(w, x) for any x ∈ R d</formula><p>where given two vectors a, b ∈ R d , then their product is defined as a, b = ∑ d i=1 a i b i . Moreover, if x ∈ R, then sign(x) = 1 if x ≥ 0 and sign(x) = −1 otherwise. A classic result in passive PAC learning states that O( d ε log( 1 ε ) + 1 ε log( 1 δ )) datapoints are needed to learn f w <ref type="bibr" target="#b54">[56]</ref>. On the other hand, several works propose active learning algorithms for F d,HS with sample complexity 2 ˜ O(d log( 1 ε )) (under certain distributional assumptions). For example, if the underlying distribution is log-concave, there exists an active learning algorithm with sample complexity˜Ocomplexity˜ complexity˜O(d log( 1 ε )) <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b61">63]</ref>. This general reduction in the sample complexity for F d,HS is easy to infer when d = 1. In this case, the data-points lie on the real line and their labels are a sequence of −1's followed by a sequence of +1's. The goal is to discover a point w where the change from −1 to +1 happens. PAC learning theory states that this can be achieved with˜Owith˜ with˜O <ref type="formula">(</ref>  </p><formula xml:id="formula_3">f w (x) = −1 if w, x &lt; −1 +1 otherwise R −1 −1 −1 +1 +1 +1 +1 +1</formula><p>w * <ref type="figure">Figure 2</ref>: Halfspace classification in dimension 1.</p><p>the other hand, an active learning algorithm that uses a simple binary search can achieve the same task with O(log( 1 ε )) queries <ref type="bibr" target="#b18">[20]</ref> (refer <ref type="figure">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Query Synthesis (QS) active learning</head><p>In this scenario, the learner can request labels for any instance in the input space X, including points that the learner generates de novo, independent of the distribution D (e.g., L</p><p>can ask for labels for those x that have zero-probability of being sampled according to D). Query synthesis is reasonable for many problems, but labeling such arbitrary instances can be difficult if the oracle is a human annotator. Thus, this scenario better represents real-world applications where the oracle is automated (e.g., results from synthetic experiments <ref type="bibr" target="#b30">[32]</ref>). Since the data-points are independent of the distribution, generalization error is not an appropriate measure of accuracy of the hypothesisˆfhypothesisˆ hypothesisˆf , and other types of error are typically used. These new error formulations depend on the concrete hypothesis class F considered. For example, if F is the class of boolean functions from {0, 1} n to {0, 1}, then the uniform error is used. Assume that the oracle O knows f * ∈ F and uses it as labeling function (realizable case), then the uniform error of the hypothesisˆfhypothesisˆ hypothesisˆf is defined as</p><formula xml:id="formula_4">Err u ( ˆ f ) = Pr x∼{0,1} n [ ˆ f (x) 񮽙 = f * (x)]</formula><p>where x is sampled uniformly at random from the instance space {0, 1} n . Recent work <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b14">16]</ref>, for the class of halfspaces </p><formula xml:id="formula_5">Err 2 ( f w ) = ||w * − w|| 2</formula><p>where || · || 2 is the 2-norm.</p><p>In both active learning scenarios (PAC and QS), the learner needs to evaluate the usefulness of an unlabeled instance x, which can either be generated de novo or sampled from the given distribution, in order to decide whether to query the oracle for the corresponding label. In the state of the art, we can find many ways of formulating such query strategies. Most of existing literature presents strategies where efficient search through the hypothesis space is the goal (refer the survey by Settles <ref type="bibr" target="#b48">[50]</ref>). Another point of consideration for an active learner L is to decide when to stop. This is essential as active learning is geared at improving accuracy while being sensitive to new data acquisition cost (i.e., reducing the query complexity). While one school of thought relies on the stopping criteria based on the intrinsic measure of stability or self-confidence within the learner, another believes that it is based on economic or other external factors (refer <ref type="bibr">[50, Section 6.7]</ref>).</p><p>Given this diversity within active learning, we enhance the standard definition of a learning algorithm and propose the definition of an active learning system, which is geared towards model extraction. Our definition is informed by the MLaaS APIs that we investigated (more details in <ref type="table">Table 1</ref>).</p><p>Definition 2 (Active learning system). Let F be a hypothesis class with instance space X and label space Y. An active learning system for F is given by two entities, the learner L and the oracle O, interacting via membership queries: L sends to O an instance x ∈ X; O answers with a label y ∈ Y. We indicate via the notation O f * the realizable case where O uses a specific labeling function f * ∈ F , i.e. y = f * (x). The behavior of L is described by the following parameters:</p><p>1. Scenario: this is the rule that describes the generation of the input for the querying process (i.e. which instances x ∈ X can be queried). In the PAC scenario, the instances are sampled from the underlying distribution D. In the query synthesis (QS) scenario, the instances are generated by the learner L;</p><p>2. Query strategy: given a specific scenario, the query strategy is the algorithm that adaptively decides if the label for a given instance x i is queried for, given that the queries x 1 , . . . , x i−1 have been answered already. In the query synthesis scenario, the query strategy also describes the procedure for instance generation.</p><p>3. Stopping criteria: this is a set of considerations used by L to decide when it must stop querying.</p><p>Any system (L, O) described as above is an active learning system for F if one of the following holds:</p><formula xml:id="formula_6">-(PAC scenario) For any D on X × Y and any ε, δ ∈ (0, 1), if L is allowed to interact with O using q L (ε, δ) queries, then L outputsˆfoutputsˆ outputsˆf ∈ F such that Err D ( ˆ f ) ≤ min f ∈F Err D ( f ) + ε with probability at least 1 − δ.</formula><p>-(QS scenario) Fix an error measure Err for the functions</p><formula xml:id="formula_7">in F . For any f * ∈ F , if L is allowed to interact with O f * using q L (ε, δ) queries, then L outputsˆfoutputsˆ outputsˆf ∈ F such that Err( ˆ f ) ≤ ε with probability at least 1 − δ.</formula><p>We refer to q L (ε, δ) as the query complexity of L.</p><p>As we will show in the following section (in particular, refer § 3.2), the query synthesis scenario is more appropriate in casting model extraction attack as active learning when we make no assumptions about the adversary's prior knowledge.</p><p>Note that, other types queries have been studied in literature. This includes the equivalence query <ref type="bibr" target="#b2">[4]</ref>. Here the learner can verify if a hypothesis is correct or not. We do not consider equivalence queries in our definition because we did not see any of the MLaaS APIs support them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Extraction</head><p>In § 3.1, we begin by formalizing the process of model extraction. We then draw parallels between model extraction and active learning in § 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model Extraction Definition</head><p>We begin by describing the operational ecosystem of model extraction attacks in the context of MLaaS systems. An entity learns a private model f * from a public class F , and provides it to the MLaaS server. The server provides a client-facing query interface for accessing the model for prediction. For example, in the case of logistic regression, the MLaaS server knows a model represented by parameters a 0 , a 1 , · · · , a d . The client issues queries of the form </p><formula xml:id="formula_8">x = (x[1], · · · , x[d]) ∈ R d ,</formula><formula xml:id="formula_9">(x) = a 0 + ∑ d i=1 a i x[i].</formula><p>Model extraction is the process where an adversary exploits this interface to learn more about the proprietary model f * . The adversary can be interested in defrauding the description of the model f * itself (i.e., stealing the parameters a i as in a reverse engineering attack), or in obtaining an approximation of the model, sayˆfsayˆ sayˆf ∈ F , that he can then use for free for the same task as the original f * was intended for. To capture the different goals of an adversary, we say that the attack is successful if the extracted model is "close enough" to f * according to an error function on F that is context dependent. Since many existing MLaaS providers operate in a pay-per-query regime, we use query complexity as a measure of efficiency of such model extraction attacks.</p><p>Formally, consider the following experiment: an adversary A, who knows the hypothesis class F , has oracle access to a proprietary model f * from F . This can be thought of as A interacting with a server S that safely stores f * . The interaction has several rounds. In each round, A chooses an instance x and sends it to S. The latter responds with f * (x). After a few rounds, A outputs a functionˆffunctionˆ functionˆf that is the adversary's candidate approximation of f * ; the experiment considersˆfconsidersˆ considersˆf a good approximation if its error with respect to the true function f * held by the server is less then a fixed threshold ε. The error function Err is defined a priori and fixed for the extraction experiment on the hypothesis class F .</p><formula xml:id="formula_10">Experiment 1 (Extraction experiment). Given a hypothesis class F = { f : X → Y}, fix an error function Err : F → R.</formula><p>Let S be a MLaaS server with the knowledge of a specific f * ∈ F , denoted by S( f * ). Let A be an adversary interacting with S with a maximum budget of q queries. The extraction experiment Exp ε F (S( f * ), A,q) proceeds as follows 1. A is given a description of F and oracle access to f * through the query interface of S. That is, if A sends x ∈ X to S, it gets back y = f * (x). After at most q queries, A eventually outputsˆfoutputsˆ outputsˆf ;</p><p>2. The output of the experiment is 1 if Err( ˆ f ) ≤ ε. Otherwise the output is 0.</p><p>Informally, an adversary A is successful if with high probability the output of the extraction experiment is 1 for a small value of ε and a fixed query budget q. This means that A likely learns a good approximation of f * by only asking q queries to the server. More precisely, we have the following definition.</p><p>Definition 3 (Extraction attack). Let F be a public hypothesis class and S an MLaaS server as explained before. We say that an adversary A, which interacts with S, implements an ε-extraction attack of complexity q and confidence γ against</p><formula xml:id="formula_11">the class F if Pr[Exp ε F (S( f * ), A,q) = 1] ≥ γ, for any f * ∈ F .</formula><p>The probability is over the randomness of A.</p><p>In other words, in Definition 3 the success probability of an adversary constrained by a fixed budget for queries is explicitly lower bounded by the quantity γ.</p><p>Before discussing the connection between model extraction and active learning, we provide an example of a hypothesis class that is easy to extract.</p><p>Example 2 (Equation-solving attack for linear regression).</p><p>Let F d,R be the hypothesis class of regression models from R d to R. A function f a in this class is described by d + 1 parameters a 0 , a 1 , . . . , a d from R and defined by: for any</p><formula xml:id="formula_12">x ∈ R d , f a (x) = a 0 + ∑ d i=1 a i x i . Consider the adversary A ES that queries x 1 , . . . , x d+1 (d + 1 instances from R d ) chosen</formula><p>in such a way that the set of vectors {(1, x i )} i=1,...,d+1 is linearly independent in R d+1 . A ES receives the corresponding d + 1 labels, y 1 , . . . , y d+1 , and can therefore solve the linear system given by the equations f a (x i ) = y i . Assume that f a * is the function known by the MLaaS server (i.e., While our model operates in the black-box setting, we discuss other attack models in more detail in Remark 2</p><formula xml:id="formula_13">y i = f a * (x i )). It is easy to see that if we fix Err( f a ) = ||a * − a|| 1 , then Pr[Exp 0 F d,R (S( f a * ), A ES , d + 1) = 1] = 1. That is, A ES imple-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Active Learning and Extraction</head><p>From the description presented in the § 2, it is clear that model extraction in the MLaaS system context closely resembles active learning. The survey of active learning in § 2.2 contains a variety of algorithms and scenarios which can be used to implement model extraction attacks (or to study its impossibility).</p><p>However, different scenarios of active learning impose different assumptions on the adversary's prior knowledge. Here, we focus on the general case of model extraction with an adversary A that has no knowledge of the data distribution D.</p><p>In particular, such an adversary is not restricted to only considering instances x ∼ D to query. For this reason, we believe that query synthesis (QS) is the right active learning scenario to investigate in order to draw a meaningful parallelism with model extraction. Recall that the query synthesis is the only framework where the query inputs can be generated de novo (i.e., they do not conform to a distribution).</p><p>Observation 1: Given a hypothesis class F and an error function Err, let (L, O) be an active learning system for F in the QS scenario (Definition 2). If the query complexity of L is q L (ε, δ), then there exists an adversary A that implements ε-extraction with complexity q L (ε, δ) and confidence 1 − δ against the class F .</p><p>The reasoning for this observation is as follows: consider the adversary A that is the learner L (i.e., A deploys the query strategy procedure and the stopping criteria that describe L). This is possible because (L, O) is in the QS scenario and L is independent of any underlying (unknown) distribution. Let q = q L (ε, δ) and observe that</p><formula xml:id="formula_14">Pr[Exp ε F (S( f * ), A,q) = 1] = Pr[A outputsˆfoutputsˆ outputsˆf and Err( ˆ f ) ≤ ε] = Pr[L outputsˆfoutputsˆ outputsˆf and Err( ˆ f ) ≤ ε] ≥ 1 − δ</formula><p>Our observation states that any active learning algorithm in the QS scenario can be used to implement a model extraction attack. Therefore, in order to study the security of a given hypothesis class in the MLaaS framework, we can use known techniques and results from the active learning literature. Two examples of this follow.</p><p>Example 3 (Decision tree extraction via QS active learning).</p><p>Let F n,BF denote the set of boolean functions with domain {0, 1} n and range {−1, 1}. The reader can think of −1 as 0 and +1 as 1. Using the range of {−1, +1} is very common in the literature on learning boolean functions. An interesting subset of F n,BF is given by the functions that can be represented as a boolean decision tree. A boolean decision tree T is a labeled binary tree, where each node v of the tree is labeled by L v ⊆ {1, · · · , n} and has two outgoing edges. Every leaf in this tree is labeled either +1 or −1. Given an n-bit string x = (b 1 , · · · , b n ), b i ∈ {0, 1} as input, the decision tree defines the following computation: the computation starts at the root of the tree T . When the computation arrives at an internal node v, we calculate the parity of ∑ i∈L v b i and go left if the parity is 0 and go right otherwise. The value of the leaf that the computation ends up in is the value of the function. We denote by F m n,BT the class of boolean decision trees with n-bit input and m nodes. <ref type="bibr">Kushilevitz and Mansour [35]</ref> present an active learning algorithm for the class F n,BF that works in the QS scenario. This algorithm utilizes the uniform error to determine the stopping condition (refer § 2.2). The authors claim that this algorithm has practical efficiency when restricted to the classes F m n,BT ⊂ F n,BF for any m. In particular, if the active learner L of <ref type="bibr" target="#b33">[35]</ref> interacts with the oracle O T * where T * ∈ F m n,BT , then L learns g ∈ F n,BF such that Pr x∼{0,1} n [g(x) 񮽙 = T * (x)] ≤ ε with probability at least 1 − δ using a number of queries polynomial in n, m, 1 ε and log( 1 δ ). Based on Observation 1, this directly translates to the existence of an adversary that implements ε-extraction with complexity polynomial in n, m, 1 ε and confidence 1 − δ against the class F m n,BT . Moreover, the algorithm <ref type="bibr" target="#b33">[35]</ref> can be extended to (a) boolean functions of the form f : {0, 1, . . . , k − 1} n → {−1, +1} that can be computed by a polynomial-size k-ary decision tree 4 , and (b) regression trees (i.e., the output is a real value from <ref type="bibr">[0, M]</ref>). In the second case, the running time of the learning algorithm is polynomial in M (refer § 6 of <ref type="bibr" target="#b33">[35]</ref>). Note that the attack model considered here is a stronger model than that considered by <ref type="bibr">Tramèr et al. [55]</ref> because the attacker/learner does not get any information about the internal path of the decision tree (refer Remark 2). queries, where f w * ∈ F d,HS is the labeling function used by O.</p><p>It follows from Observation 1 that an adversary utilizing this algorithm implements ε-extraction against the class F d,HS with complexity O(d log( 1 ε )) and confidence 1. We validate the practical efficacy of this attack in § 6.</p><p>Remark 2 (Extraction with auxiliary information). Observe that we define model extraction for only those MLaaS servers that return only the label value y for a well-formed query x (i.e. in the oracle access setting). A weaker model considers the case of MLaaS servers responding to a user's query x even when x is incomplete (i.e. with missing features), and returning the label y along with some auxiliary information. The work of <ref type="bibr">Tramèr et al. [55]</ref> proves that model extraction attacks in the presence of such "leaky servers" are feasible and efficient (i.e. low query complexity) for many hypothesis classes (e.g., logistic regression, multilayer perceptron, and decision trees). In particular, they propose an equation solving attack [55, Section 4.1] that uses the confidence values returned by the MLaaS server together with the labels to steal the model parameters. For example, in the case of logistic regression, the MLaaS server knows the parameters a 0 , a 1 , . . . , a d and responds to a query x with the label y (y = 0 if (1 + e −a(x) ) ≤ 0.5 and y = 1 otherwise) and the value a(x) as confidence value for y. Clearly, the knowledge of the con-fidence values allows an adversary to implement the same attack we describe in Example 2 for linear regression models. In <ref type="bibr">[55, §4.2]</ref>, the authors describes a path-finding attack that use the leaf/node identifier returned by the server, even for incomplete queries, to steal a decision tree. These attacks are very efficient (i.e., d + 1 queries are needed to steal a d-dimensional logistic regression model). However, their efficiency heavily relies on the presence of the various forms of auxiliary information provided by the MLaaS server. While the work in <ref type="bibr" target="#b53">[55]</ref> performs preliminary exploration of attacks in the black-box setting <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b36">38]</ref>, it does not consider more recent and efficient algorithms in the QS scenario. Our work explores this direction through a formalization of the model extraction framework that enables understanding the possibility of extending/improving the active learning attacks presented in <ref type="bibr" target="#b53">[55]</ref>. Furthermore, having a better understanding of model extraction attack and its unavoidable connection with active learning is paramount for designing MLaaS systems that are resilient to model extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Non-linear Classifiers</head><p>This section focuses on model extraction for two important non-linear classifiers: kernel SVMs and discrete models (i.e. decision trees and random forests). For kernel SVMs our method is a combination of the adaptive-retraining algorithm introduced by Tramèr et al. and the active selection strategy from classic literature on active learning of kernel SVMs <ref type="bibr" target="#b10">[12]</ref>. For discrete models our algorithm is based on the importance weighted active learning (IWAL) as described in <ref type="bibr" target="#b9">[11]</ref>. Note that decision trees for general labels (i.e. non-binary case) and random forests was not discussed in <ref type="bibr" target="#b9">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Kernel SVMs</head><p>In kernel SVMs (kSVMs), there is a kernel K : X × X → R associated with the SVM. Some of the common kernels are polynomials and radial-basis functions (RBFs). If the kernel function K(., .) has some special properties (required by classic theorem of Mercer <ref type="bibr" target="#b38">[40]</ref>), then K(., .) can be replaced with Φ(.) T Φ(.) for a projection/feature function Φ. In the feature space (the domain of Φ) the optimization problem is as follows <ref type="bibr" target="#b3">5</ref> :</p><formula xml:id="formula_15">min w,b 񮽙w񮽙 2 +C ∑ n i=1 η i such that for 1 ≤ i ≤ n y i ˆ y(x i ) ≥ 1 − η i η i ≥ 0</formula><p>In the formulation given above, ˆ y(x) is equal to w T Φ(x) + b. Recall that prediction of the kSVM is the sign ofˆyofˆ ofˆy(x), sô y(x) is the "pre sign" value of the prediction. Note that for some kernels (e.g. RBF) Φ is infinite dimensional, so one generally uses the "kernel trick"i.e. one solves the dual of the above problem and obtains a kernel expansion, so that</p><formula xml:id="formula_16">ˆ y(x) = n ∑ i=1 α i K(x, x i ) + b</formula><p>The vectors x 1 , · · · , x n are called support vectors. We assume that hyper-parameters of the kernel (C, η) are known; one can extract the hyper-parameters for the RBF kernel using the extract-and-test approach as Tramèr et al. Note that if Φ is finite dimensional, we can use an algorithm (including active learning strategies) for linear classifier by simply working in the feature space (i.e. extracting the domain of Φ(·)). However, there is a subtle issue here, which was not addressed by Tramèr et al. We need to make sure that if a query y is made in the feature space, it is "realizable" (i.e. there exists a x such that Φ(x) = y). Otherwise the learning algorithm is not sound.</p><p>Next we describe our model-extraction algorithm for kSVMs with kernels whose feature space is infinite dimension (e.g. RBF or Laplace kernels). Our algorithm is a modification of the adaptive training approach from Tramèr et al. Our discussion is specialized to kSVMs with RBFs, but our ideas are general and are applicable in other contexts.</p><p>Extended Adaptive Training (EAT): EAT proceeds in multiple rounds. In each round we construct h labeled instances. In the initial stage (t = 0) we draw r instances x 1 , · · · , x r from the uniform distribution, query their labels, and create an initial model M 0 . Assume that we are at round t, where t &gt; 0, and let M t−1 be model at time t − 1. Round t works as follows: create h labeled instances using a strategy St T (M t−1 , h) (note that the strategy St is oracle access to the teacher, and takes as parameters model from the previous round and number of labeled instances to be generated). Now we train M t−1 on the instances generated by St T (M t−1 , h) and obtain the updated model M t . We keep iterating using the strategy St T (·, ·) until the query budget is satisfied. Ideally, St T (M t−1 , h) should be instances that the model M t−1 is least confident about or closest to the decision boundary.</p><p>Tramèr et al. use line search as their strategy St T (M t−1 , h), which can lead to several queries (each step in the binary search leads to a query). We generate the initial model M 0 as in Tramèr et al. and then our strategy differs. Our strategy St T (M t−1 , 1) (note that we only add one labeled sample at each iteration) works as follows: we generate k random points x 1 , · · · , x k and then computê y i (x i ) for each x i (recall thatˆythatˆ thatˆy i (x i ) is the "pre sign" prediction of x i on the SVM M t−1 . We then pick x i with minimum | ˆ y i (x i ) | and query for its label and retrain the model M t−1 and obtain M t . This strategy is called active selection and has been used for active learning of SVMs <ref type="bibr" target="#b10">[12]</ref>. The argument for why this strategy finds the point closest to the boundary is given in <ref type="bibr">[12, §4]</ref>. There are other strategies described in <ref type="bibr" target="#b10">[12]</ref>, but we found active selection to perform the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decision Trees and Random Forests</head><p>Next we will describe the idea of importance weighted active learning (IWAL) <ref type="bibr" target="#b9">[11]</ref>. Our discussion will be specialized to decision trees and random forests, but the ideas that are described are general.</p><p>Let H be the hypothesis class (i.e. space of decision trees or random forests), X is the space of data, and Y is the space of labels. The active learner has a pool of unlabeled data x 1 , x 2 , · · · . For i &gt; 1, we denote by X 1:i−1 the sequence x 1 , · · · , x i−1 . After having processed the sequence X 1:i−1 , a coin is flipped with probability p i ∈ [0, 1] and if it comes up heads, the label of x i is queried. We also define a set S i (S 0 = / 0) recursively as follows: If the label for x i is not queried, then S i = S i−1 ; otherwise S i = S i−1 ∪ (x i , y i , p i ). Essentially the set S i keeps the information (i.e. data, label, and probability of querying) for all the datapoints whose label was queried. Given a hypothesis h ∈ H , we define err(h, S n ) as follows:</p><formula xml:id="formula_17">err(h, S n ) = 1 n ∑ (x,y,p)∈S n 1 p 1 h(x)񮽙 =y<label>(2)</label></formula><p>Next we define the following quantities (we assume n ≥ 1):</p><formula xml:id="formula_18">h n = argmin{err(h, S n−1 ) : h ∈ H } h ′ n = argmin{err(h, S n−1 ) : h ∈ H ∧ h(X n ) 񮽙 = h n (X n )} G n = err(h ′ n , S n−1 ) − err(h n , S n−1 )</formula><p>Recall that p n is the probability of querying for the label for X n , which is defined as follows:</p><formula xml:id="formula_19">p n = 񮽙 1 if G n ≤ µ(n) s(n) otherwise</formula><p>Where µ(n) = 񮽙 c 0 log n n−1 + c 0 log n n−1 , and s(n) ∈ (0, 1) is the positive solution to the following equation:</p><formula xml:id="formula_20">G n = c 1 √ s − c 1 + 1 · 񮽙 c 0 log n n − 1 + c 2 √ s − c 2 + 1 · c 0 log n n − 1</formula><p>Note the dependence on constants/hyperparameters c 0 , c 1 and c 2 , which are tuned for a specific problem (e.g. in their experiments for decision trees <ref type="bibr">[11, §6]</ref> the authors set c 0 = 8 and c 1 = c 2 = 1). Decision Trees: Let DT be any algorithm to create a decision tree. We start with an initial tree h 0 (this can constructed using a small, uniformly sampled dataset whose labels are queried). Let h n be the tree at step n − 1. The question is: how to construct h ′ n ? Let x n be the n th datapoint and Y = {l 1 , · · · , l r } be the set of labels. Let h n (x n ) = l j . Let h n (l) be the modification of tree h n such that h n (l) produces label l 񮽙 = h n (x n ) on datapoint x n . Let h ′ n be the tree in the set {h n (l) | l ∈ Y − {l j }} that has minimum err(·, S n−1 ). Now we can compute G n and the algorithm can proceed as described before. Random Forests: In this case we will restrict ourselves to binary classification, but the algorithm can readily extended to the case of multiple labels. As before RF 0 is the random forest trained on a small initial dataset. Since we are in the binary classification domain, the label set Y = {1, −1}. Assume that we have a random forest RF = {RF <ref type="bibr">[1]</ref> Let RF n be the random forest at time step n − 1. The question again is: how to construct RF ′ n ? Without loss of generality, let us say on x n RF n (x n ) = +1 (the case when the label is −1 is symmetric) and there are r trees in RF n (denoted by RF +1 n (x n )) such that their labels on x n are +1. Note that r &gt; ⌊ o 2 ⌋ because the majority label was +1. Define j = r −⌊ o 2 ⌋+1. Note that if j trees in RF +1</p><p>n (x n ) will "flip" their decision to −1 on x n , then the decision on x n will be flipped to −1. This is the intuition we use to compute RF ′ n . There are r j choices of trees and we pick the one with minimum error on S n−1 , and that gives us RF ′ n . Recall that r j is approximately r j , but we can be approximate by randomly picking j trees out of RF +1 n (x n ), and choosing the random draw with the minimum error to approximate RF ′ n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Defense Strategies</head><p>Our main observation is that model extraction in the context of MLaaS systems described at the beginning of § 3 (i.e., oracle access) is equivalent to QS active learning. Therefore, any advancement in the area of QS active learning directly translates to a new threat for MLaaS systems. In this section, we discuss strategies that could be used to make the process of extraction more difficult.We investigate the link between ML in the noisy setting and model extraction. The design of a good defense strategy is an open problem; we believe this is an interesting direction for future work where the ML and security communities can fruitfully collaborate.</p><p>In this section, we assume that the MLaaS server S with the knowledge of f * , S( f * ), has the freedom to modify the prediction before forwarding it to the client. More precisely, we assume that there exists a (possibly) randomized procedure D that the server uses to compute the answer˜yanswer˜ answer˜y to a query x, and returns that instead of f * (x). We use the notation S D ( f * ) to indicate that the server S implements D to protect f * . Clearly, the learner that interacts with S D ( f * ) can still try to learn a function f from the noisy answers from the server. However, the added noise requires the learner to make more queries, or could produce a less accurate model than f .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Classification case</head><p>We focus on the binary classification problem where F is an hypothesis class of functions of the form f : X → Y and Y is binary, but our argument can be easily generalized to the multi-class setting.</p><p>First, in the following two remarks we recall two known results from the literature <ref type="bibr" target="#b25">[27]</ref> that establish information theoretic bounds for the number of queries required to extract the model when any defense is implemented. Let ν be the generalization error of the model f * known by the server S D and µ be the generalization error of the model f learned by an adversary interacting with S D ( f * ). Assume that the hypothesis class F has VC dimension equal to d. Recall that the VC dimension of a hypothesis class F is the largest number d such that there exists a subset X ⊂ X of size d which can be shattered by F . A set X = {x 1 , . . . ,</p><formula xml:id="formula_21">x d } ⊂ X is said to be shattered by F if |{( f (x 1 ), f (x 2 ), . . . , f (x d )) : f ∈ F }| = 2 d .</formula><p>Remark 3 (Passive learning). Assume that the adversary uses a passive learning algorithm to compute f , such as the Empirical Risk Minimization (ERM) algorithm, where given a labeled training set {(</p><formula xml:id="formula_22">X 1 ,Y 1 ), . . . (X n ,Y n )}, the ERM algo- rithm outputsˆfoutputsˆ outputsˆf = arg min f ∈F 1 n ∑ n i=1 1[ f (X i ) 񮽙 = Y i ].</formula><p>Then, the adversary can learnˆflearnˆ learnˆf with excess error ε (i.e., µ ≤ ν + ε) with˜O with˜ with˜O( ν+ε ε 2 d) examples. For any algorithm, there is a distribution such that the algorithm needs at least˜Ωleast˜ least˜Ω( ν+ε ε 2 d) samples to achieve an excess error of ε.</p><p>Remark 4 (Active learning). Assume that the adversary uses an active learning algorithm to compute f , such as the disagreement-based active learning algorithm <ref type="bibr" target="#b25">[27]</ref>. Then, the adversary achieves excess error ε with˜Owith˜ with˜O( ν 2 ε 2 dθ) queries (where θ is the disagreement coefficient <ref type="bibr" target="#b25">[27]</ref>). For any active learning algorithm, there is a distribution such that it takes at least˜Ωleast˜ least˜Ω( ν 2 ε 2 d) queries to achieve an excess error of ε. Observe that any defense strategy D used by a server S to prevent the extraction of a model f * can be seen as a randomized procedure that outputs˜youtputs˜ outputs˜y instead of f * (x) with a given probability over the random coins of D. In the discrete case, we represent this with the notation</p><formula xml:id="formula_23">ρ D ( f * , x) = Pr[Y x 񮽙 = f * (x)],<label>(3)</label></formula><p>where Y x is the random variable that represents the answer of the server S D ( f * ) to the query x (e.g., ˜ y ← Y x ). When the function f * is fixed, we can consider the supremum of the function ρ D ( f * , x), which represents the upper bound for the probability that an answer from S D ( f * ) is wrong:</p><formula xml:id="formula_24">ρ D ( f * ) = sup x∈X ρ D ( f * , x).</formula><p>Before discussing potential defense approaches, we first present a general negative result. The following proposition states that that any candidate defense D that correctly responds to a query with probability greater than or equal to <ref type="bibr">1</ref> 2 + c for some constant c &gt; 0 for all instances can be easily broken. Indeed, an adversary that repetitively queries the same instance x can figure out the correct label f * (x) by simply looking at the most frequent label that is returned from S D ( f * ). We prove that with this extraction strategy, the number of queries required increases by only a logarithmic multiplicative factor. Proposition 1. Let F be an hypothesis class used for classification and (L, O) be an active learning system for F in the QS scenario with query complexity q(ε, δ). For any D, randomized procedure for returning labels, such that there exists f * ∈ F with ρ D ( f * ) &lt; 1 2 , there exists an adversary that, interacting with S D ( f * ), can implement an ε-extraction attack with confidence 1 − 2δ and complexity q =</p><formula xml:id="formula_25">8 (1−2ρ D ( f * )) 2 q(ε, δ) ln q(ε,δ) δ .</formula><p>The proof of Proposition 1 can be found in the appendix in <ref type="bibr">[1]</ref>. Proposition 1 can be used to discuss the following two different defense strategies:</p><p>1. Data-independent randomization. Let F denote a hypothesis class that is subject to an extraction attack using QS active learning. An intuitive defense for F involves adding noise to the query output f * (x) independent of the labeling function f * and the input query x. In other words, ρ D ( f , x) = ρ for any x ∈ X, f ∈ F , and ρ is a constant value in the interval (0, 1). It is easy to see that this simple strategy cannot work. It follows from Proposition 1 that if ρ &lt; 1 2 , then D is not secure. On the other hand, if ρ ≥ 1 2 , then the server is useless since it outputs an incorrect label with probability at least 1 2 . Example 5 (Halfspace extraction under noise). For example, we know that ε-extraction with any level of confidence can be implemented with complexity q = O(d log( 1 ε )) using QS active learning for the class F d,HS i.e. for binary classification via halfspaces (refer Example 4). It follows from the earlier discussion that any defense that flips labels with a constant flipping probability ρ does not work. This defense approach is similar to the case of "noisy oracles" studied extensively in the active learning literature <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b43">45]</ref>. For example, from the ML literature we know that if the flipping probability is exactly ρ (ρ ≤ 1 2 ), the AVERAGE algorithm (similar to our Algorithm 1, defined in Section 6) ε-extracts f * with˜O with˜ with˜O( d <ref type="bibr" target="#b0">2</ref> (1−2ρ) 2 log 1 ε ) labels <ref type="bibr" target="#b31">[33]</ref>. Under bounded noise where each label is flipped with probability at most ρ (ρ &lt; 1 2 ), the AV-ERAGE algorithm does not work anymore, but a modified Perceptron algorithm can learn with˜Owith˜ with˜O( d (1−2ρ) 2 log 1 ε ) labels <ref type="bibr" target="#b59">[61]</ref> in a stream-based active learning setting, and a QS active learning algorithm proposed by Chen et al. <ref type="bibr" target="#b14">[16]</ref> can also learn with the same number of labels. An adversary implementing the Chen et al. algorithm <ref type="bibr" target="#b14">[16]</ref> is even more efficient than the adversary˜Aadversary˜ adversary˜A defined in the proof of Proposition 1 (i.e., the total number of queries only increases by a constant multiplicative factor instead of ln q(ε, δ)). We validate the practical efficiency of this attack in § 6.</p><p>2. Data-dependent randomization. Based on the outcome of the earlier discussion, we believe that a defense that aims to protect a hypothesis class against model extraction via QS active learning should implement data-dependent perturbation of the returned labels. That is, we are interested in a defense D such that the probability ρ D ( f * , x) depends on the query input x and the labeling function f * . For example, given a class F that can be extracted using an active learner L (in the QS scenario), if we consider a defense D such that ρ D ( f * , x) ≥ 1 2 for some instances, then the proof of Proposition 1 does not work (the argument only works if there is a constant c &gt; 0 such that ρ D ( f * , x) ≤ 1 2 − c for all x) and the effectiveness of the adversary˜Aadversary˜ adversary˜A is not guaranteed anymore 6 .</p><p>Example 6 (Halfspace extraction under noise). For the case of binary classification via halfspaces, Alabdulmohsin et al.</p><p>[2] design a system that follows this strategy. They consider the class F d,HS and design a learning rule that uses training data to infer a distribution of models, as opposed to learning a single model. To elaborate, the algorithm learns the mean µ and the covariance Σ for a multivariate Gaussian distribution N (µ, Σ) on F d,HS such that any model drawn from N (µ, Σ) provides an accurate prediction. The problem of learning such a distribution of classifiers is formulated as a convex-optimization problem, which can be solved quite efficiently using existing solvers. During prediction, when the label for a instance x is queried, a new w is drawn at random from the learned distribution N (µ, Σ) and the label is computed as y = sign(w, x). The authors show that this randomization method can mitigate the risk of reverse engineering without incurring any notable loss in predictive accuracy. In particular, they use PAC active learning algorithms <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b15">17]</ref> (assuming that the underlying distribution D is Gaussian) to learn an approximationˆwapproximationˆ approximationˆw from queries answered in three different ways: (a) with their strategy, i.e. using a new model for each query, (b) using a fixed model to compute all labels, and (c) using a fixed model and adding independent noise to each label, i.e. y = sign(w, x + η) and η ← [−1, +1]. They show that the geometric error ofˆwofˆ ofˆw with respect to the true model is higher in the former setting (i.e. in (a)) than in the others. On 15 different datasets, their strategy gives typically an order of magnitude larger error. We empirically evaluate this defense in the context of model extraction using QS active learning algorithms in § 6.</p><p>Continuous case: Generalizing Proposition 1 to the continuous case does not seem straightforward, i.e. when the target model held by the MLaaS server is a real-valued function f * : X → R; a detailed discussion about the continuous case appears in the appendix in <ref type="bibr">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation and Evaluation</head><p>For all experiments described below, we use an Ubuntu 16.04 server with 32 GB RAM, and an Intel i5-6600 CPU clocking 3.30GHz. We use a combination of datasets obtained from the scikit-learn library and the UCI machine learning repository 7 , as used by Tramèr et al.. <ref type="bibr" target="#b4">6</ref> Intuitively, in the binary case if ρ D ( f * , x i ) ≥ 1 2 then the definition of y i performed by˜Aby˜ by˜A in step 2 (majority vote) is likely to be wrong. However, notice that this is not always the case in the multiclass setting: For example, consider the case when the answer to query x i is defined to be wrong with probability ≥ 1 2 and, when wrong, is sampled uniformly at random among the k − 1 classes that are different to the true class f * (x), then if k is large enough, y i defined via the majority vote is likely to be still correct.   that we train locally 8 , eliminating redundant queries to the oracle. To compare the efficiency of our algorithm, we reexecute the adaptive retraining procedure, and present our results in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>It is clear that our approach is more query efficient in comparison to <ref type="bibr">Tramèr et al. (between 5×-224×)</ref>, with comparable test accuracy. These advantages stem from (a) using a more informative metric of uncertainty than the distance from the decision boundary, and (b) querying labels of only those points which the local model is uncertain about. Q2. Decision Trees: Tramèr et al. propose a path finding algorithm to determine the structure of the server-hosted decision tree. They rely on the server's response to incomplete queries, and the addition of node identifiers to the generated outputs to recreate the tree. From our analysis presented in <ref type="table">Table 1such</ref> flexibility is not readily available in most MLaaS providers. As discussed earlier (refer § 4.2), we utilize the IWAL algorithm proposed by <ref type="bibr">Beygelzimer et al. [11]</ref> that iteratively refines a learned hypothesis. It is important to note that the IWAL algorithm is more general, and does not rely on the information needed by the path finding algorithm. We present the results of extraction using the IWAL algorithm below in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>In each iteration, the algorithm learns a new hypothesis, but the efficiency of the approach relies on the hypothesis used preceding the first iteration. To this end, we generate inputs uniformly at random. Note that in such a uniform query generation scenario, we rely on zero auxiliary information. We can see that while the number of queries required to launch such extraction attacks is greater than in the approach proposed 8 such a local model is seeded with uniformly random points labeled by the oracle by Tramèr et al., such an approach obtains comparable test error to the oracle. While the authors rely on certain distributional assumptions to prove a label complexity result, we empirically observe success using the uniform strategy. Such an approach is truly powerful; it makes limited assumptions about the MLaaS provider and any prior knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We begin our discussion by highlighting algorithms an adversary could use if the assumptions made about the operational ecosystem are relaxed. Then, we discuss strategies that can potentially be used to make the process of extraction more difficult, and shortcomings in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Varying the Adversary's Capabilities</head><p>The operational ecosystem in this work is one where the adversary is able to synthesize data-points de novo to extract a model through oracle access. In this section, we discuss other algorithms an adversary could use if this assumption is relaxed. We begin by discussing other models an adversary can learn in the query synthesis regime, and move on to discussing algorithms in other approaches.</p><p>Equivalence queries. In her seminal work, Angluin <ref type="bibr" target="#b2">[4]</ref> proposes a learning algorithm, L * , to correctly learn a regular set from any minimally adequate teacher, in polynomial time. For this to work, however, equivalence queries are also needed along with membership queries. Should MLaaS servers provide responses to such equivalence queries, different extraction attacks could be devised. To learn linear decision boundaries, <ref type="bibr">Wang et al. [59]</ref> first synthesize an instance close to the decision boundary using labeled data, and then select the real instance closest to the synthesized one as a query. Similarly, Awasthi et al.</p><p>[7] study learning algorithms that make queries that are close to examples generated from the data distribution. These attacks require the adversary to have access to some subset of the original training data. In other domains, program synthesis using input-output example pairs (e.g., <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b56">58]</ref>) also follows a similar principle.</p><p>If the adversary had access to a subset of the training data, or had prior knowledge of the distribution from which this data was drawn from, it could launch a different set of attacks based on the algorithms discussed below. Stream-based selective sampling. Atlas et al. <ref type="bibr" target="#b4">[6]</ref> propose selective sampling as a form of directed search (similar to Mitchell <ref type="bibr" target="#b39">[41]</ref>) that can greatly increase the ability of a connectionist network (i.e. power system security analysis in their paper) to generalize accurately. <ref type="bibr">Dagan et al. [18]</ref> propose a method for training probabilistic classifiers by choosing those examples from a stream that are more informative. <ref type="bibr">Linden- baum et al.</ref> [36] present a lookahead algorithm for selective sampling of examples for nearest neighbor classifiers. The algorithm looks for the example with the highest utility, taking its effect on the resulting classifier into account. Another important application of selective learning was for feature selection <ref type="bibr" target="#b35">[37]</ref>, an important preprocessing step. Other applications of stream-based selective sampling include sensor scheduling <ref type="bibr" target="#b32">[34]</ref>, learning ranking functions for information retrieval <ref type="bibr" target="#b60">[62]</ref>, and in word sense disambiguation <ref type="bibr" target="#b22">[24]</ref>. Pool-based sampling. Dasgupta <ref type="bibr" target="#b19">[21]</ref> surveys active learning in the non-separable case, with a special focus on statistical learning theory. He claims that in this setting, AL algorithms usually follow one of the following two strategies -(i) Efficient search in the hypothesis spaces (as in the algorithm proposed by <ref type="bibr">Chen et al. [16]</ref>, or by Cohn et al. <ref type="bibr" target="#b15">[17]</ref>), or (ii) Exploiting clusters in the data (as in the algorithm proposed by Dasgupta et al. <ref type="bibr" target="#b20">[22]</ref>). The latter option can be used to learn more complex models, such as decision trees. As the ideal halving algorithm is difficult to implement in practice, pool-based approximations are used instead such as uncertainty sampling and the query-by-committee (QBC) algorithm (e.g., <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b52">54]</ref>). Unfortunately, such approximation methods are only guaranteed to work well if the number of unlabeled examples (i.e. pool size) grows exponentially fast with each iteration. Otherwise, such heuristics become crude approximations and they can perform quite poorly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Complex Models</head><p>PAC active learning strategies have proven effective in learning DNNs. The work of <ref type="bibr">Sener et al. [49]</ref> selects the most representative points from a sample of the training distribution to learn the DNN. <ref type="bibr">Papernot et al. [46]</ref> employ substitute model training -a procedure where a small training subset is strategically augmented and used to train a shadow model that resembles the model being attacked. Note that the prior approaches rely on some additional information, such as a subset of the training data.</p><p>Active learning for complex models is challenging. Active learning algorithms considered in this paper operate in an iterative manner. Let H be the entire hypothesis class. At time time t ≥ 0 let the set of possible hypothesis be H t ⊆ H . Usually an active-learning algorithm issues a query at time t and updates the possible set of hypothesis to H t+1 , which is a subset of H t . Once the size of H t is "small" the algorithm stops. Analyzing the effect of a query on possible set of hypothesis is very complicated in the context of complex models, such as DNNs. We believe this is a very important and interesting direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Model Transferability</head><p>Most work in active learning has assumed that the correct hypothesis space for the task is already known i.e. if the model being learned is for logistic regression, or is a neural network and so on. In such situations, observe that the labeled data being used is biased, in that it is implicitly tied to the underlying hypothesis. Thus, it can become problematic if one wishes to re-use the labeled data chosen to learn another, different hypothesis space. This leads us to model transferability 9 , a <ref type="bibr" target="#b7">9</ref> A special case of agnostic active learning <ref type="bibr" target="#b6">[8]</ref>.</p><p>less studied form of defense where the oracle responds to any query with the prediction output from an entirely different hypothesis class. For example, imagine if a learner tries to learn a halfspace, but the teacher performs prediction using a boolean decision tree. Initial work in this space includes that of <ref type="bibr">Shi et al. [51]</ref>, where an adversary can steal a linear separator by learning input-output relations using a deep neural network. However, the performance of query synthesis active learning in such ecosystems is unclear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Limitations</head><p>We stress that these limitations are not a function of our specific approach, and stem from the theory of active learning. Specifically: (1) As noted by <ref type="bibr">Dasgupta [20]</ref>, the label complexity of PAC active learning depends heavily on the specific target hypothesis, and can range from O(log 1 ε ) to Ω( 1 ε ). Similar results have been obtained by others <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b41">43]</ref>. This suggests that for some hypotheses classes, the query complexity of active learning algorithms is as high as that in the passive setting. (2) Some query synthesis algorithms assume that there is some labeled data to bootstrap the system. However, this may not always be true, and randomly generating these labeled points may adversely impact the performance of the algorithm. (3) For our particular implementation, the algorithms proposed rely on the geometric error between the optimal and learned halfspaces. Sometimes, there is no direct correlation between this geometric error and the generalization error used to measure the model's goodness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Machine learning algorithms and systems are optimized for performance. Little attention is paid to the security and privacy risks of these systems and algorithms. Our work is motivated by the following attacks against machine learning.</p><p>1. Causative Attacks: These attacks are primarily geared at poisoning the training data used for learning, such that the classifier produced performs erroneously during test time. These include: (a) mislabeling the training data, (b) changing rewards in the case of reinforcement learning, or (c) modifying the sampling mechanism (to add some bias) such that it does not reflect the true underlying distribution in the case of unsupervised learning <ref type="bibr" target="#b46">[48]</ref>. The work of Papernot et al. <ref type="bibr" target="#b45">[47]</ref> modify input features resulting in misclassification by DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Evasion Attacks:</head><p>Once the algorithm has trained successfully, these forms of attacks provide tailored inputs such that the output is erroneous. These noisy inputs often preserves the semantics of the original inputs, are human imperceptible, or are physically realizable. The well studied area of adversarial examples is an instantiation of such an attack. Moreover, evasion attacks can also be even black-box i.e. the attacker need not know the model. This is because an adversarial example optimized for one model is highly likely to be effective for other models. This concept, known as transferability, was introduced by Carlini et al. <ref type="bibr" target="#b13">[15]</ref>.</p><p>3. Exploratory Attacks: These forms of attacks are the primary focus of this work, and are geared at learning intrinsics about the algorithm used for training. These intrinsics can include learning model parameters, hyperparameters, or training data. Typically, these forms of attacks fall in two categories -model inversion, or model extraction. In the first class, <ref type="bibr">Fredrikson et al. [23]</ref> show that an attacker can learn sensitive information about the dataset used to train a model, given access to sidechannel information about the dataset. In the second class, the work of <ref type="bibr">Tramér et al. [55]</ref> provides attacks to learn parameters of a model hosted on the cloud, through a query interface. Termed membership inference, Shokri et al. <ref type="bibr" target="#b50">[52]</ref> learn the training data used for machine learning by training their own inference models. <ref type="bibr">Wang et al. [57]</ref> propose attacks to learn a model's hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this paper, we formalize model extraction in the context of Machine-Learning-as-a-Service (MLaaS) servers that return only prediction values (i.e., oracle access setting), and we study its relation with query synthesis active learning (Observation 1). Thus, we are able to implement efficient attacks to the class of halfspace models used for binary classification ( § 6). While our experiments focus on the class of halfspace models, we believe that extraction via active learning can be extended to multiclass and non-linear models such as deep neural networks, random forests etc. We also begin exploring possible defense approaches ( § 5). To the best of our knowledge, this is the first work to formalize security in the context of MLaaS systems. We believe this is a fundamental first step in designing more secure MLaaS systems. Finally, we suggest that data-dependent randomization (e.g., model randomization as in <ref type="bibr" target="#b0">[2]</ref>) is the most promising direction to follow in order to design effective defenses.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F</head><label></label><figDesc>d,HS (refer to Example 1) use geometric error. Assume that the true labeling function used by the oracle is f w * , then the geometric error of the hypothesis f w ∈ F d,HS is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and the MLaaS server responds with 0 if (1 + e −a(x) ) −1 ≤ 0.5 and 1 otherwise, with a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>ments 0 -</head><label>0</label><figDesc>extraction of complexity d + 1 and confidence 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Example 4 (</head><label>4</label><figDesc>Halfspace extraction via QS active learning). Let F d,HS be the hypotheses class of d-dimensional half- spaces defined in Example 1. Alabdulmohsin et al. [3] present a spectral algorithm to learn a halfspace in the QS scenario that, in practice, outperformed earlier active learning strategies in the PAC scenario. They demonstrate, through several experiments that their algorithm learns f w ∈ F d,HS such that 񮽙w − w * 񮽙 2 ≤ ε with approximately 2d log( 1 ε )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, · · · , RF[o]} of trees RF[i] and on a datapoint x the label of the random forest RF(x) is the majority of the label of the trees RF[1](x), · · · , RF[o](x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Extraction of a kernel SVM model. Comparison of the query 

complexity and test accuracy (in %) obtained running Tramèr et al. adaptive 
retraining vs. extended adaptive retraining. 

Dataset 
Oracle 
Path Finding 
IWAL 
Accuracy 
Queries 
Queries Accuracy 

Adult 
81.2 
18323 
244188 
80.2 
Steak 
52.1 
5205 
1334 
73.1 
Iris 
86.8 
246 
361 
89.4 
GSShappiness 
79 
18907 
254892 
79.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Extraction of a decision tree model. Comparison of the query 

complexity and test accuracy (in %) obtained by running path finding (Tramèr 
et al.) vs. IWAL algorithm. The test accuracy (in %) of the server-hosted 
oracle is presented as a baseline. 

</table></figure>

			<note place="foot" n="4"> A k-ary decision tree is a tree in which each inner node v has k outgoing edges.</note>

			<note place="foot" n="5"> we are using the formulation for soft-margin kSVMs</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgements</head><p>This material is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, SaTC-Frontiers-1804648, CCF-1652140, CNS-1838733, CNS-1719336, CNS-1647152, CNS-1629833 and ARO grant number W911NF-17-1-0405. Kamalika Chaudhuri and Songbai Yan thank NSF under 1719133 and 1804829 for research support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adding robustness to support vector machines against adversarial reverse engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><forename type="middle">M</forename><surname>Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM</title>
		<meeting>the 23rd ACM International Conference on Conference on Information and Knowledge Management, CIKM<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11-03" />
			<biblScope unit="page" from="231" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient active learning of halfspaces via query synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Ibrahim M Alabdulmohsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2483" to="2489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning regular sets from queries and counterexamples. Information and computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Angluin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="87" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">V</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Spognardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domenico</forename><surname>Vitali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Felici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJSN</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Training connectionist networks with queries and selective sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><forename type="middle">E</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>David A Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="566" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning using local membership queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranjal</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="398" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="89" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Margin based active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory, 20th Annual Conference on Learning Theory</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-06-13" />
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active and passive learning of linear separators under log-concave distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florina</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT 2013 -The 26th Annual Conference on Learning Theory</title>
		<meeting><address><addrLine>Princeton University, NJ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="288" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Agnostic active learning without constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast kernel classifiers with online and active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyda</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005-09" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04248</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Incorporating diversity in active learning with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Brinker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03)</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-optimal active learning of halfspaces via query synthesis in the noisy setting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amin</forename><surname>Seyed Hamed Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1798" to="1804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving generalization with active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Committee-based sampling for training probabilistic classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sean P Engelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Machine Learning</title>
		<meeting>the Twelfth International Conference on Machine Learning<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="150" to="157" />
		</imprint>
	</monogr>
	<note>The Morgan Kaufmann series in machine learning</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A general agnostic active learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coarse sample complexity bounds for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005</title>
		<meeting><address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="235" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Two faces of active learning. Theoretical computer science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">412</biblScope>
			<biblScope unit="page" from="1767" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A general agnostic active learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Privacy in pharmacogenetics: An end-to-end case study of personalized warfarin dosing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selective sampling for example-based word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takenobu</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hozumi</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="597" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Synthesis from examples: Interaction models and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symbolic and Numeric Algorithms for Scientific Computing (SYNASC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="8" to="14" />
		</imprint>
	</monogr>
	<note>14th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A bound on the label complexity of agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hanneke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Theory of disagreement-based active learning. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Hanneke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="131" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalized teaching dimensions and the query complexity of learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tibor Heged˝ Us</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth annual conference on Computational learning theory</title>
		<meeting>the eighth annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Benjamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Workshop on Security and Artificial Intelligence</title>
		<meeting>the 4th ACM Workshop on Security and Artificial Intelligence<address><addrLine>AISec; Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10-21" />
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Active learning in the non-realizable case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matti</forename><surname>Kääriäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory, 17th International Conference</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10-07" />
			<biblScope unit="page" from="63" to="77" />
		</imprint>
	</monogr>
	<note>ALT</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Noisy binary search and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</title>
		<meeting>the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="881" to="890" />
		</imprint>
	</monogr>
	<note>SODA 2007</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The automation of science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jem</forename><surname>Ross D King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rowland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Aubrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Markham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larisa</forename><forename type="middle">N</forename><surname>Pir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soldatova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">5923</biblScope>
			<biblScope unit="page" from="85" to="89" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Embedding hard learning problems into gaussian space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pravesh</forename><surname>Klivans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques, APPROX/RANDOM 2014</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="793" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Algorithms for optimal scheduling and management of hidden markov model sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1382" to="1397" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning decision trees using the fourier spectrum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1331" to="1348" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selective sampling for nearest neighbor classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Rusakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI/IAAI</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A selective sampling approach to active feature selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Motoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="49" to="74" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eleventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Employing EM and pool-based active learning for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning<address><addrLine>Madison, Wisconsin, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="350" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mercer&apos;s theorem, feature maps, and smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Ha Quang Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="154" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalization as search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Version spaces: an approach to concept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom Michael Mitchell ; Stanford Univ Calif Dept Of Computer</forename><surname>Science</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Noisy bayesian active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Naghshvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2012 50th Annual Allerton Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1626" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Noisy generalized binary search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1366" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The geometry of generalized binary search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7893" to="7906" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM on Asia Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (EuroS&amp;P), 2016 IEEE European Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="372" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Towards the science of security and privacy in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arunesh</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wellman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03814</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Active learning literature survey univ. wisconsin-madison, madison, wi</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS Tech. Rep</title>
		<imprint>
			<date type="published" when="1648" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How to steal a machine learning classifier with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shi</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalin</forename><surname>Sagduyu</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Grushin</surname></persName>
			<affiliation>
				<orgName type="collaboration">HST</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>Technologies for Homeland Security</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Practical evasion of a learning-based classifier: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nedim</forename><surname>Srndic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Symposium on Security and Privacy</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05-18" />
			<biblScope unit="page" from="197" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Support vector machine active learning with applications to text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Stealing machine learning models via prediction apis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th USENIX Security Symposium, USENIX Security 16</title>
		<meeting><address><addrLine>Austin, TX, USA, Au</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A theory of the learnable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1134" to="1142" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Stealing hyperparameters in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">Zhenqiang</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05351</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Interactive query synthesis from input-output examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rastislav</forename><surname>Bodik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1631" to="1634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Active learning via query synthesis and nearest neighbour search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liantao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="426" to="434" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Active learning from imperfect labelers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Javidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2128" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Revisiting perceptron: Efficient and label-optimal learning of halfspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songbai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chicheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1056" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Svm selective sampling for ranking with application to data retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</title>
		<meeting>the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="354" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Beyond disagreement-based agnostic active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chicheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="442" to="450" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
