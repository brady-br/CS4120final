<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lessons Learned from 10k Experiments to Compare Virtual and Physical Testbeds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Crussell</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Kroeger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kavaler</surname></persName>
							<email>dkavale@sandia.gov</email>
							<affiliation key="aff0">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Phillips</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sandia National Laboratories</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lessons Learned from 10k Experiments to Compare Virtual and Physical Testbeds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Virtual testbeds are a core component of cyber experimentation as they allow for fast and relatively inexpensive modeling of computer systems. Unlike simulations, virtual testbeds run real software on virtual hardware which allows them to capture unknown or complex behaviors. However, virtualization is known to increase latency and decrease throughput. Could these and other artifacts from virtualization undermine the experiments that we wish to run? For the past three years, we have attempted to quantify where and how virtual testbeds differ from their physical counterparts to address this concern. While performance differences have been widely studied, we aim to uncover behav-ioral differences. We have run over 10,000 experiments and processed over half a petabyte of data. Complete details of our methodology and our experimental results from applying that methodology are published in previous work. In this paper, we describe our lessons learned in the process of constructing and instrumenting both physical and virtual testbeds and analyzing the results from each.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction &amp; Background</head><p>Cyber experimentation allows researchers to explore what-if scenarios in a scientifically rigorous manner. Cyber experimentation typically relies on virtual testbeds which run real software on virtualized hardware, also known as emulation.</p><p>Since emulation runs the actual software it can capture unknown or complex behaviors that would not be captured in a simulation. In some cases, researchers may have access to physical testbeds, but these are generally more expensive and less-reconfigurable than their virtual counterparts. To date, there has been limited research into where and how virtual testbeds differ from physical testbeds.</p><p>Previous work has shown that virtualization, the basis for virtual testbeds, increases network latency and lowers throughput <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref>. <ref type="bibr">Wang et al. [20]</ref> showed that for small bursts, buffering can even cause virtual machines to receive data at rates exceeding that of the underlying network. Differences like these and other anomalies could undermine the result of an experiment run on a virtual testbed. Furthermore, variation in the construction of virtual testbeds and the underlying hardware compounds the uncertainty.</p><p>Three years ago, we set out to quantify the differences between virtual and physical testbeds. We proposed to run representative workloads on both physical and virtual testbeds, collect and compare metrics from each to understand where and how they differ. For our workload, we used ApacheBench <ref type="bibr" target="#b7">[8]</ref> as an HTTP client to request fixed pages from an HTTP server. We compared metrics from the application workload, how the workload interacts with the operating system, and how the traffic traverses the network. These different levels of abstraction allow us to understand where differences occur and can inform experiments that may be sensitive to artifacts at different levels. In doing so, we aimed to create a handbook for experimenters to provide recommendations and pitfalls when building experiments. Complete details of our methodology and our experimental results from applying that methodology are published in previous work <ref type="bibr" target="#b3">[4]</ref>. In this paper, we focus on our lessons learned in the process of constructing and instrumenting both physical and virtual testbeds and analyzing the results from each. This includes trial and error, and how and why we ended up using the methods that we did, both of which were out of scope for the previous paper.</p><p>In total, we ran over 10,000 experiments across three clusters producing more than half a petabyte of data. We varied the payload size, network drivers, and network bandwidth and found notable differences in the system and network-level interactions. We found that network driver offloading behavior varied greatly between testbeds causing differences in the number of read system calls and packetization. When we compared the sequences of system calls using Markov chains, we found near-identical structures across testbeds. In subsequent experiments, we found many system call sequences to be identical once we collapsed read/polling loops.</p><p>We performed our experiments using the minimega <ref type="bibr" target="#b11">[12]</ref> toolset. The primary tool, minimega, is an orchestration tool that allows users to create and manage containers, virtual machines, and virtual networks using a scriptable API. It leverages a custom container implementation, QEMU <ref type="bibr" target="#b0">[1]</ref>, and Open vSwitch <ref type="bibr" target="#b6">[7]</ref>. Auxiliary tools in the toolset include vmbetter, a tool to build VMs and containers, protonuke, a simple traffic generation tool, and igor, a physical node scheduling tool. All these tools are open-source and available on Github.</p><p>Several of our lessons learned overlap with the motivations for DEW (Distributed Experiment Workflows) <ref type="bibr" target="#b12">[13]</ref>. It is encouraging to see the overlap in challenges between different experimenters using different tools. Several other papers have enumerated lessons learned from creating or operating testbeds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref>. These lessons include selecting physical devices, creating software to instantiate the testbed, and managing user interactions with the testbed. We were unable to find lessons learned using these testbeds to run thousands of experiments. For a description of related works on comparing virtual and physical testbeds see our full paper <ref type="bibr" target="#b3">[4]</ref>.</p><p>The remainder of this paper is organized as follows. In Section 2, we discuss our lessons learned, grouped into four topic areas: Tools Validation and Development (Section 2.1), Instrumentation (Section 2.2), Data Collection &amp; Aggregation (Section 2.3), and Data Analysis (Section 2.4). In Section 3, we conclude by discussing the applicability of our lessons learned to other testbeds and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lessons Learned</head><p>To perform experiments at scale, we rely on several tools to provide orchestration, instrumentation, data collection, and analysis. We primarily used bash as the glue between our tools and to define our experiments, instrumentation, and analysis. We also used Python and SQL in our analysis. As noted by others <ref type="bibr" target="#b12">[13]</ref>, an experiment definition language that would enable specifying experiments, instrumentation, collection and possibly even analysis would be very useful for such efforts. That said, flexibility and extensibility are critical to the success of any such language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Tool Validation and Development</head><p>Lesson: Using a testbed toolset for experimentation requires substantial effort and consideration to put tools together in a workable and validated fashion.</p><p>Improving minimega. Running thousands of repetitions of the same experiment seems simple, but we identified several issues. We use tools from the minimega toolset (minimega, protonuke, igor, and vmbetter), sysdig <ref type="bibr" target="#b8">[9]</ref>, parallel <ref type="bibr" target="#b18">[19]</ref>, SQLite <ref type="bibr" target="#b17">[18]</ref>, vmstat <ref type="bibr" target="#b20">[21]</ref>, and tcptrace <ref type="bibr" target="#b14">[15]</ref>. We found that we needed to add several features to the minimega toolset before we could run our experiments. In addition to many minor bug fixes and improvements, we discovered a rare bug in the capture API that would crash minimega. This API is used to record network packets for analysis. The crash had not occurred during our previous experiments where we might only run a small number of long-running captures. Additionally, we added a snaplen feature to the capture API to reduce the data size by only capturing packet headers. We also found several examples where state was not entirely cleaned up after an experiment finished. For example, there was a bug which caused the container filesystem to not unmount. We view these as a gap between how minimega is typically used (to instantiate an environment a handful of times) and how we wished to use it (to run many experiments with varying parameters). To run thousands of experiments successfully, toolsets must be well-tested, reset all state between tests, and properly handle edge cases.</p><p>Stand-alone C2 server. In addition to improving our tools, we also added a tool to the toolset for our physical testbed: a stand-alone command-and-control server. When using the command-and-control layer with VMs, minimega acts as the server. We repurposed the command-and-control server from minimega to control the physical nodes. This had the added benefit that our scripts to run the experiment on the virtual testbed are similar to the scripts to run the physical testbed. In an ideal setting, the same experiment would run on either testbed so that separate versions did not have to be updated and maintained in parallel. This could be achieved, for example, using DEW with different experiment realizations.</p><p>Parallelizing experiments. Another challenge was parallelizing our experiments. To avoid one crash bringing down many experiments, we created many three-node reservations using the cluster provisioning tool, igor. Each reservation had a head node and two nodes to run the VMs. We then used parallel to distribute jobs across the reservations. During reservation creation, igor partitions the experiment network directly on the cluster switch using VLANs, so we did not have to worry about leakage between reservations. To use parallel in this manner, we exposed all parameters that we wanted to test as arguments to our top-level script and ensured that all the head nodes had all dependencies (achieved through a custom automated prep script). Using these methods we ran up to 16 experiments simultaneously (using 48 nodes total) and dramatically reduced the runtime.</p><p>Robust snapshot feature. We used vmbetter to construct our experiment images. This tool allowed us to create minimal kernel and init ramdisks containing just the desired software. To decrease the time to run experiments, we pre-installed all needed software on the VMs so that we did not have to include the install overhead in every test. In the future, a more robust snapshot feature could alleviate the need for customized images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instrumentation</head><p>Lesson: Instrumentation is invaluable but it is often manually added, expensive, and experiment-specific. Integrating more forms of instrumentation into the toolset could help researchers to more rapidly develop experiments.</p><p>We needed two forms of instrumentation in our experiments: 1) to verify the functionality of the environment and 2) to understand and evaluate our experiments.</p><p>Verifying the environment can be as simple as checking that the endpoints can ping each other but may be more elaborate, depending on how the endpoints are customized after booting. We found that the minimega toolset has some capabilities to perform these verification checks but they must be created by each experimenter. Integrating these checks into the toolset could simplify the process for experimenters.</p><p>Instrumenting at multiple levels. To understand how the workload behaves in both the virtual and physical testbeds, we instrumented the systems at three levels: application, operating system, and network. Using ApacheBench as our application workload provided well-tested and detailed metrics for the application level. These metrics made testbed comparisons at this level robust and insightful but we encountered difficulties at the other levels. At the operating system-level, we found that we could not capture system call traces for containers in the same was as we did for VMs and physical machines. Because containers have limited access to the system resources, we were unable to run sysdig to capture the traces from within the container. Instead, we had to capture system call traces system-wide and then filter the captures based on process namespace. When we did so, we found that sysdig had dropped events when there were many containers running, corrupting the results.</p><p>At the network-level, we had a discrepancy in packetcapture locations. For VMs and containers, we performed the captures on the physical hosts whereas for the physical testbed, we captured the traffic on the host running the workload. This minor variation causes changes introduced by the virtual NIC to be masked from view.</p><p>In an ideal world, we would have instrumentation to understand the provenance of all events in the operating system and underlying network resulting from the application under test. However, there were numerous constraints on capturing the level of data necessary to perform these traces. There are substantial resource costs associated with capturing the instrumentation data at this fine of a granularity. When we disabled the instrumentation, the experiments were able to run 5-13% faster, and we were still not capturing nearly the fidelity necessary to comprehensively trace the experiment. The generated data sets themselves were also enormous, with the packet captures being on the order of several gigabytes worth of data for a single run. Sampling might mitigate some of these costs, but tuning the sampling to avoid missing low probability events can be difficult.</p><p>Level of effort. Another substantial cost associated with instrumentation is the effort required to enable the level of instrumentation. With the exception of packet capture which was easily configurable in minimega, all of our instrumentation needed to be built or integrated by hand. More detailed tracing of the execution could also be done using tools like bpftrace <ref type="bibr" target="#b16">[17]</ref> to track events through the Linux kernel level. However, doing so accurately would require substantial effort to handcraft the tracing. Little of this level of instrumentation has been provided in the orchestration frameworks. Most tools provide instrumentation to enable functional or performance debugging, not to capture and model application behavior.</p><p>Nevertheless, our instrumentation was invaluable to track down anomalous behaviors. For example, in analyzing the ApacheBench results for the e1000 driver, we found that a small subset of results significantly deviated from the rest. From tcptrace, we discovered that there were stalls (typically 13 seconds) where the server behaved as if it had never received a data ACK even though it was sent by the client. Further investigation of our kernel instrumentation showed that these stalls coincided with driver lockups and resets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Collection &amp; Aggregation</head><p>Lesson: Testbeds can provide a wealth of data to researchers but should do more to streamline the process of collecting and aggregating it into a usable form.</p><p>One challenge with our instrumentation was collecting it all for further processing. We had two forms of data: data collected within the VM and data collected from the host running the VM. Collecting data from the host running the VM went smoothly using the minimega file transport APIs. Collecting the data that the VM generates was more challenging.</p><p>Data collection. We explored several options to collect data from the VMs. First, we tried using minimega's command-and-control APIs to retrieve the files but that has limits on the file size. Instead, we created a qcow2 disk where the VM would write all of its data. After the experiment, we would then process the qcow2s to extract the instrumentation data. However, in cases where the VM did not cleanly shutdown we occasionally had corrupted the filesystem. This was often repairable but did lead to some data loss. We recently implemented a capability to mount the guest filesystem on the host in the minimega toolset which could simplify this process going forward.</p><p>Data aggregation. Once we extracted data from the qcow2 disks and from the other hosts in the experiment, we then processed it (using sysdig and tcptrace) to create results for the individual iterations of the experiment. We stored these results in an SQLite database. Once the iteration had been processed, we archived it to a storage cluster before continuing to the next iteration as the hosts have insufficient storage to keep hundreds of iterations. Once all iterations and parameters were complete, we aggregated the results from all of the individual iterations into a single database to use for our analysis. Processing and aggregating the results in this way allowed us to avoid reading all the instrumentation data from the storage cluster -we only needed to read the much smaller databases.</p><p>Storage reduction. We used tcptrace to process the PCAPs that we collected. This tool reassembles TCP flows and computes statistics for each flow in the PCAP such as the number of bytes, packets, and ACKs. Our small-payload experiments, where we completed up to 500,000 requests, resulted in 500,000 flows, each with 78 flow statistics (39 for each direction). Storing the full results for later aggregation vastly increased the size of the database with little benefit; thus, we decided to compute summary statistics for the flow statistics (e.g. quantiles, mean, and standard deviation) in the database. When we then computed statistics over all iterations, we looked at the mean of means for these statistics. This reduces storage issues, but introduces issues with aggregation. Information loss. With any level of aggregation, one loses information content. For example, in calculating a mean, we inherently lose information about the underlying statistical distribution. If the original distribution is parameterized solely by its mean, this aggregation is acceptable. However, the original distribution may also require a standard deviation to be adequately described. Thus, aggregation may hurt statistical robustness. With this in mind, we retained the original source in case we needed a closer look at the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Data Analysis</head><p>Lesson: Testbeds allow for many repetitions but care should be used when analyzing the data, especially in conflating statistical significance with practical importance.</p><p>As our dataset is large, we have the benefit of most measures being statistically significant. When performing statistical tests of significance, standard practice is to calculate some test statistic (e.g., t-test) and associated p-values based on said test statistic <ref type="bibr" target="#b13">[14]</ref>. Standard test statistics (and thus p-values) are proportional to sample size; i.e., a larger sample size tends to force p-values lower. Thus, with many observations (as is the case in our work), most tests are likely to be highly significant if the null hypothesis is false. This can be seen as a positive outcome; researchers have a statistically robust argument towards validity of their stated hypotheses. However, this can be an issue when arguing practical importance.</p><p>For example, a researcher may wish to see if a given system configuration is associated with lower latency as compared to a baseline. They perform experiments and find this is indeed the case with a statistically significant p-value. However, this new configuration only decreases packet latency by 0.1%. Though this difference is statistically significant, the question remains: is a 0.1% decrease in packet latency of practical importance? This depends heavily on the application. Thus, statistical significance should not be the only issue in determining importance; effect sizes should also be considered.</p><p>Multiple comparisons. Related to the issue of p-values is that of multiple comparisons <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. One may wish to perform an analysis over all collected metrics in order to identify which are important for an outcome of interest. As is the case in our work, most metrics were not of importance a priori, and we wished to identify those which were a posteriori. In this situation, one may be tempted to compare each metric to the other and identify statistically significant differences. Though this process can be helpful in identifying important (and hitherto unknown) aspects, this can be dangerous if one does not adjust resulting p-values to account for multiple comparison. In essence, when comparing a large number of metrics against each other, there is a non-zero probability that a comparison will be statistically significant due to chance alone; this risk increases as the number of comparisons increase. This can lead to spurious significant results.</p><p>Small variance. During experimentation we found that variance within a given configuration for most metrics was extremely small. In most cases, standard deviation was so small it was almost numerically 0, and in some cases was numerically 0. Though standard practice suggests running as many iterations of an experiment as possible, this result suggests it may not be necessary for this type of work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Conclusion &amp; Future Work</head><p>We have presented our lessons learned from conducting over 10,000 experiments to compare virtual and physical testbeds. We have described the improvements we made to our tools, the challenges of instrumenting our experiments to understand performance and behaviors, aggregating and processing over half a petabyte of data, and analyzing it to produce meaningful results. We hope these lessons are useful to the cyber experimentation community.</p><p>While some of our lessons learned are testbed-specific (i.e. specific to the minimega toolset), we believe that many of them are applicable across testbeds. Specifically, our lessons learned around instrumentation, data collection &amp; aggregation, and data analysis should be relevant to most general-purpose testbeds. Other, more specialized testbeds may not benefit from our lessons learned because they have been designed to instrument and analyze specific events.</p><p>The lessons learned around instrumentation, data collection &amp; aggregation, and data analysis are also more fundamental. Creating a testbed toolset that is ready to use as an experimentation platform can likely be solved with more engineering. But creating more generalized ways to instrument and analyze the data from these experiments will require further research.</p><p>There are many more experiments to run to understand the differences between testbeds. Recently, we have begun experiments to understand the effects of contention. Specifically, instead of running a single HTTP client and server on their own dedicated physical machines, we measure effects when we run an increasing number of isolated pairs, up to 80 pairs. Preliminary results show that contention degrades the workload performance sooner than we expect. However, we have seen instances with two or four pairs where each ApacheBench instance has better throughput than a single pair. Through these experiments, we hope to understand how to calibrate a cluster to the amount of workload it can support without, with some, and with prohibitive amounts of artifacts from contention. These experiments have further honed our toolset and techniques.</p></div>		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank our anonymous reviewers and our paper shepherd for their helpful comments and suggestions.</p><p>Supported by the Laboratory Directed Research and Development program at Sandia National Laboratories, a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA-0003525.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">QEMU, a fast and portable dynamic translator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Bellard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference, FREENIX Track</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Design, deployment, and use of the DETER testbed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Benzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Braden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clifford</forename><surname>Neuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Ostrenga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Sklower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DETER</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Lessons learned from the real-world deployment of a connected vehicle testbed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mashrur</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mizanur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjan</forename><surname>Rayamajhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmud</forename><surname>Sakib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mhafuzul</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zadid</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transportation Research Record</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2672</biblScope>
			<biblScope unit="page" from="10" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtually the same: Comparing physical and virtual testbeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Crussell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">M</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Phillips</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Computing, Networking and Communications (ICNC)</title>
		<imprint>
			<date type="published" when="2019-02" />
			<biblScope unit="page" from="847" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Estimation of the means of dependent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olive Jean Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="1095" to="1111" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple comparisons among means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olive Jean Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">293</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Linux Foundation</title>
		<ptr target="http://www.openvswitch.org/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Open vSwitch</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<ptr target="https://httpd.apache.org/docs/2.4/programs/ab.html" />
		<title level="m">The Apache Software Foundation. ab -Apache HTTP server benchmarking tool</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<ptr target="https://sysdig.com/" />
	</analytic>
	<monogr>
		<title level="j">Sysdig Inc. sysdig</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lessons learned constructing a wireless ad hoc network test bed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sushant</forename><surname>Jadhav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheetalkumar</forename><surname>Timothy X Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on Wireless Network Measurements</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diagnosing performance overheads in the xen virtual machine environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">Renato</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshio</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Janakiraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM/USENIX international conference on Virtual execution environments</title>
		<meeting>the 1st ACM/USENIX international conference on Virtual execution environments</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">minimega developers. minimega: a distributed vm management tool</title>
		<ptr target="http://minimega.org/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DEW: Distributed experiment workflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jelena</forename><surname>Mirkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Blythe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Workshop on Cyber Security Experimentation and Test (CSET 18)</title>
		<meeting><address><addrLine>Baltimore, MD</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Introduction to the Practice of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">P</forename><surname>David S Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce A</forename><surname>Mccabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>WH</publisher>
			<pubPlace>Freeman New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Ostermann</surname></persName>
		</author>
		<ptr target="http://www.tcptrace.org/" />
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speeding up packet I/O in virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Lettieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Maffione</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth ACM/IEEE symposium on Architectures for networking and communications systems</title>
		<meeting>the ninth ACM/IEEE symposium on Architectures for networking and communications systems</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">High level tracing language for linux ebpf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><surname>Robertson</surname></persName>
		</author>
		<ptr target="http://github.com/iovisor/bpftrace" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sqlite</forename><surname>Consortium</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sqlite</surname></persName>
		</author>
		<ptr target="https://sqlite.org/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gnu parallel -the command-line power tool. ;login: The USENIX Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tange</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-02" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="42" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The impact of virtualization on network performance of amazon EC2 data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM, 2010 Proceedings IEEE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Vmstat man pages. Linux Systems, calling the man command for vmstat</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frederick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Implementing the Emulab-PlanetLab Portal: Experience and lessons learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Lepreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WORLDS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explaining packet delays under virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Whiteaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renata</forename><surname>Teixeira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGCOMM Computer Communication Review</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="44" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
