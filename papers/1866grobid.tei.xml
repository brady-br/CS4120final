<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open access to the Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) is sponsored by USENIX. Multicore Locks: The Case Is Not Closed Yet Multicore Locks: The Case is not Closed Yet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 22-24. 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Guiroux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Lachaize</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Guiroux</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Lachaize</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Quéma</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire d&apos;Informatique de Grenoble</orgName>
								<orgName type="institution">Université Grenoble Alpes</orgName>
								<address>
									<addrLine>Vivien Quéma</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Laboratoire d&apos;Informatique de Grenoble</orgName>
								<orgName type="laboratory" key="lab2">† ‡񮽙 † Université Grenoble Alpes ‡ Grenoble INP 񮽙 LIG (CNRS UMR 5217)</orgName>
								<orgName type="institution" key="instit1">Université Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Grenoble Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open access to the Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) is sponsored by USENIX. Multicore Locks: The Case Is Not Closed Yet Multicore Locks: The Case is not Closed Yet</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16)</title>
						<meeting>the 2016 USENIX Annual Technical Conference (USENIX ATC &apos;16) <address><addrLine>Denver, CO, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page">649</biblScope>
							<date type="published">June 22-24. 2016</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-931971-30-0 https://www.usenix.org/conference/atc16/technical-sessions/presentation/guiroux USENIX Association</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>NUMA multicore machines are pervasive and many multithreaded applications are suffering from lock contention. To mitigate this issue, application and library developers can choose from the plethora of optimized mu-tex lock algorithms that have been designed over the past 25 years. Unfortunately, there is currently no broad study of the behavior of these optimized lock algorithms on realistic applications. In this paper, we attempt to fill this gap. We perform a performance study of 27 state-of-the-art mutex lock algorithms on 35 applications. Our study shows that the case is not yet closed regarding locking on multicore machines. Indeed, our conclusions include the following findings: (i) at its optimized contention level, no single lock is the best for more than 52% of the studied workloads; (ii) every lock is harmful for several applications , even if the application parallelism is properly tuned; (iii) for several applications, the best lock changes when varying the number of threads. These findings call for further research on optimized lock algorithms and dynamic adaptation of contention management.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today, multicore machines are pervasive and many multithreaded applications are suffering from bottlenecks related to critical sections and their corresponding locks. To mitigate these issues, application and library developers can choose from the plethora of optimized mutex lock algorithms that have been designed over the past 25 years but there is currently no clear study to guide this puzzling choice for realistic applications. In particular, the most recent and comprehensive empirical performance evaluation on multicore synchronization <ref type="bibr" target="#b8">[9]</ref>, due to its breadth (from hardware protocols to high-level data structures), only provides a partial coverage of locking algorithms. Indeed, the aforementioned study only considers 9 algorithms, does not consider hybrid spinning/blocking waiting policies, omits emerging approaches (e.g., loadcontrol algorithms described in §2) and provides a modest coverage of hierarchical locks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, a recent and efficient approach. Furthermore, most of the observations are based on microbenchmarks. Besides, in the case of papers that present a new lock algorithm, the empirical observations are often focused on the specific workload characteristics for which the lock was designed <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>, or mostly based on microbenchmarks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>The present paper provides a broad performance study on Linux/x86 of 27 state-of-the-art mutex lock algorithms on a set of 35 realistic and diverse applications (the PARSEC, Phoenix, SPLASH2 suites, MySQL and an SSL proxy). We make a number of observations, several of which have not been previously mentioned: (i) about 60% of the studied applications are significantly impacted by lock performance; (ii) no single lock is systematically the best, even for a fixed number of contending cores; (iii) worse, at their optimized contention level (individually tuned for each application), the best locks never dominate for more than 52% of the lock-sensitive applications; (iv) any of the locks is harmful (i.e., significantly inefficient compared to the best one) for at least several workloads; (v) across all the lock-sensitive applications, there is no clear performance hierarchy among the locks, even at a fixed number of contending cores; (vi) for a given application, the best lock varies according to both the number of contending cores and the machine; (vii) unlike previous recommendations <ref type="bibr" target="#b8">[9]</ref> advocating that standard Pthread mutex locks should be avoided for workloads using no more than one thread per core, we find that, with our studied workloads, the current Linux implementation of these locks actually yields good performance for many applications with this pattern. Moreover, we show that all these results hold even when each configuration, i.e., each (application, lock) pair, is tuned to its optimal degree of parallelism. From our performance study, we draw two main conclusions. First, specific lock algorithms should not be hardwired into the code of applications. Second, the observed trends call for further research both regarding lock algorithms and runtime support for parallel performance and contention management.</p><p>To conduct our study, manually modifying all the applications in order to retrofit the studied lock algorithms would have been a daunting task. Moreover, using a meta-library that allows plugging different lock algorithms under a common API (such as liblock <ref type="bibr" target="#b25">[26]</ref> or libslock <ref type="bibr" target="#b8">[9]</ref>) would not have solved the problem, as this would still have required a substantial re-engineering effort for each application. In addition, such meta-libraries provide no or limited support for important features like Pthread condition variables, used within many applications. Therefore, we implemented LiTL 1 , a lowoverhead library that allows transparent interposition of Pthread mutex lock operations and support for mainstream features like condition variables, without any restriction on the application-level locking discipline.</p><p>The remainder of the paper is organized as follows: §2 presents a taxonomy of existing lock designs and the list of algorithms covered by our study. §3 describes our experimental setup and the studied applications. §4 describes the LiTL library. §5 exposes the main results from our empirical observations. §6 discusses related works and §7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Lock algorithms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>The body of existing works on optimized lock algorithms for multicore architectures is rich and diverse and can be split into the following five categories: 1) Flat approaches correspond to simple algorithms (typically based on one or a few shared variables accessed by atomic instructions) such as: simple spinlock <ref type="bibr" target="#b32">[33]</ref>, backoff spinlock <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, test and test-and-set (TTAS) lock <ref type="bibr" target="#b1">[2]</ref>, ticket lock <ref type="bibr" target="#b29">[30]</ref>, partitioned ticket lock <ref type="bibr" target="#b10">[11]</ref>, and standard Pthread mutex lock. 2) Queue-based approaches correspond to locks based on a waiting queue in order to improve fairness as well as the memory traffic, such as: MCS <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref> and CLH <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>3) Hierarchical approaches are specifically aimed at providing scalable performance on large-scale NUMA machines, by attempting to reduce the rate of lock migrations among NUMA nodes. This category includes HBO <ref type="bibr" target="#b31">[32]</ref>, HCLH <ref type="bibr" target="#b27">[28]</ref>, FC-MCS <ref type="bibr" target="#b12">[13]</ref>, HMCS <ref type="bibr" target="#b4">[5]</ref>, AHMCS <ref type="bibr" target="#b5">[6]</ref> and the algorithms that stem from the lock cohorting framework <ref type="bibr" target="#b13">[14]</ref>. A cohort lock is based on a combination 1 LiTL: Library for Transparent Lock interposition. of two lock algorithms (similar or different): one used for the global lock and one used for the local locks (there is one local lock per NUMA node); in the usual C-L A -L B notation, L A and L B respectively correspond to the global and the node-level lock algorithms. The list includes C-BO-MCS, C-PTL-TKT and C-TKT-TKT (also known as Hticket <ref type="bibr" target="#b8">[9]</ref>). The BO, PTL and TKT acronyms respectively correspond to backoff lock, partitioned ticket lock, and standard ticket lock. 4) Load-control approaches correspond to algorithms that aim at limiting the number of threads that concurrently attempt to acquire a lock, in order to prevent a performance collapse. These algorithms are derived from queue-based locks. This category includes MCSTimePub 2 <ref type="bibr" target="#b18">[19]</ref> and so-called Malthusian algorithms like Malth Spin and Malth STP 3 <ref type="bibr" target="#b11">[12]</ref>. 5) Delegation-based approaches correspond to algorithms in which it is (sometimes or always) necessary for a thread to delegate the execution of a critical section to another thread. The typical benefits expected from such approaches are improved cache locality and better resilience under high lock contention. This category includes Oyama <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr">Hendler [20]</ref>, RCL <ref type="bibr" target="#b25">[26]</ref>, CC-Synch <ref type="bibr" target="#b14">[15]</ref> and DSM-Synch <ref type="bibr" target="#b14">[15]</ref>.</p><p>Another important design dimension is the waiting policy used when a thread cannot immediately obtain a requested lock <ref type="bibr" target="#b11">[12]</ref>. There are three main approaches: (i) spinning on a memory address, (ii) immediate parking (i.e., blocking the thread) either for a fixed amount of time or until the thread gets a chance to obtain the lock, and (iii) spinning-then-parking (STP), a hybrid strategy using a fixed or adaptive threshold <ref type="bibr" target="#b21">[22]</ref>. The choice of the waiting policy is mostly orthogonal to the lock design but, in practice, policies other than pure spinning are only considered for certain types of locks: the queue-based (from categories 2-4 above) and the standard Pthread mutex locks. Besides, note that the GNU C library for Linux provides two versions of Pthread mutex locks: the default one uses parking (via the futex syscall) and the second one uses an adaptive spin-thenpark strategy. The latter version can be enabled with the PTHREAD MUTEX ADAPTIVE NP option <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Studied algorithms</head><p>Our choice of studied locks is guided by the decision to focus on portable lock algorithms. We therefore exclude the following locks that require strong assumptions on 2 MCS-TimePub is mostly known as MCS-TP but we use MCTimePub to avoid confusion with MCS STP.</p><p>3 Malth Spin and Malth STP correspond to MCSCR-S and MCSCR-STP, respectively, but we do not use the latter names to avoid confusion with other MCS locks.   <ref type="table">Table 1</ref>: Hardware characteristics of the testbed platforms.</p><p>the application/OS behavior, code modifications, or fragile performance tuning: HCLH, HBO, FC-MCS, and all the delegation-based locks (see Dice et al. <ref type="bibr" target="#b13">[14]</ref> for detailed arguments).</p><p>Our study considers 27 mutex lock algorithms that are representative of both well-established and state-ofthe-art approaches. We use the Spin and STP suffixes to differentiate variants of the same algorithm that only differ in their waiting policy. The -LS tag corresponds to optimized algorithms borrowed from libslock <ref type="bibr" target="#b8">[9]</ref>. Our set includes ten flat locks (Backoff, Partitioned ticket, Phtread, Pthread adaptive, Spinlock, Spinlock-LS, Ticket, Ticket-LS, TTAS, TTAS-LS), seven queue-based locks (Alock-LS, CLH-LS, CLH Spin, CLH STP, MCS-LS, MCS Spin, MCS STP), seven hierarchical locks (C-BO-MCS Spin, C-BO-MCS STP, C-PTL-TKT, C-TKT-TKT, Hticket-LS, HMCS, AHMCS), and three loadcontrol locks (Malth Spin, Malth STP, MCS-TimePub).</p><p>3 Experimental setup and methodology</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Testbed and studied applications</head><p>Our experimental testbed consists of three Linux-based servers whose main characteristics are summarized in <ref type="table">Table 1</ref>. All the machines run the Ubuntu 12.04 OS with a 3.17.6 Linux kernel (CFS scheduler), glibc 2.15 and gcc 4.6.3. For our comparative study of lock performance, we consider (i) the applications from the PARSEC benchmark suite (emerging workloads), (ii) the applications from the Phoenix 2 MapReduce benchmark suite, (iii) the applications from the SPLASH2 high-performance computing benchmark suite 4 , (iv) the MySQL database running the Cloudstone workload, and (v) SSL proxy, an event-driven SSL endpoint that processes small messages. In order to evaluate the impact of workload changes on locking performance, we also consider so called "long-lived" variants of four of the above workloads denoted with a " ll" suffix. Note that six of <ref type="bibr" target="#b3">4</ref> We excluded the Cholesky application because of extremely short completion times. the applications cannot be evaluated on the two 48-core machines because, by design, they only accept a number of threads that correspond to a power of two: facesim, fluidanimate (from PARSEC), fft, ocean cp, ocean ncp, radix (from SPLASH2).</p><p>Most of these applications use a number of threads equal to the number of cores, except the three following ones: dedup (3× threads), ferret (4× threads) and MySQL (hundreds of threads). Two thirds of the applications use Pthread condition variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tuning and experimental methodology</head><p>For the lock algorithms that rely on static thresholds, we use the recommended values from the original papers and implementations. The algorithms based on a spinthen-park waiting policy (e.g., Malth STP <ref type="bibr" target="#b11">[12]</ref>) rely on a fixed threshold for the spinning time that corresponds to the duration of a round-trip context switch <ref type="bibr" target="#b21">[22]</ref> -in this case, we calibrate the duration using a microbenchmark on the testbed platform.</p><p>All the applications are run with memory interleaving (via the numactl utility) in order to avoid NUMA memory bottlenecks. Generally, in the experiments presented in this paper, we study the performance impact of a lock for a given contention level, i.e., the number of threads of the application. We vary the contention level at the granularity of a NUMA node (i.e., 8 cores for the A-64 machine, 6 cores for the A-48 machine, and 12 cores for the I-48 machine). For most of the experiments detailed in the paper, the application threads are not pinned to specific cores. The impact of pinning is nonetheless discussed in §5.3.</p><p>Finally, each experiment is run at least five times and we compute the average value. Overall, we observe little variability for most configurations. For all experiments, the considered application-level performance metric is the throughput (operations per time unit).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The LiTL lock interposition library</head><p>In order to carry out the lock comparison study, we have developed LiTL, an interposition library for Linux/x86 allowing transparently replacing the lock algorithm used for Pthread mutexes. We describe its design, implementation, and assess its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Design</head><p>The design of LiTL does not impose any restriction on the level of nested locking and is compatible with arbitrary locking disciplines (e.g., hand-over-hand locking <ref type="bibr" target="#b32">[33]</ref>). The pseudo-code of the main wrapper functions of the LiTL library is depicted in <ref type="figure" target="#fig_2">Figure 1</ref>.  General principles The primary role of LiTL is to maintain a mapping structure between an instance of the standard Pthread lock (pthread mutex t) and an instance of the chosen optimized lock type (e.g., MCS Spin). This implies that LiTL must keep track of the lifecycle of all the application's locks through interposition of the calls to pthread mutex init() and pthread mutex destroy(), and that each interposed call to pthread mutex lock() must trigger a lookup for the instance of the optimized lock. In addition, lock instances that are statically initialized can only be discovered and tracked upon the first invocation of pthread mutex lock() on them (i.e., a failed lookup leads to the creation of a new mapping).</p><p>The lock/unlock API of several lock algorithms requires an additional parameter (called "struct" hereafter) in addition to the lock pointer. For example, in the case of an MCS lock, this parameter corresponds to the record to be inserted in (or removed from) the lock's waiting queue. In the general case, a struct cannot be reused nor freed before the corresponding lock has been released. For instance, an application may rely on nested critical sections (i.e., a thread T must acquire a lock L 2 while holding another lock L 1 ). In this case, T must use a distinct struct for L 2 in order to preserve the integrity of L 1 's struct. In order to gracefully support the most general cases, LiTL systematically allocates exactly one struct per lock instance and per thread.</p><p>Supporting condition variables Dealing with condition variables inside each optimized lock algorithm would be complex and tedious as most locks have not been designed with condition variables in mind. We therefore use the following strategy: our wrapper for pthread cond wait() internally calls the true pthread cond wait() function. To issue this call, we need to hold a real Pthread mutex lock (of type pthread mutex t). This strategy (depicted in the pseudocode of <ref type="figure" target="#fig_2">Figure 1</ref>) does not introduce high contention on the internal Pthread lock. Indeed, for workloads that do not use condition variables, the Pthread lock is only requested by the holder of the optimized lock associated with the critical section. Furthermore, workloads that use condition variables are unlikely to have more than two threads competing for the Pthread lock: the holder of the optimized lock and a notified thread. Note that the latter claim also holds for workloads that rely on pthread cond broadcast() because the Linux implementation of this call only wakes up a single thread from the wait queue of the condition variable and directly transfers the remaining threads to the wait queue of the Pthread lock.</p><p>Support for specific lock semantics The design of LiTL is compatible with specific lock semantics when the underlying lock algorithms offer the corresponding properties. For example, LiTL supports non-blocking lock requests (pthread mutex trylock()) for all the currently implemented locks except CLH-based locks and Hticket-LS, which are not compatible with such semantics.</p><p>Although not yet implemented, LiTL could easily support blocking requests with timeouts for the socalled "abortable" locks (e.g., MCS-Try <ref type="bibr" target="#b33">[34]</ref> and MCSTimePub <ref type="bibr" target="#b18">[19]</ref>). Moreover, support for optional Pthread Figure 2: Performance comparison (throughput) of manually implemented locks (black bars) vs. transparently interposed locks using LiTL (white bars). The throughput is normalized with respect to the best performing configuration for a given application (A-64 machine).</p><p>mutex behavior like reentrance and error checks 5 could be easily integrated in the generic wrapper code by managing fields for the current owner and the lock acquisition counter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>The library relies on a scalable concurrent hash table (CLHT <ref type="bibr" target="#b9">[10]</ref>) in order to store, for each Pthread mutex instance used in the application, the corresponding optimized lock instance, and the associated perthread structs. For well-established locking algorithms like MCS, the code of LiTL borrows from other libraries <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b25">26]</ref>. Other algorithms are implemented from scratch based on the description of the original papers. For algorithms that are based on a parking or on a spinning-then-parking waiting policy, our implementation directly relies on the futex Linux system call. Finally, the source code of LiTL relies on preprocessor macros rather than function pointers. Indeed, we have observed that the use of function pointers in the critical path introduced a surprisingly high overhead. Moreover, all data structures are cache-aligned in order to mitigate the impact of false sharing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental validation</head><p>In this section, we assess the performance of LiTL using the A-64 machine. To that end, we compare the performance (throughput) of each lock on a set of applications running in two distinct configurations: manually modified applications and unmodified applications using interposition with LiTL. Clearly, one cannot expect to ob-5 Using respectively the PTHREAD MUTEX RECURSIVE and PTHREAD MUTEX ERRORCHECK attributes. tain exactly the same results in both configurations, as the setups differ in several ways, e.g., with respect to the exercised code paths, the process memory layout and the allocation of the locks (e.g., stack-vs. heap-based). However, we show that between both configurations: (i) the achieved performance is close and (ii) the general trends for the different locks remain stable.</p><p>We selected three applications: pca ll, radiosity ll and s raytrace ll. These three applications are particularly lock-intensive and the last two use Pthread condition variables. Therefore, all three represent an unfavorable case for LiTL. Moreover, we focus the discussion on the results under the highest contention level (i.e., when the application uses all the cores of the target machine), as this again represents an unfavorable case for LiTL. <ref type="figure">Figure 2</ref> shows the normalized performance (throughput) of both configurations (manual/interposed) for each (application, lock) pair: black bars correspond to manually implemented locks, whereas white bars correspond to transparently interposed locks using LiTL. In addition, <ref type="table">Table 2</ref> summarizes the performance differences for each application: number of locks for which each version performs better and, in each case, the average gain and the relative standard deviation.</p><p>We observe that, for all of the three applications, the results achieved by the two versions of the same lock are very close: the average performance difference is below 5%. Besides, <ref type="figure">Figure 2</ref> highlights that the general trends observed with the manual versions are preserved with the interposed versions. We thus conclude that using LiTL to study the behavior of lock algorithms in an application yields only very modest differences with respect to the performance behavior of a manually modified version.  <ref type="table">Table 2</ref>: Detailed statistics for the performance comparison of manually implemented locks vs. transparently interposed locks using LiTL (A-64 machine).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance study of lock algorithms</head><p>In this section, we use LiTL to compare the behavior of the different lock algorithms on different workloads and at different levels of contention. In the interest of space, we do not systematically report the observed standard deviations. However, in order to mitigate the impact of variability, when comparing the performance of two locks, we consider a margin of 5%: lock A is considered better than lock B if B's achieved performance is below 95% of A's. Besides, in order to make fair comparisons, the results presented for the Pthread locks are obtained using the same library interposition mechanism as with the other locks. Note that some configurations are not tested because of specific restrictions. First, streamcluster, streamcluster ll, and vips cannot use CLH-based locks or Hticket-LS as they do not support trylocks semantics. Second, we omit the results for most locks with MySQL: given the extremely large ratio of threads to cores, most locks yield performance close to zero. Third, some applications, e.g., dedup and fluidanimate, run out of memory for some configurations.</p><p>Finally, for the sake of space, we do not report all the results for the three studied machines. We rather focus on the A-64 machine and provide summaries of the results for the A-48 and I-48 machines. Nevertheless, the entire set of results can be found in a companion technical report <ref type="bibr" target="#b17">[18]</ref>.</p><p>The section is structured as follows. §5.1 provides preliminary observations that drive the study. §5.2 answers the main questions of the study regarding the observed lock behavior. §5.3 discusses additional observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Preliminary observations</head><p>Before proceeding with the detailed study, we highlight some important characteristics of the applications. <ref type="table" target="#tab_3">Table 3</ref> shows two metrics for each application and for different numbers of nodes on the A-64 machine: the performance gain of the best lock over the worst one, as well as the relative standard deviation for the performance of the different locks. For the moment, we only focus on the relative standard deviations at the maximum number of nodes (max nodes-highest contention) given in the 5th column (the detailed results from this table are discussed in §5.2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Selection of lock-sensitive applications</head><p>We consider that an application is lock-sensitive if the relative standard deviation for the performance of the different locks at max nodes is higher than 10% (highlighted in bold font). We observe that about 60% of the applications are impacted by locks. We observe similar trends on the three studied machines (see <ref type="table" target="#tab_4">Table 4</ref>).</p><p>In the remainder of this study, we focus on locksensitive applications.    <ref type="formula">90 95 95 87 92 92 84 79 94 90 90 88 89 85 109 84 89 125 88 107 87 105 102</ref>  <ref type="formula">107 97 114 81 70 103 124 121 89 92 96 73 87 75 111 114 82 45 103 72 73 234 49 136 60 106 173  ocean ncp 93 99 90 73 69 90 93 79 76 90 81 73 84 85 73 92 95 61 98 97 85 206 56 89 57 93</ref>   <ref type="formula">97 62 99 72 82 123 50 62 52 59 69 128 79 86 109 82 83 131 162 222 114 74 70 108 154</ref> water nsquared water spatial </p><note type="other">97 104 linear regression 44 227 12 21 132 67 45 34 7 49 44 15 25 8 51 47 24 50 10 8 38 8 21 27 matrix multiply 259 92 287 66 62 7 64 65 55 mysqld -</note><formula xml:id="formula_0">- - - - - - - - - - - - - 25 - - - - - - - ocean cp</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Selection of the number of nodes</head><p>In multicore applications, optimal performance is not always achieved at the maximum number of available nodes (abbreviated as max nodes) due to various kinds of scalability bottlenecks. Therefore, for each (application, lock) pair, we empirically determine the optimized configuration (abbreviated as opt nodes), i.e., the number of nodes that yields the best performance. For the A-64 and A-48 machines, we consider 1, 2, 4, 6, and 8 nodes. For the I-48 machines, we consider 1, 2, 3, and 4 nodes. Note that 6 nodes on A-64 and A-48 correspond to 3 nodes on I-48, i.e., 75% of the available cores.</p><p>The results for the A-64 machine are displayed in Table 5. For each (application, lock) pair, the corresponding cell indicates the performance gain of the optimized configuration with respect to the max-node configuration. The background color of a cell indicates the number of nodes for the optimized configuration. In addition, <ref type="table">Table 6</ref> provides a breakdown of the (application, lock) pairs according to their optimized number of nodes for all machines.</p><p>We observe that, for many applications, the optimized number of nodes is lower than the max number of nodes. Moreover, we observe ( <ref type="table" target="#tab_6">Table 5</ref>) that the performance gain of the optimized configuration is often extremely large. This confirms that tuning the degree of parallelism has frequently a very strong impact on performance. We also notice that, for some applications, the optimized number of nodes varies according to the chosen lock.  <ref type="table">Table 6</ref>: Breakdown of the (application, lock) pairs according to their optimized number of nodes (all machines).</p><p>In light of the above observations, the main questions investigated in the study ( §5.2) will be considered from two complementary angles: (i) comparing locks at a fixed number of nodes, and (ii) comparing locks at their optimized configurations (i.e., with possibly a different number of nodes for each). The first angle offers insight for situations in which the degree of parallelism cannot be adjusted, while the second is useful for scenarios in which more advanced application tuning is possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main questions</head><p>5.2.1 How much do locks impact applications? <ref type="table" target="#tab_3">Table 3</ref> shows, for each application, the performance gain of the best lock over the worst one at 1 node, max nodes, and opt nodes for the A-64 machine. The table also shows the relative standard deviation for the performance of the different locks.</p><p>We observe that the impact of locks on the performance of applications depends on the number of nodes. At 1 node, the impact of locks on lock-sensitive applications is moderate. More precisely, most applications exhibit a gain of the best lock over the worst one that is lower than 30%. In contrast, at max nodes, the impact of locks is very high for all lock-sensitive applications. More precisely, the gain brought by the best lock over the worst lock ranges from 42% to 3343%. Finally, at the optimized number of nodes, the impact of locks is high, but noticeably lower than at max nodes. We explain this difference by the fact that, at max nodes, some of the locks trigger a performance collapse for certain applications (as shown in <ref type="table" target="#tab_6">Table 5</ref>), which considerably increases the observed performance gaps between locks. We observe the same trends on the A-48 and I-48 machines (see the companion technical report <ref type="bibr" target="#b17">[18]</ref>). <ref type="table">Table 7</ref> shows the coverage of each lock, i.e., how often it stands as the best one (or is within 5% of the best) over all the studied applications for the A-64 machine. The results are shown for three configurations: 1 node, max nodes, and opt nodes. Besides, <ref type="table">Table 8</ref>   <ref type="table">Table 7</ref>: For each lock, fraction of the lock-sensitive applications for which the lock yields the best performance for three configurations: 1 node, max nodes, and opt nodes (A-64 machine).  <ref type="table">Table 8</ref>: Statistics on the coverage of locks for three configurations: 1 node, max nodes, and opt nodes (all machines).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Are some locks always among the best?</head><p>We make the following observations <ref type="table">(Table 8)</ref>. No lock is among the best for more than 89% of the applications at 1 node and for more than 52% of the applications both at max nodes and at the optimal number of nodes. We also observe that the average coverage is much higher at 1 node than at max nodes, and slightly higher at the optimized number of nodes than at max nodes. This is directly explained by the observations made in §5.2.1. First, at 1 node, locks have a much lower impact on applications than in other configurations and thus yield closer results, which increases their likelihood to be among the best ones. Second, at max nodes, all of the different locks cause, in turn, a performance collapse, which reduces their likelihood to be among the best locks. This latter phenomenon is not observed at the optimized number of nodes. We observe the same trends on the A-48 and I-48 machines (see the companion technical report <ref type="bibr" target="#b17">[18]</ref>). <ref type="table" target="#tab_10">Table 9</ref> shows pairwise comparisons for all locks, at max nodes on the A-64 machine. In each table, cell (rowA, colB) contains the score of lock A vs. lock B, i.e., the percentage of applications for which lock A is at least 5% better than lock B. For example, <ref type="table" target="#tab_10">Table 9</ref> shows that for 38% of the applications, AHMCS performs at least 5% better than Backoff at the optimized number of nodes. Similarly, the table shows that Backoff is at least 5% better than AHMCS for 29% of the applications. From these two values, we can conclude that the two above mentioned locks perform very closely for 33% of the applications. At the end of each line (resp. column), the table also shows the mean of the fraction of applications for which a lock is better (resp. worse) than others. Besides, the latter two metrics are summarized for the three machines in <ref type="table">Table 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Is there a clear hierarchy between locks?</head><p>We observe that there is no clear global performance hierarchy between locks. More precisely, for most pairs of locks (A, B), there are some applications for which A is better than B, and vice-versa (   <ref type="table">Table 10</ref>: For each lock, at the optimized number of nodes, mean of the fraction of applications for which the lock is better (resp. worse) than other locks (all machines).</p><p>never yields better performance than B. The results at max nodes (not shown due to lack of space) exhibit similar trends as the ones at opt nodes. Besides, we make the same observations (both at opt nodes and max nodes) on the A-48 and I-48 machines (see the companion technical report <ref type="bibr" target="#b17">[18]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Are all locks potentially harmful?</head><p>Our goal is to determine, for each lock, if there are applications for which it yields substantially lower performance than other locks and to quantify the magnitude of such performance gaps. <ref type="table">Table 11</ref> displays, for the A-64 machine, the performance gain brought by the best lock with respect to each of the other locks for each application at max nodes (top part) and at the optimized number of nodes for each lock (bottom part). For example, the top part of the table shows that for the dedup application, the best lock (0%, here Spinlock-LS) is 598% better than the Alock-LS lock. The gray cells highlight values greater than 15%. Thus, for each lock in a column, the number of grey cells corresponds to the number of applications for which the lock is beaten by a gap of 15% or more by the best lock(s) for this application. In addition, <ref type="table">Table 12</ref> displays, for each machine, the fraction of applications that are significantly hurt by a given lock.</p><p>On the three machines, we observe that, both at max  <ref type="formula">228 24 20 108 57 31 62 0 52 28 11 17 0 49 46 56 3 39 15 0 83 15 32 9</ref>  <ref type="formula">31 18 37 22 16 27 38 38 24 29 29 15 23 27 27 43 32 0 24 11 19 129 5 55 5 38 81  ocean ncp 27 28 29 30 9 25 27 28 12 28 16 10 20 22 14 36 37 11 29 31 27 118 0 25 2 29 93</ref> pca <ref type="formula">65 69 155 46 357 61 48 220 40 38 59 39 38 0 43 58 214 23 45 110 39 252 75 110 23 157 112  pca ll 47 38 251 24 664 25 51 511 30 24 41 0 18 36 17 50 526 15 27 206 68 584 128 128 17 241</ref>   <ref type="formula">47 801 9 2k 50 16 2k 35 45 3 28 59 63 62 12 2k 44 76 567 267 2k 396 614</ref>   <ref type="formula">82 1k 18 3k 96 87 3k 68 169 0 164 84 291 99 69 3k 111 157 639 335 2k 428 813 332 1k 1k</ref> ssl proxy 0 18 532 1 1k <ref type="formula">47 16 879 9 41 379 20 16 35 43 47 900 29 36 293 153</ref>   <ref type="formula">47 6 29 0 37 53 0 89 106 82 92 93 0 56 46</ref>   <ref type="formula">35 15 3 26 38 21 19 30 0 33 32 16 14 32</ref>   <ref type="table">Table 11</ref>: For each application, at max nodes (top part) and at the optimized number of nodes (bottom part), performance gain (in %) obtained by the best lock(s) with respect to each of the other locks. The grey background highlights cells for which the performance gains are greater than 15%. A line with many gray cells corresponds to an application whose performance is hurt by many locks. A column with many gray cells corresponds to a lock that is outperformed by many other locks. Dashes correspond to untested cases. (A-64 machine).</p><note type="other">19 49 matrix multiply 9 559 5 26 7 18 9 3 24 136 608 642 5 3 639 27 2 0 33 3 3 5 637 3 633 5 630 mysqld -</note><formula xml:id="formula_1">- - -30 - - - - - - - - 0 - - 7 173 -97 102 - - - - - - ocean cp</formula><p>nodes and at the optimal number of nodes, all locks are potentially harmful, yielding sub-optimal performance for a significant number of applications (Table 12). We also notice that locks are significantly less harmful at the optimized number of nodes than at max nodes. This is explained by the fact that several of the locks create performance collapses at max nodes, which does not occur at the optimized number of nodes. Moreover, we observe that, for each lock, the performance gap to the best lock can be significant <ref type="table">(Table 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Additional observations</head><p>Impact of the number of nodes. <ref type="table" target="#tab_3">Table 13</ref> shows, for each application on the A-64 machine, the number of pairwise changes in the lock performance hierarchy when the number of nodes is modified. For example, in the case of the facesim application, there are 18% of the pairwise performance comparisons between locks that change when moving from a 1-node configuration to a 2-node configuration. Similarly, there are 95% of pairwise comparisons that change at least once when considering <ref type="table" target="#tab_3">A-64  A-48  I-48  Lock  Max  Opt  Max  Opt  Max  Opt  ahmcs  62%  24%  56%  39%  39%  33%  alock-ls  87%  39%  61%  39%  58%  58%  backoff  61%  35%  68%  53%  58%  53%  c-bo-mcs spin  61%  35%  53%  58%  47%  32%  c-bo-mcs stp  71%  38%  80%  65%  55%  45%  clh-ls  84%  37%  73%  40%  69%  62%  clh</ref>   <ref type="table">Table 12</ref>: For each lock, at max nodes and at the optimized number of nodes, fraction of the applications for which the lock is harmful (all machines).</p><p>the 1-node, 2-node, 4-node and 8-node configurations. We observe that, for all applications, the lock performance hierarchy changes significantly according to the chosen number of nodes. Moreover, we observe the same trends on the A-48 and I-48 machines (see the companion technical report <ref type="bibr" target="#b17">[18]</ref>  <ref type="table" target="#tab_3">Table 13</ref>: For each application, percentage of pairwise changes in the lock performance hierarchy when changing the number of nodes (A-64 machine).</p><p>Impact of the machine. <ref type="table" target="#tab_4">Table 14</ref> shows the number of pairwise lock inversions observed between the machines (both at max nodes and at the optimized number of nodes). More precisely, for a given application at a given node configuration, we check whether two locks are in the same order or not on the target machines. We observe that the lock performance hierarchy changes significantly according to the chosen machine. Interestingly, we observe that there is approximately the same number of inversions between each pair of machines.  A note on Phtread locks. The various results presented in this paper show that the current Linux Pthread locks perform well (i.e., are among the best locks) for a significant share of the studied applications, thus providing a different insight than recent results, which were mostly based on synthetic workloads <ref type="bibr" target="#b8">[9]</ref>. Beyond the changes of workloads, these differences may also be explained by the continuous refinement of the Linux Pthread implementation. It is nevertheless important to note that on each machine, some locks stand out as the best ones for a higher fraction of the applications than Pthread locks. Finally, we note that Pthread adaptive locks perform slightly better than standard Pthread locks.</p><p>Impact of thread pinning. As explained in §3.2, all the above-described experiments were run without any restriction on the placement of threads, leaving the corresponding decisions to the Linux scheduler. However, in order to better control CPU allocation and improve locality, some developers and system administrators use pinning to explicitly restrict the placement of each thread to one or several core(s). The impact of thread pinning may vary greatly according to workloads and can yield both positive and negative effects <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>. In order to assess the generality of our observations, we also performed the complete set of experiments with an alternative configuration in which each thread is pinned to a given node, leaving the scheduler free to place the thread among the cores of the node. Note that for an experiment with a N-node configuration, the complete application runs on exactly first N nodes of the machine. We chose thread-tonode pinning rather than thread-to-core pinning because we observed that the former generally provided better performance for our studied applications, especially the ones using more threads than cores. The detailed results of our experiments with thread-to-node pinning are available in the companion technical report <ref type="bibr" target="#b17">[18]</ref>. Overall, we observe that all the conclusions presented in the paper still hold with per-node thread pinning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>The design and implementation of the LiTL lock library borrows code and ideas from previous open-source toolkits that provide application developers with a set of optimized implementations for some of the mostestablished lock algorithms: Concurrency Kit <ref type="bibr" target="#b0">[1]</ref>, liblock <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>, and libslock <ref type="bibr" target="#b8">[9]</ref>. All of these toolkits require potentially tedious source code modifications in the target applications, even in the case of algorithms that have been specifically designed to lower this burden <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. Moreover, among the above works, none of them provides a simple and generic solution for supporting Pthread condition variables. The authors of liblock <ref type="bibr" target="#b25">[26]</ref> have proposed an approach but we discovered that it suffers from liveness hazards due to a race condition. Indeed, when a thread T calls pthread cond wait(), it is not guaranteed that the two steps (releasing the lock and blocking the thread) are always executed atomically. Thus, a wake-up notification issued by another thread may get interleaved between the two steps and T may remain indefinitely blocked. Several research works have leveraged library interposition to compare different locking algorithms on legacy applications (e.g., Johnson et al. <ref type="bibr" target="#b20">[21]</ref> and Dice et al. <ref type="bibr" target="#b13">[14]</ref>) but, to the best of our knowledge, they have not publicly documented the design challenges to support arbitrary application patterns, nor disclosed the corresponding source code and the overhead of their interposition library has not been discussed.</p><p>Several studies have compared the performance of different multicore lock algorithms, either from a theoretical angle or based on experimental results <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14]</ref>. In comparison, our study encompasses significantly more lock algorithms and waiting policies. Moreover, the bulk of these studies is mainly focused on characterization microbenchmarks while we focus instead on workloads designed to mimic real applications. Two noticeable exceptions are the work from Boyd-Wickizer et al. <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr">Lozi et al.</ref> [26] but they do not consider the same context as our study. The former is focused on kernel-level locking bottlenecks, and the latter is focused on applications in which only one or a few heavily contended critical sections have been optimized (after a profiling phase). For all these reasons, we make observations that are significantly different from the ones based on all the above-mentioned studies. Other synchronization-related studies like the one from Gramoli <ref type="bibr" target="#b15">[16]</ref> have a different scope and focus on concurrent data structures, possibly based on other facilities than locks.</p><p>Finally, some tools have been proposed to facilitate the identification of locking bottlenecks in applications <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref>. These publications are orthogonal to our work. We note that, among them, the profilers based on library interposition can be stacked on top of LiTL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and future work</head><p>Optimized lock algorithms for multicore machines are abundant. However, there are currently no clear guidelines and methodologies helping developers to select the right lock for their workloads. In this paper, we have presented a broad study of 27 locks algorithms with 35 applications on Linux/x86. To perform that study, we have implemented LiTL, an interposition library allowing the transparent replacement of lock algorithms used for Pthread mutex locks. From our study, we draw several conclusions, including the following ones: at its optimized contention level, no single lock dominates for more than 52% of the lock-sensitive applications; any of the locks is harmful for at least several applications; for a given application, the best lock varies according to both the number of contending cores and the machine that executes the application. These observations call for further research on optimized lock algorithms, as well as tools and dynamic approaches to better understand and control their behavior.</p><p>The source code of LiTL and the data sets of our experimental results are available online <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Name</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>/</head><label></label><figDesc>/ Note that the pthread_cond_signal and // pthread_cond_broadcast primitives // do not need to be interposed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the pseudocode for the main wrapper functions of LiTL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>For each application, performance gain of the 
best vs. worst lock and relative standard deviation (A-64 
machine). 

A-64 
A-48 
I-48 
# tested applications 
39 
33 
33 
# lock-sensitive applications 
23 
19 
17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Number of tested applications and number of 
lock-sensitive applications (all machines). </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>For each (application, lock) pair, performance gain (in %) of the optimized configuration over the max-node 
configuration. The background color of a cell indicates the number of nodes (1, 2, 4, 6, or 8 nodes) for the optimized 
configuration: 1 2 4 6 8 . Dashes correspond to untested cases. (A-64 machine). 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>displays, for each machine (at 1 node, max nodes and opt nodes) the following metrics aggregated over the different locks: the min and max coverage, the average coverage, and the relative standard deviation of the coverage.</figDesc><table>Number of nodes 
Locks 
1 
Max 
Opt 
ahmcs 
67% 
24% 
52% 
alock-ls 
52% 
4% 
30% 
backoff 
83% 
30% 
26% 
c-bo-mcs spin 
74% 
22% 
39% 
c-bo-mcs stp 
62% 
12% 
29% 
clh-ls 
63% 
5% 
37% 
clh spin 
68% 
5% 
37% 
clh stp 
63% 
16% 
21% 
c-ptl-tkt 
57% 
22% 
35% 
c-tkt-tkt 
74% 
22% 
39% 
hmcs 
65% 
22% 
48% 
hticket-ls 
63% 
16% 
37% 
malth spin 
61% 
9% 
26% 
malth stp 
54% 
29% 
29% 
mcs-ls 
74% 
4% 
30% 
mcs spin 
70% 
22% 
48% 
mcs stp 
79% 
21% 
29% 
mcs-timepub 
54% 
38% 
29% 
partitioned 
70% 
22% 
39% 
pthread 
50% 
21% 
29% 
pthreadadapt 
58% 
33% 
29% 
spinlock 
65% 
26% 
30% 
spinlock-ls 
57% 
30% 
35% 
ticket 
74% 
22% 
39% 
ticket-ls 
74% 
13% 
35% 
ttas 
83% 
26% 
43% 
ttas-ls 
65% 
0% 
9% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 9 ). The only marginal exceptions are the cells having 0% for value. This corresponds to pairs of locks (A, B) for which A ahmcs alock</head><label>9</label><figDesc></figDesc><table>-ls 
backoff 
c-bo-mcs spin 
c-bo-mcs stp 
clh-ls 
clh spin 
clh stp 
c-ptl-tkt 
c-tkt-tkt 
hmcs 
hticket-ls 
malth spin 
malth stp 
mcs-ls 
mcs spin 
mcs stp 
mcs-timepub 
partitioned 
pthread 
pthreadadapt 

spinlock 
spinlock-ls 

ticket 
ticket-ls 

ttas 
ttas-ls 
average 

ahmcs 
19 38 48 29 22 17 61 19 48 5 
33 33 43 38 38 48 52 24 38 43 57 48 33 33 43 38 36 
alock-ls 
19 
39 30 26 16 16 58 17 22 9 
26 39 30 22 26 43 30 9 
39 43 48 39 35 30 35 39 30 
backoff 
29 35 
30 26 37 37 58 26 26 35 32 35 26 35 30 52 30 17 35 39 30 26 4 
22 0 
39 30 
c-bo-mcs spin 
33 48 43 
35 37 32 74 22 17 39 32 39 48 39 9 
48 13 22 39 39 39 43 48 39 35 65 38 
c-bo-mcs stp 
33 43 35 22 
42 32 74 17 22 30 21 22 25 26 26 42 21 13 33 33 39 26 26 22 26 61 31 
clh-ls 
22 21 37 42 32 
16 47 26 26 16 26 37 37 16 32 47 26 16 42 47 53 47 47 42 42 47 34 
clh spin 
22 32 32 32 26 32 
53 21 37 21 42 32 26 32 21 47 32 11 37 37 47 42 32 42 37 47 33 
clh stp 
33 32 5 
16 11 37 16 
26 16 26 26 16 11 21 16 11 5 
11 11 11 21 21 11 26 11 32 18 
c-ptl-tkt 
19 35 35 39 30 32 21 68 
26 22 26 26 43 30 26 57 39 17 39 35 48 35 30 30 35 57 35 
c-tkt-tkt 
24 39 35 26 39 32 26 74 26 
30 32 48 65 43 17 57 22 9 
39 43 39 43 39 43 35 65 38 
hmcs 
14 30 39 35 22 42 32 74 17 39 
32 39 35 35 26 52 39 26 39 39 48 39 30 30 30 52 36 
hticket-ls 
17 16 47 32 26 21 32 74 11 21 5 
32 42 11 26 53 32 11 42 42 53 42 37 26 47 58 33 
malth spin 
14 35 22 22 26 26 16 63 13 17 22 16 
22 22 13 39 17 4 
35 35 35 39 17 13 17 48 25 
malth stp 
24 35 22 35 21 32 37 58 17 17 26 21 4 
22 17 33 25 9 
33 29 35 22 17 17 17 48 26 
mcs-ls 
24 17 35 35 35 21 26 63 13 17 17 16 35 26 
17 39 17 4 
39 43 43 35 30 17 35 48 29 
mcs spin 
29 43 35 26 39 37 32 68 26 17 39 47 39 43 43 
43 22 22 35 39 35 43 39 30 39 61 37 
mcs stp 
29 35 9 
22 21 32 32 42 22 9 
30 26 17 17 26 9 
12 17 21 25 17 17 13 17 13 39 22 
mcs-timepub 
33 39 35 22 33 42 37 68 17 9 
30 32 39 29 22 9 
38 
13 29 33 30 35 30 30 30 57 32 
partitioned 
24 39 26 39 43 32 32 68 26 22 39 53 52 43 35 35 61 35 
43 48 48 43 26 43 35 65 41 
pthread 
29 39 22 26 25 37 32 58 22 17 39 26 30 25 35 26 46 25 13 
21 39 13 17 13 17 43 28 
pthreadadapt 
29 43 22 35 21 37 37 53 30 26 35 26 26 25 35 30 42 25 17 21 
22 22 17 17 17 43 29 
spinlock 
29 39 9 
26 17 37 32 53 35 13 39 32 43 35 35 22 39 17 22 26 30 
26 13 30 9 
35 29 
spinlock-ls 
29 39 26 30 35 26 26 63 26 30 35 16 30 30 30 30 48 30 22 43 30 48 
26 13 26 57 33 
ticket 
29 35 9 
26 26 32 32 63 26 22 35 32 30 26 30 26 48 22 13 26 39 30 26 
22 0 
39 29 
ticket-ls 
19 22 30 26 39 26 32 68 26 26 22 11 35 39 22 26 52 26 26 35 48 43 39 30 
30 52 33 
ttas 
24 35 4 
26 22 37 26 63 26 17 35 32 30 26 30 30 52 17 17 30 35 30 26 4 
26 
30 28 
ttas-ls 
19 17 9 
17 13 21 16 42 13 13 4 
5 
22 22 9 
22 30 9 
13 17 22 30 17 13 4 
9 
17 
average 
25 33 27 29 28 32 28 62 22 22 26 28 32 32 29 23 45 25 15 33 36 39 33 26 26 26 49 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11" validated="false"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table>For each pair of locks (rowA, colB) at the optimized number of nodes, score of lock A vs lock B: percentage 
of applications for which lock A performs at least 5% better than B (A-64 machine). 

Better 
Worse 
Lock 
A-64 
A-48 
I-48 
A-64 
A-48 
I-48 
ahmcs 
36% 
40% 
52% 
25% 
28% 
25% 
alock-ls 
30% 
42% 
37% 
33% 
25% 
32% 
backoff 
30% 
29% 
23% 
27% 
33% 
45% 
c-bo-mcs spin 
38% 
47% 
46% 
29% 
25% 
15% 
c-bo-mcs stp 
31% 
25% 
38% 
28% 
44% 
25% 
clh-ls 
34% 
46% 
32% 
32% 
32% 
38% 
clh spin 
33% 
38% 
33% 
28% 
34% 
37% 
clh stp 
18% 
11% 
8% 
62% 
72% 
71% 
c-ptl-tkt 
35% 
44% 
54% 
22% 
26% 
13% 
c-tkt-tkt 
38% 
42% 
51% 
22% 
27% 
15% 
hmcs 
36% 
50% 
52% 
26% 
21% 
17% 
hticket-ls 
33% 
45% 
42% 
28% 
25% 
17% 
malth spin 
25% 
36% 
31% 
32% 
37% 
35% 
malth stp 
26% 
20% 
28% 
32% 
53% 
36% 
mcs-ls 
29% 
43% 
35% 
29% 
22% 
26% 
mcs spin 
37% 
38% 
36% 
23% 
33% 
23% 
mcs stp 
22% 
23% 
20% 
45% 
59% 
52% 
mcs-timepub 
32% 
38% 
34% 
25% 
34% 
29% 
partitioned 
41% 
42% 
38% 
15% 
32% 
23% 
pthread 
28% 
33% 
34% 
33% 
43% 
35% 
pthreadadapt 
29% 
34% 
34% 
36% 
38% 
36% 
spinlock 
29% 
35% 
20% 
39% 
44% 
49% 
spinlock-ls 
33% 
41% 
38% 
33% 
30% 
31% 
ticket 
29% 
23% 
17% 
26% 
44% 
53% 
ticket-ls 
33% 
40% 
28% 
26% 
24% 
35% 
ttas 
28% 
28% 
24% 
26% 
34% 
44% 
ttas-ls 
17% 
27% 
20% 
49% 
42% 
52% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19" validated="false"><head>Table 14 :</head><label>14</label><figDesc></figDesc><table>For each pair of machines, at max nodes and 
at opt nodes, percentage of pairwise changes in the lock 
performance hierarchy (all machines). 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers and our shepherd, Tim Harris, for their insightful comments on ealier drafts of this paper. Dave Dice provided detailed answers for our questions on Malthusian locks. Baptiste Lepers provided valuable insights for some of the case studies. Pierre Neyron provided his help to set up experiments on the I-48 machine. Finally, this work has been partially supported by: LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01), EmSoc Replicanos and AGIR CAEC projects of Université Grenoble-Alpes and GrenobleINP, and the INRIA/LIG Digitalis project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Bahra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Concurrency</forename><surname>Kit</surname></persName>
		</author>
		<ptr target="http://concurrencykit.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transaction on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="1990-01" />
			<biblScope unit="page" from="6" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancement to the MCS Lock for Increased Functionality and Improved Programmability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Auslander</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Edelsohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rosen-Burg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wisniewski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">U.S. Patent Application Number</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Non-scalable Locks are Dangerous</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyd-Wickizer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeldovich</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Linux Symposium</title>
		<meeting>the Linux Symposium<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High Performance Locks for Multi-level NUMA Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chabbi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP&apos;15</title>
		<meeting>the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP&apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contentionconscious, Locality-preserving Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chabbi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP&apos;16</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP&apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Building FIFO and Priority-Queuing Spin Locks from Atomic Swap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename></persName>
		</author>
		<idno>TR 93-02-02</idno>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Continuously Measuring Critical Section Pressure with the Freelunch Profiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muller</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno>OOPSLA &apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM International Conference on Object Oriented Programming Systems Languages &amp; Applications</title>
		<meeting>the 2014 ACM International Conference on Object Oriented Programming Systems Languages &amp; Applications</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Everything You Always Wanted to Know About Synchronization but Were Afraid to Ask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trigonakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP&apos;13</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP&apos;13</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Secret to Scaling Concurrent Search Data Structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trigonakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Asynchronized</forename><surname>Concurrency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;15</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;15</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Brief Announcement: A Partitioned Ticket Lock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dice</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;11</title>
		<meeting>the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;11</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dice</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Malthusian</forename><surname>Locks</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1511.06035" />
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Flat-Combining NUMA Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dice</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shavit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;11</title>
		<meeting>the Twenty-third Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;11</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A General Technique for Designing NUMA Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dice</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shavit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohorting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Revisiting the Combining Synchronization Technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatourou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kallimanis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>PPoPP&apos;12</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">More Than You Ever Wanted to Know About Synchronization: Synchrobench, Measuring the Impact of the Synchronization on Concurrent Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gramoli</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>PPoPP&apos;15</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">LiTL source code and data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiroux</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quémaqu´quéma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<ptr target="https://github.com/multicore-locks" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multicore Locks: the Case is not Closed Yet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiroux</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quémaqu´quéma</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<ptr target="https://github.com/multicore-locks" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Preemption Adaptivity in Time-published Queue-based Spin Locks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on High Performance Computing (HiPC&apos;05</title>
		<meeting>the 12th International Conference on High Performance Computing (HiPC&apos;05</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flat Combining and the Synchronization-Parallelism Tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Incze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzafrir</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-second Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;10)</title>
		<meeting>the Twenty-second Annual ACM Symposium on Parallelism in Algorithms and Architectures (SPAA&apos;10)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Decoupling Contention Management from Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mowry</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;10)</title>
		<meeting>the 15th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;10)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Empirical Studies of Competitve Spinning for a Shared-memory Multiprocessor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karlin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owicki</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles (SOSP&apos;91</title>
		<meeting>the Thirteenth ACM Symposium on Operating Systems Principles (SOSP&apos;91</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">What is PTHREAD MUTEX ADAPTIVE NP?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kylheku</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<ptr target="http://stackoverflow.com/a/25168942" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards More Scalable Mutual Exclusion for Multicore Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lozi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename></persName>
		</author>
		<ptr target="http://www.i3s.unice.fr/˜jplozi/documents/lozi-phd-thesis.pdf" />
		<imprint>
			<date type="published" when="2014-07" />
			<pubPlace>UPMC, Paris</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Remote Core Locking: Migrating Critical-Section Execution to Improve the Performance of Multithreaded Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lozi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muller</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX Annual Technical Conference</title>
		<meeting>the 2012 USENIX Annual Technical Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast and Portable Locking for Multicore Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lozi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lawall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muller</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Linux Scheduler: A Decade of Wasted Cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lozi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Quémaqu´ Quéma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedorova</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Systems (EuroSys&apos;16</title>
		<meeting>the 11th European Conference on Computer Systems (EuroSys&apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Hierarchical CLH Queue Lock</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luchangco</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nussbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shavit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Parallel Processing</title>
		<meeting>the 12th International Conference on Parallel Processing</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Queue Locks on Cache Coherent Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnusson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Landin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagersten</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Parallel Processing</title>
		<meeting>the 8th International Symposium on Parallel Processing</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Algorithms for Scalable Synchronization on Shared-memory Multiprocessors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mellor-Crummey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Executing Parallel Programs with Synchronization Bottlenecks Efficiently</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oyama</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonezawa</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Workshop on Parallel and Distributed Computing For Symbolic And Irregular Applications (PDSIA&apos;99</title>
		<meeting>the International Workshop on Parallel and Distributed Computing For Symbolic And Irregular Applications (PDSIA&apos;99</meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hierarchical Backoff Locks for Nonuniform Communication Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radovic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagersten</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Symposium on HighPerformance Computer Architecture (HPCA&apos;03</title>
		<meeting>the 9th International Symposium on HighPerformance Computer Architecture (HPCA&apos;03</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Shared-Memory Synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scalable Queue-based Spin Locks with Timeout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming (PPoPP&apos;01</title>
		<meeting>the Eighth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming (PPoPP&apos;01</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing Lock Contention in Multithreaded Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tallent</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter-Field</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Be My Guest -MCS Lock Now Welcomes Guests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chabbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimura</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP&apos;16</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP&apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
