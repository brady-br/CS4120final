<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:02+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks and Pluribus One Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 14-16, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambra</forename><surname>Demontis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Melis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maura</forename><surname>Pintor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Roli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambra</forename><surname>Demontis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Melis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maura</forename><surname>Pintor</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Pluribus One</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Oprea</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Nita-Rotaru</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Roli</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Pluribus One</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical and Electronic Engineering</orgName>
								<orgName type="laboratory">Italy, and Pluribus One; Alina Oprea and Cristina Nita-Rotaru</orgName>
								<orgName type="institution" key="instit1">Northeastern University</orgName>
								<orgName type="institution" key="instit2">University of Cagliari</orgName>
								<orgName type="institution" key="instit3">Northeastern University</orgName>
								<orgName type="institution" key="instit4">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cagliari</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks and Pluribus One Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 28th USENIX Security Symposium</title>
						<meeting>the 28th USENIX Security Symposium <address><addrLine>Santa Clara, CA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">August 14-16, 2019</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-06-9 Open access to the Proceedings of the 28th USENIX Security Symposium is sponsored by USENIX. https://www.usenix.org/conference/usenixsecurity19/presentation/demontis</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Transferability captures the ability of an attack against a machine-learning model to be effective against a different, potentially unknown, model. Empirical evidence for transfer-ability has been shown in previous work, but the underlying reasons why an attack transfers or not are not yet well understood. In this paper, we present a comprehensive analysis aimed to investigate the transferability of both test-time evasion and training-time poisoning attacks. We provide a unifying optimization framework for evasion and poisoning attacks, and a formal definition of transferability of such attacks. We highlight two main factors contributing to attack transferabil-ity: the intrinsic adversarial vulnerability of the target model, and the complexity of the surrogate model used to optimize the attack. Based on these insights, we define three metrics that impact an attack&apos;s transferability. Interestingly, our results derived from theoretical analysis hold for both evasion and poisoning attacks, and are confirmed experimentally using a wide range of linear and non-linear classifiers and datasets.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The wide adoption of machine learning (ML) and deep learning algorithms in many critical applications introduces strong incentives for motivated adversaries to manipulate the results and models generated by these algorithms. Attacks against machine learning systems can happen during multiple stages in the learning pipeline. For instance, in many settings training data is collected online and thus can not be fully trusted. In poisoning availability attacks, the attacker controls a certain amount of training data, thus influencing the trained model and ultimately the predictions at testing time on most points in testing set <ref type="bibr">[4, 18, 20, 28-30, 34, 36, 41, 48]</ref>. Poisoning integrity attacks have the goal of modifying predictions on a few targeted points by manipulating the training process <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b41">41]</ref>. On the other hand, evasion attacks involve small manipulations of testing data points that results in misprediction at testing time on those points <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b49">49]</ref>.</p><p>Creating poisoning and evasion attack points is not a trivial task, particularly when many online services avoid disclosing information about their machine learning algorithms. As a result, attackers are forced to craft their attacks in blackbox settings, against a surrogate model instead of the real model used by the service, hoping that the attack will be effective on the real model. The transferability property of an attack is satisfied when an attack developed for a particular machine learning model (i.e., a surrogate model) is also effective against the target model. Attack transferability was observed in early studies on adversarial examples <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b42">42]</ref> and has gained a lot more interest in recent years with the advancement of machine learning cloud services. Previous work has reported empirical findings about the transferability of evasion attacks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref> and, only recently, also on the transferability of poisoning integrity attacks <ref type="bibr" target="#b41">[41]</ref>. In spite of these efforts, the question of when and why do adversarial points transfer remains largely unanswered.</p><p>In this paper we present the first comprehensive evaluation of transferability of evasion and poisoning availability attacks, understanding the factors contributing to transferability of both attacks. In particular, we consider attacks crafted with gradient-based optimization techniques (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">23]</ref>), a popular and successful mechanism used to create attack data points. We unify for the first time evasion and poisoning attacks into an optimization framework that can be instantiated for a range of threat models and adversarial constraints. We provide a formal definition of transferability and show that, under linearization of the loss function computed under attack, several main factors impact transferability: the intrinsic adversarial vulnerability of the target model, the complexity of the surrogate model used to optimize the attacks, and its alignment with the target model. Furthermore, we derive a new poisoning attack for logistic regression, and perform a comprehensive evaluation of both evasion and poisoning attacks on multiple datasets, confirming our theoretical analysis.</p><p>In more detail, the contributions of our work are:</p><p>Optimization framework for evasion and poisoning attacks. We introduce a unifying framework based on gradient-descent optimization that encompasses both evasion and poisoning attacks. Our framework supports threat models with different adversarial goals (integrity and availability), amount of knowledge available to the adversary (white-box and blackbox), as well as different adversarial capabilities (causative or exploratory). Our framework generalizes existing attacks proposed by previous work for evasion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b42">42]</ref> and poisoning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b48">48]</ref>. Under our framework, we derive a novel gradient-based poisoning availability attack against logistic regression. We remark here that poisoning attacks are more difficult to derive than evasion ones, as they require computing hypergradients from a bilevel optimization problem, to capture the dependency on how the machinelearning model changes while the training poisoning points are modified <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>Transferability definition and theoretical bound. We give a formal definition of transferability of evasion and poisoning attacks, and an upper bound on a transfer attack's success. This allows us to derive three metrics connected to model complexity. Our formal definition unveils that transferability depends on: (1) the size of input gradients of the target classifier; (2) how well the gradients of the surrogate and target models align; and (3) the variance of the loss landscape optimized to generate the attack points.</p><p>Comprehensive experimental evaluation of transferability. We consider a wide range of classifiers, including logistic regression, SVMs with both linear and RBF kernels, ridge regression, random forests, and deep neural networks (both feed-forward and convolutional neural networks), all with different hyperparameter settings to reflect different model complexities. We evaluate the transferability of our attacks on three datasets related to different applications: handwritten digit recognition (MNIST), Android malware detection (DREBIN), and face recognition (LFW). We confirm our theoretical analysis for both evasion and poisoning attacks.</p><p>Insights into transferability. We demonstrate that attack transferability depends strongly on the complexity of the target model, i.e., on its inherent vulnerability. This confirms that reducing the size of input gradients, e.g., via regularization, may allow us to learn more robust classifiers not only against evasion <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b44">44]</ref> but also against poisoning availability attacks. Second, transferability is also impacted by the surrogate model's alignment with the target model. Surrogates with better alignments to their targets (in terms of the angle between their gradients) are more successful at transferring the attack points. Third, surrogate loss functions that are stabler and have lower variance tend to facilitate gradient-based optimization attacks to find better local optima (see <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>As less complex models exhibit a lower variance of their loss function, they typically result in better surrogates.</p><p>Organization. We discuss background on threat modeling against machine learning in Section 2. We introduce our unifying optimization framework for evasion and poisoning attacks, If the adversarial sample loss is below a certain threshold (i.e., the black horizontal line), the point is correctly classified, otherwise it is misclassified. The adversarial point computed against the high-complexity model (top left) lays in a local optimum due to the irregularity of the objective. This point is not effective even against the same classifier trained on a different dataset (bottom left) due to the variance of the high-complexity classifier. The adversarial point computed against the low complexity model (top right), instead, succeeds against both low and high-complexity targets (left and right bottom, respectively).</p><p>as well as the poisoning attack for logistic regression in Section 3. We then formally define transferability for both evasion and poisoning attacks, and show its approximate connection with the input gradients used to craft the corresponding attack samples (Section 4). Experiments are reported in Section 5, highlighting connections among regularization hyperparameters, the size of input gradients, and transferability of attacks, on different case studies involving handwritten digit recognition, Android malware detection, and face recognition. We discuss related work in Section 6 and conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Threat Model</head><p>Supervised learning includes: (1) a training phase in which training data is given as input to a learning algorithm, resulting in a trained ML model; (2) a testing phase in which the model is applied to new data and a prediction is generated. In this paper, we consider a range of adversarial models against machine learning classifiers at both training and testing time.</p><p>Attackers are defined by: (i) their goal or objective in attacking the system; (ii) their knowledge of the system; (iii) their capabilities in influencing the system through manipulation of the input data. Before we detail each of these, we introduce our notation, and point out that the threat model and attacks considered in this work are suited to binary classification, but can be extended to multi-class settings.</p><p>Notation. We denote the sample and label spaces with X and Y ∈ {−1, +1}, respectively, and the training data with</p><formula xml:id="formula_0">D = (x i , y i ) n i=1</formula><p>, where n is the training set size. We use L(D, w) to denote the loss incurred by classifier f : X → Y (parameterized by w) on D. Typically, this is computed by averaging a loss function (y, x, w) computed on each data point, i.e., L(D, w) = 1 n ∑ n i=1 (y i , x i , w). We assume that the classifier f is learned by minimizing an objective function L(D,w) on the training data. Typically, this is an estimate of the generalization error, obtained by the sum of the empirical loss L on training data D and a regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Threat Model: Attacker's Goal</head><p>We define the attacker's goal based on the desired security violation. In particular, the attacker may aim to cause either an integrity violation, to evade detection without compromising normal system operation; or an availability violation, to compromise the normal system functionalities available to legitimate users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Threat Model: Attacker's Knowledge</head><p>We characterize the attacker's knowledge κ as a tuple in an abstract knowledge space K consisting of four main dimensions, respectively representing knowledge of: (k.i) the training data D; (k.ii) the feature set X ; (k.iii) the learning algorithm f , along with the objective function L minimized during training; and (k.iv) the parameters w learned after training the model. This categorization enables the definition of many different kinds of attacks, ranging from white-box attacks with full knowledge of the target classifier to black-box attacks in which the attacker has limited information about the target system. White-Box Attacks. We assume here that the attacker has full knowledge of the target classifier, i.e., κ = (D, X , f , w). This setting allows one to perform a worst-case evaluation of the security of machine-learning algorithms, providing empirical upper bounds on the performance degradation that may be incurred by the system under attack. Black-Box Attacks. We assume here that the input feature representation X is known. For images, this means that we do consider pixels as the input features, consistently with other recent work on black-box attacks against machine learning <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref>. At the same time, the training data D and the type of classifier f are not known to the attacker. We consider the most realistic attack model in which the attacker does not have querying access to the classifier.</p><p>The attacker can collect a surrogate datasetˆDdatasetˆ datasetˆD, ideally sampled from the same underlying data distribution as D, and train a surrogate modeî f on such data to approximate the target function f . Then, the attacker can craft the attacks againstˆf againstˆ againstˆf , and then check whether they successfully transfer to the target classifier f . By denoting limited knowledge of a given component with the hat symbol, such black-box attacks can be denoted withˆκwithˆ withˆκ = ( ˆ D,X , ˆ f , ˆ w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Threat Model: Attacker's Capability</head><p>This attack characteristic defines how the attacker can influence the system, and how data can be manipulated based on application-specific constraints. If the attacker can manipulate both training and test data, the attack is said to be causative.</p><p>It is instead referred to as exploratory, if the attacker can only manipulate test data. These scenarios are more commonly known as poisoning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b48">48]</ref> and evasion <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">42]</ref>. Another aspect related to the attacker's capability depends on the presence of application-specific constraints on data manipulation; e.g., to evade malware detection, malicious code has to be modified without compromising its intrusive functionality. This may be done against systems leveraging static code analysis, by injecting instructions that will never be executed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b45">45]</ref>. These constraints can be generally accounted for in the definition of the optimal attack strategy by assuming that the initial attack sample x can only be modified according to a space of possible modifications Φ(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimization Framework for Gradientbased Attacks</head><p>We introduce here a general optimization framework that encompasses both evasion and poisoning attacks. Gradientbased attacks have been considered for evasion (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b42">42]</ref>) and poisoning (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27]</ref>). Our optimization framework not only unifies existing evasion and poisoning attacks, but it also enables the design of new attacks. After defining our general formulation, we instantiate it for evasion and poisoning attacks, and use it to derive a new poisoning availability attack for logistic regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Gradient-based Optimization Algorithm</head><p>Given the attacker's knowledge κ ∈ K and an attack sample</p><p>x ∈ Φ(x) along with its label y, the attacker's goal can be defined in terms of an objective function A(x , y, κ) ∈ R (e.g., a loss function) which measures how effective the attack sample x is. The optimal attack strategy can be thus given as:</p><p>x ∈ arg max</p><formula xml:id="formula_1">x ∈Φ(x)</formula><p>A(x , y, κ) .</p><p>(1)</p><p>Note that, for the sake of clarity, we consider here the optimization of a single attack sample, but this formulation can be easily extended to account for multiple attack points. In</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Gradient-based Evasion and Poisoning Attacks</head><p>Input: x, y: the input sample and its label; A(x,y,κ): the attacker's objective; κ = (D, X , f , w): the attacker's knowledge parameter vector; Φ(x): the feasible set of manipulations that can be made on x; t &gt; 0: a small number. Output: x : the adversarial example.</p><p>1: Initialize the attack sample: x ← x 2: repeat 3:</p><p>Store attack from previous iteration: x ← x 4:</p><p>Update step: x ← Π Φ (x + η∇ x A(x,y,κ)), where the step size η is chosen with line search (bisection method), and Π Φ ensures projection on the feasible domain Φ.</p><formula xml:id="formula_2">5: until |A(x , y, κ) − A(x,y,κ)| ≤ t 6</formula><p>: return x particular, as in the case of poisoning attacks, the attacker can maximize the objective by iteratively optimizing one attack point at a time <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">48]</ref>.</p><p>Attack Algorithm. Algorithm 1 provides a general projected gradient-ascent algorithm that can be used to solve the aforementioned problem for both evasion and poisoning attacks. It iteratively updates the attack sample along the gradient of the objective function, ensuring the resulting point to be within the feasible domain through a projection operator Π Φ . The gradient step size η is determined in each update step using a line-search algorithm based on the bisection method, which solves max η A(x (η), y, κ), with x (η) = Π Φ (x + η∇ x A(x,y,κ)). For the line search, in our experiments we consider a maximum of 20 iterations. This allows us to reduce the overall number of iterations required by Algorithm 1 to reach a local or global optimum. We also set the maximum number of iterations for Algorithm 1 to 1, 000, but convergence (Algorithm 1, line 5) is typically reached only after a hundred iterations. We finally remark that non-differentiable learning algorithms, like decision trees and random forests, can be attacked with more complex strategies <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> or using gradient-based optimization against a differentiable surrogate learner <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b37">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evasion Attacks</head><p>In evasion attacks, the attacker manipulates test samples to have them misclassified, i.e., to evade detection by a learning algorithm. For white-box evasion, the optimization problem given in Eq. (1) can be rewritten as:</p><formula xml:id="formula_3">max x (y, x , w) ,<label>(2)</label></formula><formula xml:id="formula_4">s.t. x − x p ≤ ε , (3) x lb x x ub ,<label>(4)</label></formula><p>where v p is the p norm of v, and we assume that the classifier parameters w are known. For the black-box case, it suffices to use the parametersˆwparametersˆ parametersˆw of the surrogate classifierˆfclassifierˆ classifierˆf . In this work we consider (y, x , w) = −y f (x ), as in <ref type="bibr" target="#b2">[3]</ref>.</p><p>The intuition here is that the attacker maximizes the loss on the adversarial sample with the original class, to cause misclassification to the opposite class. The manipulation constraints Φ(x) are given in terms of: (i) a distance constraint x − x p ≤ ε, which sets a bound on the maximum input perturbation between x (i.e., the input sample) and the corresponding modified adversarial example x ; and (ii) a box constraint x lb x x ub (where u v means that each element of u has to be not greater than the corresponding element in v), which bounds the values of the attack sample x .</p><p>For images, the former constraint is used to implement either dense or sparse evasion attacks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b37">37]</ref>. Normally, the 2 and the ∞ distances between pixel values are used to cause an indistinguishable image blurring effect (by slightly manipulating all pixels). Conversely, the 1 distance corresponds to a sparse attack in which only few pixels are significantly manipulated, yielding a salt-and-pepper noise effect on the image <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">37]</ref>. The box constraint can be used to bound each pixel value between 0 and 255, or to ensure manipulation of only a specific region of the image. For example, if some pixels should not be manipulated, one can set the corresponding values of x lb and x ub equal to those of x.</p><p>Maximum-confidence vs. minimum-distance evasion. Our formulation of evasion attacks aims to produce adversarial examples that are misclassified with maximum confidence by the classifier, within the given space of feasible modifications. This is substantially different from crafting minimumdistance adversarial examples, as formulated in <ref type="bibr" target="#b42">[42]</ref> and in follow-up work (e.g., <ref type="bibr" target="#b33">[33]</ref>). This difference is conceptually depicted in <ref type="figure" target="#fig_18">Fig. 2</ref>. In particular, in terms of transferability, it is now widely acknowledged that higher-confidence attacks have better chances of successfully transfering to the target classifier (and even of bypassing countermeasures based on gradient masking) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>. For this reason, in this work we consider evasion attacks that aim to craft adversarial examples misclassified with maximum confidence.</p><p>Initialization. There is another factor known to improve trans-ferability of evasion attacks, as well as their effectiveness in the white-box setting. It consists of running the attack starting from different initialization points to mitigate the problem of getting stuck in poor local optima <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b50">50]</ref>. In addition to starting the gradient ascent from the initial point x, for nonlinear classifiers we also consider starting the gradient ascent from the projection of a randomly-chosen point of the opposite class onto the feasible domain. This double-initialization strategy helps finding better local optima, through the identification of more promising paths towards evasion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">47,</ref><ref type="bibr" target="#b50">50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Poisoning Availability Attacks</head><p>Poisoning attacks consist of manipulating training data (mainly by injecting adversarial points into the training set) to either favor intrusions without affecting normal system operation, or to purposely compromise normal system operation to cause a denial of service. The former are referred to as poisoning integrity attacks, while the latter are known as poisoning availability attacks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">48]</ref>. Recent work has mostly addressed transferability of poisoning integrity attacks <ref type="bibr" target="#b41">[41]</ref>, including backdoor attacks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16]</ref>. In this work we focus on poisoning availability attacks, as their transferability properties have not yet been widely investigated. Crafting transferable poisoning availability attacks is much more challenging than crafting transferable poisoning integrity attacks, as the latter have a much more modest goal (modifying prediction on a small set of targeted points).</p><p>As for the evasion case, we formulate poisoning in a whitebox setting, given that the extension to black-box attacks is immediate through the use of surrogate learners. Poisoning is formulated as a bilevel optimization problem in which the outer optimization maximizes the attacker's objective A (typically, a loss function L computed on untainted data), while the inner optimization amounts to learning the classifier on the poisoned training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b48">48]</ref>. This can be made explicit by rewriting Eq. (1) as:</p><formula xml:id="formula_5">max x L(D val , w ) = m ∑ j=1 (y j , x j , w ) (5) s.t. w ∈ arg min w L(D tr ∪ (x , y), w)<label>(6)</label></formula><p>where D tr and D val are the training and validation datasets available to the attacker. The former, along with the poisoning point x , is used to train the learner on poisoned data, while the latter is used to evaluate its performance on untainted data, through the loss function L(D val , w ). Notably, the objective function implicitly depends on x through the parameters w of the poisoned classifier. The attacker's capability is limited by assuming that the attacker can inject only a small fraction α of poisoning points into the training set. Thus, the attacker solves an optimization problem involving a set of poisoned data points (αn) added to the training data.</p><p>Poisoning points can be optimized via gradient-ascent procedures, as shown in Algorithm 1. The main challenge is to compute the gradient of the attacker's objective (i.e., the validation loss) with respect to each poisoning point. In fact, this gradient has to capture the implicit dependency of the optimal parameter vector w (learned after training) on the poisoning point being optimized, as the classification function changes while this point is updated. Provided that the attacker function is differentiable w.r.t. w and x, the required gradient can be computed using the chain rule <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b48">48]</ref>:</p><formula xml:id="formula_6">∇ x A = ∇ x L + ∂w ∂x ∇ w L ,<label>(7)</label></formula><p>where the term ∂w ∂x captures the implicit dependency of the parameters w on the poisoning point x. Under some regularity conditions, this derivative can be computed by replacing the inner optimization problem with its stationarity (KarushKuhn-Tucker, KKT) conditions, i.e., with its implicit equation <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b27">27]</ref>. 1 By differentiating this expression w.r.t. the poisoning point x, one yields:</p><formula xml:id="formula_7">∇ w L(D tr ∪ (x , y), w) = 0</formula><formula xml:id="formula_8">∇ x ∇ w L + ∂w ∂x ∇ 2 w L = 0 . (8) Solving for ∂w ∂x , we obtain ∂w ∂x = −(∇ x ∇ w L)(∇ 2 w L) −1 ,</formula><p>which can be substituted in Eq. <ref type="formula" target="#formula_6">(7)</ref> to obtain the required gradient:</p><formula xml:id="formula_9">∇ x A = ∇ x L − (∇ x c ∇ w L)(∇ 2 w L) −1 ∇ w L .<label>(9)</label></formula><p>Gradients for SVM. Poisoning attacks against SVMs were first proposed in <ref type="bibr" target="#b3">[4]</ref>. Here, we report a simplified expression for SVM poisoning, with L corresponding to the dual SVM learning problem, and L to the hinge loss (in the outer optimization):</p><formula xml:id="formula_10">∇ x c A = −α c ∂k kc ∂x c y k + α c ∂k sc ∂x c 0 K ss 1 1 0 −1 K sk 1 y k . (10)</formula><p>We use c, s and k here to respectively index the attack point, the support vectors, and the validation points for which (y, x, w) &gt; 0 (corresponding to a non-null derivative of the hinge loss). The coefficient α c is the dual variable assigned to the poisoning point by the learning algorithm, and k and K contain kernel values between the corresponding indexed sets of points.</p><p>Gradients for Logistic Regression. Logistic regression is a linear classifier that estimates the probability of the positive class using the sigmoid function. A poisoning attack against logistic regression has been derived in <ref type="bibr" target="#b24">[24]</ref>, but maximizing a different outer objective and not directly the validation loss.</p><p>One of our contributions is to compute gradients for logistic regression under our optimization framework. Using logistic loss as the attacker's loss, the poisoning gradient for logistic regression can be computed as:</p><formula xml:id="formula_11">∇ x c A = − ∇ x c ∇ θ L C z c θ ∇ 2 θ L X z C C z X C ∑ n i z i −1 X(y • σ − y) y (σ − 1) C,</formula><p>where θ are the classifier weights (bias excluded), • is the element-wise product, z is equal to σ(1 − σ), σ is the sigmoid of the signed discriminant function (each element of that vector is therefore:</p><formula xml:id="formula_12">σ i = 1 1+exp(−y i f i ) with f i = x i θ + b)</formula><p>, and:</p><formula xml:id="formula_13">∇ 2 θ L = C n ∑ i x i z i x i + I,<label>(11)</label></formula><formula xml:id="formula_14">∇ x c ∇ θ L = C(I • (y c σ c − y c ) + z c θx c )<label>(12)</label></formula><p>In the above equations, I is the identity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Transferability Definition and Metrics</head><p>We discuss here an intriguing connection among transferability of both evasion and poisoning attacks, input gradients and model complexity, and highlight the factors impacting transferability between a surrogate and a target model. Model complexity is a measure of the capacity of a learning algorithm to fit the training data. It is typically penalized to avoid overfitting by reducing either the number of classifier parameters to be learnt or their size (e.g., via regularization) <ref type="bibr" target="#b5">[6]</ref>. Given that complexity is essentially controlled by the hyperparameters of a given learning algorithm (e.g., the number of neurons in the hidden layers of a neural network, or the regularization hyperparameter C of an SVM), only models that are trained using the same learning algorithm should be compared in terms of complexity. As we will see, this is an important point to correctly interpret the results of our analysis. For notational convenience, we denote in the following the attack points as x = x + ˆ δ, where x is the initial point andˆδandˆ andˆδ the adversarial perturbation optimized by the attack algorithm against the surrogate classifier, for both evasion and poisoning attacks. We start by formally defining transferability for evasion attacks, and then discuss how this definition and the corresponding metrics can be generalized to poisoning. Transferability of Evasion Attacks. Given an evasion attack point x , crafted against a surrogate learner (parameterized byˆwbyˆ byˆw), we define its transferability as the loss attained by the target classifier f (parameterized by w) on that point, i.e., T = (y, x + ˆ δ, w). This can be simplified through a linear approximation of the loss function as:</p><formula xml:id="formula_15">T = (y, x + ˆ δ, w) (y, x, w) + ˆ δ ∇ x (y, x, w) .<label>(13)</label></formula><p>This approximation may not only hold for sufficiently-small input perturbations. It may also hold for larger perturbations if the classification function is linear or has a small curvature (e.g., if it is strongly regularized). It is not difficult to see that, for any given point x, y, the evasion problem in Eqs. (2)-(3) (without considering the feature bounds in Eq. 4) can be rewritten as:</p><formula xml:id="formula_16">ˆ δ ∈ arg max δ p ≤ε (y, x + δ, ˆ w) .<label>(14)</label></formula><p>Under the same linear approximation, this corresponds to the maximization of an inner product over an ε-sized ball:</p><formula xml:id="formula_17">max δ p ≤ε δ ∇ x (y, x, ˆ w) = ε∇ x (y, x, ˆ w) q ,<label>(15)</label></formula><p>where q is the dual norm of p .</p><p>The above problem is maximized as follows:</p><formula xml:id="formula_18">1. For p = 2, the maximum isˆδisˆ isˆδ = ε ∇ x (y,x, ˆ w) ∇ x (y,x, ˆ w) 2 ;</formula><p>2. For p = ∞, the maximum isˆδisˆ isˆδ ∈ ε · sign{∇ x (y, x, ˆ w)};</p><p>3. For p = 1, the maximum is achieved by setting the values ofˆδofˆ ofˆδ that correspond to the maximum absolute values of ∇ x (y, x, ˆ w) to their sign, i.e., ±1, and 0 otherwise.</p><p>Substituting the optimal value ofˆδofˆ ofˆδ into Eq. <ref type="formula" target="#formula_15">(13)</ref>, we can compute the loss increment ∆ = ˆ δ ∇ x (y, x, w) under a transfer attack in closed form; e.g., for p = 2, it is given as:</p><formula xml:id="formula_19">∆ = ε ∇ x ˆ ∇ x ˆ 2 ∇ x ≤ ε∇ x 2 ,<label>(16)</label></formula><p>where, for compactness, we usê = (y, x, ˆ w) and = (y, x, w). In this equation, the left-hand side is the increase in the loss function in the black-box case, while the right-hand side corresponds to the white-box case. The upper bound is obtained when the surrogate classifierˆwclassifierˆ classifierˆw is equal to the target w (white-box attacks). Similar results hold for p = 1 and p = ∞ (using the dual norm in the right-hand side).</p><p>Intriguing Connections and Transferability Metrics. The above findings reveal some interesting connections among transferability of attacks, model complexity (controlled by the classifier hyperparameters) and input gradients, as detailed below, and allow us to define simple and computationallyefficient transferability metrics.</p><p>(1) Size of Input Gradients. The first interesting observation is that transferability depends on the size of the gradient of the loss computed using the target classifier, regardless of the surrogate: the larger this gradient is, the larger the attack impact may be. This is inferred from the upper bound in Eq. (16). We define the corresponding metric S(x, y) as:</p><formula xml:id="formula_20">S(x, y) = ∇ x (y, x, w) q ,<label>(17)</label></formula><p>where q is the dual of the perturbation norm. Figure 3: Size of input gradients (averaged on the test set) and test error (in the absence and presence of evasion attacks) against regularization (controlled via weight decay) for a neural network trained on MNIST89 (see Sect. 5.1.1). Note how the size of input gradients and the test error under attack decrease as regularization (complexity) increases (decreases).</p><p>The size of the input gradient also depends on the complexity of the given model, controlled, e.g., by its regularization hyperparameter. Less complex, strongly-regularized classifiers tend to have smaller input gradients, i.e., they learn smoother functions that are more robust to attacks, and vice-versa. Notably, this holds for both evasion and poisoning attacks (e.g., the poisoning gradient in Eq. 10 is proportional to α c , which is larger when the model is weakly regularized). In <ref type="figure">Fig. 3</ref> we report an example showing how increasing regularization (i.e., decreasing complexity) for a neural network trained on MNIST89 (see Sect. 5.1.1), by controlling its weight decay, reduces the average size of its input gradients, improving adversarial robustness to evasion. It is however worth remarking that, since complexity is a model-dependent characteristic, the size of input gradients cannot be directly compared across different learning algorithms; e.g., if a linear SVM exhibits larger input gradients than a neural network, we cannot conclude that the former will be more vulnerable.</p><p>Another interesting observation is that, if a classifier has large input gradients (e.g., due to high-dimensionality of the input space and low level of regularization), for an attack to succeed it may suffice to apply only tiny, imperceptible perturbations. As we will see in the experimental section, this explains why adversarial examples against deep neural networks can often only be slightly perturbed to mislead detection, while when attacking less complex classifiers in low dimensions, modifications become more evident.</p><p>(2) Gradient Alignment. The second relevant impact factor on transferability is based on the alignment of the input gradients of the loss function computed using the target and the surrogate learners. If we compare the increase in the loss function in the black-box case (the left-hand side of Eq. 16) against that corresponding to white-box attacks (the righthand side), we find that the relative increase in loss, at least for 2 perturbations, is given by the following value: Interestingly, this is exactly the cosine of the angle between the gradient of the loss of the surrogate and that of the target classifier. This is a novel finding which explains why the cosine angle metric between the target and surrogate gradients can well characterize the transferability of attacks, confirming empirical results from previous work <ref type="bibr" target="#b21">[21]</ref>. For other kinds of perturbation, this definition slightly changes, but gradient alignment can be similarly evaluated. Differently from the gradient size S, gradient alignment is a pairwise metric, allowing comparisons across different surrogate models; e.g., if a surrogate SVM is better aligned with the target model than another surrogate, we can expect that attacks targeting the surrogate SVM will transfer better.</p><formula xml:id="formula_21">R(x, y) = ∇ x ˆ ∇ x ∇ x ˆ 2 ∇ x 2 . (18) x 񮽙(y, x, ˆ w) V (x, y)</formula><p>(3) Variability of the Loss Landscape. We define here another useful metric to characterize attack transferability. The idea is to measure the variability of the loss functionˆwhenfunctionˆ functionˆwhen the training set used to learn the surrogate model changes, even though it is sampled from the same underlying distribution. The reason is that the lossîslossˆlossîs exactly the objective function A optimized by the attacker to craft evasion attacks (Eq. 1). Accordingly, if this loss landscape changes dramatically even when simply resampling the surrogate training set (which may happen, e.g., for surrogate models exhibiting a large error variance, like neural networks and decision trees), it is very likely that the local optima of the corresponding optimization problem will change, and this may in turn imply that the attacks will not transfer correctly to the target learner.</p><p>We define the variability of the loss landscape simply as the variance of the loss, estimated at a given attack point x, y:</p><formula xml:id="formula_22">V (x, y) = E D {(y, x, ˆ w) 2 } − E D {(y, x, ˆ w)} 2 ,<label>(19)</label></formula><p>where E D is the expectation taken with respect to different (surrogate) training sets. This is very similar to what is typically done to estimate the variance of classifiers' predictions. This notion is clarified also in <ref type="figure" target="#fig_3">Fig. 4</ref>. As for the size of input gradients S, also the loss variance V should only be compared across models trained with the same learning algorithm.</p><p>The transferability metrics S, R and V defined so far depend on the initial attack point x and its label y. In our experiments, we will compute their mean values by averaging on different initial attack points. Transferability of Poisoning Attacks. For poisoning attacks, we can essentially follow the same derivation discussed before. Instead of defining transferability in terms of the loss attained on the modified test point, we define it in terms of the validation loss attained by the target classifier under the influence of the poisoning points. This loss function can be linearized as done in the previous case, yielding:</p><formula xml:id="formula_23">T L(D, w) + ˆ δ ∇ x L(D, w)</formula><p>, where D are the untainted validation points, andˆδandˆ andˆδ is the perturbation applied to the initial poisoning point x against the surrogate classifier. Recall that L depends on the poisoning point through the classifier parameters w, and that the gradient ∇ x L(D, w) here is equivalent to the generic one reported in Eq. <ref type="formula" target="#formula_9">(9)</ref>. It is then clear that the perturbationˆδperturbationˆ perturbationˆδ maximizes the (linearized) loss when it is best aligned with its derivative ∇ x L(D, w), according to the constraint used, as in the previous case. The three transferability metrics defined before can also be used for poisoning attacks provided that we simply replace the evasion loss (y, x, w) with the validation loss L(D, w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Analysis</head><p>In this section, we evaluate the transferability of both evasion and poisoning attacks across a range of ML models. We highlight some interesting findings about transferability, based on the three metrics developed in Sect. <ref type="bibr" target="#b3">4</ref>. In particular, we analyze attack transferability in terms of its connection to the size of the input gradients of the loss function, the gradient alignment between surrogate and target classifiers, and the variability of the loss function optimized to craft the attack points. We provide recommendations on how to choose the most effective surrogate models to craft transferable attacks in the black-box setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Transferability of Evasion Attacks</head><p>We start by reporting our experiments on evasion attacks. We consider here two distinct case studies, involving handwritten digit recognition and Android malware detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Handwritten Digit Recognition</head><p>The MNIST89 data includes the MNIST handwritten digits from classes 8 and 9. Each digit image consists of 784 pixels ranging from 0 to 255, normalized in <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref> by dividing such values by 255. We run 10 independent repetitions to average the results on different training-test splits. In each repetition, we run white-box and black-box attacks, using 5, 900 samples to train the target classifier, 5, 900 distinct samples to train the surrogate classifier (without even relabeling the surrogate data with labels predicted by the target classifier; i.e., we do not perform any query on the target), and 1, 000 test samples. We modified test digits in both classes using Algorithm 1 under the 2 distance constraint x − x 2 ≤ ε, with ε ∈ <ref type="bibr">[0,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>For each of the following learning algorithms, we train a high-complexity (H) and a low-complexity (L) model, by changing its hyperparameters: (i) SVMs with linear kernel (SVM H with C = 100 and SVM L with C = 0.01); (ii) SVMs with RBF kernel (SVM-RBF H with C = 100 and SVM-RBF L with C = 1, both with γ = 0.01); (iii) logistic classifiers (logistic H with C = 10 and logistic L with C = 1); (iv) ridge classifiers (ridge H with α = 1 and ridge L with α = 10); 2 (v) fully-connected neural networks with two hidden layers including 50 neurons each, and ReLU activations (NN H with no regularization, i.e., weight decay set to 0, and NN L with weight decay set to 0.01), trained via cross-entropy loss minimization; and (vi) random forests consisting of 30 trees (RF H with no limit on the depth of the trees and RF L with a maximum depth of 8). These configurations are chosen to evaluate the robustness of classifiers that exhibit similar test accuracies but different levels of complexity.</p><p>How does model complexity impact evasion attack success in the white-box setting? The results for white-box evasion attacks are reported for all classifiers that fall under our framework and can be tested for evasion with gradient-based attacks (SVM, Logistic, Ridge, and NN). This excludes random forests, as they are not differentiable. We report the complete security evaluation curves <ref type="bibr" target="#b4">[5]</ref> in <ref type="figure" target="#fig_4">Fig. 5</ref>, showing the mean test error (over 10 runs) against an increasing maximum admissible distortion ε. In <ref type="figure" target="#fig_5">Fig. 6a</ref> we report the mean test error at ε = 1 for each target model against the size of its input gradients (S, averaged on the test samples and on the 10 runs).</p><p>The results show that, for each learning algorithm, the lowcomplexity model has smaller input gradients, and it is less vulnerable to evasion than its high-complexity counterpart, confirming our theoretical analysis. This is also confirmed by the p-values reported in <ref type="table">Table 1</ref> (first column), obtained by  <ref type="figure" target="#fig_8">Fig. 8, left)</ref> for each target-surrogate pair. Pearson (P) and Kendall (K) correlations between ρ and R are also reported along with the p-values obtained from a permutation test to assess statistical significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evasion</head><p>Poisoning</p><formula xml:id="formula_24">MNIST89 DREBIN MNIST89 LFW ε = 1 ε = 1 ε = 5 ε = 30 5% 20% 5% 20%</formula><p>SVM &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 0.75 logistic &lt;1e-2 &lt;1e-2 &lt;1e-2 0.02 &lt;1e-2 &lt;1e-2 0.10 0.21 ridge &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 0.02 &lt;1e-2 0.02 0.75 SVM-RBF &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 &lt;1e-2 0.11 NN &lt;1e-2 &lt;1e-2 &lt;1e-2 0.02 <ref type="table">Table 1</ref>: Statistical significance of our results. For each attack, dataset and learning algorithm, we report the p-values of two two-sided binomial tests, to respectively reject the null hypothesis that: (i) for white-box attacks, the test errors of the high-and low-complexity target follow the same distribution; and (ii) for black-box attacks, the transfer rates of the highand low-complexity surrogate follow the same distribution. Each test is based on 10 samples, obtained by comparing the error of the high-and low-complexity models for each learning algorithm in each repetition. In the first (second) case, success corresponds to a larger test (transfer) error for the high-complexity target (low-complexity surrogate).</p><p>running a binomial test for each learning algorithm to compare the white-box test error of the corresponding high-and low-complexity models. All the p-values are smaller than 0.05, which confirms 95% statistical significance. Recall that these results hold only when comparing models trained using the same learning algorithm. This means that we can compare, e.g., the S value of SVM H against SVM L , but not that of SVM H against logistic H . In fact, even though logistic H exhibits the largest S value, it is not the most vulnerable classifier. Another interesting finding is that nonlinear classifiers tend to be less vulnerable than linear ones.</p><p>How do evasion attacks transfer between models in blackbox settings? In <ref type="figure" target="#fig_6">Fig. 7</ref> we report the results for black-box evasion attacks, in which the attacks against surrogate models (in rows) are transferred to the target models (in columns).</p><p>The top row shows results for surrogates trained using only 20% of the surrogate training data, while in the bottom row surrogates are trained using all surrogate data, i.e., a training set of the same size as that of the target. The three columns report results for ε ∈ {1, 2, 5}. It can be noted that lower-complexity models (with stronger regularization) provide better surrogate models, on average. In particular, this can be seen best in the middle column for medium level of perturbation, in which the lower-complexity models (SVM L , logistic L , ridge L , and SVM-RBF L ) provide on average higher error when transferred to other models. The reason is that they learn smoother and stabler functions, that are capable of better approximating the target function. Surprisingly, this holds also when using only 20% of training data, as the black-box attacks relying on such low-complexity models still transfer with similar test errors. This means that most classifiers can be attacked in this black-box setting with almost no knowledge of the model, no query access, but provided that one can get a small amount of data similar to that used to train the target model. These findings are also confirmed by looking at the variability of the loss landscape, computed as discussed in Sect. 4 (by considering 10 different training sets), and reported against the average transfer rate of each surrogate model in <ref type="figure" target="#fig_5">Fig. 6b</ref>. It is clear from that plot that higher-variance classifiers are less effective as surrogates than their less-complex counterparts, as the former tend to provide worse, unstable approximations of the target classifier. To confirm the statistical significance of this result, for each learning algorithm we also compare the mean transfer errors of high-and low-complexity surrogates with a binomial test whose p-values (always lower than 0.05) are reported in <ref type="table">Table 1 (second column)</ref>.</p><p>Another interesting, related observation is that the adversarial examples computed against lower-complexity surrogates have to be perturbed more to evade (see <ref type="figure" target="#fig_9">Fig. 9</ref>), whereas the perturbation of the ones computed against complex models .19 .57</p><formula xml:id="formula_25">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F</formula><formula xml:id="formula_26">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F</formula><p>.64</p><p>.55</p><p>.58 .94 (a) ε = 1 .51</p><formula xml:id="formula_27">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F</formula><formula xml:id="formula_28">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F</formula><formula xml:id="formula_29">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F</formula><p>.21</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>43</head><p>.57</p><p>.68</p><p>.56</p><p>.66  can be smaller. This is again due to the instability induced by high-complexity models into the loss function optimized to craft evasion attacks, whose sudden changes cause the presence of closer local optima to the initial attack point. On the vulnerability of random forests. A noteworthy finding is that random forests can be effectively attacked at small perturbation levels using most other models (see last two columns in <ref type="figure" target="#fig_6">Fig. 7)</ref>. We looked at the learned trees and discovered that trees often are susceptible to small changes. In one example, a node of the tree checked if a particular feature value was above 0.002, and classified samples as digit 8 if that condition holds (and digit 9 otherwise). The attack modified that feature from 0 to 0.028, causing it to be immediately misclassified. This vulnerability is intrinsic in the selection process of the threshold values used by these decision trees to split each node. The threshold values are selected among the existing values in the dataset (to correctly handle categorical attributes). Therefore, for pixels which are highly discriminant (e.g., mostly black for one class and white for the other), the threshold will be either very close to one extreme or the other, making it easy to subvert the prediction by a small change. Since 2 -norm attacks change almost all feature values, with high probability the attack modifies at least one feature on every path of the tree, causing misclassification.</p><formula xml:id="formula_30">(b) ε = 2 S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F</formula><p>Is gradient alignment an effective transferability metric? In <ref type="figure" target="#fig_8">Fig. 8</ref>, we report on the left the gradient alignment computed between surrogate and target models, and on the right the Pearson correlation coefficient ρ( ˆ δ, δ) between the perturbation optimized against the surrogate (i.e., the black-box perturbationˆδperturbationˆ perturbationˆδ) and that optimized against the target (i.e., the white-box perturbation δ). We observe immediately that gradient alignment provides an accurate measure of transferability: the higher the cosine similarity, the higher the correlation (meaning that the adversarial examples crafted against the two models are similar). We correlate these two measures in <ref type="figure" target="#fig_5">Fig. 6c</ref>, and show that such correlation is statistically significant for both Pearson and Kendall coefficients. In <ref type="figure" target="#fig_5">Fig. 6d</ref> we also correlate gradient alignment with the ratio between the test error of the target model in the black-and white-box setting (extrapolated from the matrix corresponding to ε = 1 in the bottom row of <ref type="figure" target="#fig_6">Fig. 7)</ref>, as suggested by our theoretical derivation. The corresponding permutation tests confirm statistical significance. We finally remark that gradient alignment is extremely fast to evaluate, as it does not require simulating any attack, but it is only a relative measure of the attack trans-   ferability, as the latter also depends on the complexity of the target model; i.e., on the size of its input gradients. </p><formula xml:id="formula_31">S V M H S V M L lo g is t ic H lo g is t ic L ri d g e H ri d g e L S V M -R B F H S V M -R B F L N N H N N L</formula><formula xml:id="formula_32">S V M H S V M L lo g is t ic H lo g is t ic L ri d g e H ri d g e L S V M -R B F H S V M -R B F L N N H N N L SVMH SVML</formula><formula xml:id="formula_33">SVM L SVM H SVM-RBF L SVM-RBF H ε = 1.7 ε = 0.45 ε = 1.1 ε = 0.85 ε = 2.35 ε = 0.95 ε = 2.9 ε = 2.65</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Android Malware Detection</head><p>The Drebin data <ref type="bibr" target="#b0">[1]</ref> consists of around 120,000 legitimate and around 5000 malicious Android applications, labeled using the VirusTotal service. A sample is labeled as malicious (or positive, y = +1) if it is classified as such from at least five out of ten anti-virus scanners, while it is flagged as legitimate (or negative, y = −1) otherwise. The structure and the source code of each application is encoded as a sparse feature vector consisting of around a million binary features denoting the presence or absence of permissions, suspicious URLs and other relevant information that can be extracted by statically analyzing Android applications. Since we are working with sparse binary features, we use the 1 norm for the attack. We use 30, 000 samples to learn surrogate and target classifiers, and the remaining 66, 944 samples for testing. The classifiers and their hyperparameters are the same used for MNIST89, apart from (i) the number of hidden neurons for NN H and NN L , set to 200, (ii) the weight decay of NN L , set to 0.005; and (iii) the maximum depth of RF L , set to 59.</p><p>We perform feature selection to retain those 5, 000 features which maximize information gain, i.e., |p(x k = 1|y = +1) − p(x k = 1|y = −1)|, where x k is the k th feature. While this feature selection process does not significantly affect the detection rate (which is only reduced by 2%, on average, at 0.5% false alarm rate), it drastically reduces the computational complexity of classification.</p><p>In each experiment, we run white-box and black-box evasion attacks on 1, 000 distinct malware samples (randomly selected from the test data) against an increasing number of modified features in each malware ε ∈ {0, 1, 2, . . . , 30}. This is achieved by imposing the 1 constraint x − x 1 ≤ ε. As in previous work, we further restrict the attacker to only inject features into each malware sample, to avoid compromising its intrusive functionality <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>To evaluate the impact of the aforementioned evasion attack, we measure the evasion rate (i.e., the fraction of malware samples misclassified as legitimate) at 0.5% false alarm rate (i.e., when only 0.5% of the legitimate samples are misclassified as malware). As in the previous experiment, we report the complete security evaluation curve for the white-box attack case, whereas we report only the value of test error for the black-box case. The results, reported in <ref type="figure" target="#fig_0">Figs. 10, 11, 12</ref>, and 13, along with the statistical tests in <ref type="table">Table 1</ref> (third and fourth columns) confirm the main findings of the previous experiments. One significant difference is that random forests are much more robust in this case. The reason is that the 1 -norm attack (differently from the 2 ) only changes a small number of features, and thus the probability that it will change features in all the ensemble trees is very low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Transferability of Poisoning Attacks</head><p>For poisoning attacks, we report experiments on handwritten digits and face recognition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Handwritten Digit Recognition</head><p>We apply our optimization framework to poison SVM, logistic, and ridge classifiers in the white-box setting. Designing efficient poisoning availability attacks against neural networks is still an open problem due to the complexity of the bilevel optimization and the non-convexity of the inner learning problem. Previous work has mainly considered integrity poisoning attacks against neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">41]</ref>, and it is believed that neural networks are much more resilient to poisoning availability attacks due to their memorization capability. Poisoning random forests is not feasible with gradient-based attacks, and we are not aware of any existing attacks for this ensemble method. We thus consider as surrogate learners: (i) linear SVMs with C = 0.01 (SVM L ) and C = 100 (SVM H ); (ii) logistic classifiers with C = 0.01 (logistic L ) and C = 10 (logistic H ); (iii) ridge classifiers with α = 100 (ridge L ) and α = 10 (ridge H ); and (iv) SVMs with RBF kernel with γ = 0.01 and C = 1 (SVM-RBF L ) and C = 100 (SVM-RBF H ). We additionally consider as target classifiers: (i) random forests with 100 base trees, each with a maximum depth of 6 for RF L , and with no limit on the maximum depth for RF H ; (ii) feed-forward neural networks with two hidden layers of 200 neurons each and ReLU activations, trained via cross-entropy loss minimization with different regularization (NN L with weight decay 0.01 and NN H with no decay); and (iii) the Convolutional Neural Network (CNN) used in <ref type="bibr" target="#b6">[7]</ref>.</p><p>We consider 500 training samples, 1, 000 validation samples to compute the attack, and a separate set of 1, 000 test samples to evaluate the error. The test error is computed against an increasing number of poisoning points into the training set, from 0% to 20% (corresponding to 125 poisoning points). The reported results are averaged on 10 independent, randomly-drawn data splits. How does model complexity impact poisoning attack success in the white-box setting? The results for white-box poisoning are reported in <ref type="figure" target="#fig_0">Fig. 14</ref>. Similarly to the evasion case, high-complexity models (with larger input gradients, as shown in <ref type="figure" target="#fig_0">Fig. 15a</ref>) are more vulnerable to poisoning attacks than their low-complexity counterparts (i.e., given that the same learning algorithm is used). This is also confirmed by the statistical tests in the fifth column of <ref type="table">Table 1</ref>. Therefore, model complexity plays a large role in a model's robustness also against poisoning attacks, confirming our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How do poisoning attacks transfer between models in black-box settings?</head><p>The results for black-box poisoning are reported in <ref type="figure" target="#fig_0">Fig. 16</ref>. For poisoning attacks, the best surrogates are those matching the complexity of the target, as they tend to be better aligned and to share similar local optima, except for low-complexity logistic and ridge surrogates, which seem to transfer better to linear classifiers. This is also witnessed by gradient alignment in <ref type="figure" target="#fig_0">Fig. 17</ref>, which is again not only correlated to the similarity between black-and white-box perturbations <ref type="figure" target="#fig_0">(Fig. 15c)</ref>, but also to the ratio between the blackand white-box test errors <ref type="figure" target="#fig_0">(Fig. 15d)</ref>. Interestingly, these error ratios are larger than one in some cases, meaning that attacking a surrogate model can be more effective than running a white-box attack against the target. A similar phenomenon has been observed for evasion attacks <ref type="bibr" target="#b33">[33]</ref>, and it is due to the fact that optimizing attacks against a smoother surrogate may find better local optima of the target function (e.g., by overcoming gradient obfuscation <ref type="bibr" target="#b1">[2]</ref>). According to our findings, for poisoning attacks, reducing the variability of the loss landscape (V) of the surrogate model is less important than finding a good alignment between the surrogate and the target. In fact, from <ref type="figure" target="#fig_0">Fig. 15b</ref> it is evident that increasing V is even beneficial for SVM-based surrogates (and all these results are statistically significant according to the p-values in the sixth column of <ref type="table">Table 1</ref>). A visual inspection of the poisoning digits in <ref type="figure" target="#fig_0">Fig. 18</ref> reveals that the poisoning points crafted against highcomplexity classifiers are only minimally perturbed, while the ones computed against low-complexity classifiers exhibit larger, visible perturbations. This is again due to the presence of closer local optima in the former case. Finally, a surprising result is that RFs are quite robust to poisoning, as well as NNs when attacked with low-complexity linear surrogates. The reason may be that these target classifiers have a large capacity, and can thus fit outlying samples (like the digits crafted against low-complexity classifiers in <ref type="figure" target="#fig_0">Fig. 18</ref>) without affecting the classification of the other training samples. .18  .54</p><formula xml:id="formula_34">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML</formula><formula xml:id="formula_35">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML</formula><formula xml:id="formula_36">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML</formula><p>.60</p><p>.57</p><p>.59 t r a n s fe r r a t e .14</p><formula xml:id="formula_37">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML</formula><p>.20</p><p>.20</p><p>.23</p><p>.15</p><p>.19</p><p>.15</p><p>.19</p><p>.21</p><p>.21 (b) ε = 10 t r a n s fe r r a t e .43</p><formula xml:id="formula_38">(a) ε = 5 S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML</formula><formula xml:id="formula_39">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L N N H N N L R F H R F L SVMH SVML</formula><p>.62</p><p>.53</p><p>.64</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>25</head><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>44</head><p>.51</p><p>.62</p><p>.58</p><p>.62</p><p>(c) ε = 30</p><p>Figure 12: Black-box (transfer) evasion attacks on DREBIN. See the caption of <ref type="figure" target="#fig_6">Fig. 7</ref> for further details.   </p><formula xml:id="formula_40">S V M H S V M L lo g is t ic H lo g is t ic L ri d g e H ri d g e L S V M -R B F H S V M -R B F L N N H N N L</formula><formula xml:id="formula_41">S V M H S V M L lo g is t ic H lo g is t ic L ri d g e H ri d g e L S V M -R B F H S V M -R B F L N N H N N L SVMH SVML</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Face Recognition</head><p>The Labeled Face on the Wild (LFW) dataset consists of faces of famous peoples collected on Internet. We considered the six identities with the largest number of images in the dataset.  <ref type="table">Table 1</ref> (seventh and eighth columns). In this case, there is not a significant distinction between the mean transfer rates of high-and low-complexity surrogates, probably due to the reduced size of the training sets used. Finally, in <ref type="figure" target="#fig_18">Fig. 23</ref> we report examples of perturbed faces against surrogates with different complexities, confirming again that larger perturbations are required to attack lower-complexity models.  (a) 5% poisoning t r a n s fe r r a t e .08</p><formula xml:id="formula_42">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N</formula><formula xml:id="formula_43">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N</formula><p>.06</p><p>.10</p><p>.11</p><p>.11</p><p>.12</p><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10</head><p>.07</p><p>(b) 10% poisoning    </p><formula xml:id="formula_44">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N</formula><formula xml:id="formula_45">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summary of Transferability Evaluation</head><p>We summarize the results of transferability for evasion and poisoning attacks below.</p><p>(1) Size of input gradients. Low-complexity target classifiers are less vulnerable to evasion and poisoning attacks than highcomplexity target classifiers trained with the same learning algorithm, due to the reduced size of their input gradients. In general, nonlinear models are more robust than linear models to both types of attacks. with transferability. Even though it cannot be directly measured in black-box scenarios, some useful guidelines can be derived from our analysis. For evasion attacks, lowcomplexity surrogate classifiers provide stabler gradients which are better aligned, on average, with those of the target models; thus, it is generally preferable to use stronglyregularized surrogates. For poisoning attacks, instead, gradient alignment tends to improve when the surrogate matches the complexity (regularization) of the target (which may be estimated using techniques from <ref type="bibr" target="#b46">[46]</ref>). gate classifiers provide loss landscapes with lower variability than high-complexity surrogate classifiers trained with the same learning algorithm, especially for evasion attacks. This results in better transferability.</p><p>To summarize, for evasion attacks, decreasing complexity of the surrogate model by properly adjusting the hyperparameters of its learning algorithm provides adversarial examples that transfer better to a range of models. For poisoning attacks, the best surrogates are generally models with similar levels of regularization as the target model. The reason is that the poisoning objective function is relatively stable (i.e., it has low variance) for most classifiers, and gradient alignment between surrogate and target becomes a more important factor.</p><p>Understanding attack transferability has two main implications. First, even when attackers do not know the target classifier, our findings suggest that low-complexity surrogates have a better chance of transferring to other models. Our recommendation to performing black-box evasion attacks is to choose surrogates with low complexity (e.g., by using strong regularization and reducing model variance). To perform poisoning attacks, our recommendation is to acquire additional information about the level of regularization of the target and train a surrogate model with a similar level of regularization. Second, our analysis also provides recommendations to defenders on how to design more robust models against evasion and poisoning attacks. In particular, lower-complexity models tend to have more resilience compared to more complex models. Of course, we need to take into account the bias-variance trade-off and choose models that still perform relatively well on the original prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Transferability for evasion attacks. Transferability of evasion attacks has been studied in previous work, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref>. Biggio et al. <ref type="bibr" target="#b2">[3]</ref> have been the first to consider evasion attacks against surrogate models in a limited-knowledge scenario. <ref type="bibr">Goodfellow et al. [14]</ref>, Tramer et al. <ref type="bibr" target="#b43">[43]</ref>, and Moosavi et al. <ref type="bibr" target="#b26">[26]</ref> have made the observation that different models might learn intersecting decision boundaries in both benign and adversarial dimensions and in that case adversarial examples transfer better. Tramer et al. have also performed a detailed study of transferability of modelagnostic perturbations that depend only on the training data, noting that adversarial examples crafted against linear models can transfer to higher-order models. We answer some of the open questions they posed about factors contributing to attack transferability. <ref type="bibr">Liu et al. [21]</ref> have empirically observed the gradient alignment between models with transferable adversarial examples. Papernot et al. <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b33">33]</ref> have observed that adversarial examples transfer across a range of models, including logistic regression, SVMs and neural networks, without providing a clear explanation of the phenomenon. Prior work has also investigated the role of input gradients and Jacobians. Some authors have proposed to decrease the magnitude of input gradients during training to defend against evasion attacks <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b35">35]</ref> or improve classification accuracy <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b44">44]</ref>. In <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b39">39]</ref>, the magnitude of input gradients has been identified as a cause for vulnerability to evasion attacks. A number of papers have shown that transferability of adversarial examples is increased by averaging the gradients computed for ensembles of models <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b47">47]</ref>. We highlight that we obtain similar effect by attacking a strongly-regularized surrogate model with a smoother and stabler decision boundary (resulting in a lower-variance model). The advantage of our approach is to reduce the computational complexity compared to attacking classifier ensembles. Through our formalization, we shed light on the most important factors for transferability. In particular, we identify a set of conditions that explain transferability including the gradient alignment between the surrogate and targeted models, and the size of the input gradients of the target model, connected to model complexity. We demonstrate that adversarial examples crafted against lowervariance models (e.g., those that are strongly regularized) tend to transfer better across a range of models.</p><p>Transferability for poisoning attacks. There is very little work on the transferability of poisoning availability attacks, except for a preliminary investigation in <ref type="bibr" target="#b27">[27]</ref>. That work indicates that poisoning examples are transferable among very simple network architectures (logistic regression, MLP, and Adaline). Transferability of targeted poisoning attacks has been addressed recently in <ref type="bibr" target="#b41">[41]</ref>. We are the first to study in depth transferability of poisoning availability attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>We have conducted an analysis of the transferability of evasion and poisoning attacks under a unified optimization framework. Our theoretical transferability formalization sheds light on various factors impacting the transfer success rates. In particular, we have defined three metrics that impact the transferability of an attack, including the complexity of the target model, the gradient alignment between the surrogate and  (a) 5% poisoning (b) 10% poisoning (c) 20% poisoning   target models, and the variance of the attacker optimization objective. The lesson to system designers is to evaluate their classifiers against these criteria and select lower-complexity, stronger regularized models that tend to provide higher robustness to both evasion and poisoning. Interesting avenues for future work include extending our analysis to multi-class classification settings, and considering a range of gray-box models in which attackers might have additional knowledge of the machine learning system (as in <ref type="bibr" target="#b41">[41]</ref>). Applicationdependent scenarios such as cyber security might provide additional constraints on threat models and attack scenarios and could impact transferability in interesting ways. or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes not withstanding any copyright notation here on. We would also like to thank Toyota ITC for funding this research.</p><formula xml:id="formula_46">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N</formula><formula xml:id="formula_47">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N</formula><formula xml:id="formula_48">S V M H S V M L lo g is t ic H lo g is t ic L r id g e H r id g e L S V M -R B F H S V M -R B F L R F H R F L N N H N N L C N</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Conceptual representation of transferability. We show the loss function of the attack objective as a function of a single feature x. The top row includes 2 surrogate models (high and low complexity), while the bottom row includes both models as targets. The adversarial samples are represented as red dots for the high-complexity surrogate and as blue dots for the low-complexity surrogate. If the adversarial sample loss is below a certain threshold (i.e., the black horizontal line), the point is correctly classified, otherwise it is misclassified. The adversarial point computed against the high-complexity model (top left) lays in a local optimum due to the irregularity of the objective. This point is not effective even against the same classifier trained on a different dataset (bottom left) due to the variance of the high-complexity classifier. The adversarial point computed against the low complexity model (top right), instead, succeeds against both low and high-complexity targets (left and right bottom, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Conceptual representation of maximum-confidence evasion attacks (within an 2 ball of radius ε) vs. minimumdistance adversarial examples. Maximum-confidence attacks tend to transfer better as they are misclassified with higher confidence (though requiring more modifications).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Conceptual representation of the variability of the loss landscape. The green line represents the expected loss with respect to different training sets used to learn the surrogate model, while the gray area represents the variance of the loss landscape. If the variance is too large, local optima may change, and the attack may not successfully transfer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: White-box evasion attacks on MNIST89. Test error against increasing maximum perturbation ε.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evaluation of our metrics for evasion attacks on MNIST89. (a) Test error under attack vs average size of input gradients (S) for low-(denoted with '×') and high-complexity (denoted with '•') classifiers. (b) Average transfer rate vs variability of loss landscape (V). (c) Pearson correlation coefficient ρ( ˆ δ, δ) between black-box ( ˆ δ) and white-box (δ) perturbations (values in Fig. 8, right) vs gradient alignment (R, values in Fig. 8, left) for each target-surrogate pair. Pearson (P) and Kendall (K) correlations between ρ and R are also reported along with the p-values obtained from a permutation test to assess statistical significance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Black-box (transfer) evasion attacks on MNIST89. Each cell contains the test error of the target classifier (in columns) computed on the attack samples crafted against the surrogate (in rows). Matrices in the top (bottom) row correspond to attacks crafted against surrogate models trained with 20% (100%) of the surrogate training data, for ε ∈ {1, 2, 5}. The test error of each target classifier in the absence of attack (target error) and under (white-box) attack are also reported for comparison, along with the mean transfer rate of each surrogate across targets. Darker colors mean higher test error, i.e., better transferability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Gradient alignment and perturbation correlation for evasion attacks on MNIST89. Left: Gradient alignment R (Eq. 18) between surrogate (rows) and target (columns) classifiers, averaged on the unmodified test samples. Right: Pearson correlation coefficient ρ(δ, ˆ δ) between white-box and black-box perturbations for ε = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Digit images crafted to evade linear and RBF SVMs. The values of ε reported here correspond to the minimum perturbation required to evade detection. Larger perturbations are required to mislead low-complexity classifiers (L), while smaller ones suffice to evade high-complexity classifiers (H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: White-box evasion attacks on DREBIN. Evasion rate against increasing maximum perturbation ε.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 11: Evaluation of our metrics for evasion attacks on DREBIN. See the caption of Fig. 6 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>0</head><label></label><figDesc>.27 0.44 0.40 0.56 0.08 0.20 0.50 0.44 0.44 0.44 0.29 0.78 0.37 0.68 0.04 0.16 0.41 0.82 0.50 0.78 0.27 0.57 0.38 0.47 0.05 0.14 0.39 0.56 0.52 0.48 0.26 0.76 0.34 0.63 0.03 0.11 0.33 0.73 0.46 0.80</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Gradient alignment and perturbation correlation (at ε = 30) for evasion attacks on DREBIN. See the caption of Fig. 8 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: White-box poisoning attacks on MNIST89. Test error against an increasing fraction of poisoning points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Evaluation of our metrics for poisoning attacks on MNIST89. See the caption of Fig. 6 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Black-box (transfer) poisoning attacks on MNIST89. See the caption of Fig. 7 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Gradient alignment and perturbation correlation (at 20% poisoning) for poisoning attacks on MNIST89. See the caption of Fig. 8 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>( 2 )</head><label>2</label><figDesc>Figure 18: Poisoning digits crafted against linear and RBF SVMs. Larger perturbations are required to have significant impact on low-complexity classifiers (L), while minimal changes are very effective on high-complexity SVMs (H).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>( 3 )Figure 19 :</head><label>319</label><figDesc>Figure 19: White-box poisoning attacks on LFW. Test error against an increasing fraction of poisoning points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Evaluation of our metrics for poisoning attacks on LFW. See the caption of Fig. 6 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: Black-box (transfer) poisoning attacks on LFW. See the caption of Fig. 7 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Gradient alignment and perturbation correlation (at 20% poisoning) for poisoning attacks on LFW. See the caption of Fig. 8 for further details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 23 :</head><label>23</label><figDesc>Figure 23: Adversarial examples crafted against linear and RBF SVMs. Larger perturbations are required to have significant impact on low-complexity classifiers (L), while minimal changes are very effective on high-complexity SVMs (H).</figDesc></figure>

			<note place="foot" n="1"> More rigorously, we should write the KKT conditions in this case as ∇ w L(D tr ∪ (x , y), w) ∈ 0, as the solution may not be unique.</note>

			<note place="foot" n="2"> Recall that the level of regularization increases as α increases, and as C decreases.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Neil Gong for shepherding our paper and the anonymous reviewers for their constructive feedback. This work was partly supported by the EU H2020 project ALOHA, under the European Union's Horizon 2020 research and innovation programme (grant no.780788). This research was also sponsored by the Combat Capabilities Development Command Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-13-2-0045 (ARL Cyber Security CRA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Combat Capabilities Development Command Army Research Laboratory</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Drebin: Efficient and explainable detection of android malware in your pocket</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Spreitzenbarth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st NDSS. The Internet Society</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Šrndi´cšrndi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD, Part III</title>
		<editor>H. Blockeel et al.</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">8190</biblScope>
			<biblScope unit="page" from="387" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poisoning attacks against support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th Int&apos;l Conf. on Machine Learning</title>
		<editor>J. Langford and J. Pineau</editor>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1807" to="1814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Stats</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial examples are not easily detected: Bypassing ten detection methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17</title>
		<editor>B. M. Thuraisingham et al.</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. on Sec. and Privacy</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno>abs/1712.05526</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evading classifiers by morphing in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-C</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th ACM SIGSAC Conf. on Computer and Comm. Sec., CCS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Yes, machine learning can be more secure! a case study on android malware detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Dependable and Secure Computing</title>
		<imprint/>
	</monogr>
	<note>In press</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On security and sparsity of linear classifiers for adversarial settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint IAPR Int&apos;l Workshop on Structural, Syntactic, and Statistical Patt. Rec</title>
		<editor>A. Robles-Kelly et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">10029</biblScope>
			<biblScope unit="page" from="322" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boosting adversarial examples with momentum</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial examples for malware detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESORICS (2)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">10493</biblScope>
			<biblScope unit="page" from="62" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Badnets: Identifying vulnerabilities in the machine learning model supply chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<idno>abs/1708.06733</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Machine Learning and Computer Security</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Black-box adversarial attacks with limited queries and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th ICML</title>
		<editor>J. Dy and A. Krause</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2137" to="2146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Manipulating machine learning: Poisoning attacks and countermeasures for regression learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nitarotaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. S&amp;P</title>
		<imprint>
			<publisher>IEEE CS</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="931" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evasion and hardening of tree ensemble classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kantchelian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd ICML</title>
		<imprint>
			<publisher>JMLR W&amp;CP</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2387" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the</title>
		<meeting>of the</meeting>
		<imprint>
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Int&amp;apos;l Conf</surname></persName>
		</author>
		<title level="m">on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified gradient regularization family for adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Conf. on Data Mining (ICDM)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE CS</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">00</biblScope>
			<biblScope unit="page" from="301" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using machine teaching to identify optimal training-set attacks on machine learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th AAAI Conf. Artificial Intelligence (AAAI &apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Is deep learning safe for robot vision? Adversarial examples against the iCub humanoid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW ViPAR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="751" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards poisoning of deep learning algorithms with backgradient optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Muñoz-González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paudice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wongrassamee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Lupu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ACM Workshop on AI and Sec., AISec &apos;17</title>
		<editor>B. M. Thuraisingham et al.</editor>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="27" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting machine learning to subvert your spam filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I P</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LEET &apos;08</title>
		<meeting><address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On the practicality of integrity attacks on documentlevel sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Potharaju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nita-Rotaru</surname></persName>
		</author>
		<editor>AISec</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paragraph: Thwarting signature learning by training maliciously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Newsome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RAID</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="81" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Practical black-box attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASIA CCS &apos;17</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Transferability in machine learning: from phenomena to black-box attacks using adversarial samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>abs/1605.07277</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Misleading worm signature generators using deliberate noise injection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Perdisci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fogla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. Sec. &amp; Privacy</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Antidote: understanding and defending against poisoning of anomaly detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">I</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ACM SIGCOMM Internet Measurement Conf., IMC &apos;09</title>
		<meeting><address><addrLine>NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Secure kernel machines against evasion attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Demontis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th ACM Workshop on AI and Sec., AISec &apos;16</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="59" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSAC Conf. on Comp. and Comm. Sec</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">Adversarial vulnerability of neural networks increases with input dimension. ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust large margin deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sokoli´csokoli´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R D</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Signal Proc</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="4265" to="4280" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">When does machine learning FAIL? Generalized transferability for evasion and poisoning attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marginean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dumitras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th USENIX Sec</title>
		<imprint>
			<publisher>USENIX Assoc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1299" to="1316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The space of transferable adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Csiszárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zombori</surname></persName>
		</author>
		<idno>ArXiv:1712.09936</idno>
		<title level="m">Gradient Regularization Improves Accuracy of Discriminative Models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Practical evasion of a learningbased classifier: A case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Šrndic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symp. Sec. and Privacy, SP &apos;14</title>
		<imprint>
			<publisher>IEEE CS</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="197" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stealing hyperparameters in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Z</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Enhancing the transferability of adversarial examples with noise reduced gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Is feature selection secure against training data poisoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR W&amp;CP -32nd ICML</title>
		<editor>F. Bach and D. Blei</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatically evading classifiers a case study on PDF malware classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS. Internet Society</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Adversarial feature selection against evasion attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="766" to="777" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
