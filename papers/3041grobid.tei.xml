<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Processing in Storage Class Memory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nider</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of British Columbia</orgName>
								<orgName type="institution" key="instit2">University of British Columbia</orgName>
								<orgName type="institution" key="instit3">University of British Columbia</orgName>
								<orgName type="institution" key="instit4">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Mustard</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of British Columbia</orgName>
								<orgName type="institution" key="instit2">University of British Columbia</orgName>
								<orgName type="institution" key="instit3">University of British Columbia</orgName>
								<orgName type="institution" key="instit4">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrada</forename><surname>Zoltan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of British Columbia</orgName>
								<orgName type="institution" key="instit2">University of British Columbia</orgName>
								<orgName type="institution" key="instit3">University of British Columbia</orgName>
								<orgName type="institution" key="instit4">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of British Columbia</orgName>
								<orgName type="institution" key="instit2">University of British Columbia</orgName>
								<orgName type="institution" key="instit3">University of British Columbia</orgName>
								<orgName type="institution" key="instit4">University of British Columbia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Processing in Storage Class Memory</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Storage and memory technologies are experiencing unprecedented transformation. Storage-class memory (SCM) delivers near-DRAM performance in non-volatile storage media and became commercially available in 2019. Unfortunately , software is not yet able to fully benefit from such high-performance storage. Processing-in-memory (PIM) aims to overcome the notorious memory wall; at the time of writing, hardware is close to being commercially available. This paper takes a position that PIM will become an integral part of future storage-class memories, so data processing can be performed in-storage, saving memory bandwidth and CPU cycles. Under that assumption, we identify typical data-processing tasks poised for in-storage processing, such as compression, encryp-tion and format conversion. We present evidence supporting our assumption and present some feasibility experiments on new PIM hardware to show the potential.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Problem Storage-class memory (SCM), also known as nonvolatile memory (NVRAM) is starting to be adopted as a long-term storage media, as a complement, or even as an alternative to SSD and HDD. It is a class of memory technologies (including STT-MRAM <ref type="bibr" target="#b19">[20]</ref>, PCM <ref type="bibr" target="#b21">[22]</ref>, ReRAM <ref type="bibr" target="#b29">[30]</ref> and FeRAM <ref type="bibr" target="#b3">[4]</ref>) whose defining feature is that data persists across power cycles. It also features a high level of parallelism that is inherent in the design, due to its physical properties. Often, memories in this class are also byte addressable, although not always used in the manner.</p><p>Over the past two decades, performance of storage hardware has increased two orders of magnitude. First, with the introduction of SSDs (solid state drives) then with the transition from SATA to PCIe and most recently with the innovation in non-volatile memory technology. Last year, Intel released Optane DC Persistent Memory <ref type="bibr" target="#b7">[8]</ref>, which is built on PCM with 3D-XPoint technology and sits directly on the memory bus and further reduces I/O latency. At the same time, computation capabilities have not increased at the same rate due to the end of Moore's Law and Dennard scaling. This means that CPU cycles are too precious to waste on I/O processing, especially since I/O processing overhead is increasing at a faster rate than CPUs can cope with. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates that while device access time used to dominate I/O latency, the cost of navigating the software stack is becoming more crucial as device access time shrinks. As storage performance increases, the time spent on processing the data read from the storage media is becoming a more significant percentage of the overall access time. Similarly as storage density increases, the amount of CPU resources required to process data at this higher rate also increases, eventually becoming the bottleneck in an otherwise scalable system. To overcome the problem of scalability in the storage subsystem, compute resources must scale with storage capacity. We believe this will lead to PIM technologies being integrated with SCM.</p><p>Solution Accelerators have been proposed at all levels of the storage hierarchy; from smart caches to processing in storage. Processing-in-memory (PIM) architectures <ref type="bibr" target="#b26">[27]</ref> have been proposed to either overcome the limitations of memory bandwidth (i.e. "memory wall" problem) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b32">33]</ref>, or reduce energy consumption in data movement <ref type="bibr" target="#b1">[2]</ref>. PIM processors are tightly coupled with the RAM, often being implemented on the same die, which has the unique property of scaling the processing capability with the amount of available memory.</p><p>We make the supposition that hardware will emerge that uses PIM processors to improve SCM scalability. These embedded PIM processors are not a drop-in replacement for the host CPU, because they exist in a severely limited environment. Highly constrained die area and a limited power envelope dictate the capabilities of the PIM processors, which in turn affect the scope of the software which they can execute. PIM on SCM shares many of these constraints with PIM on DRAM, but since SCM is essentially storage, the kind of operations performed by PIM on SCM will be different than that performed by PIM on DRAM.</p><p>We see use cases as the main source of difference between PIM on SCM and PIM on DRAM. How the hardware is used should drive the design of both hardware and software. The simplest use case of PIM on SCM will be to perform data transformation between the format of stored data and its in-memory form. We propose to offload simple data processing and manipulation operations (e.g., compression) from the CPU to processors embedded within the SCM. This may encompass several scenarios such as a compressed cache for a block device, preparing data for long-term storage by adding checksums, filtering, or collecting statistics on the data. By offloading these tasks, the CPU will be freed to handle more latency-sensitive or computationally intensive operations. Performing these same tasks using PIM on DRAM would involve using the CPU or a DMA engine to transfer data to DRAM and then operate on it. PIM on SCM eliminates this data movement.</p><p>Benefits PIM on SCM makes sense because it offers many benefits. The following is a list of the most important ones.</p><p>• Free the Bus Since the SCM and in-storage processors are tightly coupled, the data does not have to move across a shared bus. Thus, we can avoid the von-Neumann bottleneck <ref type="bibr" target="#b17">[18]</ref> on the memory bus between storage (SCM or disk) and RAM that current PIM on DRAM implementations suffer from. This is especially important with SCM, as many functions process more data than they return, transferring large amounts of data can be completely avoided.</p><p>• Scalable Processing As the size of the storage increases, processing capabilities should also increase in order to evenly scale the throughput of service requests in the system. PIM on SCM couples processors with storage, so as to guarantee linear scaling.</p><p>• Post-processing of Stored Data Many applications need to guarantee data durability by ensuring that the data has been written to persistent media. PIM on SCM can continue to transform the data in storage after it has become durable, possibly increasing the throughput of the application.</p><p>Contributions Our contribution is recognizing the opportunity to use PIM to solve scalability issues for low-latency persistent storage. We provide motivating experiments with two applications and present microbenchmarks on new PIM hardware to assess the feasibility of this idea. We believe that this approach warrants further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>PIM has been described in several hardware architecture papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref> which have explored various parameters in the design space. Our proposed hardware sits at the junction between processing-in-memory which generally refers to processors embedded in volatile memory, and processing in storage referring to processors embedded in block storage devices. We refer the reader to Siegl <ref type="bibr" target="#b26">[27]</ref> for a more comprehensive survey and taxonomy of NDP (near data processing) including PIM and NMP (near memory processing).</p><p>We view PIM as a collection of small processors embedded in the memory, which we refer to as Data Processing Units (DPU). Here we discuss some of the pertinent features of various related designs, and the associated trade-offs. We base our evaluations on the hardware built by UPMEM <ref type="bibr" target="#b9">[10]</ref>, which is likely to reflect their commercial offering. Even though the UPMEM hardware is implemented on DRAM, we would expect to see hardware very much like what UPMEM is producing in future persistent memory systems, perhaps with the additional features discussed below.</p><p>Inter-PIM Communication Several proposed designs feature a secondary bus (i.e. that does not connect to the host or main memory system) that is used to communicate data and control between the various DPUs, effectively making a distributed system. This has the advantage of being able to coordinate effort for dynamic load balancing at runtime, and sharing partial results to improve locality. The major drawback is the cost of integrating such a bus in silicon. Beyond the obvious costs of larger silicon area and energy consumption, such a bus also requires a more complicated front-end to handle loads and stores from multiple sources. This would also impact scalability because of the amount of communication required between processors.</p><p>Address Translation One of the requisite features for supporting virtual memory is an address translation function for converting virtual addresses to physical addresses. DPU support for virtual memory is very convenient because it means pointers can be shared verbatim between the host CPU and the DPU. Memory translation can be expensive because it requires maintenance of the translation tables. Each process on the host CPU has its own set of tables, and it would become necessary to either share or duplicate these mappings for each DPU in order to support virtual memory. In addition, most DPU designs can only access a limited range of the physical address space. For these reasons, many PIM designs do not support address translation.</p><p>Core Density We refer to the ratio of DPUs to memory as the core density of the system. Higher core density means each DPU is responsible for less memory (i.e. more cores per given memory capacity). It is a tradeoff between silicon area and power consumption vs. parallelism. Not enough cores means limiting parallelism, while too many means needlessly consuming power and die area that could have been used for other purposes. One question that we hope to be able to answer is how to determine the "correct" core density for a given application.</p><p>Instruction Set DPUs must decode and execute instructions, just like any other processor. Some designs have reused an existing ISA from some embedded processor, while others have opted to design their own custom ISAs. The main advantage to an existing ISA is a mature ecosystem, meaning the software development toolchain (compiler, linker, debugger, etc) already exists and is familiar to developers. It is plausible that there may be binary compatibility between two processors that share an ISA. On the other hand, designing a new ISA means only the necessary instructions must be implemented, which can lead to a leaner design. This comes at a cost of having to develop new tools and provide training for developers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Architecture and Limitations</head><p>The first step in evaluating feasibility and potential of our idea is to experiment with existing PIM hardware. Even though the UPMEM hardware is implemented on DRAM, SCM and DRAM have enough similarities so we may experiment freely without depending on simulation or emulation, which both have drawbacks. UPMEM DRAM is DDR4-compatible: it can be used as a drop-in replacement for DDR4 DIMMs. Their DPUs are general-purpose processors, which we believe to be a crucial feature for our problem. In the rest of this section we describe the architecture of the UPMEM PIM hardware and discuss its advantages and limitations. We conclude with the discussion of features that we believe would be beneficial to future PIM on SCM. UPMEM augments DRAM chips by including a specialized implementation of a general-purpose processor (DPU) inside each chip, which has direct access to the DRAM memory. This architecture was designed so that it can be readily integrated into existing DRAM designs. The memory is divided into ranks, and each rank has a set of dedicated, multithreaded DPUs. That way, the number of DPUs is proportional to the size of the memory of the machine.</p><p>Before a DPU can compute on the data, the data must be copied by a DMA engine from DRAM to a 64KB private SRAM buffer. Each DPU has a large number of hardware threads that can be scheduled for execution, but because it is an interleaved multithreading (IMT) design <ref type="bibr" target="#b30">[31]</ref> , only one thread can advance at each cycle. The large number of threads helps hide latency while moving data, since several DMA operations can progress concurrently. Since all of the memory chips are decoupled, all of the DPUs can process data in parallel, completely independent of one another. However, since there is no direct communication channel between DPUs, the host must carefully control the dataset and plan execution before processing begins because of the high cost of synchronization, which must be performed by the host.</p><p>Because the capabilities of the PIM processors are well below that of a common host CPU, it is necessary to ensure that the offloaded functions are simple enough to be implemented and executed efficiently on these embedded processors. Despite the simple design of the PIM processors, they are general purpose, and are applicable to a wide range of problems. They are easy to program since standard development tools (compiler, linker, debugger) are used. In many cases, code snippets can be ported directly from existing code. This is an important feature to encouraging adoption of the new technology in future memory designs. The fact that these processors use a C compiler means that engineers can feel comfortable with the programming environment, and get up to speed quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Virtual Memory</head><p>DRAM <ref type="bibr" target="#b16">[17]</ref> is organized into banks. In UPMEM memory the core density is one DPU for each bank of 64MB. The DPU can only access data in its bank. This creates a challenge, because when a host CPU writes a cache line to DRAM, that data is striped across multiple banks at the granularity of one byte -this is a common design in DRAM DIMMs to reduce latency. This striping is transparent to the host -as long as the data written is identical to the data read back. But when the host writes data that is meant for the DPU to read, the striping causes each DPU to see only every 8th byte of the data. In order to present the DPU with a contiguous dataset, the API functions must reorder the bytes before copying them into the memory attached to the DPUs. To be able to share the memory efficiently between the host and the DPUs, we need to be able to map virtual memory pages directly into the address space of the application, and access them as such. Currently this is not possible, due to the limitations incurred by striping. Mixing SCM and DRAM together in a single rank would further exacerbate the situation, due to the differences in physical layout. Since the DPUs are permanently tied to a specific memory range, we are unable to use the usual method of migrating sectors used to implement wear levelling on SCM <ref type="bibr" target="#b24">[25]</ref>, and would be forced to use in-place methods <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Wishlist for PIM architectures</head><p>Based on our understanding of PIM on DRAM, we believe PIM on SCM would benefit from some additional features that would make the data flow easier to manage, and software easier to write.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Triggered Functions Similar to the concept of Data</head><p>Triggered Threads <ref type="bibr" target="#b28">[29]</ref> and EDGE <ref type="bibr" target="#b15">[16]</ref>, we propose the idea of data triggered functions. The idea is that the host CPU can trigger a function execution on the DPU by reading from a memory address owned by that DPU. The address is first registered on the DPU as a function target. When the CPU issues a memory load, it will trigger the DPU to execute a function, and stall until the function completes, at which time the memory will return the result. This will keep the program flow simple, as no expensive API calls would be needed in order to execute a function on the DPU, and no additional mechanism (polling, interrupt, etc.) would be required to know when the results are available. This would require a memory controller that can support the longer latency of executing a function. The NVDIMM-P specification <ref type="bibr" target="#b12">[13]</ref> is expected to add support for non-deterministic access times to handle longer latencies associated with persistent memory, which may be able to also handle such an extreme case as function execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concurrent Memory Access</head><p>The PIM architecture must arbitrate between concurrent accesses to the same memory rank from the host CPU and the DPU. Currently, this is controlled by a register written by the host, to select the permitted bus master. This is a very coarse granularity, which means PIM cannot arbitrate between individual accesses from multiple sources. Effectively, it means that the DPU and CPU cannot operate on the same memory rank concurrently. In order to pipeline requests from software, it would be necessary to support a producer-consumer model while the DPU is processing buffer N the CPU could read the results of N-1 and write the buffer N+1. This is used for double-buffering, or the more general case of a queue of buffers. This would require support from the memory bus, which is not possible with the commonly used DDR4. Memory fabrics such as OpenCAPI <ref type="bibr" target="#b6">[7]</ref>, GenZ <ref type="bibr" target="#b20">[21]</ref> or CCIX <ref type="bibr" target="#b5">[6]</ref> may provide the necessary support.</p><p>Mix of Memory Types Each DPU can access a single region of physical memory (64MB on UPMEM hardware). It would be advantageous to allow the DPU to access a mix of DRAM and SCM. The SCM is used for persistent storage of application data, while the DRAM is used for holding temporary results, such as uncompressed or decrypted blocks of data that is stored in the SCM portion. Ultimately, we would like to dynamically select the ratio of SCM to DRAM in each region. Even if that were not possible, there is still an advantage to having a static partitioning of memory types at a fixed ratio.</p><p>Tuning For Performance A system is only as good as its weakest point. Therefore, it is important to balance the throughput of the components to get maximum performance while minimizing cost. While we believe current PIM designs used with DRAM would be essentially the same on SCM, the system characteristics will likely be different. For example, SCM has higher access latency than DRAM for both reads and writes. Therefore, a PIM design may require modifications such as increasing the number of threads to have more memory accesses in-flight in order to compensate for higher latency. Alternatively, increasing the core density may also provide the same result. These details are dependent on many factors, including the cost of additional silicon area, power consumption, bus frequency and memory latency. Further study is required to draw any meaningful conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Feasibility Experiments</head><p>In this section, we identify some promising applications that PIM on SCM can accelerate. We show that these use cases can have high software overhead on SCM, and that software becomes the main performance bottleneck.</p><p>Format Parsing Decoding stored data formats (JSON, XML, ORC, etc.) into in-memory representations is an expensive task. The best known CPU implementation of SIMD-JSON today can only operate at 2GB/s per-core <ref type="bibr" target="#b22">[23]</ref>. With the restricted core counts of modern processors, we envision using the numerous PIM cores to perform some or all of the parsing. SIMDJSON identifies all structural bytes of a record in parallel before traversing it. Performing the identification on PIM hardware would alleviate CPUs of this bandwidthintensive task. Another complimentary approach could be to use PIM to filter records before using a CPU to parse them, as proposed by Sparser <ref type="bibr" target="#b25">[26]</ref>.</p><p>To motivate this use case, we performed an experiment to compare the overhead of format conversion when the data is stored on Intel Optane NVRAM and on HDD. We parse 14GB of JSON store_sales from the TPC-DS benchmark suite using SIMDJSON's multi-line parsing benchmark running on a single CPU core. To ensure the data is taken from disk, we flush the block cache before the test. On NVRAM, the test takes 26s with 11s (42%) of parse time. On HDD it takes  80s with 11s (13%) of parse time. SIMDJSON operates at 1.3 GB/s in these tests, so it can easily keep up with the HDD throughput at 200MB/s, whereas NVRAM at 1.6GB/s 1 it is able to sustain a higher throughput than the CPU can manage. By using multiple DPUs, we expect to parallelize the parsing, and see an increase in throughput.</p><p>Storage Compression Compression is used to improve storage density, but requires that data be decompressed before it can be used <ref type="bibr" target="#b23">[24]</ref>. When this happens, CPU time that could be spent servicing other requests is spent on decompression.</p><p>MongoDB is the most popular NoSQL database <ref type="bibr" target="#b0">[1]</ref> and its storage engine, WiredTiger, is responsible for managing the data storage on each database node. Like many other keyvalues stores, WiredTiger converts the data between on-disk format (for persistent data) and in-memory format (when the data is live), optionally including encryption or compression. These operations must be performed each time a block is transferred between persistent storage and main memory. <ref type="bibr" target="#b0">1</ref> Optane NVRAM here was used as a block device with a file system on top (no dax option) and data was read via system calls. If accessed via a dax file system and using mmap, the throughput would be even higher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 3: Snappy decompression benchmark results</head><p>Our hypothesis is that the relative overhead of compression is higher on NVRAM device than on HDD. In general terms, the compression overhead becomes higher (relative to the total execution time) as the speed of the storage device increases. To test the hypothesis, we used the WiredTiger performance suite wtperf. We ran each workload with compression on and with compression off, and compared the throughput. We repeated this experiment on the NVRAM block device 2 and on a conventional HDD. Our test system consists of an Intel Xeon 5218 CPU @ 2.3GHz, 256GB Intel Optane NVRAM (non-interleaved in app mode @2666 MHz), with a Toshiba 300 GB SAS HDD (@10K RPM). <ref type="figure" target="#fig_2">Figure 2</ref> shows the results. Compression overhead is higher on NVRAM than on HDD (by at least 3%) for 17 of the 35 benchmark/operations 3 . Only 4 benchmark/operations suffer higher compression overhead on HDD than on the NVRAM. Higher relative overhead is an indication that either the CPU cycles spent on compression or the increased cache miss rate due to increased cache pollution becomes a more significant factor in performance when I/O is fast. In both cases, offloading these tasks from the host CPU could reduce the overhead.</p><p>Snappy Decompression To test whether or not PIM hardware would be able to keep up with decompression offloaded from the CPU, we implemented a decompressor for the Snappy <ref type="bibr" target="#b14">[15]</ref> algorithm. By using a set of test files that vary in size and compression ratio, we compiled a benchmark to measure the performance compared to a CPU (shown in <ref type="table">Table 1</ref>). <ref type="figure">Figure 3</ref> shows the speedup (slowdown) of our implementation over the same algorithm on the host CPU. For this experiment, we used a host with an AMD Ryzen CPU running at 2.2GHz, an Intel 660p 3D NAND SSD and 640 UPMEM DPUs. With the largest file size, we see a speedup </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Throughput</head><p>To expose the potential, we measured the maximum throughput between the DRAM and SRAM buffer of UPMEM PIM. Our test system is composed of nine single-rank DDR4-2400 UPMEM DIMMs, running at 267 MHz. That yields a total of 36GB of DRAM and 576 DPU cores. The host is one Intel Xeon Skylake SP (4110) socket. The experiment copies the entire DRAM bank that is accessible by the DPU (64MB) in 2KB blocks from DRAM to the SRAM buffer using the DMA engine. We maximized parallelism by starting all DPUs simultaneously, and by using 16 threads concurrently in each DPU. The threads only copied data, and did not perform any additional processing.</p><p>We copied 36 GB in 0.14 seconds, for a total throughput of approximately 252 GB/sec. Compared to the maximum theoretical bandwidth of a single DDR4-2400 channel between the DRAM and CPU of 19 GB/sec <ref type="bibr" target="#b8">[9]</ref>, this is more than a 13x larger throughput. With the expected maximum frequency of the UPMEM DPU set to be 500 MHz in the future, the bandwidth advantage would increase up to 20×. This gives us the motivation to leverage this enormous bandwidth to accelerate our applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other Use Cases</head><p>There are a few other use cases that we have conceived and investigated, but are not yet at the point that we can report results.</p><p>Encryption Databases often store their data in an encrypted form for security. When the data is to be read by the database, it must first be decrypted. Multiple client requests may require decrypting different blocks at the same time. Also, prefetching operations by the database may trigger decryption. Several data blocks can be decrypted in parallel, without intefering with the throughput of the main CPU which can continue to handle client requests.</p><p>Index Checks Checking if data is present in an index is a common task for databases, key/value stores, and analytics engines. Each index check is an independent operation, which means it can easily be parallelized. When a number of indexes need to be checked simultaneously, PIM can assist with the check, freeing up the CPU to deliver data to the client.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Implementing PIM on SCM is a promising direction in order to offload simple tasks and free up precious CPU cycles and memory bus cycles. Our motivating experiments have given some evidence supporting our belief. We plan to investigate how to best use this new architecture for software applications that are likely to gain the most benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion Topics</head><p>In keeping with the spirit of the workshop, we have composed the following list of items on which we hope to discuss with the other participants.</p><p>1. Applicability We would like to get feedback on the applicability of this architecture. We have come up with several use cases, but believe there are many more than we have not yet considered.</p><p>2. Hardware Parameters There are several hardware parameters that can be tweaked in a particular implementation that would affect the outcome of any software experiments we could run. How should we determine the correct mix of SCM and DRAM in each DPU rank? How to find the correct core density, given a particular PIM and memory technologies? This includes the items in our wishlist (section 3.2). Since many of these features are being discussed and defined in standards, we may have an opportunity to influence the community early enough to have a positive impact on technology we will be using for years to come.</p><p>3. Benchmarking One question that is not clear, is the most convincing way to benchmark this kind of architecture. Clearly, adding more CPU resources will increase overall system performance. However, we want to make a fair comparison to a system without these PIM processors, in order to show the cost vs. benefit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Latency breakdown of a 4KB block random read on various storage hardware technologies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>small-btree update-checkpoint-btree.update update-checkpoint-btree.read update-lsm.read evict-lsm update-lsm.update evict-btree-stress update-delta-mix3 evict-lsm-1 medium-lsm-compact evict-btree-1 medium-lsm medium-btree evict-btree update-btree.insert evict-btree-stress-multi.read evict-btree-stress-multi.update medium-multi-lsm-noprefix.update medium-multi-lsm.update medium-multi-lsm-noprefix.read insert-rmw.read medium-multi-lsm.read long-txn-lsm.update small-lsm long-txn-lsm.read medium-multi-btree-log update-btree.update checkpoint-schema-race.read update-btree.read insert-rmw.insert update-only-btree checkpoint-schema-race.update checkpoint-schema-race.insert update-grow-stress.update medium-multi-btree-log-partial.update SLOWDOWN WHEN COMPRESSION IS ENABLED RELATIVE TO WHEN IT IS DISABLED HDD NVRAM slows down more than HDD Slowdown on NVRAM is higher than on HDD 17 out of 35 workloads NVRAM slows down same as HDD NVRAM slows down less than HDD Slowdown on NVRAM is smaller than on HDD for only 4 workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The X-axis shows the names of benchmarks and their respective operations in the format benchmark.operation, where the operation type can be read, insert, modify or update; the operation type is omitted when the benchmark performs only reads.</figDesc></figure>

			<note place="foot" n="2"> Optane NVRAM was configured as a block device with the file system on top, so we could use WiredTiger without modifications. 3 We did not run all of the workloads due to limited space on our NVRAM device and other configuration issues. We omitted from the Figure those where the differences between compressed and uncompressed configurations were not statistically significant</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Db-Engines Ranking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Db-Engines</surname></persName>
		</author>
		<ptr target="https://db-engines.com/en/ranking" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A many-core architecture for in-memory data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Sandeep R Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Idicula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatanathan</forename><surname>Govindaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagri</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Balkesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Giannikis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nipun</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-50 &apos;17</title>
		<meeting>the 50th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-50 &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="245" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A scalable processing-in-memory accelerator for parallel graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ferroelectrics for digital information storage and switching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dudley Allen</forename><surname>Buck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Computer Library</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">1952</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flip-n-write: A simple deterministic technique to improve pram write performance, energy and endurance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjin</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 42nd Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="347" to="357" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Ccix cache coherency interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ccix</forename><surname>Consortium</surname></persName>
		</author>
		<ptr target="https://www.ccixconsortium.com/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">OpenCAPI consortium. Opencapi consortium</title>
		<ptr target="https://opencapi.org/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Intel R optane TM memory</title>
		<ptr target="https://www.intel.ca/content/www/ca/en/architecture-and-technology/optane-memory.html" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Memory deep dive: Ddr4 memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Denneman</surname></persName>
		</author>
		<ptr target="https://frankdenneman.nl/2015/02/25/memory-deep-dive-ddr4/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The true processing in memory accelerator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Devaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Hot Chips 31 Symposium (HCS)</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computedram: In-memory compute using off-the-shelf drams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tziantzioulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;52</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO &apos;52<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="100" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hrl: Efficient and flexible reconfigurable logic for near-data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overcoming system memory challenges with persistent memory and nvdimm-p</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Gervasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Hinkle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JEDEC Server Forum. JEDEC</title>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Processing in memory: the terasys massively parallel pim array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gokhale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iobst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="1995-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="http://google.github.io/snappy/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Edge: Event-driven gpu execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lubeznov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="337" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Memory systems: cache, DRAM, disk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spencer</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prins: Processingin-storage acceleration of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yavits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ginosar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Nanotechnology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="896" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From processing-in-memory to processing-in-storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Yavits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Ginosar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supercomput. Front. Innov. : Int. J</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Basic principles of stt-mram cell operation in memory arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Khvalkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Apalkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chepulskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Beach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Driskillsmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Visscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lottis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nikitin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krounbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics D: Applied Physics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page">74001</biblScope>
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gen-z dram and persistent memory theory of operation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Witkowski</surname></persName>
		</author>
		<ptr target="https://genzconsortium.org/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Current status of the phase change memory and its future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Electron Devices Meeting</title>
		<imprint>
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
	<note>pages 10.1.1-10.1.4</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parsing gigabytes of json per second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Langdale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lemire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="941" to="960" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zbd: Using transparent compression at the block level to increase storage space efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Makatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Klonatos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marazakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Flouris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bilas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 International Workshop on Storage Network Architecture and Parallel I/Os</title>
		<imprint>
			<date type="published" when="2010-05" />
			<biblScope unit="page" from="61" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Application-oriented wear-leveling optimization of 3d tsv-integrated storage class memorybased solid state drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakanishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adachi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Electronics Packaging and iMAPS All Asia Conference (ICEP-IAAC)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="27" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Filter before you parse: Faster analytics on raw data with sparser</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoumik</forename><surname>Palkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firas</forename><surname>Abuzaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1576" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-centric computing frontiers: A survey on processing-in-memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Siegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Buchty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mladen</forename><surname>Berekovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on Memory Systems, MEM-SYS &apos;16</title>
		<meeting>the Second International Symposium on Memory Systems, MEM-SYS &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A logic-in-memory computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="78" />
			<date type="published" when="1970-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Software data-triggered threads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Wei</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename><surname>Michael Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications, OOPSLA &apos;12</title>
		<meeting>the ACM International Conference on Object Oriented Programming Systems Languages and Applications, OOPSLA &apos;12<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="703" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nanoionics-based resistive switching memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Waser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Aono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nature Materials</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 16th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="1989-05" />
			<biblScope unit="page" from="273" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Top-pim: Throughput-oriented programmable processing in memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuwan</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lyashevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ignatowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC &apos;14</title>
		<meeting>the 23rd International Symposium on High-Performance Parallel and Distributed Computing, HPDC &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Massively parallel skyline computation for processing-in-memory architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Zois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divya</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vassilis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><forename type="middle">A</forename><surname>Tsotras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Francois</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;18</title>
		<meeting>the 27th International Conference on Parallel Architectures and Compilation Techniques, PACT &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Association for Computing Machinery</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
