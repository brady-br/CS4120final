<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving I/O Resource Sharing of Linux Cgroup for NVMe SSDs on Multi-core Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungyong</forename><surname>Ahn</surname></persName>
							<email>sungyong.ahn@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Memory Division</orgName>
								<orgName type="department" key="dep2">Dept. of CSE</orgName>
								<orgName type="institution">Samsung Electronics Co</orgName>
								<address>
									<settlement>Hwasung</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghyun</forename><surname>La</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Memory Division</orgName>
								<orgName type="department" key="dep2">Dept. of CSE</orgName>
								<orgName type="institution">Samsung Electronics Co</orgName>
								<address>
									<settlement>Hwasung</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Kim</surname></persName>
							<email>jihong@davinci.snu.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving I/O Resource Sharing of Linux Cgroup for NVMe SSDs on Multi-core Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In container-based virtualization where multiple isolated containers share I/O resources on top of a single operating system, efficient and proportional I/O resource sharing is an important system requirement. Motivated by a lack of adequate support for I/O resource sharing in Linux Cgroup for high-performance NVMe SSDs, we developed a new weight-based dynamic throttling technique which can provide proportional I/O sharing for container-based virtualization solutions running on NUMA multi-core systems with NVMe SSDs. By intelligently predicting the future I/O bandwidth requirement of containers based on past I/O service rates of I/O-active containers, and modifying the current Linux Cgroup implementation for better NUMA-scalable performance, our scheme achieves highly accurate I/O resource sharing while reducing wasted I/O bandwidth. Based on a Linux kernel 4.0.4 implementation running on a 4-node NUMA multi-core systems with NVMe SSDs, our experimental results show that the proposed technique can efficiently share the I/O bandwidth of NVMe SSDs among multiple containers according to given I/O weights.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Container-based virtualization is emerging as a key cloud computing platform for serving various cloud services because it allows multiple isolated instances (called containers) to share system resources more efficiently over hypervisor-based virtualization. In container-based virtualization, since multiple containers run independently on top of a single common operating system, it is important for a kernel-level resource manager to support resource isolation and sharing in an efficient and proportional fashion among multiple containers with different service requirements.</p><p>Linux Cgroup <ref type="bibr" target="#b0">[1]</ref> is such a resource control framework in Linux which supports many container-based virtualization solutions such as Linux container (LXC), Docker and libcontainer <ref type="bibr">[2,</ref><ref type="bibr" target="#b1">3]</ref>. Linux Cgroup manages, for example, the I/O bandwidth of a storage system in a proportional way so that the total I/O bandwidth of the storage system can be properly shared among multiple containers.</p><p>Although Linux Cgroup efficiently supports proportional I/O sharing for SATA-based HDDs/SSDs inside the CFQ I/O scheduler at the single-queue block layer, the current Cgroup implementation does not adequately support I/O resource sharing for recent highperformance SSDs (such as NVMe SSDs). For example, since these high-performance SSDs, which can achieve more than 1 million IOPS, need to work with the newly proposed multi-queue block layer <ref type="bibr" target="#b2">[4]</ref> for realizing its performance potential, the existing proportional I/O sharing scheme, which was implemented at the single-queue block layer, cannot be used. In this paper, we propose a weight-based dynamic throttling scheme for NVMe SSDs which can provide efficient and proportional I/O sharing. Our proposed throttling scheme is implemented as an extension to the existing I/O throttling layer of Linux Cgroup.</p><p>While implementing the proposed scheme, we also discovered that the current Linux Cgroup is not scalable on NUMA multicore systems when it works with highperformance NVMe SSDs. Since these NVMe SSDs are expected to be shared in practice by a large number of containers (because of their high bandwidth as well as their high capacity), it is an important requirement for Linux Cgroup to work in a scalable way as the number of containers increases. Furthermore, since a host system for these SSDs are likely to be based on NUMA multi-core systems, Linux Cgroup should support NUMA-aware scalable I/O sharing as well. In order to make the proposed scheme to be NUMA-scalable, we modified Linux Cgroup to employ per-container locks instead of sharing a single request-queue lock among multiple containers.</p><p>In order to understand the effectiveness of our proposed improvements to the current Cgroup implementation, we implemented the proposed scheme on Linux kernel 4.0.4 running on a 4-node NUMA multi-core system and evaluated it using Samsung XS1715 NVMe SSDs <ref type="bibr" target="#b3">[5]</ref>. The experimental results show that our scheme can efficiently share the I/O bandwidth of NVMe SSDs among multiple containers in proportion to their I/O weights with scalable performance.</p><p>The remainder of this paper is organized as follows. Sec. 2 explains the limitations of the current Linux Cgroup when NVMe SSDs are shared among containers. Sec. 3 describes the proposed I/O resource sharing scheme. Experimental results are presented in Sec. 4. Sec. 5 summarizes related work. Sec. 6 concludes with a summary and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Limitations of Linux Cgroup for NVMe SSDs</head><p>In this section, we evaluate how the existing I/O resource control mechanisms of Linux Cgroup work with NVMe SSDs in sharing I/O resource among multiple containers. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, in Linux Cgroup, I/O resource sharing can be supported at two layers, the Cgroup I/O throttling layer and single-queue block layer.</p><p>For SATA HDDs and SSDs, proportional I/O resource sharing has been supported inside the CFQ I/O scheduler of the single-queue block layer <ref type="bibr" target="#b4">[6]</ref>. However, since Linux kernel 3.13, NVMe SSDs have been supported under the multi-queue block layer because the single-queue block layer cannot achieve a high performance potential of NVMe SSDs <ref type="bibr" target="#b2">[4]</ref>. Therefore, the existing CFQ-based proportional I/O policy cannot be reused for NVMe SSDs.</p><p>Linux Cgroup also provides I/O throttling at the Cgroup I/O throttling layer which can be used for I/O resource sharing by limiting the maximum I/O bandwidth or maximum IOPS available for each container. As a simple proportional I/O sharing solution at the Cgroup I/O throttling layer, we developed a static throttling scheme, ST, which assigns different upper limits on the read bandwidth and write bandwidth to containers according to their I/O weights.</p><p>In order to quantitatively evaluate the limitation of the existing Cgroup resource sharing mechanisms (including ST) for NVMe SSDs, we performed simple experiments using four containers, , , , and , where the I/O weight ratios among four containers are given as 10:5:2.5:1. For the experiments, a Dell R920 with 4 Samsung XS1715 NVMe SSDs was used. R920 has 4 NUMA nodes where each NUMA node supports 12 CPU cores. We created four containers using LXC <ref type="bibr">[2]</ref>. Each container ran the I/O workloads summarized in <ref type="table" target="#tab_0">Table 1</ref> 1 . As shown in <ref type="figure">Fig. 2</ref>, the default Cgroup policy, BASELINE, has no support for proportional I/O sharing for NVMe SSDs, thus producing meaningless resource sharing result. Although ST, which assigns the maximum bandwidth allowed for each container based on the I/O weight of the container, works much better than BASELINE, it still performs poorly for proportional I/O sharing. For example, the required I/O weight ratio of to is 2:1, but ST achieves the ratio of 9.9 to 2.2. The poor performance of ST can be attributed to two main factors. First, although the static throttling approach used in ST is effective in guaranteeing that no container is allocated with the I/O bandwidth over the specified maximum bandwidth, it is not useful to meet required I/O weights of containers. Furthermore, ST is likely to waste the I/O bandwidth allocated for a container if the container is not I/O-intensive. For example, <ref type="figure" target="#fig_2">Fig. 3</ref> shows wastes a significant amount of the allocated read bandwidth because its read request are not intensive enough to fully consume the allocated read bandwidth.</p><p>Second, ST separately manages read bandwidth and write bandwidth (following the basic throttling mechanism of the Cgroup I/O throttling layer), making it difficult to manage the I/O bandwidth in an integrated fashion. For example, <ref type="figure" target="#fig_3">Fig. 4</ref> shows that consumed most of the allocated read bandwidth but it significantly under-utilized the allocated write bandwidth. Since this asymmetric I/O consumption pattern between reads and writes is application specific (e.g., MSNMeta is readintensive of ), ST cannot easily estimate the required read bandwidth and write bandwidth in advance. Therefore ST may waste a significant amount of the allocated read / write bandwidth.</p><p>Our proposed scheme improves these two weaknesses of ST by dynamically adjusting each container's maximum I/O bandwidth by predicting future I/O demands and managing both the read bandwidth and write bandwidth in a combined fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Weight-based Dynamic Throttling Scheme</head><p>In this section, we describe our proposed weightbased dynamic throttling scheme, WDT, for NVMe SSDs.  : :</p><p>= 10 : 5 : 2.5 : 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST BASELINE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Key Design Decisions</head><p>As the first design decision, we decided to implement WDT at the Cgroup I/O throttling layer instead of at the multi-queue block layer as shown in <ref type="figure" target="#fig_0">Fig. 1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overview of WDT</head><p>In order to support locally-proportional I/O sharing in WDT using dynamic throttling, we employ an interval-based approach. A fixed-length interval , called as the throttling window, is used as a basic unit of I/O resource control in WDT. (We denote the size of the throttling window as .) For the j-th throttling window , we associate the following three parameters for a container : , , . The credit budget of the container for indicates the total number of sectors that can request (either by reads or writes) during the j-th throttling window . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Future I/O Demand Predictor</head><p>The key step of WDT is to compute TotalCredit for each throttling window. Since the budget distributor simply divides TotalCredit based on I/O weights of containers, the efficiency of WDT largely depends on the accuracy of predicting TotalCredit. If TotalCredit is overestimated by a larger amount than an actual total number of credits necessary for the next throttling window, it may be difficult to meet proportional I/O requirements because some containers may consume too many credits while others have no usable credits left. On the other hand, if TotalCredit is underestimated, the overall I/O performance may be degraded because it may throttle I/O requests more than necessary. Therefore, accurately predicting TotalCredit is important in WDT. Furthermore, in order to reduce the overhead of updating TotalCredit, WDT only updates TotalCredit every N throttling windows (which we call the update window of TotalCredit). We denote the length of this update window as . Note that since we recompute TotalCredit every update window, 's are also updated only once per update window. However, 's and 's are still updated every throttling window. Let TotalCredit p represent TotalCredit computed at the p-th update window. In order to compute TotalCredit p+1 close to an actual I/O demand, we first es-    timate the future credit budget of the container which had the highest I/O weight at the p-th update window.</p><p>Assuming that is also the container with the highest I/O weight at the (p+1)-th window 2 , we can estimate TotalCredit p+1 as follows:</p><p>can be conservatively estimated as follows:</p><p>× where is the Nth percentile of a cumulative distribution of values. In the current WDT scheme, we used the 80th percentile value based on our empirical evaluation <ref type="bibr" target="#b1">3</ref> . Since maintaining an entire cumulative distribution histogram incurs a large overhead inside the kernel, we instead use the probit function <ref type="bibr" target="#b8">[10]</ref>, a well-known quantile function associated with the standard normal distribution. Using the probit function, the 80th percentile of can be calculated as follows:</p><formula xml:id="formula_0">( ) ( ) ( )</formula><p>where ( ) and ( ) are the average and standard deviation of a cumulative distribution of values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Residual Credit Carryover</head><p>Although the current WDT scheme focuses on achieving locally-proportional I/O sharing among locally I/O-active containers, WDT tries to improve the overall I/O performance by reducing wasted credit budgets of containers. In order to minimize wasted credits allocated for a container for , the container maintains the residual credit for each . is computed as . When the I/O behavior of suddenly changes (for example, almost no I/O requests for ), most of are wasted unless they are carried over for future usage. By using , WDT can use the unused credits in a future throttling window when needs higher I/O bandwidth. <ref type="bibr">2</ref> In most cases, this assumption holds for our experiments. For a few cases where changes at the next throttling window, values may be inaccurate. However, WDT quickly catches up this mistake within several subsequent throttling windows. <ref type="bibr" target="#b1">3</ref> Choosing a right value is not trivial. Since we estimate the future budget for using , the best is workloaddependent. Designing a better solution (e.g., choosing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Per-container Lock for Performance Scalability</head><p>values in a workload-adaptive fashion) is one of our future WDT extensions.</p><p>While developing the WDT scheme, we discovered a scalability problem of the current Cgroup throttling layer implementation on a NUMA machine with high performance NVMe SSDs. <ref type="figure" target="#fig_5">Fig. 6</ref> illustrates the NUMA scalability problem using a four-container example where each container runs three FIO processes and each FIO process intensively generates 4-KB random rad requests. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, the read bandwidth sharply drops when more than one NUMA nodes are used on a Dell R920 machine (with four NUMA nodes).</p><p>The main source of this scalability problem is that a single request-queue lock is shared among all containers (i.e., all FIO processes) whenever an I/O bandwidth threshold is checked. Since multiple containers running on different NUMA nodes will continuous incur expensive cacheline invalidation operations when the shared lock is updated, the read bandwidth is very quickly degraded as the number of containers running on different NUMA nodes increases. <ref type="figure">Fig. 7</ref> shows that CPU cache miss ratio sharply increases when multiple containers issue I/O requests from more than one NUMA node. The performance impact of the increased cache misses, however, depends on the performance level of a target storage system. For example, in slower HDDs, the performance penalty from the increased cache misses was insignificant because HDDs performed slowly. On the other hand, for NVMe SSDs, this penalty directly affects the I/O throughput as shown in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p><p>In order to solve the scalability problem, we adopted per-container locks instead of a single request-queue lock at the I/O throttling layer of Linux Cgroup. Since WDT requires container-local information only, it is not necessary to use a global lock shared by all the containers. Fine-grained per-container locks make the I/O throttling layer operate independently from other containers. The experimental result (in Sec. 4) shows that our simple modification significantly reduces performance degradation from the I/O scalability problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>The proposed WDT scheme was implemented in Linux kernel 4.0.4 and evaluated on a Dell R920 machine configuration described in Sec. 2. In evaluations, four real-world workloads (described in <ref type="table" target="#tab_0">Table 1</ref>) are used as well as a synthetic workload (based on FIO <ref type="bibr">[11]</ref>). We set to be 100 ms (which is the throttling window size used in the original Linux Cgroup). Since tends to be changed slowly, we set to the 10 times of (i.e., 1 s). We evaluate three schemes, ST, WDT, and WDT-, where WDT-works in the same way as WDT except that a single request-queue lock is used for all containers. <ref type="figure" target="#fig_6">Fig. 8</ref> shows how WDT satisfies different I/O weight combinations for four containers using read-world workloads of <ref type="table" target="#tab_0">Table 1</ref>. For four different cases, WDT very accurately satisfies the proportional sharing requirements. The number of NUMA nodes <ref type="figure">Fig. 9</ref> compares ST, WDT-and WDT for their proportional I/O support using synthetic workloads. In this evaluation, each container generates 4-KB random read and write requests intensively by using FIO processes. A ratio of read to write in each container was set differently from 90% (in ), 80% (in ), 70% (in ) and 60% (in ). Both WDT and WDT-can meet the proportional sharing requirement while ST cannot. It is because ST cannot properly handle asymmetric bandwidth consumption behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>Moreover, as shown in <ref type="figure">Fig. 9</ref>, WDT achieves much higher I/O bandwidth for four containers over WDT-. For example, achieves an I/O bandwidth of 176 MB/s under WDT while reaches only up to an I/O bandwidth of 133 MB/s under WDT-. This difference in the achieved I/O bandwidth between WDT and WDTshows that WDT significantly reduces the overhead of a shared lock at the Cgroup throttling layer. The cache miss ratio under WDT was 12.8 % only while that under WDT-was 32.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Several research groups have proposed I/O resource control schemes based on credit allocation and throttling such as SLEDS <ref type="bibr" target="#b9">[12]</ref>, RW(D) <ref type="bibr" target="#b10">[13]</ref> and SARC <ref type="bibr" target="#b11">[14]</ref>. Unlike our scheme, SLEDS and RW(D) are not fully work-conserving because they lack a mechanism for utilizing spare bandwidth. Although SARC is workconserving, it is rather ineffective in meeting the I/O proportionality because residual credits are not accounted in future credit allocation. Our scheme, on the other hand, is fully work-conserving while satisfying the required I/O proportionality very accurately by updating the total credit amounts depending on estimated future I/O demands and fully accounting residual credits for each throttling window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have presented an I/O resource management technique, WDT, for supporting proportional I/O resource sharing in Linux Cgroup on NUMA multi-core machines with NVMe SSDs. In order to overcome the shortcomings of the existing throttling policy, WDT employs a dynamic throttling approach by intelligently predicting the future I/O demands of each container and manages reads and writes in a combined fashion. Our evaluation results show that the WDT technique achieves very accurate proportional I/O resource sharing. By employing per-container locks, WDT also achieves NUMA-scalable high I/O performance as well.</p><p>The proposed WDT can be extended in several directions. For example, as described in <ref type="bibr" target="#b12">[15,</ref><ref type="bibr" target="#b13">16]</ref>, the problem of proportional I/O sharing should be solved in a crosslayer fashion. Although the current version of WDT has focused on the block layer only, we plan to extend the WDT scheme to consider multiple layers (e.g., a file system and a page cache) in an integrated fashion. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: An overview of the I/O resource control in Linux Cgroup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc>Our deci- sion is affected by three factors: 1) adding a new policy at the I/O throttling layer is easier by reusing most of the existing throttling layer code, 2) implementing a CFQ-like I/O scheduler for the multi-queue block layer can be quite expensive (because per-process I/O sched- uling queues necessary in the CFQ I/O scheduler incurs a large overhead in the multi-queue block layer), and 3) employing an I/O scheduler is not recommended for high-performance SSDs. Another important decision we made in designing the current WDT scheme was how to define I/O propor- tionality. An ideal proportional I/O sharing technique must satisfy the required I/O weight ratios among con- tainers both locally and globally. By locally- proportional I/O sharing, we mean that the I/O weight ratios are satisfied among I/O-active containers for a given short time interval. On the other hand, in a glob- ally-proportional I/O sharing technique, the total I/O resource usage of multiple containers (over entire exe- cution times) should be proportionally maintained. Since even formally defining the requirements of an ideal proportional I/O sharing technique is challenging, in the current version of WDT, we focus on locally- proportional I/O sharing only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Under-utilized read bandwidth in .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Asymmetric I/O bandwidth consumption between reads and writes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: I/O throughput of containers with varying number of NUMA nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Evaluation results of proportional I/O sharing under WDT with real-world workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Characteristics of I/O workloads in four containers.</head><label>1</label><figDesc></figDesc><table>Container Workload Request size (total / average) R : W 
Exchange 
126.7GB / 9.3KB 
0.34 : 1 
MSNMeta 
71.5GB / 5.0KB 
1.94 : 1 
MSNFS 
56.0GB / 5.1KB 
1.67 : 1 
Finance 
28.2GB / 2.7KB 
0.87 : 1 

Fig. 2: Evaluation results of proportional I/O sharing in Linux 

Cgroup. 

Single-queue Block Layer 
Multi-queue Block Layer 

Device Driver Layer 

Proportional 
I/O 
(CFQ) 

SATA HDD/SSD 
Device Driver 

NVMe SSD 
Device Driver 

... 

Container 

SATA HDDs / SSDs 
NVMe SSDs 

Hardware 

w1 

w2 

w3 

Container 
Container 

Cgroup I/O Throttling Layer 

IOPS 
throttling 

Bandwidth 
throttling 

Operating System 

Weight-based 
throttling 

I/O weight w1 
I/O weight w2 
I/O weight w3 

1.5 

9.9 

1.1 
2.2 

0.5 
1.2 
1.0 
1.0 

0 

2 

4 

6 

8 

10 

12 

BASELINE 
NAIVE 

Normalized 

I/O 

bandwidth 
C1 C2 

C3 C4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>the containers are predicted for the next throttling window , the total amount of credits required for the next throttling interval, TotalCredit, is computed. The budget distributor then updates values for the containers by distributing TotalCredit to each container based on its I/O weight.</figDesc><table>The used 
credit 
of the container 
represents the total num-
ber of credits consumed by 
during 
. The residual 
credit 
of the container 
indicates the remaining 
credits not consumed during 
. 
is carried over to 
the next throttling window 
. Whenever an I/O re-
quest of 
is serviced, 
is incremented by the num-
ber of sectors serviced. 
In order to decide whether the current I/O request 
should be issued or throttled under I/O proportionality 

requirements, we check if 
is smaller than the sum of 
and 
. If 
is smaller, that is, if there are remain-
ing credits available, the current I/O request is issued. 
Otherwise, it is throttled until the next throttling win-
dow. 
An overview of the proposed WDT scheme is shown 
in Fig. 5. The WDT scheme consists of two main func-
tions. The future I/O demand predictor is responsible 
for estimating a future I/O demand of the container . 
WDT monitors the I/O service rate of 
for 
, which 
we denote as 
(Credits per Millisecond), and 
computes the future I/O demand of 
based on the 
cumulated past 
values. Once future I/O demands 
of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>weight w2 weight w3 Credit allocation CPM Monitoring</head><label></label><figDesc></figDesc><table>I/O 

bandwidth 

(MB/s) 
Max. read B/W 
Read B/W used 
Max. write B/W 
Write B/W used 

Container 

Block Layer 

Container 
Container 

weight w1 
Data flow 

Future I/O Demand 
Predictor 

Budget Distributor 

TotalCredit Updater 

Residual Credits 
Carryover 

TotalCredit 

Monitoring 
Monitoring 
Monitoring </table></figure>

			<note place="foot" n="1"> We used the block I/O trace replay tool [7] to generate I/O requests from the workloads in Table 1. (These traces are from UMass [8] and SNIA [9]). In our experiments, these workloads were executed by 12 concurrent threads with a queue depth of 32.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We would like to thank, Vijay Chidambaram, our shepherd, and anonymous reviewers for their valuable suggestions. Jihong Kim was supported by the National Research Foundation of Korea (NRF) grant funded by the Ministry of Science, ICT and Future Planning (MSIP) (NRF-2013R1A2A2A01068260), and the NextGeneration Information Computing Development Program through the NRF funded by the MSIP (NRF-2015M3C4A70656 45).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cgroups</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Linux containers and the future cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linux Journal</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Linux block IO: introducing multi-queue SSD access on multi-core systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bjørling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 6th Int. Systems and Storage Conf</title>
		<meeting>of the 6th Int. Systems and Storage Conf</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ssd</forename><surname>Samsung Xs1715 Nvme Pcie</surname></persName>
		</author>
		<ptr target="http://www.samsung.com/us/business/oem-solutions/pdfs/XS1715_ProdOverview_2014_October_v1" />
		<imprint>
			<date type="published" when="2014-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linux block IO -present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Axboe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Ottawa Linux Symp</title>
		<meeting>of Ottawa Linux Symp</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trace-Replay</surname></persName>
		</author>
		<ptr target="https://bitbucket.org/yongseokoh/trace-replay" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="http://skuld.cs.umass.edu/traces/storage/SPC-Traces.pdf" />
		<title level="m">UMass trace repository</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Migrating server storage to SSDs: analysis of tradeoffs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th ACM European Conf. on Computer Systems</title>
		<meeting>of the 4th ACM European Conf. on Computer Systems</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Method of probits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Bliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2037</biblScope>
			<biblScope unit="page" from="38" to="39" />
			<date type="published" when="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Performance virtualization for largescale storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Chambliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 22nd Int. Symp. on Reliable Distributed Systems</title>
		<meeting>of the 22nd Int. Symp. on Reliable Distributed Systems</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interposed proportional sharing for a storage service utility</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Int. Conf. on Measurement and Modeling of Computer Systems</title>
		<meeting>of Int. Conf. on Measurement and Modeling of Computer Systems</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Storage performance virtualization via throughput and latency control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 13th IEEE Int. Symp. on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems</title>
		<meeting>of 13th IEEE Int. Symp. on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards SLO complying SSDs through OPS isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 13th USENIX Conf. on File and Storage Technologies</title>
		<meeting>of the 13th USENIX Conf. on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Split-level I/O scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the</title>
		<meeting>of the</meeting>
		<imprint>
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<title level="m">Symp. on Operating Systems Principles</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
