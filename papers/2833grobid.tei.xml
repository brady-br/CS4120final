<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automatic Extraction of Parallel Speech Corpora from Dubbed Movies</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date type="published" when="2017-08-03">August 3, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp¨oktem</forename><forename type="middle">Alp¨</forename><surname>Alp¨oktem</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mireia</forename><surname>Farrús</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farr´</forename><surname>Farrús</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Wanner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universitat Pompeu Fabra</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Catalan Institute for Research and Advanced Studies (ICREA)</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Automatic Extraction of Parallel Speech Corpora from Dubbed Movies</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 10th Workshop on Building and Using Comparable Corpora</title>
						<meeting>the 10th Workshop on Building and Using Comparable Corpora <address><addrLine>Vancouver, Canada</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="31" to="35"/>
							<date type="published" when="2017-08-03">August 3, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper presents a methodology to extract parallel speech corpora based on any language pair from dubbed movies, together with an application framework in which some corresponding prosodic parameters are extracted. The obtained parallel corpora are especially suitable for speech-to-speech translation applications when a prosody transfer between source and target languages is desired.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The availability of large parallel corpora is one of the major challenges in developing translation systems. Bilingual corpora, which are needed to train statistical translation models, are harder to acquire than monolingual corpora since they presuppose the implication of labour in translation or interpretation. Working in the speech domain introduces even more difficulties since interpretations are not sufficient to capture the paralinguistic aspects of speech. Several attempts have been recently made to acquire spoken parallel corpora of considerable size. However, these corpora either do not reflect the prosodic aspects in the interpreted speech or do not carry the traits of natural speech. Or they simply do not align well the source and the target language sides.</p><p>To account for this deficit, we propose to exploit dubbed movies where expressive speech is readily available in multiple languages and their corresponding aligned scripts are easily accessible through subtitles. Movies and TV shows have been a good resource for collecting parallel bilingual data because of the availability and open access of subtitles in different languages. With 1850 bitexts of 65 languages, the OpenSubtitles project ( <ref type="bibr" target="#b9">Lison and Tiedemann, 2016</ref>) is the largest resource of translated movie subtitles compiled so far. The time information in subtitles makes it easy to align sentences of different languages since timing is correlated to the same audio <ref type="bibr" target="#b8">(Itamar and Itai, 2008)</ref>. In the presence of multiple aligned audio for the same movie, the alignment can be extended to obtain parallel speech corpora. Popular movies, TV shows and documentaries are released with dubbed audio in many countries. Dubbing requires the voice acting of the original speech in another language. Because of this, the dubbed speech carries more or less the same paralinguistic aspects of the original speech.</p><p>In what follows, we describe our methodology for the extraction of a speech parallel corpus based on any language pair from dubbed movies. Unlike <ref type="bibr" target="#b12">Tsiartas et al. (2011)</ref>, who propose a method based on machine learning for automatically extracting bilingual audio-subtitle pairs from movies, we only need raw movie data, and do not require any training. Moreover, our methodology ensures the fulfilment of the following requirements: (a) it is easily expandable, (b) it supports multiple pairs of languages, (c) it can handle any domain and speech style, and (d) it delivers a parallel spoken language corpus with annotated expressive speech. "Expressive speech" annotation means that the corpus is prosodically rich, which is essential to be able to deal with non-neutral speech emotions, as done in increasingly popular speech-to-speech translation applications that try to cope with prosody transfer between source and target utterances <ref type="bibr" target="#b0">(Agüero et al., 2006;</ref><ref type="bibr" target="#b11">Sridhar et al., 2008;</ref><ref type="bibr" target="#b3">Anumanchipalli et al., 2012)</ref>.</p><p>The remainder of the paper is structured as follows. Section 2 reviews the main multilingual parallel speech corpora available to the research community. Section 3 presents the methodology used in the current paper, and Section 4 discusses the current state of the obtained parallel corpora so far.</p><p>In Section 5, finally, some conclusions are drawn and some aspects of our future work in the context of parallel speech corpora are mentioned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Available Parallel Speech Corpora</head><p>As already mentioned above, several attempts have been made to compile large spoken parallel corpora. Such corpora of considerable size are, e.g., the EPIC corpus ( <ref type="bibr" target="#b4">Bendazzoli and Sandrelli, 2005</ref>), the EMIME Bilingual Database <ref type="bibr" target="#b14">(Wester, 2010)</ref>, and the Microsoft Speech Language Translation (MSLT) corpus <ref type="bibr" target="#b7">(Federmann and Lewis, 2016)</ref>. All of them have been manually compiled, and all of them show one or several shortcomings. The EPIC corpus, which has been compiled from speeches from the European Parliament and their interpretations, falls short in reflecting the prosodic aspects in the interpreted speech. The EMIME database is a compilation of prompted speeches and does not capture the natural spoken language traits. The MSLT corpus has been collected in bilingual conversation settings, but there is no one-to-one alignment between sentences in different languages. A summary of the available bilingual speech corpora is listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our multimodal parallel corpus creation consists of three main stages: (1) movie sentence segmentation, (2) prosodic parameter extraction, and (3) parallel sentence alignment. The first and second stages can be seen as a monolingual data creation, as they take the audio and subtitle pairs as input in one language, and output speech/text/prosodic parameters at the sentence level. The resulting monolingual data from stages 1 and 2 are fed into stage 3, where corresponding sentences are aligned and reordered to create the corresponding parallel data. A general overview of the system is presented in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Let us discuss each of these stages in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Segmentation of movie audio into sentences</head><p>This stage involves the extraction of audio and complete sentences from the original audio and the corresponding subtitles of the movie. For subtitles, the SubRip text file format <ref type="bibr">1</ref>  The sentence segmentation stage starts with a preprocessing step in which elements that do not correspond to speech are removed. These include: Speaker name markers (e.g., JAMES: . . . ), text formatting tags, non-verbal information (laughter, horn, etc.) and speech dashes. Audio is initially segmented according to the timestamps in subtitle entries, with extra 0.5 seconds at each end. Then, each audio segment and its respective subtitle text are sent to the speech aligner software (Vocapia Scribe 2 ) to detect word boundaries. This pre-segmentation helps to detect the times of the words that end with a sentence-ending punctuation mark ('.', '?', '!', ':', '...'). Average word boundary confidence score of the word alignment is used to determine whether the sentence will be extracted successfully or not. If the confidence score is above a threshold of 0.5, the initial segment is cut from occurrences of sentence-endings. In a second pass, cut segments that do not end with a sentenceending punctuation mark are merged with the subsequent segments to form full sentences. We used Libav 3 library to perform the audio cuts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prosodic parameter extraction</head><p>This stage involves prosodic parameter extraction for each sentence segment detected in stage 1. The ProsodyPro library (Xu, 2013) (a script developed for the Praat software <ref type="bibr" target="#b5">(Boersma and Weenink, 2001)</ref>  an utterance and a TextGrid file containing word boundaries and outputs a set of objective measurements suitable for statistical analysis. We run ProsodyPro for each audio and TextGrid pair of sentences to generate the prosodic analysis files. See <ref type="table">Table 2</ref> for the list of analyses performed by ProsodyPro (Information taken from ProsodyPro webpage 4 ). The TextGrid file with word boundaries is produced by sending the sentence audio and transcript to the word-aligner software and then converting the alignment information in XML into TextGrid format. Having word boundaries makes it possible to align continuous prosodic parameters (such as pitch contour) with the words in the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Parallel sentence alignment</head><p>This stage involves the creation of the parallel data from two monolingual data obtained from different audio and subtitle pairs of the same movie. The goal is to find the corresponding sentence s 2 in language 2, given a sentence s 1 in language 1. For each s 1 with timestamps (s s 1 , e s 1 ), s 2 is searched within a sliding window among sentences that start in the time interval [s s 1 -5, s s 1 + 5]. Among candidate sentences within the range, the most similar to s 1 is found by first translating s 1 to language 2 and then choosing the {s 1 , s 2 } pair that gives the best translation similarity measure above a certain threshold. For translation, the Yandex Translate API 5 and for similarity measure the Meteor library <ref type="bibr" target="#b6">(Denkowski and Lavie, 2014</ref>) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Obtained Corpus and Discussion</head><p>We have tested our methodology on three movies, which we retrieved from the University Library: The Man Who Knew Too Much (1956), Slow West (2015) and The Perfect Guy (2015). The movies are originally in English, but also have dubbed Spanish audio. English and Spanish subtitles were ProsodyPro output file Description rawf0</p><p>Raw f0 contour in Hz f0</p><p>Smoothed f0 with trimming algorithm (Hz) smoothf0</p><p>Smoothed f0 with triangular window (Hz) semitonef0 f0 contour in semitones samplef0 f0 values at fixed time intervals (Hz) f0velocity</p><p>First derivative of f0 means f0, intensity and velocity parameters (mean, max, min) for each word normtimef0</p><p>Constant number of f0 values for each word normtimeIntensity Constant number of intensity values for each word <ref type="table">Table 2</ref>: Some of the files generated by ProsodyPro.</p><p>acquired from the opensubtitles webpage 6 . At the time of the submission, we have automatically extracted 2603 sentences in English and 1963 sentences in Spanish summing up to 80 and 49 minutes of audio respectively and annotated with prosodic parameters. 1328 of these sentences were aligned to create our current parallel bilingual corpora. We are in the process of expanding our dataset.</p><p>Due to the copyright on the movies, we are unable to distribute the corpus that we extracted. However, using our software, it is easy for any researcher to compile a corpus on their own. For testing purposes, English and Spanish subtitles and audio of a small portion of the movie The Man Who Knew Too Much, as well as the parallel data extracted with this methodology are made available on the github page of the project.  parallel sentences obtained from the three movies so far. We observe that the number of Spanish sentences extracted in stage 2 is sometimes lower than the number of English sentences. This is mainly because of the translation difference between the Spanish subtitles and the dubbed Spanish audio. Subtitles in languages other than the original language of the movie do not always correspond with the transcript used in dubbing. If the audio and the text obtained from the subtitle do not match, the word aligner software performs poorly and that sentence is skipped. This results in fewer number of extracted sentences in dubbed languages of the movie. <ref type="table">Table 4</ref> shows more in detail the effect of this. Poor audio-text alignment results in loss of 15.0% of the sentences in original audio, whereas in dubbed audio this loss increases to 49.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Movie</head><p>Another major effect on detection of sentences is the background noise. This again interferes with the performance of the word aligner software. But since samples with less background noise is desired for a speech database, elimination of these samples is not considered as a problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>We have presented a methodology for the extraction of multimodal speech, text and prosody parallel corpora from dubbed movies. Movies contain large samples of conversational speech, which makes the obtained corpus especially useful for speech-to-speech translation applications. It is also useful for other research fields such as large comparative linguistic and prosodic studies.</p><p>As long as we have access to a matching pair of audio and subtitles of movies, the corpora obtained can be extended as a multilingual speech parallel corpora adaptable to any language pair. Moreover, it is an open-source tool and it can be adapted to any other prosodic feature extraction module in order to obtain a customized prosody parallel corpus for any specific application. The code to extract multilingual parallel corpora together with a processed sample movie excerpt is open source and available to use 7 under the GNU General Public License <ref type="bibr">8</ref> .</p><p>As future work, we plan to extend our corpus in size and make the parallel prosodic parameters available online. We also plan to replace the proprietary word aligner tool we are using with an open source alternative with better precision and speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Above: Monolingual corpus creation from different audio-subtitle pairs in parallel. Below: Bilingual parallel corpus creation of the example dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>) is used to extract prosodic features from speech. As input, ProsodyPro takes the audio of</figDesc><table>Corpus 

Languages 
Speech style 
EPIC 
English, Italian, Spanish 
spontaneous/interpreted 
MSLT 
English, French, German 
constrained conversations 
EMIME 
Finnish/English, German/English prompted 
EMIME Mandarin 
Mandarin/English 
prompted 
MDA (Almeman et al., 2013) 
Four Arabic dialects 
prompted 
Farsi-English (Melvin et al., 2004) Farsi/English 
read/semi-spontaneous 

Table 1: Some available parallel speech corpora. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 lists the number of monolingual and</head><label>3</label><figDesc></figDesc><table>6 https://www.opensubtitles.org/ 

</table></figure>

			<note place="foot" n="1"> https://www.matroska.org/technical/ specs/subtitles/srt.html</note>

			<note place="foot" n="2"> https://scribe.vocapia.com/ 3 https://libav.org/</note>

			<note place="foot" n="4"> http://www.homepages.ucl.ac.uk/ ˜ uclyyix/ProsodyPro/</note>

			<note place="foot" n="5"> https://tech.yandex.com/translate/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Alicia Burga for giving the initial idea of this work. This work is part of the KRISTINA project, which has received funding from the European Union's Horizon 2020 Research and Innovation Programme under the Grant Agreement number 645012. The second author is partially funded by the Spanish Ministry of Economy, Industry and Competitiveness through the Ramón y Cajal program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Prosody generation for speech-tospeech translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">D</forename><surname>Agüero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Adell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Bonafonte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="557" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi dialect Arabic speech parallel corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalid</forename><surname>Almeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Abdulrahman Almiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Communications, Signal Processing, and their Applications (ICCSPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intent transfer in speech-tospeech machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Gopala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><forename type="middle">C</forename><surname>Anumanchipalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology (SLT) Workshop. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="153" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An approach to corpus-based interpreting studies: developing EPIC (European Parliament Interpreting Corpus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Bendazzoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annalisa</forename><surname>Sandrelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MuTra 2005-Challenges of Multidimensional Translation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Praat, a system for doing phonetics by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weenink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glot International</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="341" to="345" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL Workshop on Statistical Machine Translation</title>
		<meeting>the EACL Workshop on Statistical Machine Translation</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Microsoft Speech Language Translation (MSLT) corpus: The IWSLT 2016 release for English, French and German</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Language Translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using movie subtitles for creating a large-scale bilingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Einav</forename><surname>Itamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Itai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Lison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Language Resources and Evaluation (LREC 2016)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="923" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Creation of a doctor-patient dialogue corpus using standardized patients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">S</forename><surname>Melvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Win</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shrikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panayiotis</forename><forename type="middle">G</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadi</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganjavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Language Resources and Evaluation (LREC)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Factored translation models for enriching spoken language translation with prosody</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kumar Rangarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivas</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Bangalore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2723" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bilingual audio-subtitle extraction using automatic segmentation of movie audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Tsiartas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanta</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Panayiotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth</forename><surname>Georgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="5624" to="5627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The EMIME Bilingual Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirjam</forename><surname>Wester</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>The University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ProsodyPro -A Tool for Large-scale Systematic Prosody Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Resources for the Analysis of Speech Prosody (TRASP 2013)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
