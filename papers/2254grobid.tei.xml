<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:32+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multipath Transport for Virtual Private Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lukaszewski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science Naval Postgraduate School</orgName>
								<orgName type="institution">Naval Postgraduate School</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">G</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science Naval Postgraduate School</orgName>
								<orgName type="institution">Naval Postgraduate School</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multipath Transport for Virtual Private Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>An important class of virtual private networks (VPNs) builds secure tunnels at the transport layer leveraging TCP or UDP. Multipath TCP (MPTCP), an ongoing IETF effort that has been adopted into Linux and iOS, extends TCP to allow data to be delivered over multiple network interfaces and paths simultaneously. In this paper, using a testbed that can emulate a range of path characteristics between the VPN end points, we first empirically quantify the potential of using MPTCP tunnels to increase the goodput of VPN communications when multiple data paths are available. We further design and implement a preliminary version of Multipath UDP (MPUDP) to address the adverse effect of the duplicated congestion control actions that is known with a TCP-in-TCP tunnel. We observe that a severe asymmetry of path delays may cause an excessive amount of packet reordering at the receiving end and consequently degrade the overall performance of TCP-in-MPUDP tunnels. Moreover, we find that a packet scheduler capable of tracking path delays and allocating more packets to path(s) with shorter delay(s) to be an effective and relatively lightweight solution for MPUDP, instead of an elaborate data sequencing mechanism like the one used by MPTCP.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Virtual Private Networks (VPNs) are important to enterprises and government agencies as they provide secure data communication at reasonable costs by leveraging public or third-party network infrastructures. An important class of VPNs builds secure tunnels at the transport layer leveraging TCP or UDP. Several widely deployed open source software systems such as <ref type="bibr">OpenVPN [4]</ref> are of this kind.</p><p>As computer hosts and mobile devices are increasingly multi-homed, the Internet Engineering Task Force (IETF) recently proposed a multipath TCP (MPTCP) extension (i.e. RFC 6824) to leverage the additional data paths to increase throughput and support seamless mobility. Essentially, MPTCP enables one or more TCP connections (called subflows) to be added to a primary TCP connection between two MPTCP capable end hosts. Each subflow is added using the standard three-way handshake along with a new TCP header option that has been created to facilitate the creation of subflows as well as identification of such packets at the receiving end.</p><p>In addition, MPTCP performs four major functions. First, it incorporates a data sequence signal (DSS) mechanism to map the sequence numbers of individual subflows into an overall master sequence number space at the receiving end. Second, it employs a path manager to identify available paths (i.e. socket pairs with distinct IP address pairs or source ports) for creating new subflows. Third, it runs a packet scheduler to distribute packets among the active subflows. The default scheduler sends more packets on paths with shorter round trip times (RTTs) <ref type="bibr" target="#b11">[12]</ref>. Lastly, it introduces several new congestion control algorithms to ensure that multiple subflows of a same MPTCP connection will not be too aggressive (i.e. take an unfair share of bandwidth) when sharing a bottleneck link with regular TCP flows.</p><p>Several MPTCP implementations have been created <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b15">16]</ref>; the most frequently deployed being those created for the Linux kernel and Apple iOS. Given this availability, we have built an emulation testbed to evaluate the potential of using multipath VPN tunnels to increase the performance of VPN communications when multiple data paths are available from the VPN client to the VPN server. The evaluation focused on utilizing TCP services (e.g. file downloads and database transactions) over VPN tunnels as these are the typical tasks performed by a VPN user.</p><p>There is a well-known TCP-in-TCP effect <ref type="bibr" target="#b13">[14]</ref>, where the application throughput may be unnecessarily degraded due to duplicated congestion control actions of the two TCP connections involved. Therefore, in addition to an evaluation of MPTCP, this paper describes our design and evaluation of a MPUDP prototype. More specifically, our contributions are as follows:</p><p>1. We conducted an empirical performance evaluation of MPTCP specific to VPN scenarios. The results show that MPTCP can leverage an additional data path between the VPN end points to increase the application performance for a wide range of packet loss and propagation delay configurations of the two paths.</p><p>2. We designed and implemented basic MPUDP functionality into the Linux transport layer and evaluated its performance using the same VPN scenarios. The results show that by using a packet scheduler that allocates more packets to the path with the shorter RTT, the relatively lightweight MPUDP can outperform MPTCP even when the paths have a high degree of asymmetry in terms of their propagation delays.</p><p>The rest of the paper is organized as follows. Section 2 discusses related work and we present the evaluation of MPTCP tunnels in Section 3. The design and evaluation of MPUDP is described in Section 4, followed by a short discussion of limitations and future work in Section 5. Finally, Section 6 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a large collection of empirical studies (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>) done by the MPTCP designers and other researchers to understand and improve the performance of MPTCP. Specific to VPN scenarios, Boccassi et al. developed a system called Binder <ref type="bibr" target="#b0">[1]</ref>, which uses two OpenVPN gateways and an MPTCP tunnel between them to aggregate the throughput of multiple data paths within the core of a network. Our evaluation of potential VPN performance boosts from MPTCP is complementary to these prior studies. We additionally conduct experiments with a bursty packet loss model <ref type="bibr" target="#b12">[13]</ref> to understand the relative performance of MPTCP under more stressful conditions where network congestion may cause consecutive packet losses to a user connection.</p><p>In comparison, there is little work on MPUDP we can find from the open literature. The most relevant study known to us is a system called Multipath Mobile Shell (MOSH) <ref type="bibr" target="#b2">[3]</ref>, which allows two communicating end hosts to set up multiple UDP connections between them. Due to its signaling with application specific messages, the system is not a general transport layer solution like the one presented in this paper.</p><p>Additionally, there are several studies covering bandwidth aggregation techniques at the transport layer for mobile hosts (e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>). Unlike the approach of MPTCP, these solutions require modifications of applications. However, it should be noted that some of the path monitoring mechanisms they employ may be useful for further development of MPUDP.</p><p>Finally, the adverse effects of using a TCP over TCP connection is well documented. Olaf Titz <ref type="bibr" target="#b13">[14]</ref> in the early 2000's explained how the outer TCP layer, the tunnel, may have a shorter retransmission timeout (RTO) value than the inner TCP layer, which may trigger retransmissions earlier than necessary and ultimately cause the connection throughput to degrade significantly. Our work provides evidence that MPTCP tunnels may suffer from similar effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation of MPTCP Tunnels</head><p>We first ran a series of experiments to validate the reported performance benefits of MPTCP, and more importantly, to obtain baseline data for comparison with MPUDP. To achieve a comprehensive and reproducible performance evaluation, we decided to use an emulation testbed, with which we can test production VPN and MPTCP software implementations that generate real packets and at the same time, have fine-grain control over the performance characteristics of data paths. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the testbed consists of five nodes, which are 1.86GHz dual-core desktop computers with 4GB of RAM and running the Ubuntu 14.04 version of Linux. Only the tunnel end points, i.e. the "VPN Client" and "VPN Server" nodes, are MPTCP enabled with the version 0.90 of the MPTCP software distribution, and additionally, these nodes are installed with the OpenVPN 2.3.12 software. The "Web Server" node acts as an enterprise intranet resource for the remote VPN client and is configured to run the Apache2 web server software. 10-Mbps Ethernet links are used for physical node to node connections between the VPN end points to mimic the typical range of WiFi and cellular data rates, while a Gigabit Ethernet link is used to connect the "VPN Server" and "Web Server" nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Testbed Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Path Configurations</head><p>A wide range of performance characteristics of the primary and secondary data paths, in terms of the round trip time (RTT) of signal propagation and packet loss rate, can be precisely controlled by running the Linux traffic control command tc <ref type="bibr" target="#b7">[8]</ref> with appropriate parameters on the "Primary Link" and "Secondary Link" nodes. Specifically, the following configurations are used for the experiments reported in this paper.</p><p>Packet loss rate: We used two random models to simulate packet losses on the data paths: (i) uniform perpacket loss probability, which has been extensively used in prior work, and (ii) Gilbert-Elliot (GE) packet loss model <ref type="bibr" target="#b12">[13]</ref>, which we believe can capture more accurately the likely occurrences of bursty packet loss events on the Internet due to router buffer overflows. The GE model uses two states: a Gap mode and a Burst mode, and it is customizable with four parameters (p, r, k, h) as follows. The model starts in the Gap mode and will shift between the Gap and Burst mode with probability p and r, respectively, after processing each packet. It will drop a packet with probability 1 − k and 1 − h while in the Gap and Burst mode, respectively, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We have estimated the overall packet loss rates for various (p, r, k, h) combinations by collecting 100,000 ping statistics over ten 10,000 ping intervals for each combination. From the results, we decided to fix three of the parameters as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref> and vary only (1 − k) to control the overall loss rate for the GE model while ensuring that most of the loss bursts (&gt; 90%) consist of no more than two packets as one would expect for one TCP or MPTCP flow during typical network congestion. <ref type="table" target="#tab_0">Table 1</ref> shows the set of (1 − k) values used in our GE model experiments and their corresponding observed overall loss rates.  It is often the case that the multiple packet forwarding paths available for a remote VPN client to reach its home VPN server have different RTTs as these paths typically are provided by different network operators. For this reason, we decided to also evaluate the performance of MPTCP tunnels in the presence of asymmetric path conditions. For the asymmetric tests, both the primary and secondary paths were configured with the same loss rates, but different RTT values. More specifically, we experimented with five primary to secondary path RTT ratios. <ref type="table" target="#tab_1">Table 2</ref> shows these primary and secondary path RTT values and their ratios. MPTCP path managers: The 0.90 version of MPTCP software comes with two path managers for controlling the creation of subflows over available data paths. The Fullmesh path manager is designed to explore all paths between two hosts by attempting to connect each interface of one host to each interface of the other <ref type="bibr" target="#b1">[2]</ref>. By default, it creates one subflow per path; this value can be increased with additional configuration.</p><p>The Ndiffports path manager is designed to "exploit the equal costs multiple paths that are available in a data center" <ref type="bibr" target="#b1">[2]</ref>. Additionally, it allows nodes with only one interface to utilize MPTCP by creating multiple subflows for one path (i.e. the same source and destination IP address pair) through the use of different source port numbers. By default, it creates two subflows per path; this value can be increased with additional configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Test Scenario and Performance Metric</head><p>We modeled the typical VPN scenario where a remote user establishes a VPN connection with a home VPN server and then downloads data from an intranet web server. Specifically, for each experiment, we first set up the VPN tunnel with a specific transport protocol and path configuration, and then ran the wget utility on the "VPN Client" node to download a 16-MB file from the "Web Server" 20 times.</p><p>As the tunnel carries standard TCP traffic with built-in congestion control, we use the average goodput as the main performance metric and skip other common transport layer issues such as fairness of bandwidth sharing. The goodput of a successful download was calculated by dividing the file size (i.e. 16 MB) by the total time required for the download. The average goodput and 95% confidence interval for each experiment were then derived from the 20 goodput samples.</p><p>It should be noted that the VPN tunnel was established without the use of compression or encryption to allow for ease of data collection and analysis. Furthermore, after preliminary tests and survey of guidelines from various sources regarding TCP performance tuning, we decided to optimize some of the TCP specific OS parameters for our specific testbed using the sysctl command. The results are illustrated in <ref type="table" target="#tab_2">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MPTCP over Symmetric Paths</head><p>Our first set of experiments over symmetric paths was aimed to compare the Cubic and BALIA congestion control algorithms. The default TCP congestion control algorithm used in Linux is Cubic. Cubic does not key on ACK messages to adjust the congestion window. Instead, Cubic will quickly increase the TCP congestion window to a threshold level and then probe for additional bandwidth that may be available. This form of congestion control has been shown to be optimal for single paths with high latency. However, prior work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> reports that when Cubic is used with MPTCP and there is a shared bottleneck with other TCP connections, the MPTCP user is able to obtain a larger share of the bottleneck's bandwidth. As one of the solutions to this problem, the BALIA algorithm <ref type="bibr" target="#b14">[15]</ref> has been developed. <ref type="figure" target="#fig_2">Figure 3</ref> shows the performance of the two algorithms under a uniform packet loss rate of 0.1%. (The trend is similar with no packet loss or other loss rate settings.) The results confirm that Cubic is much more aggressive than BALIA in terms of bandwidth usage. Given Cubic's known unfriendly behavior toward regular TCP connections <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, we used BALIA exclusively for the rest of the experiments. We introduced the GE packet loss model in the next set of experiments. <ref type="figure" target="#fig_4">Figure 4</ref> shows the results under 1% packet losses. As expected, the fullmesh MPTCP VPN tunnel configuration shows a noticeable improvement over the single path TCP tunnel. This improvement becomes less significant as the path propagation RTTs are increased. Surprisingly, MPTCP performed better under the GE bursty loss model. We believe this is due to the TCP fast recovery mechanism (RFC 2001), which halves the congestion window only once upon consecutive packet losses. It is also interesting to note the performance of the ndiffports MPTCP path manager. Some MPTCP developers consider this path manager to have little practical use <ref type="bibr" target="#b1">[2]</ref>, but our results show that using ndiffports can improve the goodput experienced. Finally, the relative performance trend seen in <ref type="figure" target="#fig_4">Figure 4</ref> remains much the same for different packet loss rate settings. We omit those plots for brevity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MPTCP over Asymmetric Paths</head><p>In this set of experiments, we evaluated only the fullmesh path manager. The ndiffports path manager would not make sense for asymmetric testing since it is designed for hosts with a single interface. The packet loss rates were controlled using the GE model according to <ref type="table" target="#tab_0">Table 1</ref>, and the path RTTs were varied according to <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The results are shown in <ref type="figure">Figure 5</ref>. As expected, the best performance was obtained with the path RTT ratio of 1:1. Interestingly, the performance degradations due to path asymmetry, while noticeable, did not worsen as the degree of asymmetry was increased. The results show that MPTCP's built in mechanisms for handling path asymmetry <ref type="bibr" target="#b10">[11]</ref> were largely effective in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design and Evaluation of MPUDP</head><p>The previous section demonstrated the potential VPN performance gains from using a MPTCP tunnel when multiple physical network paths are available. However, as MPTCP applies standard TCP congestion con- <ref type="figure">Figure 5</ref>: MPTCP performance over asymmetric paths under the GE packet loss model trol for each subflow, we hypothesized that the use of MPTCP will face the same adverse effects of nested and often excessive congestion control as in typical TCP over TCP settings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14]</ref>. To confirm such effects, using the physical testbed and symmetric link test procedures from Section 3, we compared single path UDP and TCP tunnel performance. <ref type="figure" target="#fig_5">Figure 6</ref> plots the percent of goodput increase by a TCP-in-UDP VPN file download relative to that of a baseline TCP-in-TCP connection for a spectrum of RTT and packet loss rate combinations. The results clearly show the performance advantages of using UDP. For this reason the default transport protocol for various VPN software is set to UDP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adding MPUDP to Linux with Loadable Kernel Modules</head><p>In order to evaluate our hypothesis, we have chosen to design and implement basic MPUDP functionality into one of the latest MPTCP capable Linux kernels (i.e. v. 90). While it is feasible to add this functionality by revising the core Linux networking kernel code, as has been done for MPTCP <ref type="bibr" target="#b10">[11]</ref>, to expedite debugging and testing, we decided to implement MPUDP primarily as loadable kernel modules (LKMs) as doing so requires a minimum amount of kernel recompilation. Specifically, we have created two LKMs in C, with source files named MPUDP send.c and MPUDP recv.c respectively. We have also added 6 lines of code 1 to the Linux kernel implementation of UDP (i.e. udp.c) for invoking functions defined in the LKMs as required. The main function of the send side LKM identifies an additional outgoing interface to support a new subflow and splits traffic in a 50/50 probabalistic fashion (i.e. 1:1) among the two outgoing interfaces while the function of the receive side LKM coalesces the two subflows into a single input stream of packets to the destination UDP socket used by the VPN tunnel. More details of our MPUDP prototype are described in reference <ref type="bibr" target="#b8">[9]</ref>, which includes the design of a general method for the two tunnel end points to enable MPUDP and agree on the set of interfaces to leverage for multipath communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>To evaluate the performance of MPUDP vs. MPTCP, we first performed file downloads using each in symmetric link settings with zero simulated packet losses. For additional comparison, we also performed single path UDP downloads under the same conditions. The results are plotted in <ref type="figure" target="#fig_6">Figure 7</ref>. They show that MPUDP performs better than MPTCP and the difference becomes more significant as the RTT increases. This confirms our hypothesis that MPTCP is prone to TCP-in-TCP performance issue; in this case, the TCP subflows are slow to grab the available bandwidth due to the TCP slow start and congestion avoidance algorithms. To better see the effect of TCP-in-MPTCP, we examine the 40ms RTT test of <ref type="figure" target="#fig_6">Figure 7</ref> more closely using packet traces collected at the web server. We randomly select one trace each for MPTCP and MPUDP and use Wireshark to drill down to the evolution of the send sequence number (blue curve), acknowledge number (brown curve) and expected received byte count (green curve) during the first 0.3 seconds of the downloads. <ref type="figure">Figure 8</ref> illustrates that the MPTCP tunnel was much slower than its MPUDP counterpart in grabbing the available bandwidth. Next, we introduced packet losses into the evaluation. We used two types of loss distributions: (i) uniform, and (ii) GE burst model, as done in Section 3. The representative results are plotted in <ref type="figure" target="#fig_8">Figure 9</ref>. They show that with packet losses, MPUDP still outperformed MPTCP; however, the advantage is not as significant because the redundant congestion control with MPTCP likely had some positive effect in reducing packet retransmissions.</p><p>Last, our MPUDP implementation does not have any mechanism to re-sequence packets from multiple subflows while MPTCP has a complex method to ensure packets to be in order at the receiver. To understand the performance impact of this design difference, we repeated the downloads with asymmetric link RTTs that should exacerbate the packet reordering problem. We experimented with each of the five RTT configurations as given in <ref type="table" target="#tab_1">Table 2</ref> under four different packet loss rates as given in <ref type="table" target="#tab_0">Table 1</ref>. We illustrate the results in <ref type="figure" target="#fig_0">Figure 10</ref>, where the percentage of goodput change from MPTCP to MPUDP is plotted. It is clear that our MPUDP implementation, which remains lightweight like the standard UDP -by not having a re-sequencing mechanism and splitting traffic using a probabilistic 50/50 method, can perform poorly relative to MPTCP when the paths have different delay characteristics. As expected, the effect was more pronounced for a higher degree of link delay asymmetry. Interestingly, the relative MPUDP performance improved as the loss rate increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Path Aware Traffic Splitting</head><p>One observation from <ref type="figure" target="#fig_0">Figure 10</ref> is that the packet losses didn't seem to compound the effect of packet re-ordering as one might expect. That led us to a hypothesis that a path aware traffic splitting method, i.e. one that takes the path RTT difference into consideration might mitigate to a large extent the negative effect of asymmetric link delays on MPUDP performance.</p><p>To evaluate this hypothesis, we revised the function of the send LKM to support custom traffic split ratio (i.e. x%/(100−x)%), and then repeated the downloads of <ref type="figure" target="#fig_0">Figure 10</ref> using two new splits: 75%/25% and 90%/10%. The results are plotted in <ref type="figure" target="#fig_0">Figure 11</ref>. They show that by shifting traffic away from the link of larger RTT, the relative performance of MPUDP improved.</p><p>Therefore, if the MPUDP sender has the ability to estimate the RTT differences among the available paths, it should split traffic according to the RTT estimates, proportionally putting more packets on the faster path(s). One method to acquire such estimates might be to selectively add timestamps to a subset of packets on each path and have the receiver echo back these timestamps. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this section, we discuss additional performance factors beyond the scope of our evaluation. We also identify some limitations of our study and pertinent future work.</p><p>First, from <ref type="figure" target="#fig_6">Figures 7, 9</ref>, and other relevant data, we observe that the performance of MPTCP, MPUDP and UDP all declined sharply for paths with moderate to long RTTs (20ms to 100ms) and low to moderate loss rates (0.1% to 3%). One can hypothesize that a forward error correction (FEC) scheme might improve the overall goodput in these settings. The main trade-off would be the communication and computational overhead incurred for providing FEC.</p><p>Second, our experiments with asymmetric link configurations did not include asymmetric link transmission rates. As discussed in Section 4.2.1, it may be important for MPUDP to be informed of such rate differences in order to maximize performance. Therefore, the question of how to maintain up to date knowledge of individual path performance characteristics should be a fertile ground for future work on MPUDP.</p><p>Last, some enterprise applications use UDP, and typically their performance concern is not goodput, but message timing and/or fair sharing of bandwidth with other applications. While this paper focuses on TCP as the end-to-end transport protocol, it will be beneficial to study the impact of multipath VPN transport on UDP applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The empirical results presented in this paper provide strong evidence that MPTCP can increase the performance of TCP applications over a VPN tunnel when multiple data paths are available between the VPN end points. Consistent performance gains have been observed for a wide spectrum of path characteristics in terms of packet losses, propagation delays, and path asymmetry.</p><p>The additional performance gains from the basic MPUDP functionality, which is the case even in the presence of path asymmetry, is both surprising and encouraging, given the design's simplicity and relatively weak assumption of a path-aware packet scheduler. We believe more studies are necessary to understand the TCPin-MPUDP dynamics and find a good balance point between controlling complexity and maximizing performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multipath emulation testbed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Gilbert-Elliot model in action, with three of the parameters fixed: p = 1%, r = 75%, and 1 − h = 50%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance of Cubic vs. BALIA congestion control under 0.1% uniform packet losses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of MPTCP vs. single path TCP under 1% packet losses</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Goodput increase by using UDP vs. TCP for a single path VPN tunnel with uniform packet loss rates. Error bars show 95% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of MPUDP vs. MPTCP and single path UDP under zero packet loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>Figure 8: Close up analysis of MPTCP and MPUDP performance with path RTT = 40ms</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of MPUDP vs. MPTCP and single path UDP under packet losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Percent of goodput change from MPTCP to MPUDP under different link RTT ratios and loss rates. A positive (negative) percentage value indicates an increase (decrease) of performance with MPUDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>(</head><label></label><figDesc>Figure 11: MPUDP relative performance improved by sending less traffic on the path of larger RTT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>GE Model Configuration 

1 − k Observed Packet Loss Rate 
0% 
1% 
0.5% 
2% 
1% 
3% 
3% 
7% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 : Path RTTs Used in Asymmetric Tests</head><label>2</label><figDesc></figDesc><table>RTT Ratio Primary Path Secondary Path 
1:1 
20ms 
20ms 
1:2 
20ms 
40ms 
1:3 
20ms 
60ms 
1:4 
20ms 
80ms 
1:5 
20ms 
100ms 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 : Baseline TCP Configuration</head><label>3</label><figDesc></figDesc><table>TCP specific OS Parameters Value 
net.ipv4.tcp congestion control Cubic or BALIA 
net.ipv4.tcp rmem 
4096, 87380, 
(min, default, max) 
6291456 
net.ipv4.tcp wmem 
4096, 16384, 
(min, default, max) 
4194304 
net.core.rmem max 
212992 
net.core.rmem default 
212992 
net.core.wmem max 
212992 
net.core.wmem default 
212992 
net.ipv4.tcp no metrics save 
Enabled 

</table></figure>

			<note place="foot" n="1"> The code of our implementation is available at Github: https://github.com/danluke2/mpudp vpn thesis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Henry Foster and anonymous reviewers for their helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A system to aggregate multiple Internet gateways in community networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boccassi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marina</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MobiCom Workshop on Lowest Cost Denominator Networking for Universal Access</title>
		<meeting>ACM MobiCom Workshop on Lowest Cost Denominator Networking for Universal Access</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Blog entry: Recommended Multipath TCP configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonaventure</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">User-space Multipath UDP in MOSH</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boutier</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chroboczek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Beginning OpenVPN 2. 0. 9: Build and Integrate Virtual Private Networks Using OpenVPN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feilner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graf</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Packt Publishing Ltd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding TCP over TCP: Effects of TCP tunneling on end-to-end throughput and latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honda</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ohsaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Imase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murayama</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6011</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ptcp: An end-to-end transport layer protocol for striped connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsieh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivakumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network Protocols, 2002. Proceedings. 10th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MPTCP is not pareto-optimal: performance issues and a possible solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khalili</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gast</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Popovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Boudec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. on Emerging networking experiments and technologies</title>
		<meeting>ACM Int. Conf. on Emerging networking experiments and technologies</meeting>
		<imprint>
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linux traffic control (tc)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuznetsov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename></persName>
		</author>
		<ptr target="https://linux.die.net/man/8/tc" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Multipath transport for virtual private networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukaszewski</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>Naval Postgraduate School</orgName>
		</respStmt>
	</monogr>
<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transport level mechanisms for bandwidth aggregation on mobile hosts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magalhaes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kravets</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="165" to="171" />
		</imprint>
	</monogr>
	<note>Network Protocols</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improving Multipath TCP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paasch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014-11" />
			<pubPlace>Belgium</pubPlace>
		</imprint>
		<respStmt>
			<orgName>UC Louvain</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multipath TCP in the Linux kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paasch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barre</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017-01-15" />
		</imprint>
	</monogr>
	<note>Accessed</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Gilbert-Elliot bit error model and the efficient use in packet level simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willig</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
		<respStmt>
			<orgName>Technical University Berlin</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Why TCP over TCP is a bad idea</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titz</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<ptr target="http://sites.inka.de/bigred/devel/tcp-tcp.html.Accessed" />
		<imprint>
			<date type="published" when="2017-01-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Balanced linked adaptation congestion control algorithm for MPTCP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016-01" />
			<pubPlace>Internet Engineering Task Force</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Internet-draft</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design, implementation and evaluation of congestion control for multipath TCP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wischik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raiciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Greenhalgh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And Hand-Ley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX conference on Networked systems design and implementation</title>
		<meeting>USENIX conference on Networked systems design and implementation</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="99" to="112" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
