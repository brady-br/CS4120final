<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stable and Consistent Membership at Scale with Rapid Stable and Consistent Membership at Scale with Rapid</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 11-13. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahlia</forename><surname>Malkhi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Gopalan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vmware</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lalith</forename><surname>Suresh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahlia</forename><surname>Malkhi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Gopalan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Porto</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">One Concern</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carreiro</forename><forename type="middle">⋄</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Lokhandwala</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vmware</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vmware</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Ivan Porto Carreiro</orgName>
								<orgName type="institution" key="instit2">One Concern; Zeeshan Lokhandwala</orgName>
								<address>
									<country>VMware</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stable and Consistent Membership at Scale with Rapid Stable and Consistent Membership at Scale with Rapid</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18)</title>
						<meeting>the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18) <address><addrLine>Boston, MA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 11-13. 2018</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 2018 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc18/presentation/suresh This paper is included in the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present the design and evaluation of Rapid, a distributed membership service. At Rapid&apos;s core is a scheme for multi-process cut detection (CD) that revolves around two key insights: (i) it suspects a failure of a process only after alerts arrive from multiple sources, and (ii) when a group of processes experience problems, it detects failures of the entire group, rather than conclude about each process individually. Implementing these insights translates into a simple membership algorithm with low communication overhead. We present evidence that our strategy suffices to drive unanimous detection almost-everywhere, even when complex network conditions arise, such as one-way reachability problems, firewall misconfigurations, and high packet loss. Furthermore, we present both empirical evidence and analyses that proves that the almost-everywhere detection happens with high probability. To complete the design, Rapid contains a leaderless consensus protocol that converts multi-process cut detections into a view-change decision. The resulting membership service works both in fully decentralized as well as logically centralized modes. We present an evaluation of Rapid in moderately scal-able cloud settings. Rapid bootstraps 2000 node clusters 2-5.8x faster than prevailing tools such as Memberlist and ZooKeeper, remains stable in face of complex failure scenarios, and provides strong consistency guarantees. It is easy to integrate Rapid into existing distributed applications , of which we demonstrate two.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large-scale distributed systems today need to be provisioned and resized quickly according to changing demand. Furthermore, at scale, failures are not the exception but the norm <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b28">30]</ref>. This makes membership management and failure detection a critical component of any distributed system.</p><p>Our organization ships standalone products that we do not operate ourselves. These products run in a wide range of enterprise data center environments. In our experience, many failure scenarios are not always crash failures, but commonly involve misconfigured firewalls, one-way connectivity loss, flip-flops in reachability, and some-but-not-all packets being dropped (in line with observations by <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b39">41]</ref>). We find that existing membership solutions struggle with these common failure scenarios, despite being able to cleanly detect crash faults. In particular, existing tools take long to, or never converge to, a stable state where the faulty processes are removed ( §2.1).</p><p>We posit that despite several decades of research and production systems, stability and consistency of existing membership maintenance technologies remains a challenge. In this paper, we present the design and implementation of Rapid, a scalable, distributed membership system that provides both these properties. We discuss the need for these properties below, and present a formal treatment of the service guarantees we require in §3.</p><p>Need for stability. Membership changes in distributed systems trigger expensive recovery operations such as failovers and data migrations. Unstable and flapping membership views therefore cause applications to repeatedly trigger these recovery workflows, thereby severely degrading performance and affecting service availability. This was the case in several production incidents reported in the Cassandra <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b7">9]</ref> and Consul <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b25">27]</ref> projects. In an end-to-end experiment, we also observed a 32% increase in throughput when replacing a native system's failure detector with our solution that improved stability (see §7 for details).</p><p>Furthermore, failure recovery mechanisms may be faulty themselves and can cause catastrophic failures when they run amok <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b38">40]</ref>. Failure recovery workflows being triggered ad infinitum have led to Amazon EC2 outages <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6,</ref><ref type="bibr" target="#b2">4]</ref>, Microsoft Azure outages <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b46">48]</ref>, and "killer bugs" in Cassandra and HBase <ref type="bibr" target="#b37">[39]</ref>.</p><p>Given these reasons, we seek to avoid frequent oscillations of the membership view, which we achieve through stable failure detection.</p><p>Need for consistent membership views. Many systems require coordinated failure recovery, for example, to correctly handle data re-balancing in storage systems <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b26">28]</ref>. Consistent changes to the membership view simplify reasoning about system behavior and the development of dynamic reconfiguration mechanisms <ref type="bibr" target="#b63">[65]</ref>.</p><p>Conversely, it is challenging to build reliable clustered services on top of a weakly consistent membership ser-vice <ref type="bibr" target="#b9">[11]</ref>. Inconsistent view-changes may have detrimental effects. For example, in sharded systems that rely on consistent hashing, an inconsistent view of the cluster leads to clients directing requests to servers that do not host the relevant keys <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b1">3]</ref>. In Cassandra, the lack of consistent membership causes nodes to duplicate data re-balancing efforts when concurrently adding nodes to a cluster <ref type="bibr" target="#b9">[11]</ref> and also affects correctness <ref type="bibr" target="#b10">[12]</ref>. To work around the lack of consistent membership, Cassandra ensures that only a single node is joining the cluster at any given point in time, and operators are advised to wait at least two minutes between adding each new node to a cluster <ref type="bibr" target="#b9">[11]</ref>. As a consequence, bootstrapping a 100 node Cassandra cluster takes three hours and twenty minutes, thereby significantly slowing down provisioning <ref type="bibr" target="#b9">[11]</ref>.</p><p>For these reasons, we seek to provide strict consistency, where membership changes are driven by agreement among processes. Consistency adds a layer of safety above the failure detection layer and guarantees the same membership view to all non-faulty processes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our approach</head><p>Rapid is based on the following fundamental insights that bring stability and consistency to both decentralized and logically centralized membership services:</p><p>Expander-based monitoring edge overlay. To scale monitoring load, Rapid organizes a set of processes (a configuration) into a stable failure detection topology comprising observers that monitor and disseminate reports about their communication edges to their subjects. The monitoring relationships between processes forms a directed expander graph with strong connectivity properties, which ensures with a high probability that healthy processes detect failures. We interpret multiple reports about a subject's edges as a high-fidelity signal that the subject is faulty.</p><p>Multi-process cut detection. For stability, processes in Rapid (i) suspect a faulty process p only upon receiving alerts from multiple observers of p, and (ii) delay acting on alerts about different processes until the churn stabilizes, thereby converging to detect a global, possibly multi-node cut of processes to add or remove from the membership. This filter is remarkably simple to implement, yet it suffices by itself to achieve almosteverywhere agreement -unanimity among a large fraction of processes about the detected cut.</p><p>Practical consensus. For consistency, we show that converting almost-everywhere agreement into full agreement is practical even in large-scale settings. Rapid's consensus protocol drives configuration changes by a low-overhead, leaderless protocol in the common case: every process simply validates consensus by counting the number of identical cut detections. If there is a quorum containing three-quarters of the membership set with the same cut, then without a leader or further communication, this is a safe consensus decision.</p><p>Rapid thereby ensures all participating processes see a strongly consistent sequence of membership changes to the cluster, while ensuring that the system is stable in the face of a diverse range of failure scenarios.</p><p>In summary, we make the following key contributions:</p><p>• Through measurements, we demonstrate that prevailing membership solutions guarantee neither stability nor consistency in the face of complex failure scenarios.</p><p>• We present the design of Rapid, a scalable membership service that is robust in the presence of diverse failure scenarios while providing strong consistency. Rapid runs both as a decentralized as well as a logically centralized membership service.</p><p>• In system evaluations, we demonstrate how Rapid, despite offering much stronger guarantees, brings up 2000 node clusters 2-5.8x faster than mature alternatives such as Memberlist and ZooKeeper. We demonstrate Rapid's robustness in the face of different failure scenarios such as simultaneous node crashes, asymmetric network partitions and heavy packet loss. Rapid achieves these goals at a similar cost to existing solutions.</p><p>• Lastly, we report on our experience running Rapid to power two applications; a distributed transactional data platform and a service discovery use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation and Related work</head><p>Membership solutions today fall into two categories. They are either managed for a cluster through an auxiliary service <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b41">43]</ref>, or they are gossip-based and fully decentralized <ref type="bibr" target="#b43">[45,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b67">69,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b62">64]</ref>.</p><p>We studied how three widely adopted systems behave in the presence of network failure scenarios: (i) of the first category, ZooKeeper <ref type="bibr" target="#b13">[15]</ref>, and of the second, (ii) Memberlist <ref type="bibr" target="#b45">[47]</ref>, the membership library used by Consul <ref type="bibr" target="#b43">[45]</ref> and Serf <ref type="bibr" target="#b42">[44]</ref> and (iii) Akka Cluster <ref type="bibr" target="#b67">[69]</ref> (see §7 for the detailed setup). For ZooKeeper and Memberlist, we bootstrap a 1000 process cluster with standalone agents that join and maintain membership using these solutions (for Akka Cluster, we use 400 processes because it began failing for cluster sizes beyond 500). We then drop 80% of packets for 1% of processes, simulating high packet loss scenarios described in the literature <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b47">49]</ref> that we have also observed in practice. <ref type="figure">Figure 1</ref> shows a timeseries of membership sizes, as viewed by each non-faulty process in the cluster (every dot indicates a single measurement by a process). Akka Cluster is unstable as conflicting rumors about processes propagate in the cluster concurrently, even resulting in benign processes being removed from the membership. Memberlist and ZooKeeper resist removal of the faulty processes from the membership set but are unstable over <ref type="figure">Figure 1</ref>: Akka Cluster, ZooKeeper and Memberlist exhibit instabilities and inconsistencies when 1% of processes experience 80% packet loss (similar to scenarios described in <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b47">49]</ref>). Every process logs its own view of the cluster size every second, shown as one dot along the time (X) axis. Note, the y-axis range does not start at 0. X-axis points (or intervals) with different cluster size values represent inconsistent views among processes at that point (or during the interval).</p><p>a longer period of time. We also note extended periods of inconsistencies in the membership view.</p><p>Having found existing membership solutions to be unstable in the presence of typical network faults, we now proceed to discuss the broader design space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparison of existing solutions</head><p>There are three membership service designs in use today, each of which provides different degrees of resiliency and consistency.</p><p>Logically centralized configuration service. A common approach to membership management in the industry is to store the membership list in an auxiliary service such as ZooKeeper <ref type="bibr" target="#b13">[15]</ref>, etcd <ref type="bibr" target="#b31">[33]</ref>, or Chubby <ref type="bibr" target="#b21">[23]</ref>.</p><p>The main advantage of this approach is simplicity: a few processes maintain the ground truth of the membership list with strong consistency semantics, and the remaining processes query this list periodically.</p><p>The key shortcoming here is that relying on a small cluster reduces the overall resiliency of the system: connectivity issues to the cluster, or failures among the small set of cluster members themselves, may render the service unavailable (this led Netflix to build solutions like Eureka <ref type="bibr" target="#b56">[58,</ref><ref type="bibr" target="#b59">61]</ref>). As the ZooKeeper developers warn, this also opens up new failure modes for applications that depend on an auxiliary service for membership <ref type="bibr" target="#b15">[17]</ref>.</p><p>Gossip-based membership. van Renesse et al. <ref type="bibr" target="#b70">[72,</ref><ref type="bibr" target="#b69">71]</ref> proposed managing membership by using gossip to spread positive notifications (keepalives) between all processes. If a process p fails, other processes eventually remove p after a timeout. SWIM <ref type="bibr" target="#b27">[29]</ref> was proposed as a variant of that approach that reduces the communication overhead; it uses gossip to spread "negative" alerts, rather than regular positive notifications.</p><p>Gossip-based membership schemes are widely adopted in deployed systems today, such as Cassandra <ref type="bibr" target="#b6">[8]</ref>  <ref type="bibr" target="#b54">[56]</ref>, and some systems at Twitter <ref type="bibr" target="#b53">[55]</ref>.</p><p>The main advantage of gossip-based membership is resiliency and graceful degradation (they tolerate N − 1 failures). The key disadvantages include their weak consistency guarantees and the complex emergent behavior that leads to stability problems.</p><p>Stability is a key challenge in gossip-based membership: When communication fails between two processes which are otherwise live and correct, there are repeated accusations and refutations that may cause oscillations in the membership views. As our investigation of leading gossip-based solutions showed <ref type="figure">(Figure 1</ref>), these conflicting alerts lead to complex emergent behavior, making it challenging to build reliable clustered services on top of. Indeed, stability related issues with gossip are also observed in production settings (see, e.g., Consul <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b25">27]</ref> and Cassandra <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b8">10]</ref>).</p><p>Lastly, FireFlies <ref type="bibr" target="#b48">[50]</ref> is a decentralized membership service that tolerates Byzantine members. FireFlies organizes monitoring responsibilities via a randomized k-ring topology to provide a robust overlay against Byzantine processes. While the motivation in FireFlies was different, we believe it offers a solution for stability; accusations about a process by a potentially Byzantine monitor are not acted upon until a conservative, fixed delay elapses. If a process does not refute an accusation about it within this delay, it is removed from the membership. However, the FireFlies scheme is based on a gossipstyle protocol involving accusations, refutations, rankings, and disabling (where a process p announces that a monitor should not complain about it). Furthermore, FireFlies' refutations resist process removals as much as possible, which is undesirable in non-Byzantine settings. For example, in the 80% packet loss scenario described in <ref type="figure">Figure 1</ref>, a faulty process p may still succeed in disseminating refutations, thereby resisting removal from the membership. As we show in upcoming sections, our scheme is simple in comparison and requires little bookkeeping per process. Unlike FireFlies, we aggregate reports about a process p from multiple sources to decide whether to remove p, enabling timely and coordinated membership changes with low overhead.</p><p>Group membership. By themselves, gossip-based membership schemes do not address consistency, and allow the membership views of processes to diverge. In this sense, they may be considered more of failure detec-tors, than membership services.</p><p>Maintaining membership with strict consistency guarantees has been a focus in the field of fault tolerant statemachine replication (SMR), starting with early foundations of SMR <ref type="bibr" target="#b50">[52,</ref><ref type="bibr" target="#b58">60,</ref><ref type="bibr" target="#b61">63]</ref>, and continuing with a variety of group communication systems (see <ref type="bibr" target="#b22">[24]</ref> for a survey of GC works). In SMR systems, membership is typically needed for selecting a unique primary and for enabling dynamic service deployment. Recent work on Census <ref type="bibr" target="#b26">[28]</ref> scales dynamic membership maintenance to a locality-aware hierarchy of domains. It provides fault tolerance by running the view-change consensus protocol only among a sampled subset of the membership set.</p><p>These methods may be harnessed on top of a stable failure detection facility, stability being orthogonal to the consistency they provide. As we show, our solution uses an SMR technique that benefits from stable failure detection to form fast, leaderless consensus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Rapid Service</head><p>Our goal is to create a membership service based on techniques that apply equally well to both decentralized as well as logically centralized designs. For ease of presentation, we first describe the fully decentralized Rapid service and its properties in this section, followed by its design in §4. We then relax the resiliency properties in §5 for the logically centralized design.</p><p>API Processes use the membership service by using the Rapid library and invoking a call JOIN(HOST:PORT, SEEDS, VIEW-CHANGE-CALLBACK). Here, HOST:PORT is the process' TCP/IP listen address. Internally, the join call assigns a unique logical identifier for the process (ID). If a process departs from the cluster either due to a failure or by voluntarily leaving, it rejoins with a new ID. This ID is internal to Rapid and is not an identifier of the application that is using Rapid. SEEDS is an initial set of process addresses known to everyone and used to contact for bootstrapping. VIEW-CHANGE-CALLBACK is used to notify applications about membership change events.</p><p>Configurations A configuration in Rapid comprises a configuration identifier and a membership-set (a list of processes). Each process has a local view of the configuration. All processes use the initial seed-list as a bootstrap configuration C 0 . Every configuration change decision triggers an invocation of the VIEW-CHANGE-CALLBACK at all processes, that informs processes about a new configuration and membership set. At time t, if C is the configuration view of a majority of its members, we say that C is the current configuration. Initially, once a majority of C 0 start, it becomes current.</p><p>Failure model We assume that every pair of correct processes can communicate with each other within a known transmission delay bound (an assumption required for failure detection). When this assumption is violated for a pair of (otherwise live) processes, there is no obvious definition to determine which one of them is faulty (though at least one is). We resolve this using the parameters L and K as follows. Every process p (a subject) is monitored by K observer processes. If L-of-K correct observers cannot communicate with a subject, then the subject is considered observably unresponsive. We consider a process faulty if it is either crashed or observably unresponsive.</p><p>Cut Detection Guarantees Let C be the current configuration at time t. Consider a subset of processes F ⊂ C where |F| |C| &lt; 1 2 . If all processes in C \ F remain nonfaulty, we guarantee that the multi-process cut will eventually be detected and a view-change C \ F installed 1 :</p><p>• Multi-process cut detection: With high probability, every process in C \ F receives a multi-process cut detection notification about F. In Rapid, the probability is taken over all the random choices of the observer/subject overlay topology, discussed in §4. The property we use is that with high probability the underlying topology remains an expander at all times, where the expansion is quantified in terms of its second eigenvalue.</p><p>A similar guarantee holds for joins. If at time t a set J of processes join the system and remain non-faulty, then every process in C ∪ J is notified of J joining.</p><p>Joins and removals can be combined: If a set of processes F as above fails, and a set of processes J joins, then (C \ F) ∪ J is eventually notified of the changes.</p><p>• View-Change: Any view-change notification in C is by consensus, maintaining Agreement on the viewchange membership among all correct processes in C; and Liveness, provided a majority of C ∪ J (J = / 0 if there are no joiners) remain correct until the VC configuration becomes current.</p><p>Our requirements concerning configuration changes hold when the system has quiesced. During periods of instability, intermediate detection(s) may succeed, but there is no formal guarantee about them.</p><p>Hand-off Once a new configuration C j+1 becomes current, we abstractly abandon C j and start afresh: New failures can happen within C j+1 (for up to half of the membership set), and the Cut Detection and View Change guarantees must hold.</p><p>We note that liveness of the consensus layer depends on a majority of both C j and C j+1 remaining correct to perform the 'hand-off': Between the time when C j becomes current and until C j+1 does, no more than a minority fail in either configuration. This dynamic model borrows the dynamic-interplay framework of <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b64">66]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Decentralized Design</head><p>Rapid forms an immutable sequence of configurations driven through consensus decisions. Each configuration may drive a single configuration-change decision; the next configuration is logically a new system as in the virtual synchrony approach <ref type="bibr" target="#b20">[22]</ref>. Here, we describe the algorithm for changing a known current configuration C , consisting of a membership set (a list of process identities). When clear from the context, we omit C or explicit mentions of the configuration, as they are fixed within one instance of the configuration-change algorithm.</p><p>We start with a brief overview of the algorithm, breaking it down to three components: (1) a monitoring overlay; (2) an almost-everywhere multi-process cut detection (CD); and (3) a fast, leaderless view-change (VC) consensus protocol. The response to problems in Rapid evolves through these three components (see <ref type="figure">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monitoring</head><p>We organize processes into a monitoring topology such that every process monitors K peers and is monitored by K peers. A process being monitored is referred to as a subject and a process that is monitoring a subject is an observer (each process therefore has K subjects and K observers). The particular topology we employ in Rapid is an expander graph <ref type="bibr" target="#b36">[38]</ref> realized using K pseudo-random rings <ref type="bibr" target="#b32">[34]</ref>. Other observer/subject arrangements may be plugged into our framework without changing the rest of the logic.</p><p>Importantly, this topology is deterministic over the membership set C ; every process that receives a notification about a new configuration locally determines its subjects and creates the required monitoring channels.</p><p>There are two types of alerts generated by the monitoring component, REMOVE and JOIN. A REMOVE alert is broadcast by an observer when there are reachability problems to its subject. A JOIN alert is broadcast by an observer when it is informed about a subject joiner request. In this way, both types of alerts are generated by multiple sources about the same subject. Any best-effort broadcast primitive may be used to disseminate alerts (we use gossip-based broadcast).</p><p>Multi-process cut detection (CD) REMOVE and JOIN alerts are handled at each process independently by a multi-process cut detection (CD) mechanism. This mechanism collects evidence to support a single, stable multi-process configuration change proposal. It outputs the same cut proposal almost-everywhere; i.e., unanimity in the detection among a large fraction of processes.</p><p>The CD scheme with a K-degree monitoring topology has a constant per-process per-round communication cost, and provides stable multi-process cut detection with almost-everywhere agreement.</p><p>View change (VC) Finally, we use a consensus protocol that has a fast path to agreement on a view-change. If the protocol collects identical CD proposals from a Fast Paxos quorum (three quarters) of the membership, then it can decide in one step. Otherwise, it falls back to Paxos to form agreement on some proposal as a view-change.</p><p>We note that other consensus solutions could use CD as input and provide view-change consistency. VC has the benefit of a fast path to decision, taking advantage of the identical inputs almost-everywhere.</p><p>We now present a detailed description of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Expander-based Monitoring</head><p>Rapid organizes processes into a monitoring topology that is an expander graph <ref type="bibr" target="#b36">[38]</ref>. Specifically, we use the fact that a random K-regular graph is very likely to be a good expander for K ≥ 3 <ref type="bibr" target="#b32">[34]</ref>. We construct K pseudorandomly generated rings with each ring containing the full list of members. A pair of processes (o, s) form an observer/subject edge if o precedes s in a ring. Duplicate edges are allowed and will have a marginal effect on the behavior. <ref type="figure">Figure 2</ref> depicts the neighborhood of a single process p in a 4-Ring topology.</p><p>Topology properties. Our monitoring topology has three key properties. The first is expansion: the number of edges connecting two sets of processes reflects the relative sizes of the set. This means that if a small subset F of processes V are faulty, we should see roughly |V |−|F| |V | fraction of monitoring edges to F emanating from the set V \F of healthy processes. This ensures with high probability that healthy processes detect failures, as long as the set of failures is not too large. The size of failures we can detect depends on the expansion of the topology as quantified by the value of its second eigenvalue ( § 8). Second, every process monitors K subjects, and is monitored by K observers. Hence, monitoring incurs O(K) overhead per process per round, distributing the load across the entire cluster. The fixed observer/subject approach distinguishes Rapid from gossip-based techniques, supporting prolonged monitoring without sacrificing failure detection scalability. At the same time, we compare well with the overhead of gossip-based solutions ( §7). Third, every process join or removal results only in 2 · K monitoring edges being added or removed.</p><p>Joins New processes join by contacting a list of K temporary observers obtained from a seed process (deterministically assigned for each joiner and C pair, until a configuration change reflects the join). The temporary observers generate independent alerts about joiners. In this way, multiple JOIN alerts are generated from distinct sources, in a similar manner to alerts about failures.</p><p>Pluggable edge-monitor. A monitoring edge between an observer and its subject is a pluggable component in Rapid. With this design, Rapid can take advantage of diverse failure detection and monitoring techniques, e.g., history-based adaptive techniques as used by popular frameworks like <ref type="bibr">Hystrix [57]</ref> and Finagle <ref type="bibr" target="#b66">[68]</ref>; phiaccrual failure detectors <ref type="bibr" target="#b29">[31]</ref>; eliciting indirect probes <ref type="bibr" target="#b27">[29]</ref>; flooding a suspicion and allowing a timeout period for self-rebuttal <ref type="bibr" target="#b48">[50]</ref>; using cross-layer information <ref type="bibr" target="#b52">[54]</ref>; application-specific health checks; and others.</p><p>Irrevocable Alerts When the edge-monitor of an observer indicates an existing subject is non-responsive, the observer broadcasts a REMOVE alert about the subject. Given the high fidelity made possible with our stable edge monitoring, these alerts are considered irrevocable, thus Rapid prevents spreading conflicting reports. When contacted by a subject, a temporary observer broadcasts JOIN alert about the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-process Cut Detection</head><p>Alerts in Rapid may arrive at different orders at each process. Every process independently aggregates these alerts until a stable multi-process cut is detected. Our approach aims to reach agreement almost everywhere with regards to this detection. Our mechanism is based on a simple key insight: A process defers a decision on a single process until the alert-count it received on all processes is considered stable. In particular, it waits until there is no process with an alert-count above a low-watermark threshold L and below a high-watermark threshold H.</p><p>Our technique is simple to implement; it only requires maintaining integer counters per-process and comparing them against two thresholds. This state is reset after each configuration change.</p><p>Processing REMOVE and JOIN alerts Every process ingests broadcast alerts by observers about edges to their subjects. A REMOVE alert reports that an edge to the subject process is faulty; a JOIN alert indicates that an edge to the subject is to be created. By design, a JOIN alert can only be about a process not in the current configuration C , and REMOVE alerts can only be about processes in C . There cannot be JOIN and REMOVE alerts about the same process in C .</p><p>Every process p tallies up distinct REMOVE and JOIN alerts in the current configuration view as follows. For each observer/subject pair (o, s), p maintains a value M(o, s) which is set to 1 if an alert was received from observer o regarding subject s; and it is set to (default) 0 if no alert was received. A tally(s) for a process s is the sum of entries M( * , s).</p><p>Stable and unstable report modes We use two parameters H and L, 1 ≤ L ≤ H ≤ K. A process p considers a process s to be in a stable report mode if |tally(s)| ≥ H at p. A stable report mode indicates that p has received at least H distinct observer alerts about s, hence we consider it "high fidelity"; A process s is in an unstable report mode if tally(s) is in between L and H. If there are fewer than L distinct observer alerts about s, we consider it noise. Recall that Rapid does not revert alerts; hence, a stable report mode is permanent once it is reached. Note that, the same thresholds are used for REMOVE and JOIN reports; this is not mandatory, and is done for simplicity.</p><p>Aggregation Each process follows one simple rule for aggregating tallies towards a proposed configuration change: delay proposing a configuration change until there is at least one process in stable report mode and there is no process in unstable report mode. Once this condition holds, the process announces a configuration change proposal consisting of all processes in stable report mode, and the current configuration identifier. The proposed configuration change has the almosteverywhere agreement property, which we analyze in §8 and evaluate in §7. <ref type="figure" target="#fig_1">Figure 4</ref> depicts the almost everywhere agreement mechanism at a single process.</p><p>Ensuring liveness: implicit detections and reinforcements There are two cases in which a subject process might get stuck in an unstable report mode and not accrue H observer reports. The first is when the observers themselves are faulty. To prevent waiting for stability forever, for each observer o of s, if both o and s are in the unstable report mode, then an implicit-alert is applied from o to s (i.e., an implicit REMOVE if s is in C and a JOIN otherwise; o is by definition always in C ).</p><p>The second is the case when a subject process has good connections to some observers, and bad connections to others. In this case, after a subject s has been in the unstable reporting mode for a certain timeout period, each observer o of s reinforces the detection: if o did not send a REMOVE message about s already, it broadcasts a REMOVE about s to echo existing REMOVEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">View-change Agreement</head><p>We use the result of each process' CD proposal as input to a consensus protocol that drives agreement on a single view-change.</p><p>The consensus protocol in Rapid has a fast, leaderless path in the common case, that has the same overhead as simple gossip. The fast path is built around the Fast Paxos algorithm <ref type="bibr" target="#b51">[53]</ref>. In our variation, we use the CD result as initial input to processes, instead of having an explicit proposer populating the processes with a proposal. Fast Paxos reaches a decision if there is a quorum larger than three quarters of the membership set with an identical proposal. Due to our prudent almost-everywhere CD scheme, with high probability, all processes indeed have an identical multi-process cut proposal. In this case, the VC protocol converges simply by counting the number of identical CD proposals.</p><p>The counting protocol itself uses gossip to disseminate and aggregate a bitmap of "votes" for each unique proposal. Each process sets a bit in the bitmap of a proposal to reflect its vote. As soon as a process has a proposal for which three quarters of the cluster has voted, it decides on that proposal.</p><p>If there is no fast-quorum support for any proposal because there are conflicting proposals, or a timeout is reached, Fast Paxos falls back to a recovery path, where we use classical Paxos <ref type="bibr" target="#b50">[52]</ref> to make progress.</p><p>In the face of partitions <ref type="bibr" target="#b34">[36]</ref>, some applications may need to maintain availability everywhere (AP), and others only allow the majority component to remain live to provide strong consistency (CP). Rapid guarantees to reconfigure processes in the majority component. The remaining processes are forced to logically depart the system. They may wait to rejoin the majority component, or choose to form a separate configuration (which Rapid facilitates quickly). The history of the members forming a new configuration will have an explicit indication of these events, which applications can choose to use in any manner that fits them (including ignoring).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Logically Centralized Design</head><p>We now discuss how Rapid runs as a logically centralized service, where a set of auxiliary nodes S records the membership changes for a cluster C . This is a similar model to how systems use ZooKeeper to manage membership: the centralized service is the ground truth of the membership list.</p><p>Only three minor modifications are required to the protocol discussed in §4:</p><p>1. Nodes in the current configuration C continue monitoring each other according to the k-ring topology (to scale the monitoring load). Instead of gossiping these alerts to all nodes in C , they report it only to all nodes in S instead. 2. Nodes in S apply the CD protocol as before to identify a membership change proposal from the incoming alerts. However, they execute the VC protocol only among themselves. 3. Nodes in C learn about changes in the membership through notifications from S (or by probing nodes in S periodically). The resulting solution inherits the stability and agreement properties of the decentralized protocol, but with reduced resiliency guarantees; the resiliency of the overall system is now bound to that of S (F = S 2 − 1) -as with any logically centralized design. For progress, members of C need to be connected to a majority partition of S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation</head><p>Rapid is implemented in Java with 2362 lines of code (excluding comments and blank lines). This includes all the code associated with the membership protocol as well as messaging and failure detection. In addition, there are 2034 lines of code for tests. Our code is open-sourced under an Apache 2 license <ref type="bibr">[2]</ref>.</p><p>Our implementation uses gRPC and Netty for messaging. The counting step for consensus and the gossipbased dissemination of alerts are performed over UDP. Applications interact with Rapid using the APIs for joining and receiving callbacks described in §3. The logical identifier ( §3) for each process is generated by the Rapid library using UUIDs. The join method allows users to supply edge failure detectors to use. Similar to APIs of existing systems such as Serf <ref type="bibr" target="#b42">[44]</ref> and Akka Cluster <ref type="bibr" target="#b67">[69]</ref>, users associate application-specific metadata with the process being initialized (e.g., "role":"backend").</p><p>Our default failure detector has observers send probes to their subjects and wait for a timeout. Observers mark an edge faulty when the number of communication exceptions they detect exceed a threshold (40% of the last 10 measurement attempts fail). Similar to Memberlist and Akka Cluster, Rapid batches multiple alerts into a single message before sending them on the wire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>Setup We run our experiments on a shared internal cloud service with 200 cores and 800 GB of RAM (100 VMs). We run multiple processes per VM, given that the workloads are not CPU bottlenecked. We vary the number of processes (N) in the cluster from 1000 to 2000. We compare Rapid against (i) ZooKeeper <ref type="bibr" target="#b13">[15]</ref> accessed using Apache Curator <ref type="bibr" target="#b11">[13]</ref>, (ii) Memberlist <ref type="bibr" target="#b45">[47]</ref>, the SWIM implementation used by Serf <ref type="bibr" target="#b42">[44]</ref> and Consul <ref type="bibr" target="#b43">[45]</ref>. For Rapid, we use the decentralized variant unless specified otherwise (Rapid-C, where a 3-node ensemble manages the membership of N processes).</p><p>We also tested Akka Cluster <ref type="bibr" target="#b67">[69]</ref> but found its bootstrap process to not stabilize for clusters beyond 500 processes, and therefore do not present further (see §2.1 and <ref type="figure">Figure 1</ref>). All ZooKeeper experiments use a 3-node ensemble, configured according to <ref type="bibr" target="#b14">[16]</ref>. For Memberlist, we use the provided configuration for single data center settings (called DefaultLANConfig). Rapid uses {K, H, L} = {10, 9, 3} for all experiments and we also show a sensitivity analysis. We seek to answer:</p><p>• How quickly can Rapid bootstrap a cluster?</p><p>• How does Rapid react to different fault scenarios?</p><p>• How bandwidth intensive is Rapid?</p><p>• How sensitive is the almost-everywhere agreement property to the choice of K,H,L? • Is Rapid easy to integrate with real applications?</p><p>Bootstrap experiments We stress the bootstrap protocols of all three systems under varying cluster sizes. For Memberlist and Rapid, we start each experiment with a single seed process, and after ten seconds, spawn a subsequent group of N − 1 processes (for ZooKeeper, the 3-node ZooKeeper cluster is brought up first). Every process logs its observed cluster size every second. Every measurement is repeated five times per value of N. We measure the time taken for all processes to converge to  ZooKeeper suffers from herd behavior during the bootstrap process (as documented in <ref type="bibr" target="#b16">[18]</ref>), resulting in its bootstrap latency increasing by 4x from when N=1000 to when N=2000. Group membership with ZooKeeper is done using watches. When the i th process joins the system, it triggers i − 1 watch notifications, causing i − 1 processes to re-read the full membership list and register a new watch each. In the interval between a watch having triggered and it being replaced, the client is not notified of updates, leading to clients observing different sequences of membership change events <ref type="bibr" target="#b15">[17]</ref>. This behavior with watches leads to the eventually consistent client behavior in <ref type="figure" target="#fig_4">Figure 7</ref>. Lastly, we emphasize that this is a 3-node ZooKeeper cluster being used exclusively to manage membership for a single cluster. Adding even one extra watch per client to the group node at N=2000 inflates bootstrap latencies to 400s on average.</p><p>Memberlist processes bootstrap by contacting a seed. The seed thereby learns about every join attempt. However, non-seed processes need to periodically execute a push-pull handshake with each other to synchronize their views (by default, once every 30 seconds). Memberlist's convergence times are thereby as high as 95s on average when N = 2000 <ref type="figure" target="#fig_4">(Figure 7)</ref>. Similar to Memberlist, Rapid processes bootstrap by contacting a seed. The seed aggregates alerts until it bootstraps a cluster large enough to support a Paxos quorum (minimum of three processes). The remaining processes are admitted in a subsequent one or more view System N=1000 N=1500 <ref type="table" target="#tab_2">N=2000  ZooKeeper  1000  1500  2000  Memberlist  901  1383  1858  Rapid-C  9  10  7  Rapid  4  8  4   Table 1</ref>: Number of unique cluster sizes reported by processes in bootstrapping experiments. changes. For instance, in <ref type="figure" target="#fig_4">Figure 7</ref>, Rapid transitions from a single seed to a five node cluster, before forming a cluster of size 2000. We confirm this behavior across runs in <ref type="table">Table 1</ref>, which shows the number of unique cluster sizes reported for different values of N. In the ZooKeeper and Memberlist experiments, processes report a range of cluster sizes between 1 and N as the cluster bootstraps. Rapid however brings up large clusters with very few intermediate view changes, reporting four and eight unique cluster sizes for each setting. Our logically centralized variant Rapid-C, behaves similarly for the bootstrap process. However, processes in Rapid-C periodically probe the 3-node ensemble for updates to the membership (the probing interval is set to be 5 seconds, the same as with ZooKeeper). This extra step increases bootstrap times over the decentralized variant; in the latter case, all processes participate in the dissemination of votes through aggregate gossip.</p><p>Crash faults We now set N = 1000 to compare the different systems in the face of crash faults. At this size, we have five processes per-core in the infrastructure, leading to a stable steady state for all three systems. We then fail ten processes and observe the cluster membership size reported by every other process in the system. <ref type="figure" target="#fig_5">Figure 8</ref> shows the cluster size timeseries as recorded by each process. Every dot in the timeseries represents a cluster size recording by a single process. With Memberlist and ZooKeeper, processes record several different cluster sizes when transitioning from N to N − F. Rapid on the other hand concurrently detects all ten process failures and removes them from the membership using a 1-step consensus decision. Note, our edge failure detector performs multiple measurements before announcing a fault for stability ( §6), thereby reacting roughly 10 seconds later than Memberlist does. The results are identical when the ten processes are partitioned away completely  from the cluster (we do not show the plots for brevity).</p><p>Asymmetric network failures We study how each system responds to common network failures that we have seen in practice. These scenarios have also been described in <ref type="bibr" target="#b47">[49,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b65">67,</ref><ref type="bibr" target="#b39">41]</ref>.</p><p>Flip-flops in one-way connectivity. We enforce a "flipflopping" asymmetric network failure. Here, 10 processes lose all packets that they receive for a 20 second interval, recover for 20 seconds, and then repeat the packet dropping. We enforce this by dropping packets in the iptables INPUT chain. The timeseries of cluster sizes reported by each process is shown in <ref type="figure" target="#fig_6">Figure 9</ref>.</p><p>ZooKeeper does not react to the injected failures because clients do not receive packets on the ingress path, but send heartbeats to the ZooKeeper nodes. Reversing the direction of connectivity loss as in the next experiment does cause ZooKeeper to react. Memberlist never removes all the faulty processes from the membership, and oscillates throughout the duration of the failure scenario. We also find several intervals of inconsistent views among processes. Unlike ZooKeeper and Memberlist, Rapid detects and removes the faulty processes.</p><p>High packet loss scenario. We now run an experiment where 80% of outgoing packets from the faulty processes are dropped. We inject the fault at t = 90s. <ref type="figure" target="#fig_7">Fig- ure 10</ref> shows the resulting membership size timeseries. ZooKeeper reacts to the failures at t = 200s, and does not remove all faulty processes from the membership. <ref type="figure" target="#fig_7">Figure 10</ref> also shows how Memberlist's failure detector is conservative; even a scenario of sustained high packet loss is insufficient for Memberlist to conclusively remove a set of processes from the network. Further-   more, we observe view inconsistencies with Memberlist near t = 400s. Rapid, again, correctly identifies and removes only the faulty processes.</p><p>Memory utilization. Memberlist (written in Go) used an average of 12MB of resident memory per process. With Rapid and ZooKeeper agents (both Java based), GC events traced using -XX:+PrintGC report min/max heap utilization of 10/25MB and 3.5/16MB per process.</p><p>Network utilization. K, H, L sensitivity study We now present the effect of K, H and L on the almost-everywhere agreement property of our multi-process detection technique. We initialize 1000 processes and select F random processes to fail. We generate alert messages from the F processes' observers and deliver these alerts to each process in a uniform random order. We count the number of processes that announce a membership proposal that did not include all F processes (a conflict). We run all parameter combinations for H = {6, 7, 8, 9}, L = {1, 2, 3, 4}, F =  <ref type="figure" target="#fig_8">Figure 11</ref> shows the results. As our analysis ( §8) predicts, the conflict rate is highest when the gap between H and L is lowest (H = 6, L = 4) and the number of failures F is 2. This setting causes processes to arrive at a proposal without waiting long enough. As we increase the gap H − L and increase F, the algorithm at each process waits long enough to gather all the relevant alerts, thereby diminishing the conflict probability. Our system is thereby robust across a range of values; for H − L = 5 and F = 2, we get a 2% conflict rate for different values of H and L. Increasing to H −L = 6 drops the probability of a conflict by a factor of 4.</p><p>Experience with end-to-end workloads We integrated Rapid within use cases at our organization that required membership services. Our goal is to understand the ease of integrating and using Rapid.</p><p>Distributed transactional data platform. We worked with a team that uses a distributed data platform that supports transactions. We replaced the use of its inhouse gossip-based failure detector that uses all-to-all monitoring, with Rapid. The failure detector recommends membership changes to a Paxos-based reconfiguration mechanism, and we let Rapid provide input to the re-configuration management instead. Our integration added 62 and removed 25 lines of code. We also ported the system's failure detection logic such that it could be supplied to Rapid as an edge failure detector, which involved an additional 123 lines of code.</p><p>We now describe a use case in the context of this system where stable monitoring is required. For total ordering of requests, the platform has a transaction serialization server, similar to the one used in Google Megastore <ref type="bibr" target="#b18">[20]</ref> and Apache Omid <ref type="bibr" target="#b12">[14]</ref>. At any moment in time, the system has only one active serialization server, and its failure requires the cluster to identify a new candidate server for a failover. During this interval, workloads are paused and clients do not make progress. We ran an experiment where two update-heavy clients (read-write ratio of 50-50) each submit 500K read/write operations, batched as 500 transactions. We injected a failure that drops all packets between the current serialization server and one other data server (resembling a packet blackhole as observed by <ref type="bibr" target="#b39">[41]</ref>). Note, this fault does not affect communication links between clients and data servers. We measured the impact of this fault on the end-to-end latency and throughput.</p><p>With the baseline failure detector, the serialization server was repeatedly added and removed from the membership. The repeated failovers caused a degradation of end-to-end latency and a 32% drop in throughput <ref type="figure" target="#fig_9">(Fig- ure 12)</ref>. When using Rapid however, the system continued serving the workload without experiencing any interruption (because no node exceeded L reports).</p><p>Service discovery. A common use case for membership is service discovery, where a fleet of servers need to be discovered by dependent services. We worked with a team that uses Terraform <ref type="bibr" target="#b44">[46]</ref> to drive deployments where a load balancer discovers a set of backend servers using Serf <ref type="bibr" target="#b42">[44]</ref>. We replaced their use of Serf in this workflow with an agent that uses Rapid instead (the Rapid specific code amounted to under 20 lines of code). The setup uses nginx <ref type="bibr" target="#b0">[1]</ref> to load balance requests to 50 web servers (also using nginx) that serve a static HTML page. All 51 machines run as t2.micro instances on Amazon EC2. Both membership services update nginx's configuration file with the list of backend servers on every membership change. We then use an HTTP workload generator to generate 1000 requests per-second. 30 seconds into the experiment, we fail ten nodes and observe the impact on the end-to-end latency ( <ref type="figure" target="#fig_10">Figure 13)</ref>.</p><p>Rapid detects all failures concurrently and triggers a single reconfiguration because of its multi-node membership change detection. Serf, which uses Memberlist, detects multiple independent failures that result in several updates to the nginx configuration file. The load balancer therefore incurs higher latencies when using Serf at multiple intervals (t=35s and t=46s) because nginx is reconfiguring itself. In the steady state where there are no failures, we observe no difference between Rapid and Serf, suggesting that Rapid is well suited for service discovery workloads, despite offering stronger guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary of Proofs</head><p>In the interest of space, we report the complete proof of correctness in a tech report <ref type="bibr" target="#b49">[51]</ref>, and only present the key take aways here. Our consensus engine is standard, and borrows from known literature on consensus algorithms <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b50">52]</ref>. We do not repeat its proof of Agreement and Liveness.</p><p>It is left to prove that faced with F failures in a configuration C, the stable failure detector detects and outputs F at all processes with high probability. We divide the proof into two parts.</p><p>Detection guarantee For parameters L and K, we can detect a failure of a set F as long as |F| is bounded by the relationship</p><formula xml:id="formula_0">|F| |C| ≤ (1 − L K − λ 2K ).</formula><p>Here, λ is the second eigenvalue of the underlying monitoring topology, and is tied to the expansion properties of the topology. In our experiments, with K = 10, we have observed consistently that λ 2K &lt; 0.45 (which, for L = 3, yields |F| |C| ≤ 0.25). Almost-everywhere agreement Second, we prove the almost-everywhere agreement property about our multiprocess cut protocol. We assume that there are t failures, and that nodes receive alerts about these failures in a uniform random order. Let Pr[B(z)] be the probability that the CD protocol at z outputs a subset of t that differs from the output at other nodes. We show that if multiple processes fail simultaneously, Pr[B(z)] exponentially decreases with increasing K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusions</head><p>In this paper, we demonstrated the effectiveness of detecting cluster-wide multi-process cut conditions, as opposed to detecting individual node changes. The high fidelity of the CD output prevents frequent oscillations or incremental changes when faced with multiple failures. It achieves unanimous detection almost-everywhere, enabling a fast, leaderless consensus protocol to drive membership changes. Our implementation successfully bootstraps clusters of 2000 processes 2-5.8x times faster than existing solutions, while being stable against complex network failures. We found Rapid easy to integrate end-to-end within a distributed transactional data platform and a service discovery use case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: p's neighborhood in a K = 4-Ring topology. p's observers are {v, w, x, y}; p's subjects are {r, s,t, u}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Almost everywhere agreement protocol example at a process p, with tallies about q, r, s,t and K = 10, H = 7, L = 2. K is the number of observers per subject. The region between H and L is the unstable region. The region between K and H is the stable region. Left: stable = {r, s,t}; unstable = {q}. Right: q moves from unstable to stable; p proposes a view change {q, r, s,t}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Bootstrap convergence measurements showing the time required for all nodes to report a cluster size of N. Rapid bootstraps a 2000 node cluster 2-2.32x faster than Memberlist, and 3.23-5.81x faster than ZooKeeper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Bootstrap latency distribution for all systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Timeseries showing the first 150 seconds of all three systems bootstrapping a 2000 node cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Experiment with 10 concurrent crash failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Asymmetric network failure with one-way network partition on the network interface of 1% of processes (ingress path).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Experiment with 80% ingress packet loss on the network interface of 1% of processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Almost-everywhere agreement conflict probability for different combinations of H, L and failures F when K=10. Note the different y-axis scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Transaction latency when testing an in-house gossip-style failure detector and Rapid for robustness against a communication fault between two processes. The baseline failure detector triggers repeated failovers that reduce throughput by 32%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Service discovery experiment. Rapid's batching reduces the number of configuration reloads during a set of failures, thereby reducing tail latency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Mean, 99 th percentile and maximum</head><label>2</label><figDesc></figDesc><table>network 
bandwidth utilization per process. 

H=8 
H=9 

H=6 
H=7 

L=1 
L=2 
L=3 
L=4 
L=1 
L=2 
L=3 
L=4 

L=1 
L=2 
L=3 
L=4 
L=1 
L=2 
L=3 
L=4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>the mean, 99th 
and 100th percentiles of network utilization per sec-
ond across processes during the crash fault experiment 
(1000 processes). Rapid has a peak utilization of 9.56 
KB/s received (and 11.37 KB/s transmitted) versus 7.36 
KB/s received (8.04 KB/s transmitted) for Memberlist. 
Rapid therefore provides stronger guarantees than Mem-
berlist for a similar degree of network bandwidth utiliza-
tion. ZooKeeper clients have a peak ingress utilization of 
38.86 KB/s per-process on average to learn the updated 
view of the membership. 

</table></figure>

			<note place="foot" n="1"> The size of cuts |F| we can detect is a function of the monitoring topology. The proof is summarized in §8, and a full derivation appears in a tech report [51]. 390 2018 USENIX Annual Technical Conference USENIX Association</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nginx</surname></persName>
		</author>
		<ptr target="http://nginx.org/en/docs/http/load_balancing.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Slicer: Auto-sharding for datacenter applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adya</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fulger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhuvanagiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shraer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev-Ari</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>GA, 2016</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="739" to="753" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Summary of the Amazon EC2, Amazon EBS, and Amazon RDS Service Event in the EU West Region</title>
		<ptr target="https://aws.amazon.com/message/2329B7/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>AMAZON</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Summary of the Amazon EC2 and Amazon RDS Service Disruption in the US East Region</title>
		<ptr target="https://aws.amazon.com/message/65648/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>AMAZON</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<ptr target="https://aws.amazon.com/message/680587/" />
	</analytic>
	<monogr>
		<title level="j">Amazon ELB Service Event in the US-East Region</title>
		<imprint>
			<date type="published" when="2012-12-24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Summary of Windows Azure Service Disruption on Feb 29th</title>
		<ptr target="https://azure.microsoft.com/en-us/blog/summary-of-windows-azure\-service-disruption-on-feb-29th-2012/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>AMAZON</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache Cassandra. Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="http://cassandra.apache.org/" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cassandra-3831: scaling to large clusters in GossipStage impossible due to calculatePendingRanges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-3831" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cassandra-6127: vnodes don&apos;t scale to hundreds of nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-6127" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cassandra-9667: strongly consistent membership and ownership</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-9667" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Cassandra-11740: Nodes have wrong membership view of the cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Cassandra</surname></persName>
		</author>
		<ptr target="https://issues.apache.org/jira/browse/CASSANDRA-11740" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Curator. Apache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Curator</surname></persName>
		</author>
		<ptr target="https://curator.apache.org/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Omid. Apache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Omid</surname></persName>
		</author>
		<ptr target="http://omid.incubator.apache.org/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Zookeeper. Apache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zookeeper</surname></persName>
		</author>
		<ptr target="https://zookeeper.apache.org/" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ZooKeeper Administrator&apos;s Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Zookeeper</surname></persName>
		</author>
		<ptr target="https://zookeeper.apache.org/doc/trunk/zookeeperAdmin.html" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Programmer&apos;s Guide</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Zookeeper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zookeeper</surname></persName>
		</author>
		<ptr target="https://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Zookeeper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zookeeper Recipes</surname></persName>
		</author>
		<ptr target="https://zookeeper.apache.org/doc/trunk/recipes.html" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The network is reliable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kingsbury</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Providing scalable, highly available storage for interactive services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Furman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khor-Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushprakh</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Megastore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Innovative Data system Research (CIDR</title>
		<meeting>the Conference on Innovative Data system Research (CIDR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="223" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The Datacenter As a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barroso</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Clidaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hölzle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Morgan and Claypool Publishers</publisher>
		</imprint>
	</monogr>
	<note>2nd ed. Synthesis Lectures on Computer Architecture</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Replication</surname></persName>
		</author>
		<title level="m">History of the Virtual Synchrony Replication Model</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="91" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The chubby lock service for loosely-coupled distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burrows</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Symposium on Operating Systems Design and Implementation</title>
		<meeting>the 7th Symposium on Operating Systems Design and Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="335" to="350" />
		</imprint>
	</monogr>
	<note>OSDI &apos;06, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Group communication specifications: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chockler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Keidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitenberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="427" to="469" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Issue 1212: Node health flapping -EC2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Consul</forename><surname>Project</surname></persName>
		</author>
		<ptr target="https://github.com/hashicorp/consul/issues/1212" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Issue 916: Frequent membership loss for 50-100 node cluster in AWS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Consul</forename><surname>Project</surname></persName>
		</author>
		<ptr target="https://github.com/hashicorp/consul/issues/916" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Issue 916: node health constantly flapping in large cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Consul</forename><surname>Project</surname></persName>
		</author>
		<ptr target="https://github.com/hashicorp/consul/issues/1337" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Census: Location-aware membership management for large-scale distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cowling</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ports</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R K</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaikwad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno>USENIX&apos;09</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on USENIX Annual Technical Conference</title>
		<meeting>the 2009 Conference on USENIX Annual Technical Conference<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="12" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Swim: Scalable weakly-consistent infection-style process group membership protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Motivala</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. International Conference on</title>
		<meeting>International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
	<note>Dependable Systems and Networks</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barroso</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Tail At Scale. Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="74" to="80" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The φ accrual failure detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Defago</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hayashibara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katayama</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RR IS-RR-2004-010</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="66" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Consensus in the presence of partial synchrony</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dwork</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stockmeyer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="323" />
			<date type="published" when="1988-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etcd</forename></persName>
		</author>
		<ptr target="https://github.com/coreos/etcd" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the second eigenvalue of random regular graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szemerédi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-first Annual ACM Symposium on Theory of Computing</title>
		<meeting>the Twenty-first Annual ACM Symposium on Theory of Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="587" to="598" />
		</imprint>
	</monogr>
	<note>STOC &apos;89, ACM</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Elastic Configuration Maintenance via a Parsimonious Speculating Snapshot Solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gafni</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malkhi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LNCS 9363 of 29th International Symposium on Distributed Computing, Toshimitsu Masuzawa and Koichi Wada</title>
		<editor>Y. Moses and M. Roy</editor>
		<meeting><address><addrLine>Tokyo, France; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
	<note>DISC 2015</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Perspectives on the cap theorem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lynch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="30" to="36" />
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Understanding network failures in data centers: Measurement, analysis, and implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gill</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nagappan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2011 Conference</title>
		<meeting>the ACM SIGCOMM 2011 Conference<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="350" to="361" />
		</imprint>
	</monogr>
	<note>SIGCOMM &apos;11, ACM</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Studies in complexity and cryptography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldreich</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="451" to="464" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>Basic Facts About Expander Graphs</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">What bugs live in the cloud? a study of 3000+ issues in cloud systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunawi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Patana-Anake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adityatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eliazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Laksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa-Tria</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>SOCC &apos;14, ACM</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Why does the cloud stop computing?: Lessons from hundreds of service outages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunawi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suminto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Laksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Satria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Adityatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliazar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM Symposium on Cloud Computing</title>
		<meeting>the Seventh ACM Symposium on Cloud Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
	<note>SoCC &apos;16, ACM</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pingmesh: A large-scale system for data center network latency measurement and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurien</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM)</title>
		<meeting>the ACM SIGCOMM Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="139" to="152" />
		</imprint>
	</monogr>
	<note>SIGCOMM &apos;15, ACM</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Failure recovery: When the cure is worse than the disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mcdirmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bergan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Musuvathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 14th Workshop on Hot Topics in Operating Systems</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadoop</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<ptr target="http://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashicorp</forename><surname>Serf</surname></persName>
		</author>
		<ptr target="https://www.serf.io/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashicorp</forename></persName>
		</author>
		<ptr target="https://www.consul.io/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashicorp</forename><surname>Terraform</surname></persName>
		</author>
		<ptr target="https://www.terraform.io/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hashicorp</forename></persName>
		</author>
		<ptr target="https://github.com/hashicorp/memberlist" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gray failure: The achilles&apos; heel of cloud-scale systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chintalapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Workshop on Hot Topics in Operating Systems</title>
		<meeting>the 16th Workshop on Hot Topics in Operating Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="150" to="155" />
		</imprint>
	</monogr>
	<note>HotOS &apos;17, ACM</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Dean</forename><surname>Designs</surname></persName>
		</author>
		<ptr target="http://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf" />
		<title level="m">Lessons and Advice from Building Large Distributed Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fireflies: A secure and scalable membership and gossip service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johansen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Vigfusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo-Hansen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Malkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Porto Car-Reiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lokhandwala</surname></persName>
		</author>
		<ptr target="https://github.com/lalithsuresh/rapid/blob/master/docs/tech-report.pdf" />
		<title level="m">Consistent and Stable Membership at Scale with Rapid</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The part-time parliament</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamport</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="169" />
			<date type="published" when="1998-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast paxos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lamport</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="79" to="103" />
			<date type="published" when="2006-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Detecting Failures in Distributed Systems with the Falcon Spy Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leners</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walfish</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="279" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Building Scalable Stateful Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mccaffrey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<ptr target="https://speakerdeck.com/caitiem20/building-scalable-stateful-services" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netflix</forename><surname>Dynomite</surname></persName>
		</author>
		<ptr target="https://github.com/Netflix/dynomite" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title/>
		<ptr target="http://goo.gl/h9brP0" />
	</analytic>
	<monogr>
		<title level="j">NETFLIX. Introducing Hystrix for Resilience Engineering</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netflix</forename><surname>Eureka</surname></persName>
		</author>
		<ptr target="https://github.com/Netflix/eureka" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Optimizing distributed actor systems for dynamic interactive services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Newell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kliot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Akiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silberstein</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Systems (EuroSys)</title>
		<meeting>the European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<publisher>ACM -Association for Computing Machinery</publisher>
			<date type="published" when="2016-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Viewstamped replication: A new primary copy method to support highly-available distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oki</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liskov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh annual ACM Symposium on Principles of distributed computing</title>
		<meeting>the seventh annual ACM Symposium on Principles of distributed computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="8" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kelley</surname></persName>
		</author>
		<ptr target="https://tech.knewton.com/blog/2014/12/eureka-shouldnt-use\-zookeeper-service-discovery/" />
	</analytic>
	<monogr>
		<title level="j">Eureka! Why You Shouldn&apos;t Use ZooKeeper for Service Discovery</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Redis</forename><surname>Redis</surname></persName>
		</author>
		<ptr target="http://redis.io/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Implementing fault-tolerant services using the state machine approach: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">B</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="299" to="319" />
			<date type="published" when="1990-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scylla</forename><surname>Scylladb</surname></persName>
		</author>
		<ptr target="http://www.scylladb.com/" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Dynamic reconfiguration of primary/backup clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shraer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Malkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqueira</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX Conference on Annual Technical Conference</title>
		<meeting>the 2012 USENIX Conference on Annual Technical Conference<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="39" to="39" />
		</imprint>
	</monogr>
	<note>USENIX ATC&apos;12, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Dynamic reconfiguration: Abstraction and optimal asynchronous solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spiegelman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keidar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malkhi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">On failure in managed enterprise networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mogul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snoeren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
		<idno>HPL-2012-101</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">HP Labs</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Twitter</forename><surname>Finagle</surname></persName>
		</author>
		<ptr target="https://goo.gl/ITebZs" />
		<title level="m">A Protocol-Agnostic RPC System</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Typesafe</forename><surname>Akka</surname></persName>
		</author>
		<ptr target="http://akka.io/" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uber</forename><surname>Ringpop</surname></persName>
		</author>
		<ptr target="https://ringpop.readthedocs.io/en/latest/index.html" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Astrolabe: A robust and scalable technology for distributed system monitoring, management, and data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vogels</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="164" to="206" />
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">A gossipstyle failure detection service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Minsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayden</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IFIP International Conference on Distributed Systems Platforms and Open Distributed Processing</title>
		<meeting>the IFIP International Conference on Distributed Systems Platforms and Open Distributed Processing<address><addrLine>London, UK, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="70" />
		</imprint>
	</monogr>
	<note>Middleware &apos;98</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
