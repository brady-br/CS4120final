<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Automated Experiment-Driven Management of (Database) Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivnath</forename><surname>Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nedyalko</forename><surname>Borisov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyun</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herodotos</forename><surname>Herodotou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsidhar</forename><surname>Thummala</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Automated Experiment-Driven Management of (Database) Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In this position paper, we argue that an important piece of the system administration puzzle has largely been left untouched by researchers. This piece involves mechanisms and policies to identify as well as collect missing instru-mentation data; the missing data is essential to generate the knowledge required to address certain administrative tasks satisfactorily and efficiently. We introduce the paradigm of experiment-driven management which encapsulates such mechanisms and policies for a given administrative task. We outline the benefits that automated experiment-driven management brings to several long-standing problems in databases as well as other systems, and identify research challenges as well as initial solutions.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of administering a large system continues to remain notoriously hard. There have been a number of efforts in recent years to simplify system administration (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>). These efforts include system-level mechanisms like virtualization, computational frameworks like mapreduce, and tools that leverage statistical machine-learning techniques to analyze instrumentation data collected from systems. In spite of these efforts, current solutions for administrative tasks like benchmarking, tuning, troubleshooting, and capacity-planning remain far from satisfactory.</p><p>Let us begin with an example scenario. <ref type="figure" target="#fig_0">Figure 1</ref> shows the typical installation of an enterprise database system that consists of the production database, one or more standby databases for high availability, a test database used by database administrators (DBAs) and developers, and possibly a staging database for staged updates as they are moved from development to production. Suppose the DBA notices a slowdown of the production database due to some unknown cause. The DBA may collect some monitoring data on the production database in an attempt to diagnose the problem. However, data collection can increase the load on an already under-performing database, forcing the DBA to shift to the test database. The DBA's usual course of action would be: 1. Create a replica of the production environment on the test database. 2. Get more insight into system behavior by performing runs of the production workload on the test database, and collecting instrumentation data. Multiple runs may be required because of system variability. 3. Form hypotheses regarding potential causes of the performance problem. Do further runs under different system configurations to refine or confirm these hypotheses. For example, new indexes, statistics about the data, or resources may be added; hints may be given to the database query optimizer to force it to choose specific query execution plans; database configuration parameter settings may be changed; and so on. 4. When a fix is found, possibly after much trial and error, a careful validation is done to ensure that the fix will work on the production system. Validation may require multiple runs to test correctness and stability.</p><p>Note that the above process required the DBA to do a number of experiments. Each experiment involved setting up the system in a desired configuration, running a specific workload, and collecting instrumentation data for analysis. Experiments were used (i) to better understand the problem, (ii) during the search process of finding the fix, and (iii) for validating that an accurate and stable fix has been found. We call the overall process an instance of experiment-driven management.</p><p>Are experiments really needed in the above scenario? Quoting researchers from Oracle <ref type="bibr" target="#b10">[11]</ref>: "it is almost impossible to predict the impact of such changes on query performance before actually trying them." Here, "such changes" refer to changes to the database schema (e.g., adding or dropping indexes), updating the statistics (about the data) used by the query optimizer to pick plans, changes to database configuration parameters (e.g., buffer pool sizes), upgrades to the database software or hardware, and others.</p><p>Techniques like performance modeling and machine learning <ref type="bibr" target="#b3">[4]</ref> applied to system instrumentation data can reduce the need for experiments. However, experiment-driven management is and will remain part and parcel of an administrator's job in the foreseeable future. There are two predominant ways in which instrumentation data is generated from systems today:</p><p>• Preproduction testing: Instrumentation data can be collected from runs of the system before it goes into production use, e.g., when load/stress tests are done.</p><p>• Production-time monitoring: Once the system is in production, a variety of products (e.g., HP OpenView, IBM Tivoli) are available for monitoring performance.</p><p>Instrumentation data collected by these methods may not be representative of the full space of system behavior. We only get to observe the performance of query plans, query mixes, and database configurations that were actually used. System performance is a complex function of a number of factors. The collected data may quickly become unrepresentative of system behavior if workload or configuration changes (e.g., the mere addition of an index can change the patterns of I/O that a database issues to the storage system). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions of this Position Paper</head><p>• We observe that experiment-driven management is an essential part of system administration. The process is labor-intensive, and hence, expensive in terms of total cost of system ownership. This domain has largely been overlooked by researchers.</p><p>• Solutions that partially or fully automate experimentdriven management have the potential to solve longstanding problems in system management. We provide examples from our own work and the work of others.</p><p>• We envision experiments being supported as first-class citizens in database and general systems. We also envision an ecosystem of tools and a well-founded methodology (ideally, a science) that guides how experiments are designed and conducted. Towards this end, we identify research challenges and design principles that address: how experiments are set up, where/when are experiments run, and which experiments are done.</p><p>• We give a case study where automated experimentdriven management is applied to tune the large number of configuration parameters in a database system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Role of Experiments in System Management</head><p>We begin with a series of examples showing the critical role of experiments in many aspects of system administration.</p><p>Benchmarking: Researchers, developers, and administrators devote a lot of time and resources to running experiments as part of benchmarking. An important benchmarking activity is response surface mapping (RSM) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> which involves plotting system performance over a large space of workloads and/or system configurations. RSM is a powerful tool to evaluate design and cost tradeoffs, explore the interactions of workloads and system choices, and identify interesting points such as optima, crossover points, or the bounds of the effective operating range for design choices. <ref type="figure" target="#fig_1">Figure 4</ref> shows a response surface generated by running a TPC-H benchmark query in a PostgreSQL database for different settings of the effective cache size and shared buffers parameters. The value of effective cache size is used to determine the chances of a logical I/O hitting in the OS file cache; so its recommended setting is the size of the OS file cache. shared buffers is the size of PostgreSQL's primary buffer cache. Generating each point in <ref type="figure" target="#fig_1">Figure 4</ref> can involve a number of experiments; so many hours to days of effort can go into generating such surfaces. In <ref type="bibr" target="#b6">[7]</ref>, we developed an automated feedback-driven controller that designs and conducts a series of experiments to approximate a response surface efficiently and with statistical rigor.</p><p>Tuning configuration parameters: Database, application, and storage servers ship with a large number of configuration parameters like buffer cache sizes, number of I/O daemons, and parameters input to the database query optimizer's cost model. Finding good settings for these parameters is a challenging task because of the complex ways in which parameter settings can affect performance <ref type="bibr" target="#b8">[9]</ref>. One technique used by administrators to tune configuration parameters is to conduct a series of experiments where the value of one parameter is varied at a time. Such "oneparameter-at-a-time" techniques can have undesirable consequences when significant cross-parameter interactions exist. We will revisit this problem in Section 4.</p><p>Query interactions: A query Q 1 that runs concurrently with another query Q 2 in a database system can impact Q 2 's performance negatively or positively. The resource demands of Q 1 and Q 2 could interfere at physical resources like CPU, L1 or L2 cache, and I/O bandwidth, or at internal resources like latches, locks, and buffer caches. On the other hand, Q 1 may benefit Q 2 by reading data useful for Q 2 into the buffer cache. Traditionally, such interactions were not modeled due to the complexity involved. Experiment-driven modeling-where selected query mixes are scheduled, and the collected data used to train statistical models-holds promise <ref type="bibr" target="#b0">[1]</ref>, even to scheduling map-reduce and scientific jobs <ref type="bibr" target="#b5">[6]</ref>.</p><p>Testing and troubleshooting: Experiments arise naturally in problem debugging and diagnosis. Automated helpdesks ask questions (e.g., did you try rebooting?) to cus- tomers calling about service problems. These questions are generated dynamically based on current information. The answers may lead to more questions until there is enough information to diagnose the root cause. A similar approach has been advocated to diagnose system problems <ref type="bibr" target="#b3">[4]</ref>.</p><p>Towards the elusive self-tuning database system: When a performance problem arises in a self-tuning database system, there is a large arsenal of potential fixes to choose amongst-reallocating resources like CPU and memory, changing settings of configuration parameters like buffer cache sizes, changing the physical design like indexes, or running tasks like defragmentation and statistics gathering. It is nontrivial to pick the best fix, especially if the fix is some combination of the above fixes. To solve this problem, <ref type="bibr" target="#b9">[10]</ref> argues that we need advanced mathematical or statistical models that can map the joint space of workload parameters, state from applying fixes to database performance metrics. More importantly, we need representative data to train and validate these models, and to maintain the models in the face of workload, resource, and configuration changes. Such data can only come from experiments!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dissecting Experiment-driven Management</head><p>Having shown the critical role of experiments in system administration, we now attempt to break experiment-driven management down to its component tasks; which helps in identifying challenges and initial solutions. For ease of presentation and concreteness, we will focus on experiments in database systems. However, the challenges and ideas are more generally applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setting Up an Experiment</head><p>Consider the scenario from Section 1 where a DBA needs to conduct one or more experiments. Setting up these experiments is currently labor-intensive. Automating the tasks involved will be very useful, but it poses research challenges:</p><p>• Ensuring representative workloads: How do we find a representative workload to use in an experiment? For example, experiments for troubleshooting a deadlock may need a workload that preserves the fine-grained interleaving of transactions in the production workload. However, such a workload may be invalid for experiments in configuration parameter tuning because changing parameter settings may change the interleaving.</p><p>• Ensuring representative data: Should experiments be run on a full copy of the production data, or would (faster) experiments on a sample suffice (and if so, can the sample be picked automatically)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Where and When to Run Experiments?</head><p>Before the database goes into production use, experiments can be done on the production platform itself <ref type="figure" target="#fig_0">(Figure 1</ref>). If the database is already in production use and serving real users and applications, then experiments could be done on an offline test platform. These solutions are reasonable, but not sufficient because workloads may change and necessitate new experiments, or a test database platform may not exist. We will describe an initial solution that we have prototyped to address such concerns. This solution also serves to highlight the challenges that arise. The guiding principle behind our solution is: exploit underutilized resources in the production environment for experiments, but never harm the production workload. The two salient features of our solution are:</p><p>• Workbench: Users designate which resources can be used for running experiments. All resources in <ref type="figure" target="#fig_0">Figure 1</ref> are candidates; the production database is the default.</p><p>• Policies: A policy is specified with each resource that dictates when the resource can be used for experiments. A default policy associated with each resource in Figure 1 could be: "if the CPU, memory, and disk utilization of the resource for its home use is below 10% for the past 10 minutes, then the resource can be used for experiments." Home use denotes the regular (i.e., nonexperimental) use of the resource. The design of the workbench is based on splitting the functionality of each resource into two: (i) home use, where the resource is used directly or indirectly to support the production workload, and (ii) garage use, where the resource is used to run experiments. We will describe this design for standby databases, and then generalize to other resources. All commercial databases support one or more hot standby databases whose home use is to keep up to date with the (primary) production database by applying redo logs shipped from the primary. If the primary fails, a standby will quickly take over as the new primary. Hence, standby databases run the same hardware and software as the production database. Standby databases usually have very low utilization since they only have to apply redo records.</p><p>Thus, the standby databases are a valuable and underutilized asset that can be used for on-demand experiments without impacting user-facing queries. However, their home use should not be affected, i.e., the recovery time on failure should not have any noticeable increase. We achieve this property using two resource containers: the home container for home use, and the garage container for running experiments. Our current implementation of resource containers uses the zones feature in Solaris <ref type="bibr" target="#b7">[8]</ref> which allows resources to be allocated dynamically to a zone, with isolation between different zones. Alternately, resource containers can be implemented using virtual machine technology.</p><p>The home container is always responsible for applying the redo log records. When the standby machine is not running experiments, the home container runs on it using all available resources; the garage lies idle. The garage is booted-similar to a machine booting, but much fasteronly when the policy kicks in and allows experiments to be scheduled on the standby machine. During an experiment, both the home and the garage containers will be active, with a partitioning of resources as determined by the experiment scheduler. <ref type="figure">Figure 2</ref> provides an illustration. Both the home and the garage containers run a full and exactly the same copy of the database software. However, on booting, the garage is given a snapshot of the current data in the database. The garage's snapshot is logically separate from the home container's snapshot, but it is physically the same except for copy-on-write semantics. Our current implementation of snapshots and copy-on-write semantics leverages the Zettabyte File System <ref type="bibr" target="#b7">[8]</ref>, and is very efficient.</p><p>When experiments are completed or if the primary fails or there is a policy violation, the garage is halted immediately. All resources are then released to the home container continues to function as a pure standby or takes over as the primary as needed. Booting the garage (including snapshots and resource allocation) takes less than a minute; halting takes even less time. The whole process is so efficient that recovery time is not increased by more than a few seconds. While this description focused on the standby resource, the home/garage design applies to all other resources used by the workbench (including the production database).</p><p>Further details and an experimental evaluation are given in <ref type="bibr" target="#b2">[3]</ref>. Our prototype exposes a number of challenges:</p><p>• What mechanisms should systems provide to simplify the task of running live experiments on production systems without affecting the user-facing workload? • How can we avoid or filter out interference in measurements between the home and garage containers? • How should policies be specified, e.g., at the resource level or through end-to-end service level agreements? • Will the home/garage abstraction extend to experiments in large-scale systems, e.g., multi-tier services, systems distributed geographically, and network configuration? • Can cloud computing-which provides cheap and elastic resources-be leveraged to run live experiments?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Which Experiments to Run?</head><p>Given the infrastructure for conducting experiments before or during production use, let us now consider the question of which experiments to run. Sometimes the answer is easy, e.g., when we want to find the impact of a specific change like the addition of an index. However, the space of potential experiments for various management tasks is often large. Finding the best subset of experiments to do within a limited cost or time budget is nontrivial. While some general guiding principles exist, our experience suggests that good algorithms for experiment selection can differ on a case-by-case basis. Section 4 goes into the details of experiment selection for our case study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case Study: An Advisor for Tuning Database Configuration Parameters</head><p>Our case study focuses on the problem of tuning the large number of configuration parameters like buffer pool sizes, number of I/O daemons, and parameters input to the query optimizer's cost model in database systems. Many database users face issues with the default settings, and resort to trailand-error tuning or rules-of-thumb specified by database experts <ref type="bibr" target="#b8">[9]</ref>. Unfortunately, the behavior of modern database systems is too complex to be captured by static rules. The following observations can be made from <ref type="figure" target="#fig_1">Figure 4</ref>:</p><p>• This surface is complex and contains nonmonotonic and unexpected behavior (performance drops sharply as shared buffers goes above 15% of available memory).</p><p>• Rules-of-thumb settings for shared buffers and effective cache size are respectively 25% and 50% of available memory. Following these rules gives around 100% worse performance compared to a well-tuned setting.</p><p>• The performance impact of changing effective cache size depends on what shared buffers is set to; indicating an interaction between the two.</p><p>Given such complex behavior, experiments (which led to <ref type="figure" target="#fig_1">Figure 4</ref>) are a must for proper database tuning. Knowing the true response surface gives a lot of useful information: Q 1 : Which parameters impact performance the most? Q 2 : Which parameters display strong interactions that make "tune-one-parameter-at-a-time" efforts futile? Q 3 : What is a high-performance setting of the parameters? Q 4 : What are robust regions in the response surface where performance is both satisfactory and stable?</p><p>But, how can a tuning advisor generate such surfaces efficiently? Naively conducting experiments for all possible combinations of the parameters will not scale. For example, conducting all experiments for a 5-parameter space with 6 distinct settings per parameter and average run-time of 10 minutes per experiment takes 60 days! The tuning advisor should aim to produce reasonablyaccurate results for Q 1 -Q 4 while running as few experiments as possible. We outline a technique, called Adaptive Sampling, that can achieve this goal. Adaptive Sampling, illustrated in <ref type="figure">Figure 3</ref>, consists of two phases: bootstrapping and sequential sampling. Because of space constraints, the detailed technical description and empirical evaluation (with up to 30 configuration parameters) of Adaptive Sampling are provided in <ref type="bibr" target="#b2">[3]</ref>. Sequential sampling phase: The sequential sampling phase runs in a loop analyzing the data collected from experiments done so far (and other available data), and plans the experiments to do next. Ideally, the next experiment conducted should be the one that brings in the instrumentation data that improves the accuracy of answers to Q 1 -Q 4 the most. Therefore, we need techniques to estimate the potential improvement in accuracy that results from conducting a candidate experiment. These techniques vary depending on which of Q 1 -Q 4 we are interested in.</p><p>Since space constraints preclude us from giving details, we limit the discussion to some insights. For example, to address Q 1 , we need experiments that check whether changing a parameter can cause significant changes in performance. However, to address Q 3 , we need experiments that quickly hit high-performance regions. The promising experiments for Q 3 are from two types of regions: (i) regions around high-performing settings known so far, (exploitation), and (ii) regions with high uncertainty (exploration). For (ii), we need to capture the uncertainty (or confidence intervals) around predicted performance values in different regions. Various criteria-e.g., based on time, cost of experiments, or expected improvement-can be used to decide when to stop sequential sampling. The sequential sampling phase can also plan batches of experiments that are done in parallel (e.g., in a cloud computing platform).</p><p>We have done an extensive empirical evaluation of Adaptive Sampling using PostgreSQL and the TPC-H, TPC-W, and RUBiS benchmarks. We give a glimpse of the effectiveness of Adaptive Sampling by presenting its results on tuning PostgreSQL for TPC-H Query 18. (Results for more complex workloads are in <ref type="bibr" target="#b2">[3]</ref>.) The full response surface for this query <ref type="figure" target="#fig_1">(Figure 4</ref>) was generated using 99 experiments.</p><p>In the bootstrapping phase of Adaptive Sampling, we ran 10 experiments and estimated the individual impact (sensitivity) of each parameter on performance (by averaging out the effects of other parameters). <ref type="figure" target="#fig_2">Figure 5</ref> shows the sensitivity analysis result for five (among 30) important parameters in PostgreSQL. shared buffers (with an impact score of 44.8) and effective cache size are identified as the most important parameters. Note that with only 10 experiments, <ref type="figure" target="#fig_2">Figure 5</ref> is able to capture the non-monotonic impact of shared buffers and effective cache size on performance; which is consistent with what we observe in <ref type="figure" target="#fig_1">Figure 4</ref>. The sequential sampling phase of Adaptive Sampling focused on the two important parameters identified, and conducted 10 more experiments. The third experiment done in this phase found a setting whose performance was within 95% of the best performance in <ref type="figure" target="#fig_1">Figure 4</ref>. This sample result shows how a principled approach like Adaptive Sampling can save considerable time and effort in parameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>The availability of a powerful workbench for automated, online experiments enables us to rethink the implementation of several system administration tasks. Emerging mechanisms like virtualization and cloud computing provide the foundations for such a workbench. In general, our Adaptive Sampling algorithm-starting with a small bootstrap set of experiments, and then doing experiments based on estimated benefits (and costs)-applies to many tasks. However, we expect the details to differ, sometimes markedly, posing challenging research problems. In closing, we note that experiments will not fully replace current model-based management practices; rather, there is interesting synergy between them that needs to be explored.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Production Database System (DBMS) in an enterprise Figure 2: Workbench for experiments Figure 3: Adaptive sampling for experiment-driven management</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: 2D projection of a response surface for TPC-H Query 18; Database size = 4GB, Memory = 1GB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Bootstrapping phase:Figure 5 :</head><label>5</label><figDesc>Figure 5: Sensitivity analysis plot for Figure 4 the space of possible experiments. More efficient variants include, e.g., Latin Hypercube Sampling [5].</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>We thank Jeff Chase and Piyush Shivam for helpful discussions, and the anonymous reviewers for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling and exploiting query interactions in database systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aboulnaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Munagala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conf. on Info. and Knowledge Management</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Correlating Instrumentation Data to System States: A Building Block for Automated Diagnosis and Control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of OSDI</title>
		<meeting>of OSDI</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Tuning Database Configuration Parameters with iTuned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Thummala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
		<respStmt>
			<orgName>Duke Univ.</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Three research challenges at the intersection of machine learning, statistical induction, and systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fundamental Concepts in the Design of Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Active and accelerated learning of cost models for optimizing scientific applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Very Large Databases (VLDB)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cutting corners: Workbench automation for server benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Marupadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Subramaniam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Tech. Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ZFS and Zones in Solaris</title>
		<ptr target="www.sun.com/software/solaris" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Using Probabilistic Reasoning to Automate Software Tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sullivan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selftuning database technology and information services: from wishful thinking to viable engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moenkeberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zabback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Oracle&apos;s SQL Performance Analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yagoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Belknap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
