<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Forest-Based Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ehime University</orgName>
								<address>
									<settlement>Matsuyama</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Utiyama</surname></persName>
							<email>mutiyama@nict.go.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@hit.edu.cntamura@cs.ehime-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichiro</forename><surname>Sumita</surname></persName>
							<email>eiichiro.sumita@nict.go.jp</email>
							<affiliation key="aff2">
								<orgName type="institution">National Institute of Information and Communications Technology</orgName>
								<address>
									<settlement>Kyoto</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Forest-Based Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-string NMT model). The BLEU score of the proposed method is higher than that of the string-to-string NMT, tree-based NMT, and forest-based SMT systems .</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems ( ; tree-to-string systems ( <ref type="bibr" target="#b6">Eriguchi et al., 2016</ref><ref type="bibr" target="#b7">Eriguchi et al., , 2017</ref>; and string-totree systems ( <ref type="bibr" target="#b0">Aharoni and Goldberg, 2017;</ref><ref type="bibr">Nade- jde et al., 2017</ref>). Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information ( , and can conveniently incorporate prior knowledge ( . * Contribution during internship at National Institute of Information and Communications Technology.</p><p>† Corresponding author</p><p>Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays.</p><p>Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network ( <ref type="bibr" target="#b6">Eriguchi et al., 2016;</ref><ref type="bibr" target="#b28">Zaremoodi and Haffari, 2017)</ref>, representing trees by linearization ( <ref type="bibr" target="#b26">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Dyer et al., 2016;</ref><ref type="bibr" target="#b13">Ma et al., 2017</ref>). Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation. Therefore we focus on this kind of methods in this paper.</p><p>In spite of impressive performance of tree-based NMT systems, they suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors <ref type="bibr" target="#b19">(Quirk and Corston-Oliver, 2006</ref>). For SMT, forest-based methods have employed a packed forest to address this problem <ref type="bibr" target="#b9">(Huang, 2008)</ref>, which represents exponentially many parse trees rather than just the 1-best one ( . But for NMT, (computationally efficient) forestbased methods are still being explored <ref type="bibr">1</ref> .</p><p>Because of the structural complexity of forests, the inexistence of appropriate topological ordering, and the hyperedge-attachment nature of weights (see Section 3.1 for details), it is not trivial to linearize a forest. This hinders the development of forest-based NMT to some extent.</p><p>Inspired by the tree-based NMT methods based on linearization, we propose an efficient forestbased NMT approach (Section 3), which can en-code the syntactic information of a packed forest on the basis of a novel weighted linearization method for a packed forest (Section 3.1), and can decode the linearized packed forest under the simple sequence-to-sequence framework <ref type="bibr">(Section 3.2)</ref>. Experiments demonstrate the effectiveness of our method (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We first review the general sequence-to-sequence model (Section 2.1), then describe tree-based NMT systems based on linearization (Section 2.2), and finally introduce the packed forest, through which exponentially many trees can be represented in a compact manner (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sequence-to-sequence model</head><p>Current NMT systems usually resort to a simple framework, i.e., the sequence-to-sequence model ( ). Given a source sequence (x 0 , . . . , x T ), in order to find a target sequence (y 0 , . . . , y T ) that maximizes the conditional probability p(y 0 , . . . , y T | x 0 , . . . , x T ), the sequence-to-sequence model uses one RNN to encode the source sequence into a fixed-length context vector c and a second RNN to decode this vector and generate the target sequence. Formally, the probability of the target sequence can be calculated as follows:</p><formula xml:id="formula_0">p(y 0 , . . . ,y T | x 0 , . . . , x T ) = T t=0 p(y t | c, y 0 , . . . , y t−1 ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">p(y t | c, y 0 , . . . , y t−1 ) = g(y t−1 , s t , c),<label>(2)</label></formula><formula xml:id="formula_2">s t = f (s t−1 , y t−1 , c), (3) c = q(h 0 , . . . , h T ),<label>(4)</label></formula><formula xml:id="formula_3">h t = f (e t , h t−1 ).<label>(5)</label></formula><p>Here, g, f , and q are nonlinear functions; h t and s t are the hidden states of the source-side RNN and target-side RNN, respectively, c is the context vector, and e t is the embedding of x t .  introduced an attention mechanism to deal with the issues related to long sequences ( ). Instead of encoding the source sequence into a fixed vector c, the attention model uses different c i -s when calculating the target-side output y i at time step i:</p><formula xml:id="formula_4">c i = T j=0 α ij h j ,<label>(6)</label></formula><formula xml:id="formula_5">α ij = exp(a(s i−1 , h j )) T k=0 exp(a(s i−1 , h k )) .<label>(7)</label></formula><p>The function a(s i−1 , h j ) can be regarded as representing the soft alignment between the target-side RNN hidden state s i−1 and the source-side RNN hidden state h j . By changing the format of the source/target sequences, this framework can be regarded as a string-to-string NMT system ), a tree-to-string NMT system ( , or a string-to-tree NMT system (Aharoni and Goldberg, 2017).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Linear-structured tree-based NMT systems</head><p>Regarding the linearization adopted for tree-tostring NMT (i.e., linearization of the source side), <ref type="bibr" target="#b21">Sennrich and Haddow (2016)</ref> encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while  traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-totree NMT (i.e., linearization of the target side), <ref type="bibr" target="#b20">Nadejde et al. (2017)</ref> used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank ( <ref type="bibr" target="#b14">Marcus et al., 1993)</ref>. <ref type="bibr" target="#b27">Wu et al. (2017)</ref> used transition actions to linearize a dependency tree, and employed the sequence-to-sequence framework for NMT.</p><p>It can be seen all current tree-based NMT systems use only one tree for encoding or decoding. In contrast, we hope to utilize multiple trees (i.e., a forest). This is not trivial, on account of the lack of a fixed traversal order and the need for a compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Packed forest</head><p>The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list [6] <ref type="bibr">[7]</ref> [8]</p><p>[9]</p><p>[10] <ref type="bibr">[11]</ref> (a) Packed forest  ( <ref type="bibr" target="#b9">Huang, 2008)</ref>. <ref type="figure" target="#fig_1">Figure 1a</ref> shows a packed forest, which can be unpacked into two constituent trees ( <ref type="figure" target="#fig_1">Figure 1b</ref> and <ref type="figure" target="#fig_1">Figure 1c)</ref>. Formally, a packed forest is a pair V, E, where V is the set of nodes and E is the set of hyperedges. Each v ∈ V can be represented as X i,j , where X is a constituent label and i, j ∈ [0, n] are indices of words, showing that the node spans the words ranging from i (inclusive) to j (exclusive). Here, n is the length of the input sentence. Each e ∈ E is a three-tuple head(e), tails(e), score(e), where head(e) ∈ V is similar to the head node in a constituent tree, and tails(e) ∈ V * is similar to the set of child nodes in a constituent tree. score(e) ∈ R is the logarithm of the probability that tails(e) represents the tails of head(e) calculated by the parser. Based on score(e), the score of a constituent tree T can be calculated as follows:</p><formula xml:id="formula_6">score(T ) = −λn + e∈E(T ) score(e), (8)</formula><p>where E(T ) is the set of hyperedges appearing in tree T , and λ is a regularization coefficient for the sentence length 2 .</p><p>2 Following the configuration of Charniak and Johnson</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Forest-based NMT</head><p>We first propose a linearization method for the packed forest (Section 3.1), then describe how to encode the linearized forest (Section 3.2), which can then be translated by the conventional decoder (see Section 2.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Forest linearization</head><p>Recently, several studies have focused on the linearization methods of a syntax tree, both in the area of tree-based NMT (Section 2.2) and in the area of parsing ( <ref type="bibr" target="#b26">Vinyals et al., 2015;</ref><ref type="bibr" target="#b5">Dyer et al., 2016;</ref><ref type="bibr" target="#b13">Ma et al., 2017)</ref>. Basically, these methods follow a fixed traversal order (e.g., depthfirst), which does not exist for the packed forest (a directed acyclic graph (DAG)). Furthermore, the weights are attached to edges of a packed forest instead of the nodes, which further increase the difficulty.</p><p>Topological ordering algorithms for DAG <ref type="bibr" target="#b10">(Kahn, 1962;</ref><ref type="bibr" target="#b25">Tarjan, 1976)</ref> are not good solutions, because the outputted ordering is not always optimal for machine translation. In particular, a topo- <ref type="bibr">(2005)</ref>, for all the experiments in this paper, we fixed λ to log 2 600.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Linearization of a packed forest</head><formula xml:id="formula_7">1: function LINEARIZEFOREST(V, E, w) 2: v ← FINDROOT(V ) 3: r ← [] 4: EXPANDSEQ(v, r, V, E, w) 5: return r 6: function FINDROOT(V ) 7:</formula><p>for v ∈ V do 8: if v has no parent then 9:</p><p>return v 10: procedure EXPANDSEQ(v, r, V, E, w) 11:</p><p>for e ∈ E do 12:</p><p>if head(e) = v then 13:</p><p>if tails(e) = ∅ then 14:</p><p>for t ∈ SORT(tails(e)) do Sort tails(e) by word indices. 15:</p><formula xml:id="formula_8">EXPANDSEQ(t, r, V, E, w) 16: l ← LINEARIZEEDGE(head(e), w) 17: r.append(l, σ(0.0)) σ is the sigmoid function, i.e., σ(x) = 1 1+e −x , x ∈ R. 18: l ← c LINEARIZEEDGES(tails(e), w) c</formula><p>is a unary operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>19:</head><p>r.append(l, σ(score(e))) 20:</p><formula xml:id="formula_9">else 21: l ← LINEARIZEEDGE(head(e), w) 22: r.append(l, σ(0.0)) 23: function LINEARIZEEDGE(Xi,j, w) 24: return X ⊗ ( j−1 k=i w k ) 25: function LINEARIZEEDGES(v, w) 26: return ⊕v∈vLINEARIZEEDGE(v, w)</formula><p>logical ordering could ignore "word sequential information" and "parent-child information" in the sentences. For example, for the packed forest in <ref type="figure" target="#fig_1">Figure 1a</ref>, <ref type="bibr">[11]</ref>" is a valid topological ordering, the word sequential information of the words (e.g., "John" should be located ahead of the period), which is fairly crucial for translation of languages with fixed pragmatic word order such as Chinese or English, is lost.</p><formula xml:id="formula_10">although "[10]→[1]→[2]→ · · · →[9]→</formula><p>As another example, for the packed forest in <ref type="figure" target="#fig_1">Figure 1a</ref>, nodes <ref type="bibr">[2]</ref>, <ref type="bibr">[9]</ref>, and <ref type="bibr">[10]</ref> are all the children of node <ref type="bibr">[11]</ref>. However, in the topological or-</p><formula xml:id="formula_11">der "[1]→[2]→ · · · →[9]→[10]→[11]," node [2]</formula><p>is quite far from node <ref type="bibr">[11]</ref>, while nodes <ref type="bibr">[9]</ref> and <ref type="bibr">[10]</ref> are both close to node <ref type="bibr">[11]</ref>. The parent-child information cannot be reflected in this topological order, which is not what we would expect.</p><p>To address the above two problems, we propose a novel linearization algorithm for a packed forest (Algorithm 1). The algorithm linearizes the packed forest from the root node (Line 2) to leaf nodes by calling the EXPANDSEQ procedure (Line 15) recursively, while preserving the word order in the sentence <ref type="bibr">(Line 14)</ref>. In this way, word sequential information is preserved. Within the EXPANDSEQ procedure, once a hyperedge is linearized (Line 16), the tails are also linearized immediately (Line 18). In this way, parent-child information is preserved. Intuitively, different parts of constituent trees should be combined in different ways, therefore we define different operators ( c , ⊗, ⊕, or ) to represent the relationships between different parts, so that the representations of these parts can be combined in different ways (see Section 3.2 for details). Words are concatenated by the operator "" with each other, a word and a constituent label is concatenated by the operator "⊗", the linearization results of child nodes are concatenated by the operator "⊕" with each other, while the unary operator " c " is used to indicate that the node is the child node of the previous part. Furthermore, each token in the linearized sequence is related to a score, representing the confidence of the parser.</p><formula xml:id="formula_12">NNP⊗John / NP⊗John / c NNP⊗John / VBZ⊗has / DT⊗a / NN⊗dog / NP⊗adog / c DT⊗a⊕NN⊗dog / NP⊗adog / c DT⊗a⊕NN⊗dog / S⊗adog / c NP⊗adog / VP⊗hasadog / c VBZ⊗has⊕NP⊗adog / c VBZ⊗has⊕S⊗adog / .⊗. / S⊗Johnhasadog. / c NP⊗John⊕VP⊗hasadog⊕.⊗.</formula><p>The linearization result of the packed forest in <ref type="figure" target="#fig_1">Figure 1a</ref> is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. Tokens in the linearized sequence are separated by slashes. Each token in the sequence is composed of different types of symbols and combined by different operators. We can see that word sequential information is preserved. For example, "NNP⊗John" (linearization result of node <ref type="bibr">[1]</ref>) is in front of "VBZ⊗has" (linearization result of node <ref type="bibr">[3]</ref>), which is in front of "DT⊗a" (linearization result of node <ref type="bibr">[4]</ref>). Moreover, parent-child information is also preserved. For example, "NP⊗John" (linearization result of node <ref type="bibr">[2]</ref>) is followed by " c NNP⊗John" (linearization result of node <ref type="bibr">[1]</ref>, the child of node <ref type="bibr">[2]</ref>).</p><p>Note that our linearization method cannot fully recover packed forest. What we want to do is not to propose a fully recoverable linearization method. What we actually want to do is to encode syntax information as much as possible, so that we can improve the performance of NMT. As will be shown in Section 4, this goal is achieved.</p><p>Also note that there is one more advantage of our linearization method: the linearized sequence is a weighted sequence, while all the previous studies ignored the weights during linearization.</p><p>As will be shown in Section 4, the weights are actually important not only for the linearization of a packed forest, but also for the linearization of a single tree. By preserving only the nodes and hyperedges in the 1-best tree and removing all others, our linearization method can be regarded as a treelinearization method. Compared with other treelinearization methods, our method combines several different kinds of information within one symbol, retaining the parent-child information, and incorporating the confidence of the parser in the sequence. We examine whether the weights can be useful not only for linear structured tree-based NMT but also for our forest-based NMT.</p><p>Furthermore, although our method is nonreversible for packed forests, it is reversible for constituent trees, in that the linearization is processed exactly in the depth-first traversal order and all necessary information in the tree nodes has been encoded. As far as we know, there is no previous work on linearization of packed forests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding the linearized forest</head><p>The linearized packed forest forms the input of the encoder, which has two major differences from the input of a sequence-to-sequence NMT system. First, the input sequence of the encoder consists of two parts: the symbol sequence and the score sequence. Second, each symbol in the symbol sequence consists of several parts (words and constituent labels), which are combined by certain operators ( c , ⊗, ⊕, or ). Based on these observations, we propose two new frameworks, which are illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>. Formally, the input layer receives the sequence</p><formula xml:id="formula_13">(l 0 , ξ 0 , . . . , l T , ξ T )</formula><p>, where l i denotes the i-th symbol and ξ i its score. Then, the sequence is fed into the score layer and the symbol layer. The score and symbol layers receive the sequence and output the score sequence ξ = (ξ 0 , . . . , ξ T ) and symbol sequence l = (l 0 , . . . , l T ), respectively, from the input. Any item l ∈ l in the symbol layer has the form</p><formula xml:id="formula_14">l = o 0 x 1 o 1 . . . x m−1 o m−1 x m ,<label>(9)</label></formula><p>where each x k (k = 1, . . . , m) is a word or a constituent label, m is the total number of words and constituent labels in a symbol, o 0 is " c " or empty, and each o k (k = 1, . . . , m − 1) is either "⊗", "⊕", or "". Then, in the node/operator layer, the x-s and o-s are separated and rearranged as x = (x 1 , . . . , x m , o 0 , . . . , o m−1 ), which is fed to the pre-embedding layer. The pre-embedding layer generates a sequence p = (p 1 , . . . , p m , . . . , p 2m ), which is calculated as follows:</p><formula xml:id="formula_15">p = W emb [I(x)].<label>(10)</label></formula><p>Here, the function I(x) returns a list of the indices in the dictionary for all the elements in x, which consist of words, constituent labels, or operators. In addition, W emb is the embedding matrix of size (|w word | + |w label | + 4) × d word , where |w word | and |w label | are the total number of words and constituent labels, respectively, d word is the dimension of the word embedding, and there are four possible operators: " c ," "⊗," "⊕," and "." Note that p is a list of 2m vectors, and the dimension of each vector is d word .</p><p>Because the length of the sequence of the input layer is T + 1, there are T + 1 different ps in the pre-embedding layer, which we denote by P = (p 0 , . . . , p T ). Depending on where the score layer is incorporated, we propose two frameworks: Score-on-Embedding (SoE) and Score-onAttention (SoA). In SoE, the k-th element of the embedding layer is calculated as follows:</p><formula xml:id="formula_16">e k = ξ k p∈p k p,<label>(11)</label></formula><p>while in SoA, the k-th element of the embedding layer is calculated as</p><formula xml:id="formula_17">e k = p∈p k p,<label>(12)</label></formula><p>where k = 0, . . . , T . Note that e k ∈ R d word . In this manner, the proposed forest-to-string NMT framework is connected with the conventional sequence-to-sequence NMT framework. After calculating the embedding vectors in the embedding layer, the hidden vectors are calculated using Equation 5. When calculating the context vector c i -s, SoE and SoA differ from each other. For SoE, the c i -s are calculated using Equation 6 and 7, while for SoA, the α ij -s used to calculate the c i -s are determined as follows:</p><formula xml:id="formula_18">α ij = exp(ξ j a(s i−1 , h j )) T k=0 exp(ξ k a(s i−1 , h k )) .<label>(13)</label></formula><p>Then, using the decoder of the sequence-tosequence framework, the sentence of the target language can be generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We evaluate the effectiveness of our forest-based NMT systems on English-to-Chinese and Englishto-Japanese translation tasks 3 . The statistics of the corpora used in our experiments are summarized in <ref type="table">Table 1</ref> which the parser cannot generate the packed forest successfully and the sentences longer than 80 words. For NIST datasets, we simply choose the first reference among the four English references of NIST corpora, because all of them are independent with each other, according to the documents of NIST datasets. For Chinese sentences, we used Stanford segmenter 5 for segmentation. For Japanese sentences, we followed the preprocessing steps recommended in WAT 2017 <ref type="bibr">6</ref> . We implemented our framework based on nematus 8 ( <ref type="bibr" target="#b20">Sennrich et al., 2017</ref>). For optimization, we used the Adadelta algorithm <ref type="bibr" target="#b29">(Zeiler, 2012)</ref>. In order to avoid overfitting, we used dropout ( <ref type="bibr" target="#b22">Srivastava et al., 2014</ref>) on the embedding layer and hidden layer, with the dropout probability set to 0.2. We used the gated recurrent unit ( ) as the recurrent unit of RNNs, which are bi-directional, with one hidden layer.</p><p>Based on the tuning result, we set the maximum length of the input sequence to 300, the hidden layer size as 512, the dimension of word embedding as 620, and the batch size for training as 40. We pruned the packed forest using the algorithm of <ref type="bibr" target="#b9">Huang (2008)</ref>, with a threshold of 5. If the linearization of the pruned forest is still longer than 300, then we linearize the 1-best parsing tree instead of the forest. During decoding, we used beam search, and fixed the beam size to 12. For the case of Forest (SoA), with 1 core of Tesla K80 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed is about 10 sentences per second.  <ref type="table" target="#tab_4">Table 2</ref>: English-Chinese experimental results (character-level BLEU). "FS," "TN," and "FN" denote forest-based SMT, tree-based NMT, and forest-based NMT systems, respectively. We performed the paired bootstrap resampling significance test <ref type="bibr" target="#b11">(Koehn, 2004</ref>) over the NIST MT 03 to 05 corpus, with respect to the s2s baseline, and list the p values in the  <ref type="table">Table 3</ref>: English-Japanese experimental results (character-level BLEU). To avoid the affect of segmentation errors, the performance were evaluated by character-level BLEU ( <ref type="bibr" target="#b18">Papineni et al., 2002</ref>). We compare our proposed models (i.e., Forest (SoE) and Forest (SoA)) with three types of baseline: a string-to-string model (s2s), forest-based models that do not use score sequences (Forest (No score)), and tree-based models that use the 1-best parsing tree (1-best (No score, SoE, SoA)). For the 1-best models, we preserve the nodes and hyperedges that are used in the 1-best constituent tree in the packed forest, and remove all other nodes and hyperedges, yielding a pruned forest that contains only the 1-best constituent tree. For the "No score" configurations, we force the input score sequence to be a sequence of 1.0 with the same length as the input symbol sequence, so that neither the embedding layer nor the attention layer are affected by the score sequence. In addition, we also perform a comparison with some state-of-the-art tree-based systems that are  <ref type="formula" target="#formula_1">(2008)</ref>, we use the implementation of cicada 11 . For , we reimplemented the "Mixed RNN Encoder" model, because of its outstanding performance on the NIST MT corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental results</head><p>We can see that for both English-Chinese and English-Japanese, compared with the s2s baseline system, both the 1-best and forest-based configurations yield better results. This indicates syntactic information contained in the constituent trees or forests is indeed useful for machine translation. Specifically, we observe the following facts. First, among the three different frameworks SoE, SoA, and No-score, the SoA framework performs the best, while the No-score framework per- <ref type="bibr">[Source]</ref> In the Czech Republic , which was ravaged by serious floods last summer , the temperatures in its border region adjacent to neighboring Slovakia plunged to minus 18 degrees Celsius .</p><p>[  forms the worst. This indicates that the scores of the edges in constituent trees or packed forests, which reflect the confidence of the correctness of the edges, are indeed useful. In fact, for the 1-best constituent parsing tree, the score of the edge reflects the confidence of the parser. By using this information, the NMT system succeed to learn a better attention, paying much attention to the confident structure and not paying attention to the unconfident structure, which improved the translation performance. This fact is ignored by previous studies on tree-based NMT. Furthermore, it is better to use the scores to modify the values of attention instead of rescaling the word embeddings, because modifying word embeddings carelessly may change the semantic meanings of words. Second, compared with the cases that only using the 1-best constituent trees, using packed forests yields statistical significantly better results for the SoE and SoA frameworks. This shows the effectiveness of using more syntactic information. Compared with one constituent tree, the packed forest, which contains multiple different trees, describes the syntactic structure of the sentence in different aspects, which together increase the accuracy of machine translation. However, without using the scores, the 1-best constituent tree is preferred. This is because without using the scores, all trees in the packed forest are treated equally, which makes it easy to import noise into the encoder.</p><p>Compared with other types of state-of-the-art systems, our systems using only the 1-best tree (1-best(SoE, SoA)) are better than the other treebased systems. Moreover, our NMT systems using the packed forests achieve the best performance. These results also support the usefulness of the scores of the edges and packed forests in NMT.</p><p>As for the efficiency, the training time of the SoA system was slightly longer than that of the SoE system, which was about twice of the s2s baseline. The training time of the tree-based system was about 1.5 times of the baseline. For the case of Forest (SoA), with 1 core of Tesla P100 GPU and LDC corpus as the training data, training spent about 10 days, and decoding speed was about 10 sentences per second. The reason for the relatively low efficiency is that the linearized sequences of packed forests were much longer than word sequences, enlarging the scale of the inputs. Despite this, the training process ended within reasonable time. <ref type="figure" target="#fig_5">Figure 4</ref> illustrates the translation results of an English sentence using several different configurations: the s2s baseline, using only the 1-best tree (SoE), and using the packed forest (SoE). This is a sentence from NIST MT 03, and the training corpus is the LDC corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative analysis</head><p>For the s2s case, no syntactic information is utilized, and therefore the output of the system is not a grammatical Chinese sentence. The attributive phrase of "Czech border region" is a complete sentence. However, the attributive is not allowed to be a complete sentence in Chinese.</p><p>For the case of using 1-best constituent tree, the output is a grammatical Chinese sentence. However, the phrase "adjacent to neighboring Slovakia" is completely ignored in the translation result. After analyzing the constituent tree, we found that this phrase was incorrectly parsed as an "adverb phrase", so that the NMT system paid little attention to it, because of the low confidence given by the parser.</p><p>In contrast, for the case of the packed forest, we can see this phrase was not ignored and was translated correctly. Actually, besides "adverb phrase", this phrase was also correctly parsed as an "adjective phrase", and covered by multiple different nodes in the forest, making it difficult for the encoder to ignore the phrase.</p><p>We also noticed that our method performed better on learning attention. For the example in <ref type="figure" target="#fig_5">Fig- ure 4</ref>, we observed that for s2s model, the decoder paid attention to the word "Czech" twice, which causes the output sentence contains the Chinese translation of Czech twice. On the other hand, for our forest model, by using the syntax information, the decoder paid attention to the phrase "In the Czech Republic" only once, making the decoder generates the correct output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Incorporating syntactic information into NMT systems is attracting widespread attention nowadays. Compared with conventional string-to-string NMT systems, tree-based systems demonstrate a better performance with the help of constituent trees or dependency trees.</p><p>The first noteworthy study is <ref type="bibr" target="#b6">Eriguchi et al. (2016)</ref>, which used Tree-structured LSTM <ref type="bibr" target="#b24">(Tai et al., 2015)</ref> to encode the HPSG syntax tree of the sentence in the source-side in a bottom-up manner. Then, <ref type="bibr" target="#b3">Chen et al. (2017)</ref> enhanced the encoder with a top-down tree encoder.</p><p>As a simple extension of <ref type="bibr" target="#b6">Eriguchi et al. (2016)</ref>, very recently, <ref type="bibr" target="#b28">Zaremoodi and Haffari (2017)</ref> proposed a forest-based NMT method by representing the packed forest with a forest-structured neural network. However, their method was evaluated in small-scale MT settings (each training dataset consists of under 10k parallel sentences). In contrast, our proposed method is effective in a largescale MT setting, and we present qualitative analysis regarding the effectiveness of using forests in NMT.</p><p>Although these methods obtained good results, the tree-structured network used by the encoder made the training and decoding relatively slow, therefore restricts the scope of application.</p><p>Other attempts at encoding syntactic trees have also been proposed. <ref type="bibr" target="#b7">Eriguchi et al. (2017)</ref> combined the Recurrent Neural Network Grammar ( <ref type="bibr" target="#b5">Dyer et al., 2016</ref>) with NMT systems, while  linearized the constituent tree and encoded it using RNNs. The training of these methods is fast, because of the linear structures of RNNs. However, all these syntax-based NMT systems used only the 1-best parsing tree, making the systems sensitive to parsing errors.</p><p>Instead of using trees to represent syntactic information, some studies use other data structures to represent the latent syntax of the input sentence. For example, <ref type="bibr" target="#b8">Hashimoto and Tsuruoka (2017)</ref> proposed translating using a latent graph. However, such systems do not enjoy the benefit of handcrafted syntactic knowledge, because they do not use a parser trained from a large treebank with human annotations.</p><p>Compared with these related studies, our framework utilizes a linearized packed forest, meaning the encoder can encode exponentially many trees in an efficient manner. The experimental results demonstrated these advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future work</head><p>We proposed a new NMT framework, which encodes a packed forest for the source sentence using linear-structured neural networks, such as RNN. Compared with conventional string-tostring NMT systems and tree-to-string NMT systems, our framework can utilize exponentially many linearized parsing trees during encoding, without significantly decreasing the efficiency. This represents the first attempt at using a forest under the string-to-string NMT framework. The experimental results demonstrate the effectiveness of our framework.</p><p>As future work, we plan to design some more elaborate structures to incorporate the score layer in the encoder. Further improvement in the translation performance is expected to be achieved for the forest-based NMT system. We will also apply the proposed linearization method to other tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of (a) a packed forest. The numbers in the brackets located at the upper-left corner of each node in the packed forest show one correct topological ordering of the nodes. The packed forest is a compact representation of two trees: (b) the correct constituent tree, and (c) an incorrect constituent tree. Note that the terminal nodes (i.e., words in the sentence) in the packed forest are shown only for illustration, and they do not belong to the packed forest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Linearization result of the packed forest in Figure 1a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The framework of the forest-based NMT system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>publicly available, including an SMT system (Mi et al., 2008) and the NMT systems (Eriguchi et al. (2016) 9 , Chen et al. (2017) 10 , and Li et al. (2017)). For Mi et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Chinese translation results of an English sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>John S 0,5</head><label></label><figDesc></figDesc><table>NP 0,1 

VP 1,4 

. 4,5 

NP 2,4 

NNP 0,1 
VBZ 1,2 

has 

DT 2,3 

a 
dog 

NN 3,4 

. 

NP 2,4 

S 2,4 

-3.9490 
4.7280 
5.0983 

-1.3092 

-6.7403 -18.1946 

5.8665 

[1] 

[2] 

[3] 
[4] 
[5] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>. The packed forests of English sentences are obtained by the constituent parser proposed by Huang (2008) 4 . We filtered out the sentences for 3 English is commonly chosen as the target language. We chose English as the source language because a high- performance forest parser is not available for</figDesc><table>other languages. 
4 http://web.engr.oregonstate.edu/ 

˜ huanlian/software/forest-reranker/ 
forest-charniak-v0.8.tar.bz2 

Language 
Corpus 
Usage 
#Sent. 

English-Japanese 
ASPEC 

train 
100,000 
dev. 
1790 
test 
1812 

English-Chinese 

LDC 7 
train 
1,423,695 
FBIS 
233,510 
NIST MT 02 
dev. 
876 
NIST MT 03 
test 

919 
NIST MT 04 
1,788 
NIST MT 05 
1,082 

Table 1: Statistics of the corpora. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>table .</head><label>.</label><figDesc></figDesc><table>Types 
Systems &amp; BLEU 
p value 
Configurations 
(test) 
FS 
Mi et al. (2008) 
34.13 
-

TN 

Eriguchi et al. (2016) 
37.52 
-
Chen et al. (2017) 
36.94 
-
Li et al. (2017) 
36.21 
-

FN 

s2s 
37.10 
-
1-best (No score) 
38.01 
&lt; 0.05 
1-best (SoE) 
38.53 
&lt; 0.01 
1-best (SoA) 
39.42 
&lt; 0.001 
Forest (No score) 
37.92 
&lt; 0.1 
Forest (SoE) 
41.35 
&lt; 0.01 
Forest (SoA) 
42.17 
&lt; 0.005 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 and 3 summarize the experimental results.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> Zaremoodi and Haffari (2017) have proposed a forestbased NMT method based on a forest-structured neural network recently, but it is computationally inefficient (see Section 5).</note>

			<note place="foot" n="5"> https://nlp.stanford.edu/software/ stanford-segmenter-2017-06-09.zip 6 http://lotus.kuee.kyoto-u.ac.jp/WAT/ WAT2017/baseline/dataPreparationJE.html 7 LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08, and LDC2005T06 8 https://github.com/EdinburghNLP/ nematus</note>

			<note place="foot" n="9"> https://github.com/tempra28/tree2seq 10 https://github.com/howardchenhd/ Syntax-awared-NMT 11 https://github.com/tarowatanabe/ cicada</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We are grateful to the anonymous reviewers for their insightful comments and suggestions. We thank Lemao Liu from Tencent AI Lab for his suggestions about the experiments. We thank Atsushi Fujita whose suggestions greatly improve the readability and the logical soundness of this paper. This work was done during the internship of Chunpeng Ma at NICT. Akihiro Tamura is supported by JSPS KAKENHI Grant Number JP18K18110. Tiejun Zhao is supported by the National Natural Science Foundation of China (NSFC) via grant 91520204 and State High-Tech Development Plan of China (863 program) via grant 2015AA015405.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards string-to-tree neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="132" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coarseto-fine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved neural machine translation with a syntax-aware encoder and decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1936" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07776</idno>
		<title level="m">Recurrent neural network grammars</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to parse and translate improves neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural machine translation with source-side latent graph parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02265</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Forest reranking: Discriminative parsing with non-local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="586" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topological sorting of large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arthur B Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="558" to="562" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical significance tests for machine translation evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2004</title>
		<meeting>EMNLP 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="388" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling source syntax for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="688" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deterministic attention for sequence-to-sequence constituent parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumita</forename><surname>Eiichiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3237" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Mitchell P Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Forest-based translation rule extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="206" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Forestbased translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-08: HLT</title>
		<meeting>ACL-08: HLT</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Nadejde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01147</idno>
	</analytic>
	<monogr>
		<title level="m">Philipp Koehn, and Alexandra Brich. 2017. Syntax-aware neural machine translation using ccg</title>
		<imprint/>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The impact of parse quality on syntactically-informed statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon Corston-</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2006 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="62" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nematus: a toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Hitschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Läubli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Valerio Miceli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jozef</forename><surname>Barone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mokry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nadejde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the Software Demonstrations of the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="65" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02892</idno>
		<title level="m">Linguistic input features improve neural machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edge-disjoint spanning trees and depth-first search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robert Endre Tarjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="185" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Incorporating syntactic uncertainty in neural machine translation with forest-to-sequence model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poorya</forename><surname>Zaremoodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07019</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthew D Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<title level="m">Adadelta: an adaptive learning rate method</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Prior knowledge integration for neural machine translation using posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1514" to="1523" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
