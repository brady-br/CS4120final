<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 16th USENIX Conference on File and Storage Technologies. Open access to the Proceedings of the 16th USENIX Conference on File and Storage Technologies is sponsored by USENIX. Clay Codes: Moulding MDS Codes to Yield an MSR Code Clay Codes: Moulding MDS Codes to Yield an MSR Code</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myna</forename><surname>Vajha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Ramkumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhagyashree</forename><surname>Puranik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Kini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elita</forename><surname>Lobo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birenjith</forename><surname>Sasidharan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Vijay</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandar</forename><surname>Barg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Narayanamurthy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Hussain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Nandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myna</forename><surname>Vajha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinayak</forename><surname>Ramkumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhagyashree</forename><surname>Puranik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Kini</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elita</forename><surname>Lobo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Birenjith</forename><surname>Sasidharan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Vijay</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Barg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Ye</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Narayanamurthy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Hussain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Nandi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bangalore</roleName><forename type="first">Netapp</forename><surname>Atg</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<addrLine>February 12-15</addrLine>
									<postCode>2018 â€¢</postCode>
									<settlement>Oakland, Bangalore</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">NetApp ATG</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 16th USENIX Conference on File and Storage Technologies. Open access to the Proceedings of the 16th USENIX Conference on File and Storage Technologies is sponsored by USENIX. Clay Codes: Moulding MDS Codes to Yield an MSR Code Clay Codes: Moulding MDS Codes to Yield an MSR Code</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://www.usenix.org/conference/fast18/presentation/vajha</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>With increase in scale, the number of node failures in a data center increases sharply. To ensure availability of data, failure-tolerance schemes such as Reed-Solomon (RS) or more generally, Maximum Distance Separable (MDS) erasure codes are used. However, while MDS codes offer minimum storage overhead for a given amount of failure tolerance, they do not meet other practical needs of today&apos;s data centers. Although modern codes such as Minimum Storage Regenerating (MSR) codes are designed to meet these practical needs, they are available only in highly-constrained theoretical constructions, that are not sufficiently mature enough for practical implementation. We present Clay codes that extract the best from both worlds. Clay (short for Coupled-Layer) codes are MSR codes that offer a simplified construction for decoding/repair by using pairwise coupling across multiple stacked layers of any single MDS code. In addition, Clay codes provide the first practical implementation of an MSR code that offers (a) low storage overhead, (b) simultaneous optimality in terms of three key parameters: repair bandwidth, sub-packetization level and disk I/O, (c) uniform repair performance of data and parity nodes and (d) support for both single and multiple-node repairs, while permitting faster and more efficient repair. While all MSR codes are vector codes, none of the distributed storage systems support vector codes. We have modified Ceph to support any vector code, and our contribution is now a part of Ceph&apos;s master codebase. We have implemented Clay codes, and integrated it as a plu-gin to Ceph. Six example Clay codes were evaluated on a cluster of Amazon EC2 instances and code parameters were carefully chosen to match known erasure-code deployments in practice. A particular example code, with storage overhead 1.25x, is shown to reduce repair network traffic by a factor of 2.9 in comparison with RS codes and similar reductions are obtained for both repair time and disk read.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The number of failures in storage subsystems increase as data centers scale <ref type="bibr" target="#b9">[11]</ref> [17] <ref type="bibr" target="#b27">[29]</ref>. In order to ensure data availability and durability, failure-tolerant solutions such as replication and erasure codes are used. It is important for these solutions to be highly efficient so that they incur low cost in terms of their utilization of storage, computing and network resources. This additional cost is considered an overhead, as the redundancy introduced for failure tolerance does not aid the performance of the application utilizing the data.</p><p>In order to be failure tolerant, data centers have increasingly started to adopt erasure codes in place of replication. A class of erasure codes known as maximum distance separable (MDS) codes offer the same level of failure tolerance as replication codes with minimal storage overhead. For example, Facebook <ref type="bibr" target="#b17">[19]</ref> reported reduced storage overhead of 1.4x by using Reed-Solomon (RS) codes, a popular class of MDS codes, as opposed to the storage overhead of 3x incurred in triple replication <ref type="bibr" target="#b11">[13]</ref>. The disadvantage of the traditional MDS codes is their high repair cost. In case of replication, when a node or storage subsystem fails, an exact copy of the lost data can be copied from surviving nodes. However, in case of erasure codes, dependent data that is more voluminous in comparison with the lost data, is copied from surviving nodes and the lost data is then computed by a repair node, which results in a higher repair cost when compared to replication. This leads to increased repair bandwidth and repair time.</p><p>A class of erasure codes, termed as minimum storage regenerating (MSR) codes, offer all the advantages of MDS codes but require lesser repair bandwidth. Until recently, MSR codes lacked several key desirable properties that are important for practical systems. For example, they were computationally more complex <ref type="bibr" target="#b12">[14]</ref>, or demonstrated non-uniform repair characteristics for different types of node failures <ref type="bibr" target="#b16">[18]</ref>, or were able to recover from only a limited (one or two) number of failures <ref type="bibr" target="#b18">[20]</ref>, or they lacked constructions of common erasure code configurations <ref type="bibr" target="#b22">[24]</ref>, <ref type="bibr" target="#b18">[20]</ref>. The first theoretical construction that offered all the desirable properties of an MSR code was presented by Ye and Barg <ref type="bibr" target="#b33">[35]</ref>.</p><p>This paper presents Clay codes that extend the theoretical construction presented in <ref type="bibr" target="#b33">[35]</ref>, with practical considerations. Clay codes are constructed by placing any MDS code in multiple layers and performing pair-wise coupling across layers. Such a construction offers efficient repair with optimal repair bandwidth, causing Clay codes to fall in the MSR arena.</p><p>We implement Clay codes and make it available as open-source under LGPL. We also integrate Clay codes as a plugin with Ceph, a distributed object storage system. Ceph supports scalar erasure codes such as RS codes. However, it does not support vector codes. We modified Ceph to support any vector code, and our contribution is now included in Ceph's master codebase <ref type="bibr" target="#b2">[4]</ref>.</p><p>In erasure coding terminology, scalar codes require block-granular repair data, while vector codes can work at the sub-block granularity for repair. In Ceph, the equivalent of an erasure-coded block is one chunk of object. By this, we mean that Ceph supports chunkgranular repair data, while our contribution extended it to sub-chunk granularity. To the best of our knowledge, after our contribution, Ceph has become the first distributed storage system to support vector codes. Also, if Clay codes become part of Ceph's codebase, this will be the first-ever implementation of an MSR code that provides all desirable practical properties, and which is integrated to a distributed storage system.</p><p>Our contributions include (a) the construction of Clay codes as explained in Section 3, (b) the modification made to Ceph in order to support any vector code, explained in Section 4, and (c) the integration of Clay codes as a plugin to Ceph, explained in Section 4. We conducted experiments to compare the performance of Clay codes with RS codes available in Ceph and the results are presented in Section 5. One of the example Clay codes that we evaluated, which has a storage overhead of 1.25x, was able to bring down the repair network traffic by a factor of 2.9 when compared with the RS code of same parameters. Similar reductions were also obtained for disk read and repair time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Preliminaries</head><p>Erasure Code Erasure codes are an alternative to replication for ensuring failure tolerance in data storage. In an <ref type="bibr">[n, k]</ref> erasure-coded system, data pertaining to an object is first divided into k data chunks and then encoded to obtain m = n âˆ’ k parity chunks. When we do not wish to distinguish between a data or parity chunk, we will simply refer to the chunk as a coded chunk. The collection of n coded chunks obtained after encoding are stored in n distinct nodes. Here, by node, we mean an independent failure domain such as a disk or a storage node of a distributed storage system (DSS). The storage efficiency of an erasure code is measured by storage overhead defined as the ratio of the number of coded chunks n to the number of data chunks k. Every erasure code has an underlying finite field over which computations are performed. For the sake of simplicity, we assume here that the field is of size 2 8 and hence each element of the finite field can be represented by a byte <ref type="bibr" target="#b0">1</ref> . It is convenient to differentiate at this point, between scalar and vector codes. Scalar Codes Let each data chunk be comprised of L bytes. In the case of a scalar code, one byte from each of the k data chunks is picked and the k bytes are linearly combined in m different ways, to obtain m parity bytes. The resultant set of n = k + m bytes so obtained is called a codeword. This operation is repeated in parallel for all the L bytes in a data chunk to obtain L codewords. This operation will also result in the creation of m parity chunks, each composed of L bytes (see <ref type="figure">Fig. 1</ref>). As mentioned above, every coded chunk is stored on a different node.</p><p>Data chunks Parity chunks Codeword Byte <ref type="figure">Figure 1</ref>: A pictorial representation of a scalar code. The L = 6 horizontal layers are the codewords and the n = 6 vertical columns, the chunks, with the first k = 4 chunks corresponding to data chunks and the last (n âˆ’ k) = 2 chunks, the parity chunks. Each unit (tiny rectangle) in the figure corresponds to a single byte.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector Codes</head><p>The difference in the case of vector codes is that here, one works with ordered collections of Î± â‰¥ 1 bytes at a time. For convenience, we will refer to such an ordered collection of Î± bytes as a superbyte. In the encoding process, a superbyte from each of the k data chunks is picked and the k superbytes are then linearly combined in m different ways, to obtain m parity superbytes. The resultant set of n = k + m superbytes is called a (vector) codeword. This operation is repeated in parallel for all the N = L Î± superbytes in a data chunk to obtain N codewords. <ref type="figure" target="#fig_0">Figure 2</ref> shows a simple example where each superbyte consists of just two bytes.</p><p>The number Î± of bytes within a superbyte is termed the sub-packetization level of the code. Scalar codes such as RS codes can be regarded as having subpacketization level Î± = 1. Seen differently, one could view a vector code as replacing Î± scalar codewords with a single vector codeword. The advantage of vector codes is that repair of a coded chunk in a failed node can potentially be accomplished by accessing only a subset of the Î± bytes within the superbyte, present in each of the remaining coded chunks, corresponding to the same codeword. This reduces network traffic arising from node repair.</p><p>Sub-chunking through Interleaving In <ref type="figure" target="#fig_0">Fig. 2</ref>, we have shown the Î± bytes associated to a superbyte as being stored contiguously. When the sub-packetization level Î± is large, given that operations involving multiple codewords are carried out in parallel, it is advantageous, from an ease-of-memory-access viewpoint, to interleave the bytes so that the corresponding bytes across different codewords are stored contiguously as shown in <ref type="figure">Fig. 3</ref>. This is particularly true, when the number N of superbytes within a chunk is large, for example, when L = 8KB and Î± = 2, contiguous access to N = 4K bytes is possible. With interleaving, each data chunk is partitioned into Î± subsets, which we shall refer to as subchunks. Thus each sub-chunk within a node, holds one byte from each of the N codewords stored in the node. Figure 3: This figure shows the interleaving of the corresponding bytes within a superbyte across codewords, for the particularly simple case of two bytes within a superbyte. This results in a partitioning of the data chunk into sub-chunks and can lead to improved-memory-access performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MDS Codes</head><p>The sub-class of (n, k) erasure codes, either scalar or vector, having the property that they can recover from the failure of any (n âˆ’ k) nodes are called MDS codes. For a fixed k, these codes have the smallest storage overhead n k among any of the erasure codes that can recover from a failure of a fixed number of n âˆ’ k nodes. Examples include RS, Row-Diagonal Parity <ref type="bibr" target="#b7">[9]</ref> and EVENODD <ref type="bibr" target="#b5">[7]</ref> codes, see <ref type="bibr" target="#b3">[5]</ref> for additional examples. Facebook data centers <ref type="bibr" target="#b26">[28]</ref> have employed an (14, 10) RS code in their data warehouse cluster.</p><p>Node Repair The need for node repair in a distributed storage system can arise either because a particular hardware component has failed, is undergoing maintenance, is being rebooted or else, is simply busy serving other simultaneous requests for data. A substantial amount of network traffic is generated on account of node-repair operations. An example cited in <ref type="bibr" target="#b26">[28]</ref>, is one of a Facebook data-warehouse, that stores multiple petabytes of data, where the median amount of data transferred through top-of-rack switches for the purposes of node repair, is in excess of 0.2 petabytes per day. The traffic arising from node-repair requests, eats into the bandwidth available to serve user requests for data. The time taken for node repair also directly affects system availability. Thus there is strong interest in coding schemes that minimize the amount of data transfer across the network, and the time taken to repair a failed node. Under the conventional approach to repairing an RS code for instance, one would have to download k times the amount of data as is stored in a failed node to restore the failed node, which quite clearly, is inefficient.</p><p>MSR Codes MSR codes <ref type="bibr" target="#b8">[10]</ref> are a sub-class of vector MDS codes that have the smallest possible repair bandwidth. To restore a failed node containing Î± bytes in an (n, k) MSR code, the code first contacts an arbitrarilychosen subset of d helper nodes, where d is a design parameter that can take on values ranging from k to (n âˆ’ 1). It then downloads Î² = Î± dâˆ’k+1 bytes from each helper node, and restores the failed node using the helper data. The total amount dÎ² of bytes downloaded is typically much smaller than the total amount kÎ± bytes of data stored in the k nodes. Here Î± is the sub-packetization level of an MSR code. The total number dÎ² of bytes downloaded for node repair, is called the repair bandwidth. Let us define the normalized repair bandwidth to be the quantity</p><formula xml:id="formula_0">dÎ² kÎ± = d k(dâˆ’k+1) .</formula><p>The normalization by kÎ± can be motivated by viewing a single MSR codeword having sub-packetization level Î± as a replacement for Î± scalar RS codewords. The download bandwidth under the conventional repair of Î± scalar RS codes equals kÎ± bytes, corresponding to a normalized repair bandwidth of 1. For the particular case d = (n âˆ’ 1), the normalized value equals nâˆ’1 k(nâˆ’k) . It follows that the larger the number (n âˆ’ k) of parity chunks, the greater the reduction in repair traffic. We will also use the parameter M = kÎ± to denote the total number of databytes contained in an MSR codeword. Thus an MSR code has associated parameter set given by {(n, k), d, (Î±, Î² ), M} with Î² = Î± dâˆ’k+1 and M = kÎ±.</p><p>Additional Desired Attributes: Over and above the low repair-bandwidth and low storage-overhead attributes of MSR codes, there are some additional properties that one would like a code to have. These include (a) uniform- repair capability, i.e., the ability to repair data and parity nodes with the same low repair bandwidth, (b) minimal disk read, meaning that the amount of data read from disk for node repair in a helper node is the same as the amount of data transferred over the network from the helper node and (c) low value of sub-packetization parameter Î±, and (d) a small size of underlying finite field over which the code is constructed. In MSR codes that possess the disk read optimal property, both network traffic and number of disk reads during node repair are simultaneously minimized and are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related Work</head><p>The problem of efficient node repair has been studied for some time and several solutions have been proposed. Locally repairable codes such as the Windows Azure Code <ref type="bibr" target="#b13">[15]</ref> and Xorbas <ref type="bibr" target="#b26">[28]</ref> trade the MDS property to allow efficient node-repair by accessing a smaller number of helper nodes. The piggy-backed RS codes introduced in <ref type="bibr" target="#b24">[26]</ref> achieve reductions in network traffic while retaining the MDS property but they do not achieve the savings that are possible with an MSR code. Though there are multiple implementations of MSR codes, these are lacking in one or the other of the desired attributes (see <ref type="table">Table 1</ref>). In <ref type="bibr" target="#b6">[8]</ref>, the authors present 2-parity FMSR codes, that allow efficient repair, but reconstruct a function of the data that is not necessarily same as the failed node data. This demands an additional decoding operation to be performed to retrieve original data. In <ref type="bibr" target="#b22">[24]</ref>, the authors implement a modified productmatrix MSR construction <ref type="bibr" target="#b25">[27]</ref>. Although the code displays optimal disk I/O performance, the storage overhead is on the higher side and of the form (2 âˆ’ 1 k ). In <ref type="bibr" target="#b18">[20]</ref>, the authors implement an MSR code known as the Butterfly code and experimentally validate the theoreticallyproven benefits of reduced data download for node repair. However, the Butterfly code is limited to (n âˆ’ k) = m = 2 and has large value of sub-packetization 2 kâˆ’1 . The restriction to small values of parameter m limits the efficiency of repair, as the normalized repair bandwidth can be no smaller than 1 2 . In <ref type="bibr" target="#b16">[18]</ref>, the authors propose a class of MDS array codes named as HashTag codes with Î± â‰¤ (n âˆ’ k) k/nâˆ’k that permit flexibility in choice of Î± at the expense of repair bandwidth. However, the code supports efficient repair only for systematic nodes, requires computations at helper nodes, and involves operations in a large finite-field. The authors have presented an evaluation of HashTag codes in Hadoop.</p><p>In a parallel line of work, many theoretical constructions of MSR codes are proposed in literature. The product-matrix MSR codes proposed in <ref type="bibr" target="#b25">[27]</ref> operate with very low sub-packetization and small finite-field size, however require a large storage overhead. In a second notable construction known as zig-zag codes <ref type="bibr" target="#b28">[30]</ref>, the authors present the first theoretical construction of low-storage-overhead MSR codes for every n, k, when d = (n âˆ’ 1). The construction of zig-zag code is nonexplicit in the sense that the finite-field coefficients determining the parities have to be found by computer search. Thus, despite the many theoretical constructions and a smaller number of practical implementations, the search for an MSR code having all of the desirable properties described above and its practical evaluation continued to remain elusive. The recent theoretical results of Ye and Barg <ref type="bibr" target="#b33">[35]</ref> have resulted in an altered situation. In this work, the authors provide a construction that permits storage overhead as close to 1 as desired, sub-packetization level close to the minimum possible, finite field size no larger than n, optimal disk I/O, and all-node optimal repair.Clay codes offer a practical perspective and an implementation of the Ye-Barg theoretical construction, along with several additional attributes. In other words, Clay codes possess all of the desirable properties mentioned above, and also offer several additional advantages compared to the Ye-Barg code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Refinements over Ye-Barg Code</head><p>The presentation of the Clay code here is from a coupledlayer perspective that leads directly to implementation, whereas the description in <ref type="bibr" target="#b33">[35]</ref> is primarily in terms of parity-check matrices. For example, using the coupledlayer viewpoint, both data decoding (by which we mean recovery from a maximum of (n âˆ’ k) erasures) as well as node-repair algorithms can be described in terms of two simple operations: (a) decoding of the scalar MDS code, and (b) an elementary linear transformation between pairs of bytes (see Section 3). While this coupledlayer view-point was implicit in the Ye-Barg paper <ref type="bibr" target="#b33">[35]</ref>, we make it explicit here.</p><p>In addition, Clay codes can be constructed using any scalar MDS code as building blocks, while Ye-Barg code is based only on Vandermonde-RS codes. Therefore, scalar MDS codes that have been time-tested, and best suited for a given application or workload need not be modified in order to make the switch to MSR codes. By using Clay codes, these applications can use the same MDS code in a coupled-layer architecture and get the added benefits of MSR codes. The third important distinction is that, in <ref type="bibr" target="#b33">[35]</ref>, only the single node-failure case is discussed. In the case of Clay codes, we have come up with a generic algorithm to repair multiple failures, that has allowed us to repair many instances of multiple node repair with reduced repair bandwidth. Our refinements over Ye-Barg code primarily aiming at its practical realization precede certain theoretical developments that are to come later. In a recent work <ref type="bibr" target="#b4">[6]</ref>, it is proved that the sub-packetization of Clay codes is the minimum possible for any disk-read-optimal MSR code. In <ref type="bibr" target="#b29">[31]</ref>, authors propose a permuatation-based transformation that converts a non-binary (n, k) MDS code to another MDS code permitting efficient repair of a set of (n âˆ’ k) nodes, at the cost of increasing the sub-packetization (nâˆ’k) times. An MSR code obtained by repeated application of the transformation results in the same sub-packetization as that of the Ye-Barg code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Construction of the Clay Code</head><p>Single Codeword Description In Section 2, we noted that each node stores a data chunk and that a data chunk is comprised of L bytes from N codewords. In the present section we will restrict our attention to the case of a single codeword, i.e., to the case when N = 1, L = Î±. Parameters of Clay Codes Evaluated <ref type="table" target="#tab_2">Table 2</ref> lists the parameters of the Clay codes evaluated here. As can be seen, the normalized repair bandwidth can be made much smaller by increasing the value of (d âˆ’ k + 1). For example, the normalized repair bandwidth for a <ref type="bibr" target="#b18">(20,</ref><ref type="bibr" target="#b14">16)</ref> code equals 0.297, meaning that the repair bandwidth of a Clay code, is less than 30% of the corresponding value for Î± = 1024 layers of a (20, 16) RS code. Explaining Through Example We will describe the Clay code via an example code having parameters:</p><formula xml:id="formula_1">{(n = 4, k = 2), d = 3, (Î± = 4, Î² = 2), M = 8}.</formula><p>The codeword is stored across n = 4 nodes of which k = 2 are data nodes and n âˆ’ k = 2 are parity nodes. Each node stores a superbyte made up of Î± = 4 bytes. The code has storage overhead nÎ± kÎ± = n k = 2 which is the ratio of    Let us assume that a (4, 2) RS code M is used to encode and store data on these 4 nodes. We assume that nodes (0, 0), (1, 0) store data, nodes (0, 1), (1, 1) store parity. Two nodes are said to be in same y-section, if they have the same y-coordinate.</p><formula xml:id="formula_2">(n, k) d (Î±, Î² ) (dÎ² )/(kÎ±)<label>(6,</label></formula><formula xml:id="formula_3">{(x, y) | (x, y) âˆˆ J}, J = {(0, 0), (1, 0), (0, 1), (1, 1)}}. (0,0) (0,1) (1,0) (1,1)</formula><p>The Uncoupled Code Next, consider storing on the same 4 nodes, 4 codewords drawn from the same RS code M . Thus each node now stores 4 bytes, each associated to a different codeword. We will use the parameter z âˆˆ {0, 1, 2, 3} to index the 4 codewords. Together these 4 codewords form the uncoupled code U , whose bytes are denoted by {U(x, y, z) | (x, y) âˆˆ J, z âˆˆ {0, 1, 2, 3}}. These 16 bytes can be viewed as being stored in a data cube composed of 4 horizontal layers (or planes), with 4 bytes to a layer <ref type="figure" target="#fig_4">(Fig. 5)</ref>. The data cube can also be viewed as being composed of 4 (vertical) columns, each column composed of 4 cylinders. Each column stores a superbyte while each of the 4 cylinders within a column stores a single byte. It can be verified that the uncoupled code inherits the property that data stored in the 4 nodes can be recovered by connecting to any 2 nodes. As one might expect, this code offers no savings in repair bandwidth over that of the constituent RS codes, since we have simply replicated the same RS code 4 times. We show below how the uncoupled code can be used to create a new coupledlayer (Clay) code that is an MSR code having the desired optimal, repair bandwidth. Using a Pair of Coordinates to Represent a Layer The coupling of the layers is easier explained in terms of a binary representation (z 0 , z 1 ) of the layer-index z, defined by z = 2z 0 + z 1 i.e., 0 â‡’ (0, 0), 1 â‡’ (0, 1), 2 â‡’ (1, 0) and 3 â‡’ (1, 1). We color in red, vertices within a layer for which x = z y as a means of identifying the layer. For example in <ref type="figure" target="#fig_5">Fig. 6</ref>, in layer (z 0 , z 1 ) = (1, 1), the vertices (1, 0), (1, 1) are colored red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX</head><p>Pairing of Vertices and Bytes We will abbreviate and write p = (x, y, z) in place of (x, y, z) and introduce a pairing (p, p * ) of vertices within the data cube. The vertices that are colored red are unpaired. The remaining vertices are paired such that a vertex p and its companion p * both belong to the same y-section. In the data cube of our example code, there are a total of 4 * 4 = 16 vertices of which 8 are unpaired. The remaining 8 vertices form 4 pairs. Each pair is shown in the data cube appearing on the left in <ref type="figure" target="#fig_7">Fig. 7</ref> using a pair of yellow rectangles linked by a dotted line. Mathematically, p * is obtained from p = (x, y, z) simply by interchanging the values of x and z y . Examples are presented in <ref type="table" target="#tab_3">Table 3</ref>. As mentioned earlier, each vertex p of the data cube is associated to a byte U(p) = U(x, y, z) of data in the uncoupled code U . We will use U * (p) to denote the companion U(p * ), of the byte U(p).  In the reverse direction, we have U(p) = C(p) respectively if p is unpaired. Else, U(p),C(p) are related by the pairwise reverse transform (PRT):</p><formula xml:id="formula_4">Vertex p = (x, y, z 0 , z 1 ) Companion p * (interchange x, z y ) (0, 0, 1, 0) (1, 0, 0, 0) (1, 1, 1, 0) (0, 1, 1, 1) (0, 1, 1, 0) (0, 1, 1, 0) a red vertex, (p = p * )</formula><formula xml:id="formula_5">C(p) C * (p) = 1 Î³ Î³ 1 âˆ’1 U(p) U * (p) .<label>(1)</label></formula><formula xml:id="formula_6">U(p) U * (p) = 1 Î³ Î³ 1 C(p) C * (p) .<label>(2)</label></formula><p>We assume Î³ to be chosen such that Î³ = 0, Î³ 2 = 1, and under this condition, it can be verified that any two bytes in the set {U(p),U * (p),C(p),C * (p)} can be recovered from the remaining two bytes.  In <ref type="figure" target="#fig_9">Fig. 9</ref>, which shows a portion of the bytes in C , the dotted column corresponds to the failed node having coordinates (x, y) = (1, 0). To repair the node, only the two layers z = (1, 0) and z = (1, 1) corresponding to the presence of red dots within the dotted column are called upon for node repair. Thus each helper node contributes only 2 bytes, as opposed to 4 in an RS code, towards node repair and this explains the savings in repair bandwidth. To understand how repair is accomplished, we turn to <ref type="figure" target="#fig_11">Fig. 11</ref>. As shown in the figure, the PRT allows us to determine from the the bytes in layers z = (1, 0) and z = (1, 1) belonging to y-section y = 1 in data cube C, the corresponding bytes in data cube U. RS decoding allows us to then recover the bytes U(p) belonging to y-section y = 0 in the same two planes. At this point, we have access to the bytes C(p),U(p) for p corresponding to vertices lying in planes z = (1, 0) and z = (1, 1) and lying in y-section y = 0. This set includes 2 of the bytes C(p) in the column corresponding to the failed node. The remaining two bytes C(p) in the failed column can be determined using properties of the PFT.</p><p>Intersection Score To explain decoding, we introduce the notion of an Intersection Score (IS). The IS of a layer is given by the number of hole-dot pairs, i.e., the vertices that correspond to erased bytes and which are at the same time colored red. For example in <ref type="figure">Fig. 10</ref>, when nodes (0, 0), (0, 1) are erased, layers (0, 0), (0, 1), (1, 1) have respective IS=2, 1, 0. Decoding The "Decode" algorithm of the Clay code is able to correct the erasure of any n âˆ’ k = 2 nodes. Decoding is carried out sequentially, layer-by-layer, in order of increasing IS. This is explained in <ref type="figure" target="#fig_0">Fig.12</ref> for the case when nodes (0, 0), (0, 1) are erased and for layers having IS= 0, IS= 1. In a layer with IS= 0, U bytes can be computed for all non-erased vertices from the known symbols. The erased U bytes are then calculated using RS code decoding. For a layer with IS= 1, to compute U bytes for all non-erased vertices, we make use of U bytes recovered in layers with IS= 0. Thus the processing of a layer with IS = 0 has to take place prior to processing a layer with IS = 1 and so on. Once all the U bytes are recovered, the C bytes can be computed using the PFT. As a result of the simple, pairwise nature of the PFT and PRT, encoding and decoding times are not unduly affected by the coupled-layer structure.  Clay code parameters Clay codes can be constructed for any parameter set of the form:</p><formula xml:id="formula_7">(0,0) (0,1) (1,0) (1,1) (a) IS=2 (0,0) (0,1) (1,0) (1,1) (b) IS=1 (0,0) (0,1) (1,0) (1,1)<label>(c)</label></formula><p>(n = qt, k, d) (Î± = q t , Î² = q tâˆ’1 ), with q = (d âˆ’ k + 1), for any integer t â‰¥ 1 over any finite field of size Q &gt; n. The encoding, decoding and repair algorithms can all be generalized for the parameters above. However, in the case d &lt; n âˆ’ 1, during single node repair, while picking the d helper nodes, one must include among the d helper nodes, all the nodes belonging to the failed node's y-section.</p><p>Clay codes for any (n, k, d) The parameters indicated above have the restriction that q = (d âˆ’ k + 1) divide n. But the construction can be extended in a simple way to the case when q is not a factor of n. For example, for parameters (n = 14, k = 10, d = 13), q = d âˆ’ k + 1 = 4. We construct the Clay code taking n = 16, the nearest multiple of q larger than n, and k = k + (n âˆ’ n) = 12. While encoding, we set data bytes in s = (n âˆ’n) = 2 systematic nodes as zero, and thus the resultant code has parameters (n = 14, k = 10, d = 13). The technique used is called shortening in the coding theory literature. We use s temporary buffers each of size equal to chunk size during the encoding, decoding and repair operations. Our implementation of Clay code includes this generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ceph and Vector MDS Codes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Introduction to Ceph</head><p>Ceph <ref type="bibr" target="#b30">[32]</ref> is a popular, open-source distributed storage system <ref type="bibr" target="#b31">[33]</ref>, that permits the storage of data as objects.</p><p>Object Storage Daemon (OSD) is the daemon process of Ceph, associated with a storage unit such as a solid-state or hard-disk drive, on which user data is stored. Ceph supports multiple erasure-codes, and a code can be chosen by setting attributes of the erasure-codeprofile. Objects will then be stored in logical partitions referred to as pools associated with an erasure-codeprofile. Each pool can have a single or multiple placement groups (PG) associated with it. A PG is a collection of n OSDs, where n is the block length of the erasure code associated to the pool.</p><p>The allocation of OSDs to a PG is dynamic, and is carried out by the CRUSH algorithm <ref type="bibr" target="#b32">[34]</ref>. When an object is streamed to Ceph, the CRUSH algorithm allocates a PG to it. It also performs load balancing dynamically whenever new objects are added, or when active OSDs fail. Each PG contains a single, distinct OSD designated as the primary OSD (p-OSD). When it is required to store an object in a Ceph cluster, the object is passed on to the p-OSD of the allocated PG. The p-OSD is also responsible for initiating the encoding and recovery operations.</p><p>In Ceph, the passage from data object to data chunks by the p-OSD is carried out in two steps as opposed to the single-step description in Section 2. For a large object, the amount of buffer memory required to perform encoding and decoding operations will be high. Hence, as an intermediate step, an object is first divided into smaller units called stripes, whose size is denoted by S (in bytes). If an object's size is not divisible by S, zeros are padded. The object is then encoded by the p-OSD one stripe at a time. The stripe-size is to be specified within the cluster's configuration file. Both zero padding and system performance are important factors to be considered while fixing a stripe-size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sub-Chunking through Interleaving</head><p>To encode, the p-OSD first zero pads each stripe as necessary in order to ensure that the strip size S is divisible by kÎ±. The reason for the divisibility by a factor of k is because as described earlier, the first step in encoding is to break up each stripe into k data chunks of equal size. The reason for the additional divisibility requirement by a further factor Î± arises because we are dealing with a vector code and as explained in Section 2, operations in a vector code involve superbytes, where each superbyte contains Î± bytes. In what follows, we will assume that S is divisible by kÎ±.</p><p>The encoding of a stripe is thus equivalent to encoding N = S kÎ± codewords at a time. The next step as explained in Section 2, is interleaving at the end of which one obtains Î± sub-chunks per OSD, each of size N bytes. We note that the parameter L introduced in Section 2, is the number of bytes per data chunk and is thus given by L = S k . This notion of sub-chunk is not native to Ceph, but rather is a modification to the Ceph architecture proposed here, to enable the support of vector codes.</p><p>The advantage of a vector code is that it potentially enables the repair of an erased coded chunk by passing on a subset of the Î± sub-chunks. For example, in the Clay code implemented in Ceph is an MSR code, it suffices for each node to pass on Î² sub-chunks. However, when these Î² sub-chunks are not sequentially located within the storage unit, it can result in fragmented reads. We analyze such disk read performance degradation in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation in Ceph</head><p>Our implementation makes use of the Jerasure <ref type="bibr" target="#b20">[22]</ref> and GF-Complete <ref type="bibr" target="#b19">[21]</ref> libraries which provide implementations of various MDS codes and Galois-field arithmetic. We chose in our implementation to employ the finite field of size 2 8 to exploit the computational efficiency for this field size provided by the GF-complete library in Ceph.</p><p>In our implementation, we employ an additional buffer, termed as U-buffer, that stores the sub-chunks associated with the uncoupled symbols U introduced in Section 3. This buffer is of size nL = S n k bytes. The Ubuffer is allocated once for a PG, and is used repetitively during encode, decode and repair operations of any object belonging to that PG. Pairwise Transforms We introduced functions that compute any two sub-chunks in the set {U,U * ,C,C * } given the remaining two sub-chunks.</p><p>We implemented these functions using the function jerasure matrix dotprod(), which is built on top of function galois w08 region multiply(). Encoding Encoding of an object is carried out by p-OSD by pretending that m parity chunks have been erased, and then recovering the m chunks using the k data chunks by initiating the decoding algorithm for the code. Pairwise forward and reverse transforms are the only additional computations required for Clay encoding in comparison with MDS encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Enabling Selection Between Repair &amp; Decoding</head><p>When one or more OSDs go down, multiple PGs are affected. Within an affected PG, recovery operations are triggered for all associated objects. We introduced a boolean function is repair() in order to choose between a bandwidth, disk I/O efficient repair algorithm and the default decode algorithm. For the case of single OSD failure, is repair() always returns true. There are multiple failure cases as well for which is repair() returns true i.e., efficient repair is possible. We discuss these cases in detail in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Helper-Chunk</head><p>Identification In the current Ceph architecture, when a failure happens, minimum to decode() is called in order to determine the k helper chunk indices. We introduced a function minimum to repair() to determine the d helper chunk indices when repair can be performed efficiently i.e., when is repair() returns true. OSDs corresponding to these indices are contacted to get information needed for repair/decode. When there is a single failure, minimum to repair() returns d chunk indices such that all the chunks that fall in the y-cross-section of the failed chunk are included. We describe the case of multiple erasure cases in detail in Appendix A Fractional Read For the case of efficient repair, we only read a fraction of chunk, this functionality is implemented by feeding repair parameters to an existing structure ECSubRead that is used in inter-OSD communication. We have also introduced a new read function with Filestore of Ceph that supports sub-chunk reads. Decode and Repair Either the decode or repair function is called depending on whether if is repair() returns true or false respectively. The decoding algorithm is described in Section 3. Our repair algorithm supports in addition to single-node failure (Section.3), some multipleerasure failure patterns as well (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Contributions to Ceph</head><p>Enabling vector codes in Ceph: We introduced the notion of sub-chunking in order to enable new vector erasure code plugins. This contribution is currently available in Ceph's master codebase <ref type="bibr" target="#b2">[4]</ref>. Clay codes in Ceph: We implemented Clay codes as a technique (cl msr) within the jerasure plugin. The current implementation gives flexibility for a client to pick any n, k, d parameters for the code. It also gives an option to choose the MDS code used within to be either a Vandermonde-based-RS or Cauchy-original code. The Clay code <ref type="bibr">[2]</ref> is yet to be part of Ceph's master codebase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>The experiments conducted to evaluate the performance of Clay codes in Ceph while recovering from a single node failure are discussed in the present section. Experimental results relating multiple node-failure case can be found in Section 6.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview and Setup</head><p>Codes Evaluated While Clay codes can be constructed for any parameter set (n, k, d), we have carried out experimental evaluation for selected parameter sets close to those of codes employed in practice, see Table 4. Code C1 has (n, k) parameters comparable to that of the RDP code <ref type="bibr" target="#b7">[9]</ref>, Code C2 with the locally repairable code used in Windows Azure <ref type="bibr" target="#b14">[16]</ref>, and Code C3 with the (20, 17)-RS code used in Backblaze <ref type="bibr" target="#b0">[1]</ref>. There are three other codes C4, C5 and C6 that match with the (14, 10)-RS code used in Facebook data-analytic clusters <ref type="bibr" target="#b23">[25]</ref>. Results relating to Codes C4-C6 can be found in Section 6.1, which focuses on repair in the multiple-erasure case.  The experimental results for Clay codes are compared against those for RS codes possessing the same (n, k) parameters. By an RS code, we mean an MDS-code implementation based on the cauchy orig technique of Ceph's jerasure plugin. The same MDS code is also employed as the MDS code appearing in the Clay-code construction evaluated here.</p><p>Experimental Setup All evaluations are carried out on Amazon EC2 instances of the m4.xlarge (16GB RAM, 4 CPU cores) configuration. Each instance is attached to an SSD-type volume of size 500GB. We integrated the Clay code in Ceph Jewel 10.2.2 to perform evaluations. The Ceph storage cluster deployed consists of 26 nodes. One server is dedicated for the MON daemon, while the remaining 25 nodes each run one OSD. Apart from the installed operating system, the entire 500GB disk is dedicated to the OSD. Thus the total storage capacity of the cluster is approximately 12.2TB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Distribution Model</head><p>Object # Objects Total, T Stripe size (MB) (GB) size, S Fixed <ref type="table" target="#tab_2">(W 1 )  64  8192  512  64MB  64  6758  Variable  32  820  448  1MB  (W 2 )</ref> 1 614 Overview Experiments are carried out on both fixed and variable object-size workloads, respectively referred to as W 1 and W 2 . Workload W 1 has all objects of fixed size 64MB, while in the W 2 workload we choose objects of sizes 64MB, 32MB and 1MB distributed in respective proportions of 82.5%, 10% and 7.5%. Our choices of object sizes cover a good range of medium (1MB), medium/large(32MB) and large (64MB) objects <ref type="bibr" target="#b1">[3]</ref>, and the distribution is chosen in accordance with that in the Facebook data analytic cluster reported in <ref type="bibr" target="#b21">[23]</ref>. The workloads used for evaluation are summarized in Table 5. The stripe-size S is set as 64MB and 1MB for workloads W 1 and W 2 respectively, so as to avoid zeropadding.</p><p>The failure domain is chosen to be a node. Since we have one OSD per node, this is equivalent to having a single OSD as the failure domain. We inject node failures into the system by removing OSDs from the cluster. Measurements are taken using nmon and NMONVisualizer tools. We run experiments with a single PG, and validate the results against the theoretical prediction. We also run the same experiments with 512 PGs, which we will refer to as the multiple-PG case. Measurements are made of (a) repair network traffic, (b) repair disk read, (c) repair time, (d) encoding time and (e) I/O performance for degraded, normal operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluations</head><p>Network Traffic: Single Node Failure Network traffic refers to the data transferred across the network during single-node repair. Repair is carried out by the p-OSD, which also acts as a helper node. The network traffic during repair includes both the transfer of helper data to the primary OSD and the transfer of recovered chunk from primary OSD to the replacement OSD. The theoretical estimate for the amount of network traffic is</p><formula xml:id="formula_8">T k ((d âˆ’1) Î² Î± +1</formula><p>) bytes for a Clay code, versus T bytes for an RS code. Our evaluations confirm the expected savings, and we observed reductions of 25%, 52% and 66%, (a factor of 2.9Ã—) in network traffic for codes C1, C2 and C3 respectively in comparison with the corresponding RS codes under fixed and variable workloads (see <ref type="figure">Fig. 13</ref>(a), 13(d).) As can be seen, the code C3 with the largest value of q = (d âˆ’ k + 1) offer the largest savings in network traffic.</p><p>In Ceph, the assignment of OSDs and objects to PGs are done in a dynamic fashion. Hence, the number of objects affected by failure of an OSD can vary across different runs of multiple-PG experiment. We present an network bandwidth performance with 512 PGs under the W 1 workload averaged across 3 runs in <ref type="figure" target="#fig_3">Fig. 14</ref>. It was observed that in certain situations, an OSD that is already part of the PG can get reassigned as a replacement for the failed OSD. In such cases, the number of failures are treated as two resulting in inferior network-traffic performance in multiple-PG setting.</p><p>Disk Read: Single Node Failure The amount of data read from the disks of the helper nodes during the repair of a failed node is referred to as disk read and is an important parameter to minimize.</p><p>Depending on the index of the failed node, the subchunks to be fetched from helper nodes in a Clay code can be contiguous or non-contiguous. Non-contiguous reads in HDD volumes lead to a slow-down in performance <ref type="bibr" target="#b18">[20]</ref>. Even for SSD volumes that permit reads at a granularity of 4kB, the amount of disk read needed depends on the sub-chunk-size. Let us look at, for instance, disk read from a helper node in the case of single node failure for code C3 in workload W2. The stripe-size S = 1MB, and the chunk size is given by L = S/k = 64kB. During repair of a node, L/(d âˆ’ k + 1) = 16kB of data is to be read from each helper node. In the best-case scenario (for example, a systematic node failure), the 16kB data is contiguous, whereas for the worst-case scenario (as in the case of parity node failure) the reads are fragmented. In the latter case, Î² = 256 fragments with each of size L/Î± = 64 bytes are read. As a consequence, when 4kB of data is read from the disk, only 1kB ends up being useful for the repair operation. Therefore, the disk read is 4 times the amount of data needed for repair. This is evident in disk read measurements from a helper node in the worst-case as shown in <ref type="figure">Fig. 13(f)</ref>. A similar analysis shows that for workload W2, the code C2 leads to additional disk read while C1 does not. This is observed experimentally as well.</p><p>On the other hand, for workload W1 with stripe-size S = 64MB, all the three codes C1, C2, and C3 do not cause any additional disk read as shown in <ref type="figure">Fig. 13(b)</ref>. For instance, with code C3, fragments of size S/kÎ± = 4kB are to be read in the worst-case scenario. As the size is aligned to the granularity of SSD reads, disk read for the worst-case is equal to 256 * 4kB=1MB. This is exactly the amount read during best-case as well. (see <ref type="figure">Fig. 13(f)</ref>). In summary, all the three codes result in disk I/O savings for the W1 workload whereas for workload W2 only C1 results in an advantage.</p><p>The expected disk read from all helper nodes during repair is T dÎ² kÎ± bytes for a Clay code in contrast to T bytes for an RS code. In experiments with fixed object-size (see <ref type="figure">Fig. 13(b)</ref>), we obtain savings of 37.5%, 59.3% and 70.2% (a factor of 3.4Ã—) for codes C1, C2 and C3 respectively, when compared against the corresponding RS code. <ref type="figure" target="#fig_3">Fig. 14</ref> shows the disk read in the multiple-PG setting. I/O Performance We measured the normal and degraded (i.e., with a repair executing in the background) I/O performance of Clay codes C1-C3, and RS codes with same parameters. This was done using the standard Ceph benchmarking tests for read and write operations.</p><p>The results are shown in <ref type="figure" target="#fig_4">Fig. 15</ref>. Under the normal operation, the write, sequential-read and random-read performances are same for both Clay and RS codes. However in the degraded situation, the I/O performance of Clay codes is observed to be better in comparison with RS codes. In particular, the degraded write, read throughput of <ref type="bibr" target="#b18">(20,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b17">19</ref>) Clay code is observed to be more than the (20, 16) RS code by 106% and 27% respectively. This can possibly be attributed to the reduced amount of repair data that is read, transmitted and computed on to build the lost data in the erased node. Repair Time and Encoding Time We measure the time taken for repair by capturing the starting and stopping times of network activity within the cluster. We observed a significant reduction in repair time for Clay codes in comparison with an RS code. For the code C3 in a single-PG setting, we observe a reduction by a factor of 3Ã— in comparison with an RS code. This is mainly due to reduction in network traffic and disk I/O required during repair. Every affected object requires recovery of (1/k)-th fraction of the object size, and the average repair time per object is plotted in <ref type="figure">Fig. 13(c)</ref>. We define the time required by the RADOS utility to place an object into Ceph object-store as the encoding time. The encoding time includes times taken for computation, disk-I/O operations, and data transfer across the network. We define the time taken for computing the code chunks based on the encoding algorithm as the encode computation time. During encoding, the network traffic and I/O operations are the same for both the classes of codes. Although the encode computation time of Clay code is higher than that of the RS code (See <ref type="figure" target="#fig_5">Fig. 16.)</ref>   <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr" target="#b22">[24]</ref>. The significant savings in network traffic and disk reads during node repair are a sufficient incentive for putting up with overheads in the encode computation time. The decoding time will be almost same as encoding time, since we perform encoding using the decoding function as described in Section 4.3. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Handling Failure of Multiple Nodes</head><p>The Clay code is capable of recovering from multiple node-failures with savings in repair bandwidth. In the case of multiple erasures, the bandwidth needed for repair varies with the erasure pattern. In <ref type="figure" target="#fig_7">Fig. 17</ref>, we show the average network traffic of Clay codes with parameters (n = 14, k = 10, d) for d = 11, 12, 13 while repairing f = 1, 2, 3, and 4 node failures. The average network traffic for repairing f nodes is computed under the assumption that all the f -node-failure patterns are equally likely. Detailed analysis of savings in network traffic for multiple erasures is relegated to Appendix A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation of Multiple Erasures</head><p>Network Traffic and Disk Read While the primary benefit of the Clay code is optimal network traffic and disk read during repair of a single node failure, it also yields savings over RS counterpart code in the case of a large number of multiple-node failure patterns. We evaluate the performance of codes C4-C6 (see <ref type="table" target="#tab_4">Table 4</ref>) under W 1 workload injecting multiple node-failures in a setting of 512PGs. The plots for network traffic and disk read are shown in <ref type="figure" target="#fig_9">Fig. 18, 19</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>Clay codes extend the theoretical construction presented by Ye &amp; Barg with practical considerations from a coupled-layer perspective that leads directly to implementation. Within the class of MDS codes, Clay codes have minimum possible repair bandwidth and disk I/O. Within the class of MSR codes, Clay codes possess the least possible level of sub-packetization. A natural question to ask is if these impressive theoretical credentials of the Clay code result in matching practical performance.</p><p>We answer this in the affirmative here by studying the real-world performance of the Clay code in a Ceph setting, with respect to network traffic for repair, disk I/O during repair, repair time and degraded I/O performance. Along the way, we also modified Ceph to support any vector code, and our contribution is now a part of Ceph's master code-base. A particular Clay code, with storage overhead 1.25x, is shown to reduce repair network traffic, disk read and repair times by factors of 2.9, 3.4 and 3 respectively. Much of this is made possible because Clay codes can be constructed via a simple two-step process where one first stacks in layers, Î± codewords drawn from an MDS code; in the next step, elements from different layers are paired and transformed to yield the Clay code. The same construction with minor modifications is shown to offer support for handling multiple erasures as well. It is our belief that Clay codes are well-poised to make the leap from theory to practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Handling Failure of Multiple Nodes</head><p>The failure patterns that can be recovered with bandwidth-savings are referred to as repairable failure patterns. Non repairable failure patterns are recovered by using the decode algorithm.</p><p>Repairable Failure Patterns (i) d &lt; n âˆ’ 1: Clay codes designed with d &lt; n âˆ’ 1 can recover from e failures with savings in repair bandwidth when e â‰¤ n âˆ’ d, with a minor exception described in Remark 1. The helper nodes are to be chosen in such a way that if a y-section contains a failed node, then all the surviving nodes in that y-section must act as helper nodes. If no such choice of helper nodes is available then it is not a repairable failure pattern. For example, consider the code with parameters (n = 14, k = 10, d = 11). The nodes can be put in a (2Ã—7) grid, as q = d âˆ’k +1 = 2 and t = n q = 7. In <ref type="figure" target="#fig_0">Fig.20</ref>, we assume that nodes (0, 0) and (0, 1) have failed, and therefore nodes (1, 0) and (1, 1) along with any 9 other nodes can be picked as helper nodes. (ii) d = n âˆ’ 1: When the code is designed for d = (n âˆ’ 1), up to (q âˆ’ 1) failures that occur within a single y-section can be recovered with savings in repair bandwidth. As the number of surviving nodes is smaller than d in such a case, all the surviving nodes are picked as helper nodes. See <ref type="figure" target="#fig_0">Fig. 21</ref> for an example of a repairable failure-pattern in the case of a (14, 10, 13) Clay code. Remark 1 Whenever d e Î² e &gt; kÎ±, decode algorithm is a better option and the is repair() function takes care of these cases by returning false. For example, when there are q failures within the same y-section, every layer will have IS &gt; 0 giving Î² e = Î± and hence repair is not efficient for this case.</p><p>Repair Algorithm We present a repair algorithm in 1, that is generic for single and multiple erasures. This is invoked whenever savings in bandwidth are possible, i.e, when is repair() returns true. In the algorithm, we refer to those non-erased nodes that are not helper nodes as aloof nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 repair</head><p>1: Input: E (erasures), I (aloof nodes). <ref type="bibr">2:</ref> repair layers = get repair layers(E ). 3: set s = 1. 4: set maxIS = max of IS(E âˆª I , z) over all z from repair layers 5: while ( 1 â‰¤ s â‰¤ maxIS ) <ref type="bibr">6:</ref> for (z âˆˆ repair layers and IS(E âˆª I , z) = s) <ref type="bibr">7:</ref> if (IS(E , z) &gt; 1) G = Ï† G is set of all nodes in a's y-section.}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>E = E âˆª G âˆª I</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Compute U sub-chunks in layer z corresponding to all the nodes other than E</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>Invoke scalar MDS decode to recover U subchunks for all nodes in E </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A pictorial representation of a vector code where each superbyte consists of 2 bytes. The picture shows N = 3 codewords. A single chunk, either data or parity, stores 3 superbytes, each corresponding to a different codeword.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the total number nÎ± = 16 of bytes stored to the num- ber M = kÎ± = 8 of data bytes. During repair of a failed node, Î² = 2 bytes of data are downloaded from each of the d = 3 helper nodes, resulting in a normalized repair bandwidth of dÎ² kÎ± = d k(dâˆ’k+1) = 0.75. Starting Point: A (4, 2) Scalar RS Code We be- gin our description of the Clay code with a simple, distributed data storage setup composed of 4 nodes, where the nodes are indexed by (x, y) coordinates:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The (4, 2) MDS code M .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The uncoupled code U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The uncoupled code U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Transforming from Uncoupled to Coupled-Layer Code We now show how one can transform in a sim- ply way, a codeword belonging to the uncoupled code U to a codeword belonging to the Coupled-layer (Clay) code C . As with the uncoupled code, there are a total of 16 bytes making up each codeword in the Clay code. These 16 bytes are stored in a second, identical data cube that is again, composed of 4 horizontal layers, 4 vertical columns with 4 vertices in a layer and 4 vertices per col- umn. Each node corresponds to a column of the data cube and stores a superbyte, made up of 4 bytes. The Clay code C associates a byte C(p) with each vertex p of the data cube just as does the uncoupled code U . The bytes U(p) and C(p) are related in a simple man- ner. If p corresponds to an unpaired (and hence colored in red) vertex, we simply set C(p) = U(p). If (p, p * ) are a pair of companion vertices, p = p * , U(p),U * (p) and C(p),C * (p) are related by the the following pairwise forward transform (PFT):</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Bytes C(x, y, z) of the Clay code can be obtained from bytes U(x, y, z) of the uncoupled code through a pairwise forward transform and in the reverse direction, by the corresponding pairwise reverse transform. Vertex pairs within a data cube are identified by a pair of yellow rectangles linked by a dotted line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 8: Encoding flowchart for the Clay code. A top view of the nodes is shown on the right. The nodes in pink and blue correspond respectively, to the coupled and uncoupled codes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Identifying the failed node and helper data transferred.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>IS=0 Figure 10 :</head><label>IS=010</label><figDesc>Figure 10: Illustration of the intersection score (IS) for erasures at (0, 1), (0, 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The dotted cylinder identifies the erased node. The bytes shown on the top left represent helper data (6 bytes in all) transferred for repair. The PRT is performed on helper data in C to obtain the bytes (4 bytes) U(p) belonging to the same layers and lying y-section y = 1. RS code decoding within each of the two layers is used to obtain the 4 missing U(p) bytes. The bytes corresponding to the erased node in C can then be computed using properties of the PFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Illustrating how the Clay code recovers from 2 erasures. We begin with a layer having IS = 0 (top) before moving to a layer with IS = 1 (bottom). Symbols alongside each vertex, indicate which of the 4 bytes {C,C * ,U,U * } are known. (Left) Pink circles indicate non-erased vertices in C. (Middle) Blue vertices indicate vertices in U whose contents can be determined from the available C,U bytes. (Right) Invoking the parity-check equations in U allows all bytes in U to be recovered. Once all the U bytes are recovered, one recovers the remaining unknown bytes C using the PFT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>(</head><label></label><figDesc>Figure 13: Experimental evaluation of C1, C2 and C3 in comparison with RS codes in a single-PG setting is presented in plots (a)-(e). The plot (f) gives a relative comparison of disk read in a helper node for stripe-sizes 1MB and 64MB for code C3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Network traffic and disk read during repair of single node in a setting with 512 PGs, for W 1 workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Normal and degraded I/O performance of codes C1, C2, C3 in comparison with RS. The observed values for sequential and random reads are almost the same, and hence plotted as a single value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Comparison of average encoding times for C1, C2 and C3 in comparison with RS codes, for the W 1 workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Average theoretical network traffic during repair of 64MB object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Network traffic evaluation of C4-C6 against RS codes (W 1 workload, multiple-PG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Disk-read evaluation of C4-C6 against RS codes (W 1 workload, multiple-PG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: The (2 Ã— 7) grid of 14 nodes in (14, 10, 11) Clay code. The nodes (0, 0) and (0, 1) have failed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 21 :</head><label>21</label><figDesc>Figure 21: The (4 Ã— 4) grid containing 14 nodes in (14, 10, 13) Clay code. Note that the cells (2, 2) and (3, 2) in the grid do not represent nodes. The nodes (0, 0) and (2, 0) coming from 0-section have failed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>a</head><label></label><figDesc>= the erased node with hole-dot in layer z 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>s</head><label></label><figDesc>= s + 1 16: end while 17: Compute C chunks corresponding to all the erased nodes, from U sub-chunks in repair layers and the helper C sub-chunks in repair layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Parameters of the Clay codes evaluated here. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 : Example vertex pairings.</head><label>3</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table>Codes C1-C3 are evaluated in Ceph for single-node repair. The evalua-
tion of Codes C4-C6 is carried out for both single and multiple-node failures. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table>Workload models used in experiments. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>the encoding time of a Clay code remains close to that of the corresponding RS code. The increase in the</figDesc><table>USENIX Association 
16th USENIX Conference on File and Storage Technologies 149 

computation time for the Clay code is due to the multipli-
cations involved in PFT and PRT operations. In storage 
systems, while data-write is primarily a one-time oper-
ation, failure is a norm and thus recovery from failures 
is a routine activity </table></figure>

			<note place="foot" n="1"> The codes described in this paper can however, be constructed over a finite field whose size is significantly smaller, and approximately equal to the parameter n. Apart from simplicity, we use the word byte here since the finite field of size 2 8 is a popular choice in practice. 140 16th USENIX Conference on File and Storage Technologies USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgments</head><p>We thank our shepherd Cheng Huang and the anonymous reviewers for their valuable comments. P. V. Kumar would like to acknowledge support from NSF Grant No.1421848 as well as the UGC-ISF research program. The research of Alexander Barg and Min Ye was supported by NSF grants CCF1422955 and CCF1618603.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Backblaze data service provider</title>
		<ptr target="https://www.backblaze.com/blog/reed-solomon/.Accessed" />
		<imprint>
			<date type="published" when="2017-09-28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Red hat ceph storage: Scalable object storage on qct servers -a performance and sizing guide</title>
		<imprint/>
	</monogr>
	<note>Reference Architecture</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sub-chunks: Enabling vector codes in ceph</title>
		<ptr target="https://github.com/ceph/ceph/pull/15193/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tutorial</surname></persName>
		</author>
		<ptr target="http://web.eecs.utk.edu/Ëœplank/plank/papers/FAST-2013-Tutorial.html" />
		<title level="m">Erasure coding for storage applications</title>
		<imprint>
			<biblScope unit="page" from="2017" to="2045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A tight lower bound on the sub-packetization level of optimal-access MSR and MDS codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename></persName>
		</author>
		<idno>CoRR abs/1710.05876</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EVEN-ODD: an efficient scheme for tolerating double disk failures in RAID architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="192" to="202" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nccloud: A network-coding-based storage system in a cloud-of-clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>And Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Row-diagonal parity for double disk failure correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corbett</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>English</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grcanac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sankar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX Conference on File and Storage Technologies</title>
		<meeting>the 3rd USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Network coding for distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="4539" to="4551" />
			<date type="published" when="2010-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Availability in globally distributed storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ford</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Labelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">I</forename><surname>Stokely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quin-Lan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 9th USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM Symposium on Operating Systems Principles</title>
		<meeting>the 19th ACM Symposium on Operating Systems Principles<address><addrLine>Bolton Landing, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10-19" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghemawat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gobioff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Nineteenth ACM Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
	<note>SOSP &apos;03, ACM</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">NCCloud: applying network coding for the storage repair in a cloud-of-clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10thth USENIX Conference on File and Storage Technologies(FAST)</title>
		<meeting>the 10thth USENIX Conference on File and Storage Technologies(FAST)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Erasure coding in windows azure storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yekhanin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 2012 USENIX Annual Technical Conference</title>
		<meeting><address><addrLine>Boston, MA; USENIX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Erasure coding in Windows Azure storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Simitci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ogus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yekhanin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 USENIX conference on Annual Technical Conference</title>
		<meeting>the 2012 USENIX conference on Annual Technical Conference<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>USENIX ATC</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Are disks the dominant contributor for storage failures?: A comprehensive study of storage subsystem failure characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanevsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hashtag erasure codes: From theory to practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kralevska</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gligoroski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Verby</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data PP</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facebook&apos;s warm BLOB storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muralidhar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14</title>
		<meeting><address><addrLine>Broomfield, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="383" to="398" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Opening the chrysalis: On the real repair performance of MSR codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamies-Juarez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Blagojevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mateescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bandic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 4th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="81" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gf-complete: A comprehensive open source library for galois field arithmetic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plank</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Greenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houston</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<idno>UT-CS-13- 703</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Tennessee</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jerasure: A library in c facilitating erasure coding for storage applications-version 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Plank</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greenan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename></persName>
		</author>
		<idno>UT-EECS-14-721</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Tennessee</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ec-cache: Load-balanced, lowlatency cluster caching with online erasure coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kosaian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016</title>
		<meeting><address><addrLine>Savannah, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="401" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Having your cake and eating it too: Jointly optimal erasure codes for i/o, storage, and network-bandwidth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 13th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="81" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A solution to the network challenges of data recovery in erasure-coded distributed storage systems: A study on the facebook warehouse cluster</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th USENIX Workshop on Hot Topics in Storage and File Systems, HotStorage&apos;13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A &quot;hitchhiker&apos;s&quot; guide to fast and efficient data reconstruction in erasure-coded data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramchandran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM 2014 Conference</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="331" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optimal Exact-Regenerating Codes for Distributed Storage at the MSR and MBR Points via a Product-Matrix Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">V</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="5227" to="5239" />
			<date type="published" when="2011-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Xoring elephants: Novel erasure codes for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sathiamoorthy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asteris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Vadali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borthakur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="325" to="336" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Disk failures in the real world: What does an mttf of 1,000,000 hours mean to you?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schroeder</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gibson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 5th USENIX Conference on File and Storage Technologies<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zigzag codes: MDS array codes with optimal rebuilding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruck</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="1597" to="1616" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generic transformation for optimal repair bandwidth and rebuilding access in MDS codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Information Theory</title>
		<meeting><address><addrLine>Aachen, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-25" />
			<biblScope unit="page" from="1623" to="1627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ceph: A scalable, high-performance distributed file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maltzahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="307" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Grid resource management -CRUSH: controlled, scalable, decentralized placement of replicated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maltzahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE SC2006 Conference on High Performance Networking and Computing</title>
		<meeting>the ACM/IEEE SC2006 Conference on High Performance Networking and Computing<address><addrLine>Tampa, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">122</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">RADOS: a scalable, reliable storage service for petabyte-scale storage clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maltzahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Petascale Data Storage Workshop (PDSW &apos;07)</title>
		<meeting>the 2nd International Petascale Data Storage Workshop (PDSW &apos;07)<address><addrLine>Reno, Nevada, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
	<note>November 11</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Explicit constructions of optimal-access MDS codes with nearly optimal sub-packetization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="6307" to="6317" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
