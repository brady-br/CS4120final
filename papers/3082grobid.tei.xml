<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T03:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inferring Origin Flow Patterns in Wi-Fi with Deep Learning Inferring Origin Flow Patterns in Wi-Fi with Deep Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 18-20. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjune</forename><forename type="middle">L</forename><surname>Gwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjune</forename><forename type="middle">L</forename><surname>Gwon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Kung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">Harvard University</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inferring Origin Flow Patterns in Wi-Fi with Deep Learning Inferring Origin Flow Patterns in Wi-Fi with Deep Learning</title>
					</analytic>
					<monogr>
						<title level="m">• Philadelphia, PA USENIX Association 11th International Conference on Autonomic Computing</title>
						<imprint>
							<biblScope unit="page">73</biblScope>
							<date type="published">June 18-20. 2014</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 11th International Conference on Autonomic Computing (ICAC &apos;14) is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a novel application of deep learning in networking. The envisioned system can learn the original flow characteristics such as a burst size and inter-burst gaps conceived at the source from packet sampling done at a receiver Wi-Fi node. This problem is challenging because CSMA introduces complex, irregular alterations to the origin pattern of the flow in the presence of competing flows. Our approach is semi-supervised learning. We first work through multiple layers of feature extraction and subsampling from unlabeled flow measurements. We use a feature extractor based on sparse coding and dictionary learning, and our subsampler performs overlapping max pooling. Given the layers of learned feature mapping, we train SVM classifiers with deep feature representation resulted at the top layer. The proposed scheme has been evaluated empirically in a custom wireless simulator and OPNET. The results are promising that we achieve superior classification performance over ARMAX, Na¨ıveNa¨ıve Bayes classifiers, and Gaussian mixture models optimized by the EM algorithm.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning plays an increasingly important role in the complex, data-intensive tasks required by today's sensing and computing systems. A flow is a sequence of data packets sharing the same context (e.g., TCP connection, media stream) sent from a source to its destination. Accurate knowledge about the flow characteristics such as a burst size (in number of packets or bytes) and interburst gap, as were originated at the source, can be used beneficially to manage scarce networking resources. One motivating example would be software-defined networking (SDN) <ref type="bibr" target="#b0">[3]</ref>, which can leverage detailed flow knowledge to program routers and access points (APs) for scheduling a congested data traffic or mitigating wireless interferences more intelligently. This paper describes our first work in developing inference schemes to learn the original properties of a flow from packet sampling at a receiver that is not necessarily the destination of the flow. We focus on the case where the source and the receiver are Wi-Fi nodes, and there are other Wi-Fi nodes that transmit their own flows. In particular, the receiver for our case is a network node such as a Wi-Fi AP that forwards or broadcasts packets, being a spot of aggregating different flows. The key challenge is how to unravel the work of CSMA that introduces a complicated mixture of competing flows. We believe that the approach of this paper can be extended for various other wireless and wired networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Problem</head><p>Figure 1 explains our origin flow inference problem. WiFi node A is the source of flow f A transmitted to Node B, a Wi-Fi AP. We denote x A|B a sample of f A measured by B. We use vector notation f to represent an origin flow pattern over time, and x its measurement. (Section 2 will explain how we describe patterns of a flow in detail; for now, consider f and x finite sequences of numbers.) Notice that there are other Wi-Fi nodes in the channel, namely nodes C, D, E, F, and G, that transmit own flows, creating contentions.</p><p>Distributed Coordination Function (DCF) provides the fundamental mechanism to access wireless media for the IEEE 802.11 Wi-Fi <ref type="bibr">[1]</ref>. DCF employs Carrier Sense Multiple Access (CSMA) with a random backoff drawn from an exponentially growing window. Mixed with other transmissions, the sample x A|B could hardly preserve the original patterns in f A . For example, the received packet burst lengths and gaps between bursts can be altered significantly. The exactness of such alteration is difficult to estimate, but there are both linear (e.g., geometric increase of burst lengths) and nonlinear (e.g., packet loss, retransmission, timeout) distortions. Among all possible causes, the main culprit should be CSMA. Figure 1: Flow inference problem illustrated</p><p>We aim to solve the following.</p><p>1. Classify received frame/packet pattern sampled at a receiver Wi-Fi node to the origin flow pattern; 2. Infer the original properties of a flow such as burst sizes and inter-burst gaps originated at the source.</p><p>We clarify several points. First, the origin flow pattern conveys context of application-level data. This is depicted under node A in <ref type="figure" target="#fig_2">Figure 1</ref>. f A instantiates a pattern of the data formation mostly passed from the application layer (and the lesser from Transport/IP/MAC) to the transmit unit. Thus, our inference problem can lead to important understanding of application context mining. Secondly, our primary work domain is the MAC layer. We deduce observable patterns of a conventional TCP/UDP/IP flow from measuring directly the 802.11 MAC frames over time. Lastly, a sampled flow pattern at best is the origin flow pattern shifted (delayed) in time. For our inference to be effective, it is crucial to learn invariance such as some preserved original burst lengths that can spread widely over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Motivating Applications</head><p>Traffic monitoring capabilities are crucial for network management and security. Wireless bandwidth is among the most precious networking resources. Accurate origin flow inference can help derive efficient scheduling for wireless channels. The inferred information can also be used to classify legitimate traffic from malicious attacks. Programming network nodes. Software-defined networking (SDN) is an emerging paradigm to build highly dynamic networks. Inferred origin traffic information can help program SDN nodes. For example, we can improve transmit-receive scheduling and avoid interferences. Resource provisioning. The state-of-the-art networks can provision almost all networking resources elastically. The origin traffic inference will reveal the original properties of a flow that resource provisions such as communication bandwidth, flow cache, and compute cycles should strive to satisfy.</p><p>Queue management. A network node (e.g., router, AP, switch) can leverage the source sending rates of large flows to manage its receive buffers and scheduling mechanisms dynamically. With knowledge on origin characteristics of a flow, networks can improve overall fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Our Contributions</head><p>The main contribution of our work is to demonstrate the effectiveness of learning algorithms applied to an important networking problem mostly studied under parametric, model-based frameworks. Our approach is semisupervised learning. We set up and train deep, unsupervised feature learning that constitutes multiple layers of sparse coding and pooling units. Given the learned feature mapping, we train classifiers in supervised learning. We have identified the key attributes for successful learning approaches to enhance our baseline such as forcing incoherency for sparse coding dictionary to extract more discriminative features, dense arrangement of sparse coding units, and max pooling on overlapping intervals. We have also explored and experimented with other learning methods from classical autoregressive time-series prediction and Na¨ıveNa¨ıve Bayes classifiers to the EM-optimized Gaussian mixtures. Our evaluation empirically confirms superior performance of the proposed learning methods in recovering the original properties of a flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Related Work</head><p>There is considerable work in model-based estimation for origin flow properties. Basu and Mukherjee <ref type="bibr" target="#b2">[5]</ref> discuss numerous time-series models for Internet data traffic, including the autoregressive moving average process helpful for some of our formulation in Sections 2 and 3. Claffy et al. <ref type="bibr" target="#b6">[9]</ref> present one of the earliest work to infer the original packet size distribution of a flow from packet sampling at routers. <ref type="bibr">Duffield et al. [12]</ref> analyze methods to infer the original frequencies of flow lengths from sparse packet sampling.</p><p>The way sparse representations are used in computer vision and pattern recognition has inspired our method. Wright et al. <ref type="bibr" target="#b24">[27]</ref> have developed a face recognition system that performs classification with sparse representation of features, which is based on a similar idea as ours. The idea of pooling sparse representations of features provides an important primitive to construct higher-level features as studied by Raina et al. <ref type="bibr" target="#b19">[22]</ref>, although pooling techniques date back to Riesenhuber and Poggio <ref type="bibr" target="#b21">[24]</ref>. <ref type="bibr">Coates and Ng [10]</ref> propose to pool over multiple features for deep learning.</p><p>Heisele, Ho, and Poggio <ref type="bibr" target="#b11">[14]</ref> explain useful techniques of applying SVM for multi-class classification, which is inherent in our origin flow pattern inference problem. There are a number of existing techniques to learn incoherent dictionary atoms. <ref type="bibr">Ramirez et al. [23]</ref>, <ref type="bibr">Zhang &amp; Li [28]</ref>, and Lin et al. <ref type="bibr" target="#b15">[18]</ref> have proposed similar ideas that force orthonormal dictionary columns as we maximize incoherency among dictionary columns in §5.1. Our idea of accompanying sparsity relaxation ( §5.2) for sparse coding with forced incoherent dictionary atoms is new.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5">Outline</head><p>Section 2 explains the time-series representation and processing of a flow. In Section 3, we explore supervised learning methods for the origin flow inference. Section 4 describes our baseline semi-supervised learning method. We propose several enhancements to the baseline method in Section 5 and evaluate the learning methods with a custom simulator and OPNET in Section 6. The paper concludes in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Time-series Representation of Flow</head><p>The runs-and-gaps model <ref type="bibr" target="#b13">[16]</ref> gives a concise way to describe a flow. In <ref type="figure" target="#fig_0">Figure 2</ref>, characteristic patterns of an example flow are captured by packet runs and gaps measurable over time. As indicated earlier, we perform our flow measurements directly at the MAC layer rather than at the transport or IP layers by sampling and processing Wi-Fi frames. Here, an important parameter is the unit interval T s or sampling period during which each element w t is sampled and recorded. The total measurement time or observation window is N × T s . Alternatively, we have vector x = [x 1 x 2 ... x t ... x N ] for w where x t is the corresponding byte count of the total payload at time t. Hence, a zero in x (or w) indicates a gap. If w 7 = 3 and x 7 = 1, 492, we have 3 packets for the flow at t = 7, and the sum of the payloads from the 3 packets is 1,492 bytes. We will call either w or x a measurable input vector for inference, which contains extractable features. While both w and x carry unique information, we mainly work with x throughout this paper.</p><p>We designate an origin flow (pattern) with another vector f. Just like x, f is a sequence of byte counts uniformly sampled, but the difference is that f reflects the initial pattern (or signature) originated at its source. Note f ∈ R M whereas x ∈ R N , and M and N are not necessarily equal. We use notation x i,k to refer kth measurement on flow i since there can be many measurements on f i . We also use f i,k to designate the kth instance of origin flow i because there could be many origin patterns, or the pattern can be a stochastic process and changes dynamically over time. In summary, f, w, and x are all finite time-series representations of a flow.</p><p>Consider sampling and processing of three example flows in <ref type="figure">Fig. 3</ref> at a receiver. The receive buffer first timestamps each arriving data frame and marks with flow ID. At t = 1, the received frame for flow 1 contains 2 packets whose payload sizes are 50 and 50 bytes, denoted in (2, 50/50B). At t = 6, flow 3 has two received frames. The first frame contains 2 packets with sizes 100 and 400 bytes whereas the second frame contains only one packet with 1,000 bytes. The example results in the following:</p><p>1. The core of an inference system comprises a feature extractor (FE) and a classifier (CL) that need to be trained. <ref type="figure">Figure 4</ref>   <ref type="figure">Figure 4</ref>: Supervised learning framework maps an input x to its feature y and CL : y → ˆ l that performs classification on extracted features of the input. The inference system learns the mappings FE and CL from training examples and their labels. Once trained, when an unknown data x comes in, the system makes an inference by classifying it to a classîclassˆclassî.</p><formula xml:id="formula_0">񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 {xi, li} T i=1 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 x ˆ l y</formula><p>Supervised learning for the origin flow inference problem considers a training dataset</p><formula xml:id="formula_1">{x i , �f l i , l i �} T i=1</formula><p>collected at the Wi-Fi receiver of interest. x i is a time-series representation of flow l i sampled at the receiver. We also make f l i available, the corresponding origin flow timeseries representation. So, when a measured sample x is classified as l j , we can infer the original flow properties from looking up f l j 's.</p><p>We now explore supervised learning methods. We note that most of these methods naturally lead to binary classifiers. Our origin flow inference problem, however, is a multi-class classification. We will revisit this issue in §4.3. For clarity of explanation, this section accompanies binary classification. Autoregressive moving average with exogenous inputs (ARMAX) <ref type="bibr" target="#b16">[19]</ref> is a widely-studied model for linear system identification. ARMAX models the current output of a system with the previous (delayed) output and input values. With ARMAX, we can directly estimate the origin flow time-series</p><formula xml:id="formula_2">f = [ f 1 f 2 ... f t−1 f t ] from the measurement x = [x 0 x 1 ... x t−2 x t−1 ] in a linear difference equation: f t + a 1 f t−1 + ... + a n f t−n = b 1 x t−1 + ... + b m x t−m + ε.</formula><p>Note that ε gives the model error, which itself can be written elaborately over time,</p><formula xml:id="formula_3">i.e., c 1 ε t−1 + ... + c m ε t−m . The ARMAX matrix form is ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ f t f t−1 . . . f 1 ⎤ ⎥ ⎥ ⎥ ⎥ ⎦ � �� � f = ⎡ ⎢ ⎢ ⎣ f t−1 ··· f t−n x t−1 ··· x t−m . . . . . . . . . . . . f 0 ··· f 1−n x 0 ··· x 1−m ⎤ ⎥ ⎥ ⎦ � �� � Φ Φ Φ ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ −a 1 . . . −a n b 1 . . . b m ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ � �� � θ θ θ</formula><p>Under supervised learning, if we have many training ex-</p><formula xml:id="formula_4">amples {x i , �f l i , l i �} T i=1</formula><p>, we will have a massively overconstrained system for our inference problem. Least squares can train θ θ θ . However, when the normal equationˆθ equationˆ equationˆθ θ θ = (Φ Φ Φ T Φ Φ Φ) −1 Φ Φ Φ T y becomes unstable, the recursive least squares via Kalman filtering <ref type="bibr" target="#b5">[8]</ref> can be used instead.</p><p>Na¨ıveNa¨ıve Bayes Classifier. The key for Na¨ıveNa¨ıve Bayes is to define a good feature extractor for a flow measurement x. We use a feature y = � ˆ μ run sizê μ gap length � by computing the sample mean values of run size (bytes) and gap length (unit intervals) from x. The classifier is constructed from computing empirical conditional distribution p(y|l i ) of the train dataset. In runtime, the trained classifiers infer the origin flow:</p><formula xml:id="formula_5">l i if p(l i |y) ≥ p(l j |y) ⇔ p(y|l i )p(l i )/p(y|l j )p(l j ) ≥ 1.</formula><p>Here, we use a simple decision rule that compares the learned likelihood ratios p(y|l i ) and p(y|l j ) for binary classification. Support vector machine (SVM). Boser, Guyon &amp; Vapnik <ref type="bibr" target="#b3">[6]</ref> first proposed SVM. SVM is a binary classification model searching for a hyperplane that maximizes separation between two classes. The hyperplane is orthogonal to the shortest line connecting the convex hulls of the classes. Support vectors are the data points along the shortest line. The hyperplane has the form</p><formula xml:id="formula_6">h(x) = ∑ T i=1 α i l i x i · x + b</formula><p>, where x i is a training example for flow i, and α i , b the solution of a quadratic programming (QP) problem. Class label l i ∈ {−1, 1} for each x i must be provided during the training. Classification of a runtime input x computes sign(h(x)).</p><p>The SVM kernel trick can work with nonlinearity of data. A kernel function K(.) (e.g., radial basis, sigmoid) maps the input x to a higher dimensional feature space where a better margin is possible. The hyperplane with the kernel trick becomes h kern (x) = ∑ T i=1 α i l i K(x i , x) + b. Gaussian mixture model (GMM). Strictly speaking, GMM is an unsupervised feature learning method that is later paired with supervised classifier training. GMM assumes the probability density of input data as a weighted sum of K Gaussian distributions. GMM can be thought as a model-based version for K-means clustering. Parameterized by {w j , μ j , Σ j } K j=1 , the feature for an input x is a combination of posterior membership probabilities evaluated by the Gaussians. For jth Gaussian, we have</p><formula xml:id="formula_7">p j (x) = 1 (2π) N/2 |Σ j | 1/2 · exp � − 1 2 (x − μ j ) � Σ −1 j (x − μ j ) �</formula><p>Expectation maximization (EM) <ref type="bibr" target="#b8">[11]</ref> trains GMMs iteratively. In E-step, EM creates a function evaluating the expected log-likelihood with the current estimate of the parameters. M-step computes new parameter values that maximizes the expected log-likelihood of the E-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Origin Flow Inference with Deep Learning</head><p>Deep learning refers to multiple layers of extracting features and nonlinear aggregation of the extracted features.</p><p>This section presents our deep learning approach for the origin flow inference problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>We propose an inference system based on semisupervised learning. At the first stage, the system performs unsupervised feature learning over multiple layers.</p><p>1. Do sparse coding and dictionary learning with unlabeled training dataset 2. Pool sparse representations of the training dataset to reduce the number of features 3. Pass the resulting features (i.e., pooled sparse representations) to next layer and repeat by treating current layer's features as input data for next layer Given multiple layers of the learned dictionaries and features, the system next performs supervised learning.</p><p>1. Do multi-layer sparse coding and pooling with labeled training dataset 2. Train (linear) SVM classifiers with the final form of feature vector resulted at the top layer</p><p>At runtime, the system takes a sample measurement of a flow, performs the multi-layer inference (i.e., sparse coding and pooling), and predicts the origin flow pattern and properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Feature Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Sparse coding</head><p>We use sparse coding <ref type="bibr" target="#b17">[20]</ref> as the primary means to extract features from the sampled time-series data. Con-</p><formula xml:id="formula_8">sider unlabeled dataset {x k } T k=1 with each x k ∈ R N . We pack {x k } T k=1 to the columns of X = 񮽙 x � 1 x � 2 ... x � T 񮽙 .</formula><p>Note X ∈ R N×T . Sparse coding requires a dictionary D ∈ R N×P learned from X. We adopt K-SVD <ref type="bibr" target="#b1">[4]</ref> that learns D in the following optimization</p><formula xml:id="formula_9">min D,Y �X − DY� 2 F s.t. �y k � 0 ≤ K ∀k<label>(1)</label></formula><p>Here, the columns of Y ∈ R P×T or {y k } T k=1 , are the sparse representations of {x k } T k=1 . (Note y k ∈ R P .) K-SVD is a fast iterative algorithm and requires to compute sparse code y k for each x k with current D. We use orthogonal matching pursuit (OMP) <ref type="bibr" target="#b18">[21]</ref> for computing sparse codes. Our choice of OMP is merely based on its computational efficiency, and there are other algorithms such as LASSO <ref type="bibr" target="#b23">[26]</ref> and LARS <ref type="bibr" target="#b10">[13]</ref> that also work well.</p><p>The columns of D, {d j } P j=1 , are dictionary atoms. Hence, each element in vector y reflects a degree of membership to the corresponding dictionary atom. To represent unbiased membership, dictionary atoms are normalized such that 񮽙 񮽙 d j 񮽙 񮽙 2 � 2 = 1. To make every d j meaningful, we need more training samples than dictionary atoms, so T ≥ P. For discriminative convenience, D is overcomplete-i.e., P &gt; N. This means that sparse code y has a higher dimensionality than x, but y is Ksparse, that is, only K � N entries of y are nonzero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Max pooling</head><p>Every input vector x k is mapped to the corresponding feature vector y k via sparse coding. If all feature vectors were used straightforwardly, we could overwhelm the unsupervised feature learning process. It is customary to reduce the number of extracted features by subsampling (or summarizing over) feature vectors-note, this is by no means to discard any useful information.</p><p>Pooling, popular in convolutional neural networks <ref type="bibr" target="#b14">[17]</ref>, operates over multiple (sparse) feature representations and aggregates to a higher level of features in reduced dimension. An important property of pooled feature representation is translation invariance <ref type="bibr" target="#b21">[24]</ref>. We use max pooling <ref type="bibr" target="#b4">[7]</ref> that takes the maximum value for the elements in the same position over a group of feature vectors. For example, consider max pooling of L sparse codes {y 1 , y 2 ,...,y L } that yields z = max pool(y 1 , y 2 ,..., y L ) in <ref type="figure">Figure 5</ref>. 2 -gives the final feature representation for {x 1 , x 2 , x 3 , x 4 } in this 2-layer deep learning.</p><p>In general, a depth or the number of layers reflects the coverage of deep learning. Layer 1 extracts small, local features over multiple intervals spanned by consecutive input vectors. The resulting feature representations are subsampled with max pooling before passed to layer 2. Layer 2 builds larger features using its own dictionary. Because the feature coverage by layer 1 coding and pooling is over a subregion for the layer 2 coverage, the features aggregated at layer 2 are novel and could not be seen at layer 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Supervised Learning</head><p>We embrace the supervised learning that largely consists of training SVM classifiers. The SVM classification framework is generic and can directly work on x without any feature extraction or pooling. SVMs could be trained with a single-layer sparse representation y (1) subject to x = D (1) y (1) . Under our 2-layer deep learning setup, we train linear SVM classifiers using the final pooled feature vectors z <ref type="bibr">(2)</ref> .</p><p>Considering there are many data patterns generated by applications, the origin flow inference is a multiclass classification problem. There are two approaches for SVM, which is inherently a binary classifier, to classify q origin flow patterns. The first approach is to train all q(q−1) 2 1-vs-1 SVMs exhaustively. Each SVM is dedicated to distinguish between any pair out of q origin flows and infers the original flow properties mapped by the classification result.</p><p>The second approach is to train q 1-vs-all SVMs. For flow i, training 1-vs-all SVM will require two datasets X i with label l i = 1 consisting of measured patterns of flow i only and X \i with label l j = −1 containing measurements for all other flows j ∀ j � = i. Ideally, X \i should contain unbiased mix of the other flows. Our empirical evaluation in Section 6 considers the 1-vs-all approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Enhancements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Incoherent Dictionary Learning</head><p>Dictionary learning algorithms address the performance of sparse coding in two aspects: 1) reconstructive accuracy and 2) discriminative ability of the learned dictionary atoms (i.e., the column vectors of D). We emphasize the latter aspect because our primary objective is classification rather than compressing data. Discriminative ability of a dictionary is related to making its atoms as incoherent as possible. Sparse coding with a dictionary consisting of more incoherent column vectors should improve the margin of an SVM, which results in a better classification performance.</p><p>The maximally incoherent D is constrained such that D T D = I. In other words, an incoherent dictionary matrix has orthonormal columns. This is equivalent to minimizing</p><formula xml:id="formula_10">񮽙 񮽙 D T D − I 񮽙 񮽙 2 F</formula><p>. We can also think of having the two</p><formula xml:id="formula_11">conditions d T k d j = 0 ∀k � = j (orthogonal columns) and 񮽙 񮽙 d T k d k 񮽙 񮽙 2 = 1 (normalized).</formula><p>Since we use K-SVD, we add the incoherence optimization term to Equation <ref type="formula" target="#formula_9">(1)</ref> min</p><formula xml:id="formula_12">D,Y �X − DY� 2 F + γ 񮽙 񮽙 D T D − I 񮽙 񮽙 2 F s.t. �y k � 0 ≤ K ∀k (2)</formula><p>The new optimization here, however, is not a trivial task.</p><p>For the time being, we propose a two-stage algorithm presented below instead. In the outer for loop, we run K-SVD unmodified. The resulting D then enters the inner while loop that implements the gradient descent algorithm <ref type="bibr" target="#b22">[25]</ref> to regularize the incoherence term in Eq. <ref type="bibr">(2)</ref>. γ is the step size for gradient search and decayed by 0 &lt; δ &lt; 1 within the inner loop until initialized back to the default value γ 0 in the outer loop after running K-SVD with the next training vector. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Sparsity Relaxation</head><p>Strictly speaking, the way we force the dictionary incoherence is flawed. The gradient descent takes over after K-SVD, but K-SVD and the gradient descent regularization have to take place jointly as Equation <ref type="formula">(2)</ref> suggests. For this reason, the resulting effect of the outer loop computation perturbs the K-SVD optimization. In other words, improving the dictionary incoherence comes with the cost of reconstructive accuracy.</p><p>As a result, using the same value of K for sparse coding may be too tight to meet the minimal error criterion. Therefore, we must relax the original sparsity K for sparse coding to K � such that K � &gt; K. We do sparsity relaxation as follows. Let D � represent the incoherent dictionary resulted from Algorithm 1. We repeat sparse coding with</p><formula xml:id="formula_13">D � to find Y � min Y � 񮽙 񮽙 X − D � Y � 񮽙 񮽙 2 F s.t. 񮽙 񮽙 y � k 񮽙 񮽙 0 ≤ K � ∀k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Dense Sparse Coding and Overlapping Pooling</head><p>The baseline deep learning scheme in <ref type="figure">Figure 6</ref> does batch sparse coding of {x 1 , x 2 , x 3 , x 4 } and max pooling of {y 1 , y 2 } and {y 3 , y 4 }. We can enhance this baseline by performing sparse coding on shifted x k 's and max pooling over the resulting, overlapping sparse codes. This is illustrated in <ref type="figure">Figure 7</ref>. The overlapping intervals are formed by shifting (i.e., delaying) the elements in x k 's altogether by τ. Note that τ = 1 gives the fully overlapped intervals while there is no overlapping for τ = 4 · N, which equals the baseline. (In our evaluation, we use a 95% overlap between consecutive x's.) Dense sparse coding can substantially reduce chances to miss a feature with increased cost of computing. Overlapping pooling further improves on the translational invariance of a feature. Also, according to <ref type="bibr">Krizhevsky et al. [15]</ref>, overlapping pooling reduces overfit in classifiers.  <ref type="figure">Figure 7</ref>: Enhanced 2-layer deep learning with dense sparse coding and overlapping max pooling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate the proposed baseline and enhanced inference schemes in comparison to classical machine learning approaches described in Section 3. We have implemented a simple setup featuring three Wi-Fi nodes in a custom MATLAB simulator and used OPNET Modeler to test more elaborated, seven Wi-Fi node scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Flow generation</head><p>We generate flows based on the runs-and-gaps model explained in Section 2. The triplet �t r , s r ,t g � describes the generative pattern of a flow, where t r and t g are the run and gap lengths in number of unit intervals (T s ), and s r denotes the size of payload in bytes generated per each unit interval of a run. A flow type can be constant, stochastic, or mixed. A constant flow has deterministic t r , s r , and t g values. We consider 10 origin flows summarized in <ref type="table" target="#tab_4">Table 1</ref>. Notice we also use the normal (N) and uniform (U) distributions. We round fractions, discard negative numbers drawn from a normal distribution and regenerate. Using T s = 10 msec, we generate 2,000 instances of time series for each flow. We use the first 1,000 instances for training and the other 1,000 for testing. Each instance is a vector Stochastic �U(4, 10), Pareto(100, 2), Exp(0.5)� Flow 8</p><p>Stochastic �N(10, 5), Pareto(40, 1), N(10, 5)� Flow 9</p><p>Mixed �1, Pareto(100, 2), 1� Flow 10 Mixed �1, Pareto(100, 2), Exp(0.25)� of 500 elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Preprocessing generated origin flow patterns</head><p>We precompute the mean run and gap lengths from the generated origin flow patterns in the training dataset. This is convenient because we enable simple lookup (of the precomputed values) based on the classification result of a measured flow in order to estimate the origin run and gap properties. In <ref type="figure">Figure 8</ref>, we have</p><formula xml:id="formula_14">񮽙 s 1 1 s 1 2 0 0 0 s 2 1 0 0 0 0 0 s 3 1 s 3 2 s 3 3 0 0 ... 񮽙 , where</formula><formula xml:id="formula_15">s 1 = ∑ 2 k=1 s 1 k , s 2 = ∑ 1 k=1 s 2 k , s 3 = ∑ 3 k=1 s 3 k</formula><p>give total bytes of the three bursts. We can then compute the mean burst size for this pattern. We also compute {t 1 r ,t 2 r ,t 3 r ,...},</p><formula xml:id="formula_16">{t 1 g ,t 2 g ,t 3 g ,..</formula><p>.}, and their mean values. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙 񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>񮽙</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Evaluation metrics</head><p>We are foremost interested in the accuracy of classifying a measured pattern x to its ground-truth origin flow pattern f. We compute two metrics, recall (true positive rate) and false alarm (false positive rate), to evaluate classification performance: Without false alarm rate, we cannot truly assess the probability of detection for a classifier using a computed recall value because the classifier can be configured to declare positive only, automatically achieving to guess all positives correctly. Classification leads to inferring other important properties of a flow from its training dataset records. As our secondary evaluation metrics, we calculate errors in estimating the original mean burst size and mean gap length of the flow. </p><formula xml:id="formula_17">Recall = ∑</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Scenario 1: Three Wi-Fi Nodes</head><formula xml:id="formula_18">f C = f j ∀ j � = i.</formula><p>Node C can change its flow pattern from f j to f k , while node A still running f i , but f k is chosen such that k � = i. Wi-Fi setup. We have implemented a custom discreteevent simulator in MATLAB, assuming the IEEE 802.11g our baseline Wi-Fi system. At its core, our CSMA implementation is based on an open-source wireless simulator <ref type="bibr">[2]</ref>. The backoff mechanism works as follows. The contention window CW is initialized to aCW min. In case of timeout, CSMA doubles CW, otherwise waits until the channel becomes idle with an additional DCF interframe space (DIFS) duration. CSMA chooses a uniformly random wait time between <ref type="bibr">[1, CW]</ref>. CW can grow up to aCW max of 1,023 slots. CW is decremented only when the media is sensed idle. RTS and CTS are disabled. The Wi-Fi configuration is summarized in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>Inference schemes. We have implemented all of the inference schemes in MATLAB. We consider ARMAX- least squares, Na¨ıveNa¨ıve Bayes classifiers, Gaussian mixture models (GMM), and the two deep learning methods we proposed. The baseline deep learning method has 2 layers implemented as described in Section 4. The enhanced deep learning method also has 2 layers, but we additionally have implemented incoherent dictionary training, dense sparse coding, and overlapping max pooling as described in Section 5. We call the size of vector x observed length or observation window size and have varied 100, 200, 300, and 500.</p><p>Training. As mentioned in §6.1.1, the training dataset for each flow has 1,000 instances. For flow i, we transmit training examples {f i,k } 1000 k=1 serially. (Note the notation f i,k for kth instance of flow i.) We consider 1-vs-all linear SVM classifiers for all inference schemes.</p><p>We train ARMAX with n previously transmitted origin flow patterns and m previous inputs-i.e., for flow i, we feed {f i,k−1 ,...,f i,k−n } and {x i,k−1 ,..., x i,k−m }-at time k. After trying out various configurations, we choose n = 2 and m = 3. The least squares directly estimatê f given x. SVMs for ARMAX are trained withˆfswithˆwithˆfs.</p><p>For Na¨ıveNa¨ıve Bayes, we extract the statistic y = 񮽙ˆμ񮽙ˆ 񮽙ˆμ run sizê μ gap length 񮽙 from x by averaging run burst sizes and gap lengths and use y as a feature with label l i for flow i to build empirical distributions p(y|x, l i ). Note training Na¨ıveNa¨ıve Bayes yields classifiers, so we do not need to train any SVMs for Na¨ıveNa¨ıve Bayes. We use K = 10 GMMs. Like Bayes, we use the same static y from x as a feature to train GMMs via the EM algorithm. However, unlike Bayes, GMMs do not yield classifiers. We train SVMs with the membership probabilities from the trained K Gaussians evaluated on y given l i .</p><p>The proposed deep learning methods produce a maxpooled sparse representation z (2) at the top of layer 2 for given x. We use labeled z (2) 's to train SVMs.</p><p>Results. <ref type="figure" target="#fig_2">Figure 10</ref> presents classification recall and false alarm rate of each inference scheme for Scenario 1. Overall, deep learning (DL) yields consistently higher recall at lower false alarm. When using a small observation window length to sample x, the recall gap between the enhanced and baseline deep learning methods is noticeably larger than the case of using a large observation window. This is because dense sparse coding and overlapping max pooling in the enhanced deep learning scheme substantially reduce the probability of missing a feature. A possible explanation for poor ARMAX performance could be that CSMA introduces significant nonlinear distortions. GMMs are on par with our baseline deep learning. If optimized to their limits, GMMs seem to be a reasonable alternative to deep learning for classification. This is not surprising since GMMs are really a form of K-SVD. (Note also that our effort to fine-tune GMMs was only fair as our focus in this paper was on deep learning.) After classification, we predict the original mean burst size and gap length of a flow based on lookup of the precomputed values from the training dataset (see explanation in §6.1.2). The estimation errors in <ref type="table" target="#tab_6">Table 3</ref> are the best case, obtained with the maximum observation window size of 5 seconds that we have tried. <ref type="figure" target="#fig_2">Figure 11</ref> illustrates Scenario 2 featuring seven Wi-Fi nodes simulated in OPNET. This scenario is important for several reasons. First, we can configure more realistic application profiles for simulated nodes. We can also scale the simulation. Lastly, we can validate our schemes with OPNET's built-in Wi-Fi protocols, particularly the IEEE 802.11g, which should be more complete than our MATLAB simulator. Scenario 2 preserves nodes A, B, C and their activities the same as Scenario 1. There are five additional Wi-Fi nodes that communicate in typical Internet styles as summarized in <ref type="table" target="#tab_7">Table 4</ref>.  We test the same inference schemes and keep the training methodology of Scenario 1. In <ref type="figure" target="#fig_0">Figure 12</ref>, we plot classification recall and false alarm rate of each scheme evaluated under Scenario 2. With more nodes and increased traffic, the overall classification performance is worse. Again, we can clearly see the benefits of dense sparse coding and overlapping pooling for enhanced deep learning that consistently outperforms the other schemes over various observation window sizes. For a small observation window in particular, recall for enhanced deep learning is substantially higher while achieving the lowest false alarm rate. <ref type="table" target="#tab_6">Table 3</ref> shows the estimation errors (in parenthesis) to predict original properties of the flows, the mean burst size and mean gap length, after classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Scenario 2: Wi-Fi Nodes in OPNET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We have addressed the problem of inferring the original properties of a flow sampled from a received Wi-Fi traffic mix. This inverse problem is challenging because CSMA significantly changes the origin pattern of the flow while scheduling with other flows in competition. Machine learning can provide tools to harness complexity and nonlinearity, but it requires to apply adept domain knowledge or application-specific insights to configure the overall learning pipeline and fine-tune all model parameters to their meticulous detail. Learned from our initial, unsuccessful attempt to straightforwardly integrate sparse coding and dictionary learning into SVM classification, we have set up multiple layers of sparse feature extraction and max pooling that enable deep learning from received flow patterns. Multilayer sparse coding allows us to learn local, atomic features such as run and gap sizes of a flow accurately at the lowest layer and global features such as periodicity in traffic patterns at higher layers. Max pooling summarizes often too many extracted features while providing translation invariance. We have explained how the proposed approaches incorporate these ideas and validated their superior performance.</p><p>In summary, contributions of this paper include a novel formulation of an inverse problem to recover origin flow patterns in Wi-Fi, identification of the key attributes for successful machine learning approaches to solve such inverse problems (i.e., the use of sparse representation of features, multi-layer inference, and pooling), and demonstration of the sound working of these methods in recovering original flow properties. We have chosen not to discuss possible applications of this paper in security and network anomaly detection. We plan to address such applications in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Runs-and-gaps model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 3: Time-series processing example</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Two-stage incoherent dictionary learning Require: training dataset X = 񮽙 x � 1 ... x � T 񮽙 1: initialize D := I 2: for i = 1 to T 3: D := ksvd(D, x k , K) 4: initialize γ := γ 0 5: while 񮽙 񮽙 D T D − I 񮽙 񮽙 2 F &gt; ε 6: D := D − γD(D T D − I) 7: D := normalize columns(D) 8: γ := γ · δ 9: end 10: end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For example, flow 1 with �2, 100, 4� creates the origin flow pattern (time series) f 1 = [100 100 0 0 0 0 100 100 0 0 ... ]. For a stochastic flow, t r , s r , and t g are random variables. For example, flow 6 with �Exp(0.5), Pareto(40, 1), Exp(0.25)� has ex- ponentially distributed run and gap lengths (with mean of 1/0.5 and 1/0.25 unit intervals, respectively), and Pareto- distributed payload sizes. An instance of flow 6 could be f 6 = [518 97 0 0 0 0 0 0 32 0 ... ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 8: Computing generated flow statistics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>True positives ∑ True positives + ∑ False negatives False alarm = ∑ False positives ∑ False positives + ∑ True negatives</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 depicts Scenario 1 .</head><label>91</label><figDesc>Figure 9 depicts Scenario 1. In this simple scenario, we infer the origin time series f A sent by source node A, using x A|B measured at receiver node B. Node C, another source, contends with node A by transmitting its own flow f C . We carry out cross-validation with all 10 flow datasets by setting f A = f i ∀i ∈ {1,..., 10}, flow by flow at once. When f A = f i , we randomly set f C = f j ∀ j � = i. Node C can change its flow pattern from f j to f k , while node A still running f i , but f k is chosen such that k � = i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 9: Scenario 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Classification recall and false alarm rate for Scenario 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 11: Scenario 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Classification recall and false alarm rate for Scenario 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>}. There are two mappings, FE : x → y that</head><label></label><figDesc></figDesc><table>describes the supervised learning frame-
work. Supervised learning requires a labeled training 
dataset that consists of training examples {x 1 , . .. ,x T } 
with corresponding desired output values (i.e., labels) 
{l 1 , . .. ,l T Feature 
Extractor 
(FE) 

Classifier 
(CL) 

Feature 
Extractor 
(FE) 

Classifier 
(CL) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>], max pooling results in z j = max(y 1, j , y 2, j ,...,y L, j ). 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙񮽙has a moderate size N, we perform batch processing of multiple x k 's concatenated in series. Figure 6 showcases 4 input vectors with pooling unit configured at L = 2. If x k has a very large N on the other hand, x k can be divided 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙</head><label></label><figDesc></figDesc><table>Noting y k = 
[y k,1 ... y k,P ] and z = [z 1 ... z P 񮽙񮽙񮽙 

Figure 5: Max pooling of L sparse codes 

4.2.3 Multi-layer deep learning 

Figure 6 presents our multi-layer deep learning. We use 2 
layers. Each layer trains own dictionary and has separate 
sparse coding and pooling units. Assuming input vector 
x k 񮽙 񮽙 񮽙 
񮽙 񮽙 񮽙 
񮽙 񮽙 񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙 񮽙 
񮽙񮽙񮽙 񮽙 
񮽙 񮽙 
񮽙񮽙񮽙 񮽙 
񮽙 񮽙 
񮽙񮽙񮽙 񮽙 
񮽙 񮽙 
񮽙񮽙񮽙 񮽙 

񮽙 񮽙 
񮽙񮽙񮽙 񮽙 
񮽙 񮽙 
񮽙񮽙񮽙 񮽙 

񮽙 񮽙 
񮽙񮽙񮽙 񮽙 
񮽙 񮽙 
񮽙񮽙񮽙 񮽙 

񮽙 񮽙 
񮽙񮽙񮽙 񮽙 

Figure 6: Baseline 2-layer deep learning with sparse coding and 
max pooling 

into subpatches, and we perform sparse coding on each 
subpatch. The input vector length for sparse coding is an 
important system parameter for deep feature learning. 
Max pooling is performed over sets of two consecutive 
sparse representations {y 

(1) 

1 , y 

(1) 

2 } and {y 

(1) 

3 , y 

(1) 

4 }. We 
use notation y 

(1) 

k for kth sparse code at layer 1. The in-
termediate pooled features, z 

(1) 

1 and z 

(1) 

2 , are sent to layer 
2 for another round of sparse coding and max pooling. 
At the top, z 

(2) 

1 -obtained by pooling the layer 2 sparse 
codes y 

(2) 

1 and y 

(2) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙񮽙</head><label>񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙</label><figDesc></figDesc><table>񮽙 񮽙 񮽙 
񮽙 񮽙 񮽙 
񮽙 񮽙 񮽙 
񮽙 񮽙 񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 񮽙񮽙񮽙 񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙񮽙 񮽙 񮽙񮽙񮽙񮽙񮽙񮽙񮽙 
񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 

񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 


</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 1 : Origin flows used for evaluation</head><label>1</label><figDesc></figDesc><table>Flow 
Type 
Generative triplet �t r , s r ,t g � 
Flow 1 
Constant 
�2, 100, 4� 
Flow 2 
Constant 
�2, 500, 2� 
Flow 3 
Constant 
�5, 200, 5� 
Flow 4 
Constant 
�10, 200, 10� 
Flow 5 
Stochastic 
�Exp(1), Pareto(100, 2), Exp(0.1)� 
Flow 6 
Stochastic 
�Exp(0.5), Pareto(40, 1), Exp(0.25)� 
Flow 7 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 2 : Wi-Fi parameter configuration for Scenario 1</head><label>2</label><figDesc></figDesc><table>Parameter 
Description 
Value 
aSlotTime 
Slot time 
20 μsec 
aSIFSTime 
Short interframe space (SIFS) 
10 μsec 
aDIFSTime 
DCF interframe space (DIFS) 
50 μsec 
aCW min 
Min contention window size 
15 slots 
aCW max 
Max contention window size 
1023 slots 
tPLCPPreamble PLCP preamble duration 
16 μsec 
tPLCP SIG 
PLCP SIGNAL field duration 
4 μsec 
tSymbol 
OFDM symbol duration 
4 μsec 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 : Errors to estimate original mean burst size and mean gap length with 5-sec observation window (error percentages in parenthesis are for Scenario 2)</head><label>3</label><figDesc></figDesc><table>Scheme 
Origin burst size 
Origin gap length 
estimation error 
estimation error 
ARMAX 
39.3% (45.9%) 
28.1% (36.7%) 
Na¨ıveNa¨ıve Bayes 
31.4% (37.5%) 
15.8% (24.6%) 
GMMs 
23.2% (31.3%) 
11.7% (18.1%) 
DL (baseline) 
18.7% (28.3%) 
9.3% (16.2%) 
DL (enhanced) 
12.2% (22.8%) 
6.8% (11.4%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 : Configuration summary of Scenario 2</head><label>4</label><figDesc></figDesc><table>Node Role 
Main networking activity 
A 
Flow source 
Transmits f i 
B 
Receiver 
Intercepts flows as Wi-Fi router/AP 
C 
Flow source 
Transmits f j ∀ j � = i 
D 
Flow source 
Multimedia streaming over RTP/UDP/IP 
E 
Flow dest. 
HTTP with page size ∼ U[10,400] B 
F 
Flow dest. 
ftp file transfer with size 50000 B 
G 
Flow dest. 
DB access with inter-arrival ∼ Exp(3) sec 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank anonymous reviewers of ICAC for their valuable comments in revising the final version of this paper. This material is based on research sponsored in part by Intel Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Software-defined Networking: The New Norm for Networks. Open Networking Foundation (White Paper)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruckstein</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>K-Svd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klivansky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Time Series Models for Internet Traffic. In INFOCOM</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Training Algorithm for Optimal Margin Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vapnik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Theoretical Analysis of Feature Pooling in Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boureau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lecun</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recursive-least-squares Transversal Filters for Adaptive Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cioffi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailath</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="304" to="337" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application of Sampling Methodologies to Network Traffic Characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claffy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Polyzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Selecting Receptive Fields in Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Coates</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dempster</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating Flow Distributions from Sampled Flow Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duffield</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorup</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Least Angle Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibshirani</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Face Recognition with Support Vector Machines: Global versus Component-based Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heisele</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poggio</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krizhevsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Location-dependent Runs-and-gaps Model for Predicting TCP Performance over UAV Wireless Channel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Tarsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Vlah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hague</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Muccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suter</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MILCOM</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face Recognition: A Convolutional Neural Network Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="98" to="113" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incoherent Dictionary Learning for Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ljung</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ed</surname></persName>
		</author>
		<title level="m">System Identification</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Theory for the User</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Emergence of Simple-cell Receptive Field Properties by Learning a Sparse Code for Natural Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olshausen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And Field</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Orthogonal Matching Pursuit: Recursive Function Approximation with Applications to Wavelet Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pati</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-taught Learning: Transfer Learning from Unlabeled Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raina</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Classification and Clustering via Dictionary Learning with Structured Incoherence and Shared Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramirez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sprechmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sapiro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical Models of Object Recognition in Cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riesenhuber</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Poggio</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An Introduction to Basic Optimization Theory: Classical and New Gradient-based Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snyman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Regression Shrinkage and Selection via the Lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tibshirani</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust Face Recognition via Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wright</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And M</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative K-SVD for Dictionary Learning in Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
