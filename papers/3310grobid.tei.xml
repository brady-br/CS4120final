<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:09+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introduction to SIGHAN 2015 Bake-off for Chinese Spelling Check</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 30-31, 2015. 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuen-Hsien</forename><surname>Tseng</surname></persName>
							<email>samtseng@ntnu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Information Technology Center</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
							<email>lhlee@ntnu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="department">Information Technology Center</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Ping</forename><surname>Chang</surname></persName>
							<email>lchang@ntnu.edu.tw</email>
							<affiliation key="aff1">
								<orgName type="department">Mandarin Training Center</orgName>
								<orgName type="institution">National Taiwan Normal University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
							<email>hhchen@ntu.edu.tw</email>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Computer Science and Information Engineering</orgName>
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Introduction to SIGHAN 2015 Bake-off for Chinese Spelling Check</title>
					</analytic>
					<monogr>
						<title level="m">Association for Computational Linguistics and Asian Federation of Natural Language Processing</title>
						<meeting> <address><addrLine>Beijing, China</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="32" to="37"/>
							<date type="published">July 30-31, 2015. 2015</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>This paper introduces the SIGHAN 2015 Bake-off for Chinese Spelling Check, including task description, data preparation, performance metrics, and evaluation results. The competition reveals current state-of-the-art NLP techniques in dealing with Chinese spelling checking. All data sets with gold standards and evaluation tool used in this bake-off are publicly available for future research.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Chinese spelling checkers are relatively difficult to develop, partly because no word delimiters exist among Chinese words and a Chinese word can contain only a single character or multiple characters. Furthermore, there are more than 13 thousand Chinese characters, instead of only 26 letters in English, and each with its own context to constitute a meaningful Chinese word. All these make Chinese spell checking a challengeable task.</p><p>An empirical analysis indicated that Chinese spelling errors frequently arise from confusion among multiple-character words, which are phonologically and visually similar, but semantically distinct ( <ref type="bibr" target="#b0">Liu et al., 2011</ref>). The automatic spelling checker should have both capabilities of identifying the spelling errors and suggesting the correct characters of erroneous usages. The SIGHAN 2013 Bake-off for Chinese Spelling Check was the first campaign to provide data sets as benchmarks for the performance evaluation of Chinese spelling checkers ( <ref type="bibr" target="#b2">Wu et al., 2013</ref>). The data in SIGHAN 2013 originated from the essays written by native Chinese speakers. Following the experience of the first evaluation, the second bake-off was held in CIPS-SIGHAN Joint CLP-2014 conference, which focuses on the essays written by learners of Chinese as a Foreign Language (CFL) ( <ref type="bibr" target="#b3">Yu et al., 2014</ref>).</p><p>Due to the greater challenge in detecting and correcting spelling errors in CFL leaners' written essays, SIGHAN 2015 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker is expected to identify all possible spelling errors, highlight their locations, and suggest possible corrections.</p><p>The rest of this article is organized as follows. Section 2 provides an overview of the SIGHAN 2015 Bake-off for Chinese Spelling Check. Section 3 introduces the developed data sets. Section 4 proposes the evaluation metrics. Section 5 compares results from the various contestants. Finally, we conclude this paper with findings and offer future research directions in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Description</head><p>The goal of this task is to evaluate the capability of a Chinese spelling checker. A passage consisting of several sentences with/without spelling errors is given as the input. The checker should return the locations of incorrect characters and suggest the correct characters. Each character or punctuation mark occupies 1 spot for counting location. The input instance is given a unique passage number pid. If the sentence contains no spelling errors, the checker should return "pid, 0". If an input passage contains at least one spelling error, the output format is "pid [, location, correction]+", where the symbol "+" indicates there is one or more instance of the predicted element "[, location, correction]". "Location" and "correction", respectively, denote the location of incorrect character and its correct version. Examples are given as follows.</p><p>• Example 1</p><p>Input: (pid=A2-0047-1) 我真的洗碗我可以去看你 Output: A2-0047-1, 4, 希, 5, 望</p><p>• Example 2</p><p>Input: (pid=B2-1670-2) 在日本，大學生打工的情況 是相當普偏的。 Output: B2-1670-2, 17, 遍</p><p>• Example 3</p><p>Input: (pid=B2-1903-7) 我也是你的朋友，我會永遠 在你身邊。 Output: B2-1903-7, 0</p><p>There are 2 wrong characters in Ex. 1, and correct characters "希," and "望" should be used in locations 4, and 5, respectively. In Ex. 2, the 17 th character "偏" is wrong, and should be "遍". Location "0" denotes that there is no spelling error in Ex. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Preparation</head><p>The learner corpus used in our task was collected from the essay section of the computer-based Test of Chinese as a Foreign Language (TOCFL), administered in Taiwan. The spelling errors were manually annotated by trained native Chinese speakers, who also provided corrections corresponding to each error. The essays were then split into three sets as follows (1) Training Set: this set included 970 selected essays with a total of 3,143 spelling errors. Each essay is represented in SGML format shown in <ref type="figure">Fig. 1</ref>. The title attribute is used to describe the essay topic. Each passage is composed of several sentences, and each passage contains at least one spelling error, and the data indicates both the error's location and corresponding correction. All essays in this set are used to train the developed spelling checker.</p><p>(2) Dryrun Set: a total of 39 passages were given to participants to familiarize themselves with the final testing process. Each participant can submit several runs generated using different models with different parameter settings of their checkers. In addition to make sure that the submitted results can be correctly evaluated, participants can fine-tune their developed models in the dryrun phase. The purpose of dryrun is to validate the submitted output format only, and no dryrun outcomes were considered in the official evaluation (3) Test Set: this set consists of 1,100 testing passages. Half of these passages contained no spelling errors, while the other half included at least one spelling error. The evaluation was conducted as an open test. In addition to the data sets provided, registered participant teams were allowed to employ any linguistic and computational resources to detect and correct spelling errors. Besides, passages written by CFL learners may yield grammatical errors, missing or redundant words, poor word selection, or word ordering problems. The task in question focuses exclusively on spelling error correction.</p><p>&lt;ESSAY title="學中文的第一天"&gt; &lt;TEXT&gt; &lt;PASSAGE id="A2-0521-1"&gt; 這位小姐說：你應 該一直走到十只路口，再右磚一直走經過一家銀 行就到了。&lt;/PASSAGE&gt; &lt;PASSAGE id="A2-0521-2"&gt;應為今天是第一天， 老師先請學生自己給介紹。&lt;/PASSAGE&gt; &lt;/TEXT&gt; &lt;MISTAKE id="A2-0521-1" location="15"&gt; &lt;WRONG&gt;十只路口&lt;/WRONG&gt; &lt;CORRECTION&gt;十字路口&lt;/CORRECTION&gt; &lt;/MISTAKE&gt; &lt;MISTAKE id="A2-0521-1" location="21"&gt; &lt;WRONG&gt;右磚&lt;/WRONG&gt; &lt;CORRECTION&gt;右轉&lt;/CORRECTION&gt; &lt;/MISTAKE&gt; &lt;MISTAKE id="A2-0521-2" location="1"&gt; &lt;WRONG&gt;應為&lt;/WRONG&gt; &lt;CORRECTION&gt;因為&lt;/CORRECTION&gt; &lt;/MISTAKE&gt; &lt;/ESSAY&gt; <ref type="figure">Figure 1</ref>. An essay represented in SGML format <ref type="table" target="#tab_0">Table 1</ref> shows the confusion matrix used for performance evaluation. In the matrix, TP (True Positive) is the number of passages with spelling errors that are correctly identified by the spelling checker; FP (False Positive) is the number of passages in which non-existent errors are identified; TN (True Negative) is the number of passages without spelling errors which are correctly identified as such; FN (False Negative) is the number of passages with spelling errors for which no errors are detected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Performance Metrics</head><p>The criteria for judging correctness are determined at two levels as follows.</p><p>(1) Detection level: all locations of incorrect characters in a given passage should be completely identical with the gold standard.</p><p>(2) Correction level: all locations and corresponding corrections of incorrect characters should be completely identical with the gold standard.</p><p>In addition to achieve satisfactory detection/correction performance, reducing the false positive rate, that is the mistaken identification of errors where none exist, is also important ( <ref type="bibr" target="#b1">Wu et al., 2010)</ref>. The following metrics are measured at both levels with the help of the confusion matrix.</p><p>• False Positive Rate (FPR) = FP / (FP+TN)</p><p>• Accuracy = (TP+TN) / (TP+FP+TN+FN)</p><p>• Precision = TP / (TP+FP)</p><formula xml:id="formula_0">• Recall = TP / (TP+FN) • F1= 2 *Precision*Recall/(Precision+Recall) Confusion Matrix System Result Positive (Erroneous) Negative (Correct)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Standard</head><p>Positive For example, if 5 testing inputs with gold standards are "A2-0092-2, 0", "A2-0243-1, 3, 健, 4, 康", "B2-1923-2, 8, 誤, 41, 情", "B2-2731-1, 0", and "B2-3754-3, 10, 觀", and the system outputs the result as "A2-0092-2, 5, 玩", "A2-0243-1, 3, 件, 4, 康", "B2-1923-2, 8, 誤, 41, 情", "B2-2731-1, 0", and "B2-3754-3, 11, 觀", the evaluation tool will yield the following performance:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TP FN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FP TN</head><p>• False Positive Rate (FPR) = 0.5 (=1/2) Notes: {"A2-0092-2, 5"}/{"A2-0092-2, 0", "B2-2731-1, 0"}</p><formula xml:id="formula_1">• Detection-level</formula><p>• Accuracy =0.6 (=3/5) Notes: {"A2-0243-1, 3, 4", "B2-1923-2, 8, 41", "B2-2731-1, 0"} / {"A2-0092-2, 5", A2-0243-1, 3, 4", "B2-1923-2, 8, 41", "B2-2731-1, 0", "B2-3754-3, 11"}</p><p>• Precision = 0.5 (=2/4) Notes: {"A2-0243-1, 3, 4", "B2-1923-2, 8, 41"} / {"A2-0092-2, 5", A2-0243-1, 3, 4", "B2-1923-2, 8, 41", "B2-3754-3, 11"}</p><p>• Recall = 0.67 (=2/3).</p><p>Notes: {"A2-0243-1, 3, 4", "B2-1923-2, 8, 41"} / {A2-0243-1, 3, 4", "B2-1923-2, 8, 41", "B2-3754-3, 10"}   <ref type="table" target="#tab_3">Table 3</ref> shows the task testing results. The research team NCTU&amp;NTUT achieved the lowest false positive rate at 0.0509. For the detectionlevel evaluations, according to the test data distribution, a baseline system can achieve an accuracy level of 0.5 by always reporting all testing cases as correct without errors. The system result submitted by CAS achieved promising performance exceeding 0.7. We used the F1 score to reflect the tradeoff between precision and recall. As shown in the testing results, CAS provided the best error detection results, achieving a high F1 score of 0.6404. For correction-level evaluations, the correction accuracy provided by the CAS system (0.6918) significantly outperformed the other teams. Besides, in terms of correction precision and recall, the spelling checker developed by CAS also outperforms the others, which in turn has the highest F1 score of 0.6254. Note that it is difficult to correct all spelling errors found in the input passages, since some sentences contain multiple errors and only correcting some of them are regarded as a wrong case in our evaluation. <ref type="table">Table 4</ref> summarizes the participants' developed approaches and the usages of linguistic resources. Among 6 teams that submitted the official testing results, NCYU did not submit the report of its developed method. None of the submitted systems provided superior performance in all metrics, though those submitted by CAS and NCTU&amp;NTUT provided relatively best overall performance when different metric is considered. The CAS team proposes a unified framework for Chinese spelling correction. They used HMM-based approach to segment sentences and generate correction candidates. Then, a twostage filter process is applied to re-ranking the candidates for choosing the most promising candidates. The NCTU&amp;NTUT team proposes a word vector/conditional random field based spelling error detector. They utilize the error detection results to guide and speed up the timeconsuming language model rescoring procedure. By this way, potential Chinese spelling errors could be detected and corrected in a modified sentence with the maximum language model score.    <ref type="table">Table 4</ref>. A summary of participants' developed systems</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>This paper provides an overview of SIGHAN 2015 Bake-off for Chinese spelling check, including task design, data preparation, evaluation metrics, performance evaluation results and the approaches used by the participant teams. Regardless of actual performance, all submissions contribute to the knowledge in search for an effective Chinese spell checker, and the individual reports in the Bake-off proceedings provide useful insight into Chinese language processing. We hope the data sets collected for this Bakeoff can facilitate and expedite future development of effective Chinese spelling checkers. Therefore, all data sets with gold standards and evaluation tool are made publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan8csc.html.</p><p>The future direction focuses on the development of Chinese grammatical error correction. We plan to build new language resources to help improve existing techniques for computer-aided Chinese language learning. In addition, new data sets obtained from CFL learners will be investigated for the future enrichment of this research topic.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Participant (Ordered by abbreviations of names)</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Confusion matrix for evaluation.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 summarizes</head><label>2</label><figDesc></figDesc><table>the submission statistics for 
9 participant teams including 4 from universities 
and research institutions in China (CAS, ECNU, 
SCAU, and WHU), 4 from Taiwan (KUAS, 
NCTU &amp; NTUT, NCYU, and NTOU), and one 
private firm (Lingage). Among 9 registered 
teams, 6 teams submitted their testing results. In 
formal testing phase, each participant can submit 
at most three runs that adopt different models or 
parameter settings. In total, we received 15 runs. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 . Submission statistics for all participants</head><label>2</label><figDesc></figDesc><table>Submission 

FPR 

Detection-Level 
Correction-Level 
Acc. 
Pre. 
Rec. 
F1 
Acc. 
Pre. 
Rec. 
F1 

CAS-Run1 
0.1164 
0.6891 
0.8095 
0.4945 
0.614 
0.68 
0.8037 
0.4764 
0.5982 
CAS-Run2 
0.1309 
0.7009 
0.8027 
0.5327 
0.6404 
0.6918 
0.7972 
0.5145 
0.6254 
CAS-Run3 
0.2036 
0.6655 
0.7241 
0.5345 
0.6151 
0.6491 
0.7113 
0.5018 
0.5885 
KUAS-Run1 
0.2327 
0.5009 
0.5019 
0.2345 
0.3197 
0.4836 
0.4622 
0.2 
0.2792 
KUAS-Run2 
0.2091 
0.5164 
0.5363 
0.2418 
0.3333 
0.4982 
0.4956 
0.2055 
0.2905 
KUAS-Run3 
0.1818 
0.5318 
0.5745 
0.2455 
0.3439 
0.5145 
0.537 
0.2109 
0.3029 
NCTU&amp;NTUT-Run1 
0.0509 
0.6055 
0.8372 
0.2618 
0.3989 
0.5782 
0.8028 
0.2073 
0.3295 
NCTU&amp;NTUT-Run2 
0.0655 
0.6091 
0.8125 
0.2836 
0.4205 
0.5809 
0.7764 
0.2273 
0.3516 
NCTU&amp;NTUT-Run3 
0.1327 
0.6018 
0.7171 
0.3364 
0.4579 
0.5645 
0.6636 
0.2618 
0.3755 
NCYU-Run1 
0.1182 
0.5245 
0.586 
0.1673 
0.2603 
0.5091 
0.5357 
0.1364 
0.2174 
NTOU-Run1 
0.0909 
0.5445 
0.6644 
0.18 
0.2833 
0.5327 
0.6324 
0.1564 
0.2507 
NTOU-Run2 
0.5727 
0.4227 
0.422 
0.4182 
0.4201 
0.39 
0.3811 
0.3527 
0.3664 
SCAU-Run1 
0.5327 
0.3409 
0.2871 
0.2145 
0.2456 
0.3218 
0.2487 
0.1764 
0.2064 
SCAU-Run2 
0.1218 
0.5464 
0.6378 
0.2145 
0.3211 
0.5227 
0.5786 
0.1673 
0.2595 
SCAU-Run3 
0.6218 
0.3282 
0.3091 
0.2782 
0.2928 
0.3018 
0.2661 
0.2255 
0.2441 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 . Testing results of our Chinese spelling check task.</head><label>3</label><figDesc></figDesc><table>Participant 

Approaches 
Linguistic Resources 

CAS 

• Candidate Generation 
• Candidate Re-ranking 
• Global Decision Making 

• SIGHAN-2013 CSC Datasets 
• CLP-2014 CSC Datasets 
• SIGHAN-2015 CSC Training Data 
• Taiwan Web Pages as Corpus 
• Chinese Words and Idioms Dictionary 
• Pinyin and Cangjie Code Table 
• Web-based Resources 

KUAS 
• Rules-based Method 
• Linear Regression Model 
• Chinese Orthographic Database 

NCTU &amp; NTUT 

• Misspelling Correction Rules 
• CRF-based Parser 
• Word Vector/CRF-based Spelling 
Error Detector 
• Trigram Language Model 

• CLP-2014 CSC Datasets 
• SIGHAN-2015 CSC Training Data 
• Sinica Corpus 

NTOU 
• N-gram Model 
• Rule-based Classifier 

• SIGHAN 2013 CSC Datasets 
• CLP-2014 CSC Datasets 
• Showen Jiezi and the Four-Corner Encoding 
• Sinica Corpus 
• Google N-gram Corpus 

SCAU 
• Bi-gram Language Model 
• Tri-gram Language Model 

• SIGHAN-2013 CSC Datasets 
• CLP-2014 CSC Datasets 
• CCL 
• SOGOU 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank all the participants for taking part in our task. We would like to thank Bo-Shun Liao for developing the evaluation tool.</p><p>This research is partially supported by the "Aim for the Top University Project" and "Center of Learning Technology for Chinese" of National Taiwan Normal University (NTNU), sponsored by the Ministry of Education, Taiwan, R.O.C. and is also sponsored in part by the "International Research-Intensive Center of Excellence Program" of NTNU and Ministry of Science and Technology, Taiwan, R.O.C. under the Grant no. <ref type="bibr">MOST 104-2911</ref><ref type="bibr">-I-003-301, MOST 102-2221</ref><ref type="bibr">-E-002-103-MY3, and MOST 103- 2221</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hua</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan-Wen</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihsuan</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Ying</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transaction on Asian Language Information Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reducing the false alarm rate of Chinese character error detection and correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping-Che</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing of the 1 st CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP-10)</title>
		<meeting>essing of the 1 st CIPS-SIGHAN Joint Conference on Chinese Language essing (CLP-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chinese spelling check evaluation at SIGHAN Bake-off 2013</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Hung</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Lin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7 th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13)</title>
		<meeting>the 7 th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview of SIGHAN 2014 Bake-off for Chinese spelling check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian-Chih</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lung-Hao</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuen-Hsien</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Processing of the 3 rd CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP-14)</title>
		<meeting>essing of the 3 rd CIPS-SIGHAN Joint Conference on Chinese Language essing (CLP-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="126" to="132" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
