<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-17, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zheng</surname></persName>
							<email>longzh@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianliang</forename><surname>Li</surname></persName>
							<email>xianliang@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Zheng</surname></persName>
							<email>yaohui@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Liao</surname></persName>
							<email>xfliao@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Jin</surname></persName>
							<email>hjin@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shao</surname></persName>
							<email>zyshao@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang-Sheng</forename><surname>Hua</surname></persName>
							<email>qshua@hust.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianliang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Xue</surname></persName>
							<email>j.xue@unsw.edu.au</email>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">UNSW Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang-Sheng</forename><surname>Hua</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Research Center for Big Data Technology and System/Service Computing Technology and System Lab/Cluster and Grid Computing Lab, Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Jingling Xue</orgName>
								<orgName type="institution" key="instit1">Huazhong University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">UNSW Sydney</orgName>
								<orgName type="institution" key="instit3">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling Scaph: Scalable GPU-Accelerated Graph Processing with Value-Driven Differential Scheduling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference</title>
						<meeting>the 2020 USENIX Annual Technical Conference						</meeting>
						<imprint>
							<date type="published">July 15-17, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-14-4 Open access to the Proceedings of the 2020 USENIX Annual Technical Conference is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We introduce Scaph, a GPU-accelerated graph system that achieves scale-up graph processing on large-scale graphs that are initially partitioned into subgraphs at the host to enable iterative graph computations on the subgraphs on the GPU. For active subgraphs to be processed on GPU at an iteration, the prior work always streams each in its entirety to GPU, even though only the neighboring information for its active ver-tices will ever be used. In contrast, Scaph boosts performance significantly by reducing the amount of such redundant data transferred, thereby improving the effective utilization of the host-GPU bandwidth drastically. The key novelty of Scaph is to classify adaptively at each iteration whether a subgraph is a high-value subgraph (if it is likely to be traversed extensively in the current and future iterations) or a low-value subgraph (otherwise). Scaph then schedules a sub-graph for graph processing on GPU using two graph processing engines, one for high-value subgraphs, which will be streamed to GPU entirely and iterated over repeatedly, one for low-value subgraphs, for which only the neighboring information needed for its active vertices is transferred. Evaluation on real-world and synthesized large-scale graphs shows that Scaph outperforms the state-of-the-art, Totem (4.12×), Graphie (8.93×), and Garaph (3.71×), on average.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph processing is used in a variety of real-world applications, including path navigation <ref type="bibr" target="#b23">[23]</ref>, social network analysis <ref type="bibr" target="#b9">[9]</ref>, and financial fraud detection <ref type="bibr" target="#b27">[27]</ref>. Graph processing, typically memory-bound, often benefits substantially from memory optimizations <ref type="bibr" target="#b50">[50]</ref>. Compared to CPU-based graph systems <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b68">68]</ref>, GPU-accelerated graph systems can have high internal bandwidth and massive parallelism, therefore offering superior speedup <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b66">66]</ref>, even for graph algorithms that involve substantial light-weight integer and comparison-based operations <ref type="bibr" target="#b28">[28]</ref>.</p><p>Unfortunately, many real-world graphs still cannot fit into GPU memory to enjoy high-performance in-memory graph  processing. For example, NVIDIA's high-end Tesla V100 has 32GB global memory <ref type="bibr" target="#b42">[42]</ref>, while real-world graphs such as Facebook's can easily reach the terabyte-scale <ref type="bibr" target="#b9">[9]</ref>. This gap has spurred the development of many distributed graph systems, which partition a graph into sub-graphs and then assign these sub-graphs to different machines for distributed computing <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b67">67]</ref>. However, these distributed graph systems suffer from prohibitive communication overheads <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b58">58]</ref> and also require an extensive range of domain knowledge to maintain <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b59">59]</ref>. There is nowadays a viable alternative of turning a single machine plugged in with a GPU to support scale-up largescale graph processing. Such a GPU-accelerated heterogeneous platform is easy to use and maintain <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b68">68]</ref>. In addition, we can take advantage of the large host memory (at the terabyte scale) to store large-scale graphs while still enjoying high-performance graph processing on GPU.</p><p>In this paper, we focus on building graph systems on GPU-accelerated heterogeneous platforms to achieve scaleup graph processing for large graphs that cannot fit into GPU memory. This would enable high-performance graph analytics on large-scale graphs everywhere by simply plugging a GPU into an off-the-shelf commodity PC. In this case, a large graph must be partitioned into subgraphs at the host. Any subgraphs to be processed on GPU must be streamed asynchronously to GPU when some previously transferred subgraphs are being concurrently processed on GPU (in an overlapping manner). We consider vertex-centric graph processing <ref type="bibr" target="#b36">[36]</ref>, where a graph algorithm is performed in a sequence of iterations until convergence <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b36">36]</ref>. In each iteration, a graph algorithm processes only the active vertices (vertices with ongoing updates) in each subgraph, updates their neighbors (along their outgoing edges) and activates the neighbors whose values have been updated. In this paper, we restrict ourselves to handle large-scale graphs that can entirely fit into the host memory. Meanwhile, all the vertex data, including vertex states (active or not), are assumed to be resident in the GPU memory. In contrast, the edge data of a graph are stored at the host and partitioned into subgraphs. During graph processing, active subgraphs (containing all out-going edges of an active vertex) must be transferred to GPU for iterative processing.</p><p>Achieving scale-up graph processing for large-scale graphs on GPU-accelerated heterogeneous platforms is challenging. The power-law graphs <ref type="bibr" target="#b15">[15]</ref> can result in substantial load imbalance among threads and warps <ref type="bibr" target="#b39">[39]</ref>. Irregular data accesses made in graph algorithms often lead to non-coalesced memory accesses for GPU graph processing. Fortunately, effective techniques for addressing these performance-limiting issues exist <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b34">34]</ref>. Currently, the performance bottleneck in a GPU-accelerated graph system has shifted to the limited host-GPU bandwidth, which was relatively sufficient in the past (e.g., ∼11.4GB/s for PCI-Express 3.0). However, existing graph processing engines <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b47">47]</ref> focus still on overcoming the GPU memory capacity limitation to enable large-scale graph processing, without paying adequate attention to the effective utilization of the host-GPU bandwidth.</p><p>Simple heuristics are used to reduce the number of data transfers. Totem <ref type="bibr" target="#b14">[14]</ref> partitions a graph into two subgraphs, one for the host and one for GPU, by keeping the amount of data transfers to a minimum at the expense of severe load imbalance. Garaph <ref type="bibr" target="#b34">[34]</ref> concurrently processes all active subgraphs on both the host and GPU. Graphie <ref type="bibr" target="#b17">[17]</ref> processes all subgraphs on GPU but re-processes only the recently processed subgraphs in the next iteration (before they are removed from GPU memory). However, these graph systems always transfer an active subgraph in its entirety to GPU (even though only the neighboring information for its active vertices will usually be used), resulting in poor utilization of the host-GPU bandwidth. To see this, <ref type="figure" target="#fig_0">Figure 1</ref> compares the performance results of Graphie <ref type="bibr" target="#b17">[17]</ref> for running three graph algorithms on a large graph on a PC with three generations of GPUs (one at a time). We see little performance gains when increasingly more powerful GPUs are used. For example, P100 has over 3× as many #SMX's and 4× as much memory as GTX980, but it offers small performance improvements.</p><p>Recently, hardware vendors have launched several advanced interconnect technologies to mitigate the impact of the "bandwidth wall". For example, compared to PCI-E 3.0, NVLINK 2.0 (50GB/s per link) and PCI-E 4.0 (32GB/s) are several times faster, but still cannot keep up with the growth in GPU computing capabilities. Specifically, these advanced technologies cannot yet provide ∼500GB/s required by graph analytics under existing computing platforms <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this work, we argue that we can improve the performance of large-scale graph processing on a GPU-accelerated architecture significantly by improving the effective utilization of the host-GPU bandwidth. Our key observation is that the majority of the data in an active subgraph (once streamed to GPU) are never used in current and future iterations ( §2.2). We introduce Scaph that achieves significantly improved performance than state of the art by adopting value-driven differential scheduling for active subgraphs. The key novelty is to classify an active subgraph adaptively into a high-value subgraph (if it will be extensively traversed in current and future iterations) and a low-value subgraph (otherwise). Thus, a high-value subgraph contains a significant amount of useful data (UD) to be used by active vertices in the current iteration and of potentially useful data (PUD) to be used by its future active vertices in future iterations. On the other hand, a low-value subgraph contains a lot of never-used data (NUD) in current and future iterations.</p><p>Unlike earlier graph systems <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b47">47]</ref>, which transfer an active subgraph to GPU in its entirety (but with only its UD used usually), Scaph uses the host to stream an active sub-graph to GPU by using two graph processing engines for handling high-value and low-value subgraphs, respectively. For the high-value subgraph, it will be transferred to GPU entirely. Inspired by the data movement reduction in out-ofcore settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b69">69]</ref>, we propose to compute each highvalue subgraph multiple times to exploit its PUD ahead of schedule for accelerating convergence. Unlike these earlier efforts focusing on exploiting only the PUD in a subgraph, we present a GPU-context-friendly delayed scheduling to enable exploiting the PUD across subgraphs on GPU such that the value of the high-value subgraphs can be maximized. For the low-value subgraph, only the neighboring information for its active vertices is transferred and scheduled once.</p><p>In summary, this paper makes the following contributions:</p><p>• Subgraph Value Characterization. We quantify the value of a subgraph adaptively (dynamically) in terms of its UD and PUD used in current and future iterations.</p><p>• Value-Driven Differential Scheduling. We propose a scheduler that adaptively distinguishes high-and low-value subgraphs in each iteration and dispatches a subgraph to an appropriate graph processing engine for acceleration.</p><p>• Value-Driven Graph Processing Engines. We introduce two graph processing engines to squeeze the most value out of high-and low-value subgraphs to maximize the effective utilization of the host-GPU bandwidth in each case.</p><p>• Evaluation. We evaluate Scaph on both real-world and synthesized large graphs. Scaph outperforms state-of-the-art heterogeneous graph systems, Totem (4.12×) <ref type="bibr" target="#b14">[14]</ref>, Graphie (8.93×) <ref type="bibr" target="#b17">[17]</ref>, and Garaph (3.71×) <ref type="bibr" target="#b34">[34]</ref>, on average.  Scaph. §4 describes value-driven differential scheduling while §5 discusses how to accomplish this effectively. §6 presents results. §7 discusses the related work. Finally, §8 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>We first review the background. We then present some case studies to reveal why the poor host-GPU bandwidth utilization has limited the performance achieved by existing heterogeneous graph systems, finally motivating Scaph. <ref type="figure" target="#fig_1">Figure 2</ref> shows a representative GPU-accelerated heterogeneous architecture that integrates the hardware advantages of the host (with a larger host memory) and the GPU (with a stronger computing ability). A GPU consists of multiple streaming multiprocessors (SMXs), each of which includes hundreds of cores. Compared to the high-speed internal bandwidth (e.g., ∼720GB/s for NVIDIA Tesla P100 <ref type="bibr" target="#b41">[41]</ref>) of GPU cores accessing global memory, a GPU is generally connected to the host with a relatively slow interface. For example, the host-GPU bandwidth via PCI Express 3.0 can be limited to be as low as ∼11.4GB/s in practice <ref type="bibr" target="#b4">[5]</ref>. This significant performance gap often severely limits the performance potential achieved on a GPU-accelerated heterogeneous architecture if the host-GPU data transfers are frequent <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26]</ref>. This work makes use of a PCI Express interconnect since it is commonly used in the current commodity market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Host-GPU Heterogeneous Architectures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">A Motivating Study</head><p>Existing heterogeneous graph systems <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b47">47]</ref>, with Graphie <ref type="bibr" target="#b17">[17]</ref> as a representative compared against in our evaluation, generally use host memory to store large-scale graphs (partitioned into subgraphs) and rely on GPUs exclusively to accelerate graph analytics on these subgraphs. <ref type="figure">Figure 3</ref> depicts their generic graph processing engine used, with the function calls in blue executed on GPU. Due to the limited GPU memory, a graph G is first divided into subgraphs, ˜ G 1 , · · · , ˜ G n (line 2). During the entire iterative graph processing, the vertex data of G always reside in GPU memory, but the edge data of G, which are spread across these subgraphs, will be streamed to GPU on-demand <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b47">47]</ref>.</p><p>At each iteration (lines 5 -12), ˜ G active represents the set of active subgraphs, i.e., the ones containing some out-going edges of an active vertex. In each iteration, all active vertices Activate(e.destination_vertex) <ref type="figure">Figure 3</ref>: Existing graph processing engine on a GPUaccelerated heterogeneous architecture (with the function calls in blue executed on GPU and all the rest on the host)  Afterward, these active vertices will be processed on the GPU (lines 13 -18) to activate more destination vertices possibly. Note that Graphie <ref type="bibr" target="#b17">[17]</ref> may schedule first the subgraphs processed at the end of the previous iteration as they are still in GPU memory (line 8). This simple graph processing engine does not effectively utilize the limited, scarce host-GPU bandwidth since many vertices in an active subgraph are not active. Simply transferring an entire subgraph to GPU (line 10) but consuming only a fraction of its data (lines 14 -15) will waste a considerable amount of the host-GPU bandwidth. As a result, all the required data cannot arrive at the GPU promptly, limiting the performance that can be potentially achieved on GPU.</p><formula xml:id="formula_0">1 Procedure SimpleSubgraphEngine(Graph G) 2 Load˜GLoad˜ Load˜G's subgraphs in { ˜ G 1 , · · · , ˜ G n } into</formula><p>Let us examine the ratios of the unused over used data in the subgraphs transferred to GPU for three graph algorithms operating on two graphs, twitter (TW) <ref type="bibr" target="#b29">[29]</ref> and uk-2007 (UK) <ref type="bibr" target="#b5">[6]</ref>, by <ref type="bibr">Graphie [17]</ref> using the graph processing engine given in <ref type="figure">Figure 3</ref>. <ref type="table" target="#tab_3">Table 1</ref> gives the results obtained through an offline trace analysis, showing that these ratios range from 6.29 to 36.17. This indicates that the host-GPU bandwidth under Graphie is utilized rather ineffectively. Consequently, as shown further in <ref type="figure" target="#fig_2">Figure 4</ref>, the performance of Graphie for  <ref type="figure" target="#fig_5">Figure 5</ref>: UD, PUD, and NUD in a subgraph, which may change across the iterations, illustrated for SSSP. The weight of an edge denotes its distance. The shortest distance found so far by SSSP at a vertex is depicted next to it in orange.  <ref type="bibr" target="#b29">[29]</ref> each graph algorithm (operating on TW) has plateaued as soon as #SMXs = 4. However, mainstream GPU accelerators usually have far more than 4 SMXs. For example, NVIDIA's Tesla K80 has 26 SMXs, while P100 has been integrated with 56 SMXs. Thus, a significant gap remains between the poor provision of data and high-speed computation of GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Value-Driven Subgraph Scheduling</head><p>For a subgraph, its active vertices vary across the iterations. However, from the perspective of an active vertex, it always contains three types of edge data, as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>:</p><p>• Useful Data (UD). These are the edge data associated with all the active vertices in a subgraph, i.e.,</p><formula xml:id="formula_1">V 1 2 − → V 3 , V 1 3 − → V 4 ,</formula><p>and <ref type="figure" target="#fig_5">Figure 5</ref>. UD will definitely be used in the current iteration (lines 15 -16 in <ref type="figure">Figure 3</ref>) and must be transferred to GPU <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b47">47]</ref>.</p><formula xml:id="formula_2">V 2 1 − → V 4 in</formula><p>• Potentially Useful Data (PUD). These are the edge data associated with all the future active vertices in future iterations in a subgraph. In <ref type="figure" target="#fig_5">Figure 5</ref>, PUD will be just V 4 4 − → V 5 , since V 4 will be the only one activated by both V 1 and V 2 in current and future iterations. Unlike UD, PUD is not actually used in the current iteration, but may be transferred repeatedly to GPU if not handled carefully (as in the case of <ref type="figure">Figure 3</ref> where PUD is usually discarded).</p><p>• Never Used Data (NUD). These are the edge data that will never be used again in a subgraph, associated with its vertices that have converged and will thus never be active. In Note that the same vertex may be activated many times in different iterations. Given a subgraph, its UD, PUD, and NUD computed at different iterations can vary dynamically. <ref type="figure" target="#fig_4">Figure 6</ref> shows the amount of UD, PUD, and NUD for the  <ref type="bibr" target="#b29">[29]</ref>, partitioned sequentially into subgraphs of 32MB each. Graphie <ref type="bibr" target="#b17">[17]</ref>, a representative of existing heterogeneous graph systems <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b47">47]</ref>, wastes the host-GPU bandwidth in two ways ( <ref type="figure">Figure 3</ref>). First, PUD, usually discarded by Graphie but needed in future iterations, is substantial in earlier iterations. Second, NUD, which is becoming increasingly more dominant as the iteration progresses, is streamed to GPU redundantly. For a subgraph, it will be cost-ineffective to stream just its UD, since its PUD cannot be exploited simultaneously. Instead, our key insight for improving the effective utilization of the host-GPU bandwidth is to look beyond the current iteration, by considering not only its UD in the current iteration but also its PUD in future iterations. Based on a cost-benefit analysis, we aim to leverage rather than discard its PUD (once streamed to GPU) in iterative graph processing. Thus, the value of a subgraph at an iteration should be measured in terms of not only its UD but also its PUD. Now, how do we extract the UD and PUD from a subgraph at a given iteration so that both can be transferred to GPU? Extracting the UD from a subgraph is easy as its active vertices in the current iteration are known (lines 4 and 12 in <ref type="figure">Figure 3</ref>). However, extracting precisely the PUD (without NUD) from a subgraph is difficult, as its future active vertices are not known yet during the current iteration.</p><p>For a given subgraph, we propose to predict its PUD size at an iteration from the UD sizes in the current and past iterations. This enables to adopt a value-driven differential scheduler that computes the value of a subgraph adaptively and schedules it depending on if it has a high value (when its UD and PUD are dominant) or a low value (otherwise). <ref type="figure" target="#fig_6">Figure 7</ref> shows the workflow of Scaph, in which all the subgraphs of a graph are computed on the GPU while the host is responsible for their preparation. At each iteration, its dispatcher classifies a subgraph into either a high-value or lowvalue subgraph and sends it to its corresponding engine to facilitate value-driven differential scheduling. Both engines schedule their subgraphs for acceleration on GPU independently but concurrently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Scaph Overview</head><p>Value-Driven Subgraph Dispatcher. Conceptually, the value of a subgraph at a given iteration is proportional to the amount of its UD and PUD. The key insight here is that, for a given subgraph, although accurately computing its PUD is difficult, its PUD size can be approximated based on the UD sizes in the current and past iterations. For a subgraph at a given iteration, Scaph's subgraph dispatcher ( §4), classifies it adaptively as a high-value subgraph if it contains a sufficient amount of UD and PUD to justify its transfer in its entirety to GPU and a low-value subgraph to request only its UD to be transferred to GPU otherwise. This is done adaptively as the value of a subgraph changes as the iteration progresses.</p><p>Value-Driven Subgraph Scheduler. Scaph has two separate graph processing engines, described in §5, to process differentially high-and low-value subgraphs. For a high-value subgraph, we use a queue-assisted multi-round processing engine, which streams it entirely from the host to GPU (if it is not in GPU memory) and exploits both its UD and PUD adequately to enable faster convergence. For a low-value subgraph, Scaph relies on the graph processing engine given in <ref type="figure">Figure 3</ref> but transfers only its UD to GPU, with the UD extracted in a NUMA-aware manner on the host.</p><p>Scaph is essentially a hybrid graph system that allows outof-order computation of high-value subgraphs in each synchronous iteration. The use of asynchronous execution allows fast convergence but also changes the vertex scheduling priority of subgraphs. Therefore, a graph algorithm can use Scaph safely for preserving the convergence and the converged values, if it satisfies the correctness condition that the final vertex results are insensitive to the value propagation order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Value-Driven Subgraph Dispatching</head><p>In Section 4.1, we quantify the value of a subgraph. In Section 4.2, we discuss how to estimate the value of a subgraph to support value-driven differential scheduling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Quantifying the Value of a Subgraph</head><p>Graph computations proceed iteratively until convergence. Conceptually, the value of a subgraph˜Gsubgraph˜ subgraph˜G can be measured in terms of its UD used in the current iteration and its PUD used in future iterations. Therefore, the value of˜Gof˜ of˜G, denoted Val( ˜ G), from the current iteration Cur to the MAX-th iteration (beyond which˜Gwhich˜ which˜G is no longer active), is defined as:</p><formula xml:id="formula_3">Val( ˜ G) = MAX ∑ i=Cur ∑ v∈˜Gv∈˜ v∈˜G.SetOfVertices D(v) * A i (v)<label>(1)</label></formula><p>where D(v) represents the number of out-going edges of vertex v restricted tõ G and A i (v) ∈ {0, 1} indicates that v is active (inactive) in the i-th iteration when A i (v) = 1 (A i (v) = 0). Val( ˜ G) represents the amount of computations arising from˜G from˜ from˜G from the current iteration until convergence. According to Equation (1), the PUD of a subgraph is quantized by the number of its edges that will be used in future iterations.</p><p>The value of a subgraph depends upon its active vertices and their degrees. In the case of uniform degree distributions, the activation status of vertices can still differentiate the amount of UD, PUD, and NUD for a subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Value-Driven Differential Scheduling</head><p>Scaph emphasizes value-driven data transfers, which should directly reflect how the bandwidth is effectively utilized in order to enable faster convergence.</p><p>The intuition behind Val( ˜ G) is clear. If Val( ˜ G) is high, ˜ G should be a high-value subgraph. Then we should transfer˜G transfer˜ transfer˜G as a whole to GPU and also exploit its UD and PUD adequately by iterating over˜Gover˜ over˜G multiple times before it is removed from GPU memory. Otherwise, ˜ G should be treated as a lowvalue subgraph. In this case, we will opt to transfer only its UD to GPU and just iterate over the resulting˜Gresulting˜ resulting˜G once.</p><p>If˜GIf˜ If˜G is a high-value subgraph, then the throughput of processing˜Gcessing˜ cessing˜G on GPU can be measured as follows:</p><formula xml:id="formula_4">T HV ( ˜ G) = |UD| + λ|PUD| | ˜ G|/BW + t barrier<label>(2)</label></formula><p>The denominator | ˜ G|/BW + t barrier , which represents the data transfer time for˜Gfor˜ for˜G, is used to approximate the time elapsed on processing˜Gprocessing˜ processing˜G by assuming a complete overlap between data transfers and computations on GPU. As˜GAs˜ As˜G is transferred in its entirety to GPU, | ˜ G| denotes the amount of data thus transferred, BW represents the host-GPU bandwidth, and T barrier is the synchronization overhead for˜Gfor˜ for˜G (amortized by the number of active subgraphs processed). The numerator |UD| + λ * |PUD| represents the amount of UD and PUD accessed wheñ G is iterated over on GPU. We use a balancing factor λ to decay |PUD|, where 0 λ 1, to signify the actual amount of PUD accessed.</p><p>If˜GIf˜ If˜G is a low-value subgraph, then we have:</p><formula xml:id="formula_5">T LV ( ˜ G) = |UD| |UD|/BW + t barrier<label>(3)</label></formula><p>This time, only the UD of˜Gof˜ of˜G is streamed to GPU. Now, ˜ G is a high-value subgraph if T HV ( ˜ G) T LV ( ˜ G) and a low-value subgraph otherwise. Thus, we need to analyze:</p><formula xml:id="formula_6">|UD|+λ|PUD|(1 + t barrier |UD|/BW ) &gt; | ˜ G|<label>(4)</label></formula><p>To verify T HV ( ˜ G) T LV ( ˜ G), the key lies in determining |PUD|, which is difficult to obtain directly. In fact, for a subgraph, its PUD is technically activated from its UD, motivating us to estimate the PUD of a subgraph heuristically based on the UD of the same subgraph. In this work, we consider a subgraph to have a high value if either of the following two conditions (which we found to work well across all of our applications, as confirmed in §6) holds to simplify Equation (4):</p><formula xml:id="formula_7">1 Procedure VDDSEngine(Graph G) 2 Distribute G's subgraphs { ˜ G 1 ,· · · , ˜ G n } to NUMA nodes 3 VertexInitialization(G) 4 ˜ G active ← FindActiveSubgraph(G) 5</formula><p>Transfer VertexStates from GPU to CPU When α is relatively large, which implies that the UD in a subgraph tends to be dominant, we can determine if it is a high-value subgraph by considering only its UD. β is needed to identify the high-value subgraphs where the amount of UD is relatively low and that of PUD is potentially high. Thus, β is often smaller than α. As shown in <ref type="table" target="#tab_3">Table 1</ref>, considering both together is often more effective than considering either alone. In this work, α and β are set empirically as 50% and 30% to represent a nice point for yielding good results. <ref type="figure">Figure 8</ref> gives our value-driven differential scheduler, VDDSEngine(), for scheduling a graph G. Initially, G is partitioned into subgraphs, ˜ G 1 , · · · , ˜ G n , at the host and distributed across its NUMA nodes (to facilitate their scheduling). Scaph uses two graph processing engines, as described in §5 below, HVSPEngine() for scheduling high-value subgraphs, and LVSPEngine() for scheduling low-value subgraphs. In line 8, Scaph uses the above heuristic predictor to estimate the value of an active subgraph. Note that both engines work independently but concurrently. LVSPEngine() needs VertexStates in order to perform UD extraction for the active vertices in each subgraph. The UD extraction can be overlapped effectively with the data transfers in HVSPEngine(). At the end of each iteration (line 15), Scaph will transfer back the updated vertices from the GPU to the CPU. Edges, which are not modified, are thus not transferred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Value-Driven Subgraph Processing</head><p>Scaph has two graph processing engines. We describe the one for handling high-value subgraphs in §5.1 and the one for handling low-value subgraphs in §5.2. The PUD iñiñ G 1 can be exploited only if˜Gif˜ if˜G 2 and/or˜Gor˜ or˜G 3 are processed first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">High-Value Subgraph Processing</head><p>The key to extracting the most value out of high-value subgraphs lies in how to fully exploit their PUD. A useful idea of running each loaded subgraph multiple times is leveraged in the out-of-core settings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b69">69]</ref> to exploit the intrinsic value in a subgraph for reducing the number of I/Os between memory and disk. However, under a GPU-accelerated heterogeneous architecture, subgraphs must often be small enough (in several tens of millions of bytes <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b26">26]</ref>) against the ones in out-of-core settings, to enable fine-grained GPU scheduling. In this case, simply iterating over such a small-sized subgraph multiple times is often ineffective, since it can exploit only the PUD of its active vertices activated by its other active vertices but not active vertices from other subgraphs. In Scaph, we improve the PUD exploitation significantly by enabling exploiting the external value across the subgraphs.</p><p>Our key observation is that: given a subgraph already available in GPU memory, scheduling it again after a period of delay can expose its PUD more fully than processing it repeatedly. <ref type="figure" target="#fig_8">Figure 9</ref> illustrates this with three subgraphs, exhibiting some complex inter-subgraph data dependencies (as is often the case in practice). We see that  iñ G 1 can be activated by iñ G 2 and  iñ G 3 . Once  iñ G 1 is activated,  iñ G 1 may get activated (as shown). In this case, the edge data for →, →, and → are part of the PUD of˜Gof˜ of˜G 1 . By processing˜Ging˜ ing˜G 1 after˜Gafter˜ after˜G 2 or˜Gor˜ or˜G 3 or both (even better), we can exploit such PUD to enable faster convergence. That is, repeatedly processing˜Gprocessing˜ processing˜G 1 would not help.</p><p>Queue-Assisted Multi-Round Processing. The scheduling of high-value subgraphs at a given iteration is shown in <ref type="figure" target="#fig_0">Figure 10</ref>. We use a k-level priority queue (PQ 1 , . . . , PQ k ) to enable re-scheduling a GPU-resident subgraph after some delay, where k indicates the maximum number of times some subgraphs have been processed in the current iteration. Thus, k varies from iteration to iteration. <ref type="figure" target="#fig_0">Figure 11</ref> shows a case.</p><p>In each differential scheduling iteration orchestrated by VDDSEngine <ref type="figure">(Figure 8</ref>), HVSPEngine(worklist) is invoked, where worklist contains all the high-value subgraphs in this iteration. During the pre-processing (lines 2-6), each subgraph˜G subgraph˜ subgraph˜G i in worklist is examined in turn. ˜ G i will be enqueued into PQ 1 (if not already there) if˜Gif˜ if˜G i remains to be GPU-resident (i.e., in one of {PQ 1 , . . . , PQ k }) from the previous iteration and inserted into TransSet (waiting to be streamed to GPU) otherwise. Thus, there are two concurrently executed modules,  Subgraph Transferring and Subgraph Scheduling. The Subgraph Transferring module (lines 7 -16) is responsible for streaming asynchronously the subgraphs in TransSet to GPU. This is done by using some free GPU memory whenever possible (line 11) or making some free by dequeuing a subgraph from the multi-level queue (lines <ref type="bibr">13 -14)</ref>. Due to lines 4 and 16, all subgraphs in worklist are initially enqueued into PQ 1 , and thus assigned with the highest priority.</p><p>The Subgraph Scheduling module (lines 17 -31) is responsible for scheduling the subgraphs in PQ 1 , · · · , PQ k . The subgraphs in PQ 1 are processed first (for the first time in the current iteration) to exploit their UD (lines 21 -23). If PQ 1 = / 0 (implying that some subgraphs are still being transferred to GPU asynchronously), the scheduler will dequeue a subgraph from a non-empty PQ i , where i is the smallest, to exploit its PUD (lines 25 -29), as this will be the i-th time that the subgraph is processed (in the i-th round) of the current iteration. Simultaneously, the data transfers for PQ 1 and the computations for PQ 2 , · · · , PQ k are maximally overlapped, too. In either case, the priority of a subgraph, once processed, drops by one (lines <ref type="bibr">30 -31)</ref>. This delayed re-scheduling at-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TransSet</head><p>Scheduling Priority ...</p><formula xml:id="formula_8">PQk PQ2 PQ1 G9 G7 G4 G3 G8 G6 G5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G2 G1</head><p>Figure 11: Subgraph processing with a k-level priority queue. PQ i represents a queue PQ with the i-th priority. The smaller i is, the higher the priority is. All the subgraphs streamed from the host to GPU enter into PQ 1 initially.</p><p>tempts to maximize the PUD exploitation, by, e.g., increasing the chances for˜Gfor˜ for˜G 1 to be processed after˜Gafter˜ after˜G 2 and/or˜Gor˜ or˜G 3 in <ref type="figure" target="#fig_8">Fig- ure 9</ref> (as motivated earlier). Consider˜GConsider˜ Consider˜G 3 , which resides in PQ 1 , in <ref type="figure" target="#fig_0">Figure 11</ref>. Once we have exploited its UD, we will move it to PQ 2 so that we can exploit its PUD after˜Gafter˜ after˜G 7 , ˜ G 4 , ˜ G 8 , and˜Gand˜ and˜G 6 have been processed.</p><p>Our scheduler with a multi-level priority queue guarantees that subgraphs are scheduled fairly, preventing them from bearing too many useless computations in the sense that the data of a vertex is computed but not updated.</p><p>Time and Space Complexity Analysis. k is expected to be bounded by BW BW where BW is the internal bandwidth of GPU and BW is the host-GPU bandwidth. In our computing platform, BW = 224GB/s and BW = 11.4GB/s. Thus, k 20 is typically expected.</p><p>As for the space complexity, a k-level priority queue is used to keep track of only the indices of the active subgraphs processed in an iteration. Thus, the worst complexity is O( Mem GPU × sizeof(SubgraphIndex)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>| ˜ G|</head><p>), where Mem GPU is the global memory size and | ˜ G| is the size of a subgraph˜Gsubgraph˜ subgraph˜G. In our computing platform, we have used 4GB×4B 32MB = 0.5KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Low-Value Subgraph Processing</head><p>The key to exploiting the most value of low-value subgraphs is to extract their UD efficiently. We use multiple CPU cores at the host to parallelize the UD extraction. Due to non-uniform memory access (NUMA), however, scanning naively all the vertices in a subgraph to extract its UD can still be costly. In addition, different subgraphs exhibit different amounts of UD. Such scanning tasks are also prone to load imbalance. <ref type="figure" target="#fig_0">Figure 12</ref> gives our scheduler for low-value subgraphs. In each value-driven scheduling iteration orchestrated by VDDSEngine() in <ref type="figure">Figure 8</ref>, LVSPEngine(worklist, VertexStates) is invoked, where worklist contains all the low-value subgraphs that are active in this iteration, with their active vertices indicated in VertexStates. There are three modules, UD Extraction, Subgraph Transferring, and Subgraph Scheduling, which all execute concurrently. The major contribution here is a NUMA-aware parallel UD extraction.</p><p>UD Extraction. Initially, all the subgraphs partitioned from a graph are evenly distributed to different NUMA nodes,  with a NUMA node consisting of a CPU socket and its own memory banks (line 2 in <ref type="figure">Figure 8</ref>). The UD extraction module is given in terms of lines 2 -4 and lines 17 -30. To boost performance and improve intra-node load balancing, the UD extraction for each subgraph is done in its own thread, which is bound to the NUMA node storing the subgraph (line 3).</p><p>To improve inter-node load balancing (as a minor optimization), we also duplicate in a NUMA node an equal number of randomly selected subgraphs from the other nodes (if there is still some memory space available). We adopt a simple bitmap-based approach to extract the UD from a subgraph˜Gsubgraph˜ subgraph˜G efficiently (lines 17-30). All its vertices are stored in a bitmap, VertexStates( ˜ G).bitmap, with 1 (0) indicating that the corresponding vertex iñ G is active (inactive). To accelerate its construction, the total of active vertices is computed on GPU.</p><p>Unlike high-value subgraphs, which can each be stored in the same-sized chunk in GPU memory ( §5.1), low-value subgraphs may give rise to UD-induced subgraphs of varying sizes. To reduce fragmentation, Scaph further divides each chunk for storing a subgraph into smaller tiles (totaling 32 in our implementation). To store a UD-induced subgraph in GPU memory, Scaph will try to find consecutive tiles first in a partially filled chunk and then in a vacant chunk.</p><p>Subgraph Transferring. As in the case of high-value subgraph streaming in <ref type="figure" target="#fig_0">Figure 10</ref>, this module proceeds similarly except that a multi-level queue is no longer used.</p><p>Subgraph Scheduling. As in the case of scheduling highvalue subgraphs to GPU in <ref type="figure" target="#fig_0">Figure 10</ref>, this module schedules UD-induced subgraphs (without using a multi-level queue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluate the efficiency and scalability of Scaph by answering the following four research questions (RQs):</p><p>• RQ1: How much more efficient is Scaph over state-of-theart heterogeneous graph systems? • RQ2: How effective is Scaph's value-driven differential scheduling in helping it achieve the overall performance? • RQ3: How well does Scaph scale?</p><p>• RQ4: How much runtime overhead does Scaph introduce?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Setup</head><p>We compare Scaph with the following three state-of-the-art CPU-GPU heterogeneous graph systems:</p><p>• Totem <ref type="bibr" target="#b14">[14]</ref>. A graph is divided into two subgraphs, which are processed by CPU and GPU, respectively. At the end of each iteration, the states of the active vertices that are activated reciprocally by the two subgraphs are exchanged.</p><p>• Graphie <ref type="bibr" target="#b17">[17]</ref>. Like Scaph, a graph is initially partitioned at the host CPU and the subgraphs are then streamed to GPU for graph processing. Unlike Scaph, however, all active subgraphs are transferred to GPU in their entirety.</p><p>• Garaph <ref type="bibr" target="#b34">[34]</ref>. At an iteration, all the subgraphs that are partitioned from a graph are processed concurrently by both the host and GPU if the number of outgoing edges of all active vertices in the entire graph exceeds 50% of the total number of edges and on the host only otherwise.</p><p>Subgraph Size. For Totem, Graphie, and Garaph, the sizes of subgraphs are selected from their papers. In Scaph, a graph is partitioned into subgraphs of 32MB each for several reasons. First, the host-GPU bandwidth tends to be under utilized with smaller sizes. Second, subgraphs will be streamed to GPU more frequently with larger sizes, as they tend to contain active vertices for more iterations. Finally, the kernel launching overheads appear to be well hidden with 32MB.</p><p>Graph Applications. We consider the first three typical graph algorithms (from different categories) and the latter two actual graph workloads (with different complexities): (1) Single-Source Shortest Path (SSSP) <ref type="bibr" target="#b60">[60]</ref>-Sequential traversal, (2) Connected Components (CC) <ref type="bibr" target="#b20">[20]</ref>-Parallel traversal, (3) Minimum Spanning Tree (MST) <ref type="bibr" target="#b37">[37]</ref>-Graph mutation, (4) Neural Network Digit Recognition (NNDR) <ref type="bibr" target="#b3">[4]</ref>, and (5) Graph-based Circuit Simulation (GCS) <ref type="bibr" target="#b25">[25]</ref>. All these algorithms fit the correctness criteria discussed in §3, though NNDR and GCS are already typically executed in an asyn-  chronous way, while the other algorithms are typically run in a synchronous, iterative manner. Graph Datasets. We use (1) 6 real-world graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b31">31]</ref>) for performance evaluation, and (2) 5 large synthesized graphs (generated by the RMAT tool <ref type="bibr" target="#b6">[7]</ref>) for scalability evaluation. <ref type="table" target="#tab_6">Table 2</ref> gives all the graphs used. For SSSP and MST that work on the weighted graphs, we randomly assign each edge of an unweighted graph with a weight ranging from 1 to 100.</p><p>Computing Platform. We evaluate Scaph on a machine where the host is equipped with two Intel 14-core Xeon CPUs, E5-2680v4@2.40GHz with 512GB memory (256GB on each of the two NUMA nodes). The GPU is NVIDIA P100 (with 56 SMXs, 3584 cores, and 16GB memory), connected to the host via the PCI Express 3.0 at 16x. The host-GPU bandwidth is around 11.4GB/s. We use NVCC V8.0.61 and g++ V5.4.0 to compile all the applications under "-O3". The operating system is Ubuntu 14.04 with Linux kernel 4.13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">RQ1: Efficiency</head><p>To answer RQ1, we compare Scaph against Totem <ref type="bibr" target="#b14">[14]</ref>, Graphie <ref type="bibr" target="#b17">[17]</ref>, and Garaph <ref type="bibr" target="#b34">[34]</ref>. <ref type="table" target="#tab_7">Table 3</ref> depicts the results.</p><p>Scaph vs. Totem. The speedup of Scaph over Totem ranges from 2.23× (for SSSP on SK) to 7.64× (for CC on TW) with an average of 4.12×. Totem's critical performance bottleneck lies in its severe load imbalance, as it partitions each graph into only two subgraphs, one for the host (with 512GB memory) and one for GPU (with only 16GB memory). As a result, Totem cannot tap GPU's processing power to exploit adequately the UD and PUD in a graph. Its bottleneck is to ask the CPU to process most of the graph data, which would have been processed more efficiently by the GPU otherwise. A typical measurement for FB is for the CPU to handle 358.1GB and the GPU to handle only 16GB. In contrast, Scaph streams all subgraphs dynamically to GPU with value-driven differential scheduling, thereby exploiting more adequately GPU's processing power, and consequently, the UD and PUD in all the subgraphs. In the case of CC operating on FR, SK, and UK, their GPU portions under Totem are 39.6%, 36.7%, and 19.1%, respectively. As a result, Scaph outperforms Totem by 5.51x (FR), 2.52x (SK), and 3.77x (UK).</p><p>Scaph vs. Graphie. Scaph is faster than Graphie by 8.93× on average, with its speedup ranging from 3.03× (for NNDR on TW) to 16.41× (for SSSP on SK). Both Graphie and Scaph process all subgraphs on GPU only. So Graphie can be understood as a version of Scaph, where every subgraph is treated as a high-value subgraph except that only its UD is used but its PUD is exploited rather inadequately. Graphie is inferior to Scaph for several reasons. First, Graphie transfers an active subgraph entirely to GPU even though it contains only a few active vertices (i.e., a lot of NUD), wasting the host-GPU bandwidth. Second, Graphie exploits the UD only but PUD inadequately in an active subgraph.</p><p>Let us examine SSSP on SK, where the speedup of Scaph over Graphie is the highest (at 16.41×). Graphie converges in 75 iterations, by transferring 18,019 subgraphs totaling 374.4GB data to GPU. In contrast, Scaph converges in 16 iterations, by transferring 9,897 subgraphs totaling only 19.6GB data, comprising 13.2GB for 798 high-value subgraphs and 6.4GB for 9,099 low-value subgraphs. For Scaph, its significantly improved utilization for the host-GPU bandwidth has resulted in its significantly improved overall performance.</p><p>Scaph vs. Garaph. Scaph is faster than Garaph by 3.71×, with an overall rang from 1.93× (for NNDR on TW) to 5.62× (for CC on FB). Unlike Scaph, Garaph processes all the subgraphs on both the host and GPU if the active vertices in the entire graph have a lot of outgoing edges and on the host only otherwise ( §6.1). Despite this, Garaph cannot distinguish highvalue from low-value subgraphs as Scaph does. While being more effective than Graphie in reducing the amount of NUD transferred, Garaph is inferior to Scaph as it still transfers more NUD to GPU and exploits PUD less adequately.</p><p>Let us examine CC on FB, where the speedup of Scaph over Garaph is the highest (at 5.62×). Garaph processes all the subgraphs on the host only (as the outgoing edges of FB's active vertices over the total is under 6.9% at any iteration), by using a so-called notify-pull model. In contrast, Scaph uses a fine-grained value-driven differential scheduler to identify high-value and low-value subgraphs even though it has active vertices only in its local regions at any iteration, so that the GPU's processing power is adequately exploited.</p><formula xml:id="formula_9">T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2 8 R 2 9 R 3 0 A V G T W S K F R U K R 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">RQ2: Effectiveness</head><p>To answer RQ2, we consider four variations of Scaph: (1) Scaph-HVSP, where all the low-value subgraphs can be understood as being misidentified as high-value subgraphs, (2) Scaph-LVSP, where all the high-value subgraphs can be understood as being misidentified as low-value subgraphs, (3) Scaph-HBASE, which applies the differential processing, but every subgraph transferred to the GPU has kept computation with a specific number of times (without using queue-based delayed scheduling), as used in CLIP <ref type="bibr" target="#b1">[2]</ref>, and (4) Scaph-LBASE, a variation of Scaph-LVSP except that every subgraph is streamed to GPU entirely (without UD extraction), as used in Graphie <ref type="bibr" target="#b17">[17]</ref>. <ref type="figure" target="#fig_0">Figure 13</ref> gives the results. We see that neither of Scaph-HVSP and Scaph-LVSP is always better than the other, and also Scaph is the best performer for all the algorithms on all the graphs. Thus, Scaph's value-driven differential scheduling with heuristic subgraph identification is highly effective.</p><p>Scaph-HVSP. Scaph-HVSP achieves better speedups for the graphs where algorithms take longer iterations to converge, as this allows it to exploit PUD more adequately and thus stream less redundant data to GPU. For example, each algorithm on SK has the longest number of iterations against on other graphs, thereby delivering considerable speedups. We also see that Scaph-HBASE is significantly inferior to Scaph-HVSP. This is because small subgraphs often contain very little PUD from themselves worthy of being exploited. Our queue-based scheduling allows the availability of PUD from other subgraphs via delayed scheduling. Thus, multitime processing under Scaph-HVSP can expose significantly more PUD than that under Scaph-HBASE (i.e., by simply applying the idea from CLIP) for boosting performance.</p><p>Scaph-LVSP. Just like Scaph-HVSP, Scaph-LVSP can be quite effective in some cases. For example, the top two speedups achieved by Scaph-LVSP for MST are 5.26x and 3.58x on SK (14.8GB) and UK (27.61GB), respectively. The corresponding speedups from Scaph are <ref type="bibr">5.99x and 4.19x</ref>. However, Scaph-LVSP can be rather ineffective for the graphs that can nearly fit into the 16GB GPU memory, since Scaph-LBASE will then make GPU-resident for nearly all the subgraphs. For R28 with 16.78GB (unweighted) and 29.48GB (weighted), Scaph-LVSP offers little or even negative benefits for CC, NNDR, and GCS (on unweighted graphs) but positive ones for SSSP and MST (on weighted graphs).</p><p>Scaph. Scaph obtains the best of both worlds, Scaph-HVSP and Scaph-LVSP. For CC, SSSP, MST, NNDR, and GCS, the average speedups achieved by Scaph-HVSP (Scaph-LVSP) are </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">RQ3: Sensitivity Study</head><p>To answer RQ3, we investigate Scaph's scalability in terms of #SMXs, graph sizes, memory sizes, and GPU generations. We select Graphie as a reference on CC, MST, and NNDR.</p><p>#SMXs. <ref type="figure" target="#fig_0">Figure 14</ref>(a) compares Scaph and Graphie in terms of CC, MST, and NNDR on UK <ref type="bibr" target="#b5">[6]</ref> for varying #SMXs by using all the 8GB GPU memory available. Scaph is significantly more scalable than Graphie for all the three graph algorithms, since Scaph can utilize the host-GPU bandwidth more effectively as already motivated earlier <ref type="figure" target="#fig_0">(Figures 1 and 4)</ref>. For example, Graphie-MST reaches its plateau when #SMXs = 2, but Scaph-MST continues to offer a scalable performance improvement. CC and NNDR exhibit a similar trend.</p><p>However, Scaph's scalability degrades gradually as #SMXs increases, due to the integrated impacts of the intrinsic random accesses of graph processing on GPU <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b57">57]</ref> and the increasingly more SMXs competing for the memory bandwidth. As also shown in <ref type="figure" target="#fig_0">Figure 14</ref>(a), Groute <ref type="bibr" target="#b4">[5]</ref>, an in-memory graph system that can not handle over-subscription, on UK-2007@1M <ref type="bibr" target="#b5">[6]</ref> (a sample graph with 1M vertices and 41M edges generated from UK), suffers from exactly the same scalability problem, which is beyond the scope of this work. We leave addressing this problem in future work.</p><p>Graph Sizes. <ref type="figure" target="#fig_0">Figure 14</ref>(b) compares Scaph and Graphie as the graph size increases. For CC and NNDR working on unweighted graphs, Scaph (Graphie) can store up to 4 billion (2 billion) edges in GPU memory. For MST working on the weighted graph, these edge counts drop to roughly 2 billion and 1 billion. Both Scaph and Graphie maintain their throughput well as the graph size increases but degrade visibly for the graphs that can no longer fit into GPU memory. However <ref type="formula" target="#formula_3">, 0 4 8 12 16 20 24 28 32 36 40 44 48 52 56</ref>  Scaph has a slower performance reduction rate than Graphie, for two reasons. First, Scaph can better tap GPU's processing power due to its use of a multi-level priority queue for exploiting PUD more adequately and overlapping data transfers and GPU computation more effectively. Second, Scaph avoids transferring a large amount of NUD for low-value subgraphs.</p><p>GPU Memory Capacities. <ref type="figure" target="#fig_0">Figure 14</ref>(c) compares Scaph and Graphie for varying GPU memory sizes. Graphie is highly sensitive to the GPU memory capacity used, which determines directly how many subgraphs can be resident on GPU at an iteration and how many of these get re-processed in the ensuing iteration (before they are removed from GPU memory). In contrast, Scaph is nearly insensitive, since it exploits UD and PUD for high-value subgraphs and UD only for low-value subgraphs always. Note that Scaph is significantly faster than Graphie <ref type="table" target="#tab_7">(Table 3)</ref>. In <ref type="figure" target="#fig_0">Figure 14</ref>(c), Graphie improves over itself (normalized to 10GB) as the GPU memory size increases.</p><p>GPU Generations. <ref type="figure" target="#fig_0">Figure 14</ref>(d) characterizes the performance of Scaph on different GPU generations. Compared to Graphie that shows few speedups as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, Scaph enables the significant speedups for K40 (1.99×∼3.12×) and P100 (4.26×∼5.02×) against that of GTX980.</p><p>Varying α and β. <ref type="figure" target="#fig_0">Figure 14</ref>(e) shows the sensitivity of the performance results of Scaph with respect to α and β. Here, A1 can be understood as Scaph-HVSP and A5 as Scaph-LVSP. Looking at A3, we see that increasing α and β causes more subgraphs to be mis-identified as low-value subgraphs (A4 and A5) and decreasing α and β causes more subgraphs to be mis-identified as high-value subgraphs (A1 and A2). Thus, A3 seems to represent a nice sweet spot for yielding good performance results. As for the problem of finding an optimal setting, we leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">RQ4: Runtime Overhead</head><p>We discuss Scaph's overheads incurred in its value-driven differential scheduling (VDDS) given in <ref type="figure">Figure 8</ref>, high-value subgraph processing (HVSP) given in <ref type="figure" target="#fig_0">Figure 10</ref>, and lowvalue subgraph processing (LVSP) given in <ref type="figure" target="#fig_0">Figure 12</ref>.</p><p>VDDS. The cost of computing the subgraph value comes from computing the UD size for each iteration, on GPU, in line 8 of <ref type="figure">Figure 8</ref>. This is negligible, as shown in <ref type="figure" target="#fig_0">Figure 15(a)</ref>.  <ref type="figure" target="#fig_0">Figure 15(b)</ref>, the cost incurred per iteration is small, representing an average of 0.79% of the total processing time. This small overhead is more than offset by the benefit reaped. In particular, the iteration count is reduced since most of the PUD can be computed ahead of schedule. The per-iteration time can be improved mainly because most of the NUD is discarded (rather than transferred expensively).</p><p>LVSP. The main overhead of LVSP lies in transferring a bitmap representation for all the active vertices in a subgraph from GPU to the host. As shown in <ref type="figure" target="#fig_0">Figure 15(c)</ref>, the average cost incurred per iteration represents 4.3% of the total graph processing time. However, this cost increases relatively towards the last few iterations, reaching 57.4% at the end, where each subgraph has little UD to be acted upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Limitations</head><p>Graph Partition. Various partitions may show different value variations of subgraph at runtime. Scaph adopts a greedy vertex-cut partition <ref type="bibr" target="#b15">[15]</ref> with the time taken depending on the number of partitions. It would be interesting future work to find a more reasonable partition method that can make most of UD and PUD exploited in the early stage of graph processing for faster convergence.</p><p>Disk-based Heterogeneous Graph Systems. The performance of Scaph is insensitive to the difference between CPU and GPU memory, given that the whole graph is assumed to fit into the CPU memory. To support even larger graphs on a single machine, using the disk (e.g., SSD) as secondary storage is promising. In this case, a new dimension of performance bottleneck will be the I/O inefficiency, which has been studied in prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b55">55]</ref>. We can combine Scaph with these past disk-based solutions to cooperatively handle graphs that cannot fit into the host memory.</p><p>Performance Profitability. Scaph delivers performance benefits by processing all the subgraphs differentially. Scaph is currently not expected to be applied to graph algorithms where the set of active vertices does not shrink as computation goes on. For example, all vertices in PageRank are active in every iteration. Thus, all the data of a subgraph can be regarded as UD without any PUD. In fact, we can extend Scaph to distinguish these all-active subgraphs further for PageRank by considering not only the degrees and the activation but also the state variation rate for each vertex, which is a potential direction of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Heterogeneous Graph Systems. Such systems have been studied on a range of heterogeneous architectures equipped with varying hardware resources <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b44">44]</ref>. Compared to GPU-accelerated solutions <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b47">47]</ref>, FPGA-accelerated alternatives are advantageous in energy-efficiency <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b61">61]</ref>. In developing Scaph, we focus on improving host-accelerator bandwidth utilization. The basic idea behind can also be applied to improve the scalability of FPGA-accelerated heterogeneous graph systems with a few hardware specializations.</p><p>Distributed Graph Systems. The rationale is to aggregate multiple machines to enable processing large-scale graphs. The main challenge lies in obtaining good graph partitions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52]</ref> so as to minimize the communication overheads across the machines. Some recent studies take advantage of emerging high-speed networks (e.g., RDMA) to reduce communication overheads <ref type="bibr" target="#b49">[49,</ref><ref type="bibr" target="#b58">58]</ref>. Aspire <ref type="bibr" target="#b54">[54]</ref> designs a relaxed consistency model to exploit asynchronous parallelism for iterative algorithms. Gemini <ref type="bibr" target="#b67">[67]</ref> includes a series of adaptive runtime optimizations to enable obtaining an attractive scale-out efficiency.</p><p>Disk-based Graph Systems. Many disk-based graph systems <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b65">65]</ref> exist for supporting large-scale graph processing. <ref type="bibr">GraphChi [30]</ref> relies on parallel sliding windows to optimize disk accesses. GridGraph <ref type="bibr" target="#b68">[68]</ref> uses 2-level hierarchical partitioning to reduce the I/O overhead. TurboGraph <ref type="bibr" target="#b18">[18]</ref> applies a pin-and-slide model to exploit the multicore and I/O parallelism. Due to the low disk-to-memory bandwidth, disk-based graph systems are often at least two orders-ofmagnitude slower than heterogeneous solutions.</p><p>Data Movement Reduction. Several previous studies leverage an analogous idea of running graph partitions multiple times for different purposes. CLIP <ref type="bibr" target="#b1">[2]</ref> iterates over each loaded subgraph multiple times to squeeze out the value of each subgraph so that less amount of disk I/O is required. GraphQ <ref type="bibr" target="#b69">[69]</ref> enables computing the local subgraphs multiple times in order to tolerate long latency across the compute nodes. Unlike these efforts, Scaph emphasizes on a GPU context that often requires small-size subgraphs to enable fine-grained scheduling. Thus, simply computing a subgraph multiple times is not sufficient to exploit its PUD fully. Scaph enables value exploitation not only within a subgraph but also across the subgraphs via a delayed scheduling mechanism.</p><p>In LUMOS <ref type="bibr" target="#b53">[53]</ref>, a subgraph in an iteration can be exploited asynchronously iff its updated values are independent of the subsequent iteration. This dependency-aware technique allows enjoying the efficiency of asynchronous execution while ensuring synchronous processing semantics. Applying this technique into Scaph can help identify the high-value subgraphs that contain across-iteration dependencies, so that Scaph can be extended to handle synchronous algorithms <ref type="bibr" target="#b22">[22]</ref> safely by scheduling these high-value subgraphs once. However, the downside is that many dependency-free low-value subgraphs may also be allowed to be computed multiple times, wasting the GPU computational and storage resources.</p><p>Wonderland <ref type="bibr" target="#b63">[63]</ref> uses graph abstraction as a bridge over on-disk subgraphs to speed up convergence. However, under the context of small-sized subgraphs, such a graph abstraction is often hard to keep concise, and extracting it from the whole graph is also non-trivial. PowerLayer <ref type="bibr" target="#b7">[8]</ref> presents differentiated processing on high-degree and low-degree vertices to improve the trade-off between load balance and communication overheads in a distributed setting. However, applying the idea of PowerLayer cannot often identify the value of a subgraph accurately while Scaph does with a fine-grained solution. Mosaic <ref type="bibr" target="#b35">[35]</ref> adopts a subgraph compression technique, which can be used to work together with Scaph to improve the bandwidth-efficiency of heterogeneous graph system further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper tackles the challenge faced in achieving scale-up large-scale graph processing on a GPU-accelerated heterogeneous architecture. We introduce Scaph, a value-driven heterogeneous graph system that differentially schedules the subgraphs partitioned from a graph according to their values in order to improve the effective utilization of the host-GPU bandwidth. Scaph outperforms state of the art, as evaluated with representative graph algorithms operating on a range of graph datasets. In addition, these performance benefits scale up as more computing resources are available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of Graphie [17] for three representative graph algorithms on fb-2009 (a graph with 139.1M vertices and 12.3B edges, taking 137.8GB (unweighted) and 275.6GB (weighted)) on three different generations of GPUs plugged (separately) in a 28-core host machine with 512GB memory</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A GPU-accelerated heterogeneous architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of Graphie for TW with different number of SMXs are processed. If their out-going edges are not in the GPU, their containing (active) subgraphs are transferred to GPU in their entirety. Afterward, these active vertices will be processed on the GPU (lines 13 -18) to activate more destination vertices possibly. Note that Graphie [17] may schedule first the subgraphs processed at the end of the previous iteration as they are still in GPU memory (line 8). This simple graph processing engine does not effectively utilize the limited, scarce host-GPU bandwidth since many vertices in an active subgraph are not active. Simply transferring an entire subgraph to GPU (line 10) but consuming only a fraction of its data (lines 14 -15) will waste a considerable amount of the host-GPU bandwidth. As a result, all the required data cannot arrive at the GPU promptly, limiting the performance that can be potentially achieved on GPU. Let us examine the ratios of the unused over used data in the subgraphs transferred to GPU for three graph algorithms operating on two graphs, twitter (TW) [29] and uk-2007 (UK) [6], by Graphie [17] using the graph processing engine given in Figure 3. Table 1 gives the results obtained through an offline trace analysis, showing that these ratios range from 6.29 to 36.17. This indicates that the host-GPU bandwidth under Graphie is utilized rather ineffectively. Consequently, as shown further in Figure 4, the performance of Graphie for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The amount of UD, PUD, and NUD across the iterations for three graph algorithms on twitter (TW) [29]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 ,</head><label>5</label><figDesc>NUD are V 3 5 − → V 6 and V 3 6 − → V 7 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The workflow of Scaph active subgraphs across all the iterations for three graph algorithms operating on twitter (TW) [29], partitioned sequentially into subgraphs of 32MB each. Graphie [17], a representative of existing heterogeneous graph systems [17, 26, 47], wastes the host-GPU bandwidth in two ways (Figure 3). First, PUD, usually discarded by Graphie but needed in future iterations, is substantial in earlier iterations. Second, NUD, which is becoming increasingly more dominant as the iteration progresses, is streamed to GPU redundantly. For a subgraph, it will be cost-ineffective to stream just its UD, since its PUD cannot be exploited simultaneously. Instead, our key insight for improving the effective utilization of the host-GPU bandwidth is to look beyond the current iteration, by considering not only its UD in the current iteration but also its PUD in future iterations. Based on a cost-benefit analysis, we aim to leverage rather than discard its PUD (once streamed to GPU) in iterative graph processing. Thus, the value of a subgraph at an iteration should be measured in terms of not only its UD but also its PUD. Now, how do we extract the UD and PUD from a subgraph at a given iteration so that both can be transferred to GPU? Extracting the UD from a subgraph is easy as its active vertices in the current iteration are known (lines 4 and 12 in Figure 3). However, extracting precisely the PUD (without NUD) from a subgraph is difficult, as its future active vertices are not known yet during the current iteration. For a given subgraph, we propose to predict its PUD size at an iteration from the UD sizes in the current and past iterations. This enables to adopt a value-driven differential scheduler that computes the value of a subgraph adaptively and schedules it depending on if it has a high value (when its UD and PUD are dominant) or a low value (otherwise).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figure 8: Value-driven differential scheduling for high-and low-value subgraphs, with the calls in blue executed on GPU • |UD|/| ˜ G)| &gt; α. This indicates that UD is dominant among˜G among˜ among˜G. Intuitively, ˜ G is a high-value subgraph. • |UD current | − |UD last | &gt; 0 and |UD|/| ˜ G| &gt; β. UD remains a medium level and is also growing increasingly over iteration, indicating the potentially growing PUD. ˜ G can be thus treated as a high-value subgraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: An example illustrating value propagation across the subgraphs, with  activatable by and . The PUD iñiñ G 1 can be exploited only if˜Gif˜ if˜G 2 and/or˜Gor˜ or˜G 3 are processed first.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: High-value subgraph processing in each iteration (called from Figure 8). The Kernel function is from Figure 3. The two colored code regions are executed in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>22 if</head><label>22</label><figDesc>Figure 12: Low-value subgraph processing in each iteration (called from Figure 8). The Kernel function is from Figure 3. The three shaded code regions are executed in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Speedup of Scaph, Scaph-HVSP, and Scaph-LVSP (normalized to Scaph-LBASE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Performance of Scaph and Graphie (including Groute for (a) only) in terms of varying (a) #SMXs: UK [6] and 8GB GPU memory, (b) graph sizes: R26-R30 [7] and 16GB GPU memory and #SMX=56, (c) GPU memory capacities: FB [31] and #SMX=56, (d) GPU genreations: FB [31], and (e) configurations of (α, β): FB [31], respectively. All results are normalized to the one obtained by itself with the smallest configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Scaph's runtime overhead for running CC on UK [6] across the iterations HVSP. The main overhead of HVSP lies in its queue management. In Figure 15(b), the cost incurred per iteration is small, representing an average of 0.79% of the total processing time. This small overhead is more than offset by the benefit reaped. In particular, the iteration count is reduced since most of the PUD can be computed ahead of schedule. The per-iteration time can be improved mainly because most of the NUD is discarded (rather than transferred expensively). LVSP. The main overhead of LVSP lies in transferring a bitmap representation for all the active vertices in a subgraph from GPU to the host. As shown in Figure 15(c), the average cost incurred per iteration represents 4.3% of the total graph processing time. However, this cost increases relatively towards the last few iterations, reaching 57.4% at the end, where each subgraph has little UD to be acted upon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>The amount of 
used/unused data in the sub-
graphs transferred to GPU 

Algo. 
Used 
Unused 

TW 

CC 
12.15GB 
21.44GB 
SSSP 22.74GB 
77.42GB 
MST 25.78GB 106.47GB 

UK 

CC 
43.41GB 688.43GB 
SSSP 81.64GB 1302.85GB 
MST 134.97GB 2099.25GB 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>1 Procedure HVSPEngine(worklist) 2 foreach˜Gforeach˜ foreach˜G i ∈ worklist do 3 if˜Gif˜ if˜G i is resident in GPU memory then</head><label>1</label><figDesc></figDesc><table>4 

Push(PQ 1 , i ) 

5 

else 

6 

Push(TransSet, i ) 

/* Subgraph Transferring Module */ 

7 

while TransSet = / 
0 do 

8 

if copystream is available then 

9 

i ← Pop(TransSet) 

10 

if GPU has available memory for one subgraph then 

11 

Gbuf ← AllocateDeviceMemory( ) 

12 

else 

13 

j ← Pop(PQ k ) 

14 

Gbuf ← GetGbuf( ˜ 
G j ) 

15 

TransferData(copystream, Gbuf, ˜ 
G i , CPU2GPU) 

16 

Push(PQ 1 , i ) 

/* Subgraph Scheduling Module */ 

17 

while worklist = / 
0 do 

18 

if at least one stream in execstreams is available then 

19 

stream ← Available(execstreams) 
/* Exploit the UD of a subgraph in PQ 1 */ 

20 

if PQ 1 = / 
0 then 

21 

i ← Pop(PQ 1 ) 

22 

Kernel(stream, ˜ 
G i ) 

23 

Erase(worklist, ˜ 
G i ) 
/* Exploit the PUD of a subgraph in PQ i , where i =1 
*/ 

24 

else 

25 

for p ← 2 to k do 

26 

if PQ p = / 
0 then 

27 

i ← Pop(PQ p ) 

28 

Kernel(stream, ˜ 
G i ) 

29 

break 

30 

priority ← GetPriority( ˜ 
G i ) 

31 

Push(PQ priority+1 , ˜ 
G i ) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 2 : Graph datasets. The graph size is evaluated in the weighted edgelist representation.</head><label>2</label><figDesc></figDesc><table>Dataset 
#Vertices #Edges Avg. Degree 
Size 

twitter (TW) 
41.7M 
1.47B 
39.5 
32.8GB 
comfriend (FR) 
124.8M 1.81B 
14.5 
40.4GB 
sk-2005 (SK) 
50.6 M 
1.95B 
38.5 
43.6GB 
uk-2007 (UK) 
105.9M 3.74B 
35.3 
83.6GB 
altavista-2002 (AV) 
1.41G 
6.64B 
4.695 
148.3GB 
fb-2009 (FB) 
139.1M 12.3B 
88.7 
275.6GB 
RMAT-k (25&lt;k&lt;31) 
2 k 
2 k+4 
16 
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Execution times of Scaph, Totem, Graphie, and 
Garaph. Here, 'N/A' indicates that a graph algorithm has 
abnormally terminated due to some runtime error. 

Algorithm System 
Execution Time (Secs) 
TW 
FR 
SK 
UK 
AV 
FB 

CC 

Totem 2.41 5.01 
2.72 
9.32 
N/A 
N/A 
Graphie 1.89 4.46 16.53 23.61 57.49 133.21 
Garaph 1.17 2.53 
2.90 
7.07 
31.46 86.24 
Scaph 
0.28 0.91 
1.08 
2.47 
7.08 
15.35 

SSSP 

Totem 5.94 5.78 
7.07 
19.21 
N/A 
N/A 
Graphie 5.32 9.24 52.01 89.44 218.51 413.07 
Garaph 3.71 4.45 
6.83 
16.52 114.68 204.35 
Scaph 
0.92 1.67 
3.17 
6.64 
29.06 38.87 

MST 

Totem 7.93 10.90 21.33 42.84 
N/A 
N/A 
Graphie 8.45 16.24 32.19 53.22 198.85 304.51 
Garaph 4.14 7.38 12.35 25.82 101.25 131.45 
Scaph 
1.39 1.99 
2.93 
6.36 
25.23 35.41 

NNDR 

Totem 6.47 6.63 12.17 29.43 
N/A 
N/A 
Graphie 5.38 7.32 28.19 49.81 234.04 457.13 
Garaph 3.41 4.76 
9.28 
28.74 116.34 175.34 
Scaph 
1.77 2.08 
2.99 
5.13 
20.19 33.55 

GCS 

Totem 19.77 23.04 59.51 98.11 
N/A 
N/A 
Graphie 24.08 38.84 50.34 93.29 454.41 834.59 
Garaph 10.53 15.56 20.438 39.45 185.58 299.76 
Scaph 
3.33 4.08 10.46 16.13 39.52 54.94 

</table></figure>

			<note place="foot" n="582"> 2020 USENIX Annual Technical Conference USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments. In particular, we thank our shepherd, Xiaosong Ma, for her valuable suggestions. We would also like to thank Pengcheng Yao, Chuangyi Gui, Qinggang Wang, and Jieshao Zhao for their support. This work is supported by the Na- </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A scalable processing-inmemory accelerator for parallel graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwhan</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungpack</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiyoung</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd ACM/IEEE Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 42nd ACM/IEEE Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Squeezing out all the value of loaded data: An out-of-core graph processing system with reduced disk i/o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference (USENIX ATC)</title>
		<meeting>the USENIX Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="125" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Giraph: Large-scale graph processing infrastructure on hadoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching</forename><surname>Avery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Hadoop Summit</title>
		<meeting>the Hadoop Summit</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="5" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing cuda workloads using a detailed gpu simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Bakhoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilson</forename><forename type="middle">W L</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tor</forename><forename type="middle">M</forename><surname>Aamodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedingns of the IEEE International Symposium on Performance Analysis of Systems and Software</title>
		<meeting>eedingns of the IEEE International Symposium on Performance Analysis of Systems and Software</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="163" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Groute: An asynchronous multi-gpu programming model for irregular computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreepathi</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="235" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The webgraph framework I: Compression techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Boldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastiano</forename><surname>Vigna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International World Wide Web Conference (WWW)</title>
		<meeting>the 13th International World Wide Web Conference (WWW)<address><addrLine>Manhattan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="595" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-mat: A recursive model for graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 SIAM International Conference on Data Mining (SDM)</title>
		<meeting>the 2004 SIAM International Conference on Data Mining (SDM)</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="442" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Powerlyra: Differentiated graph computation and partitioning on skewed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th</title>
		<meeting>the 10th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<title level="m">European Conference on Computer Systems (Eurosys)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="13" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">One trillion edges: Graph processing at facebook-scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avery</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Kabiljo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dionysios</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sambavi</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1804" to="1815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Foregraph: Exploring large-scale graph processing on multi-fpga architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuze</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale graph processing on emerging storage devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Elyasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th USENIX Conference on File and Storage Technologies (USENIX FAST)</title>
		<meeting>the 17th USENIX Conference on File and Storage Technologies (USENIX FAST)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="309" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parallelizing sequential graph computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenfei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data (SIGMOD)</title>
		<meeting>the 2017 ACM International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="495" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A yoke of oxen and a thousand chickens for heavy lifting graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullah</forename><surname>Gharaibeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauro Beltrão</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizeu</forename><surname>Santos-Neto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Ripeanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 21st International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="345" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Powergraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graphx: Graph processing in a distributed dataflow framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 11th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="599" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graphie: Large-scale asynchronous graph traversals on just a gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Mawhirter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Buland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="233" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Turbograph: a fast parallel graph engine handling billionscale graphs in a single pc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wook-Shin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungyeol</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Accelerating large graph algorithms on the gpu using cuda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Harish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Petter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 International Conference on high-performance computing (HiPC)</title>
		<meeting>the 2007 International Conference on high-performance computing (HiPC)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="197" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The connected-component labeling problem: A review of state-of-the-art algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Xiwei Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="25" to="43" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient parallel graph exploration on multi-core cpu and gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungpack</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tayo</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 20th International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="78" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Polo&quot; Chau, and Christos Faloutsos. Inference of beliefs on billion-scale graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Unit</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duen</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD Workshop on Large-scale Data Mining: Theory and Applications (LDMTA)</title>
		<meeting>KDD Workshop on Large-scale Data Mining: Theory and Applications (LDMTA)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">All-pairs shortestpaths for large graphs on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">T</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware</title>
		<meeting>the 23rd ACM SIGGRAPH/EUROGRAPHICS Symposium on Graphics Hardware</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="47" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mizan: a system for dynamic load balancing in large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuhair</forename><surname>Khayyat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karim</forename><surname>Awara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amani</forename><surname>Alonazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hani</forename><surname>Jamjoom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Kalnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM European Conference on Computer Systems (Eurosys)</title>
		<meeting>the 8th ACM European Conference on Computer Systems (Eurosys)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cusha: Vertex-centric graph processing on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farzad</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laxmi</forename><forename type="middle">N</forename><surname>Bhuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on High Performance Parallel and Distributed Computing (HPDC)</title>
		<meeting>the 23rd International Symposium on High Performance Parallel and Distributed Computing (HPDC)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gts: A fast and scalable graph processing method based on streaming topology to gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyuhyeon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunseok</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Management of Data (SIGMOD)</title>
		<meeting>the 2016 International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Data mining techniques for the detection of fraudulent financial statements. Expert systems with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstathios</forename><surname>Kirkos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charalambos</forename><surname>Spathis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Manolopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="995" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gpu-accelerated graph clustering via parallel label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuke</forename><surname>Kozawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiyuki</forename><surname>Amagasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Kitagawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>ACM Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">What is twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haewoon</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on World Wide Web</title>
		<meeting>the 19th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graphchi: Large-scale graph computation on just a pc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SNAP Datasets: Stanford large network dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Krevl</surname></persName>
		</author>
		<ptr target="http://snap.stanford.edu/data" />
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graphene: Finegrained io management for graph computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howie</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 USENIX Annual Technical Conference (USENIX ATC)</title>
		<meeting>the 2015 USENIX Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed graphlab: a framework for machine learning and data mining in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Garaph: Efficient gpu-accelerated graph processing on a single machine with balanced replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference (USENIX ATC)</title>
		<meeting>the USENIX Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="195" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mosaic: Processing a trillion-edge graph on a single machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woonhak</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Systems (EuroSys)</title>
		<meeting>the 12th European Conference on Computer Systems (EuroSys)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="527" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Aart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">C</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilan</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naty</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data (SIG-MOD)</title>
		<meeting>the 2010 ACM SIGMOD International Conference on Management of Data (SIG-MOD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient minimum spanning tree algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanguthevar</forename><surname>Mamun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium on Computers and Communication (ISCC)</title>
		<meeting>the IEEE Symposium on Computers and Communication (ISCC)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1047" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph: Heterogeneity-aware graph computation with adaptive partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Adnan</forename><surname>Tariq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Rothermel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th IEEE International Conference on Distributed Computing Systems (ICDCS)</title>
		<meeting>the 36th IEEE International Conference on Distributed Computing Systems (ICDCS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="118" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scalable gpu graph traversal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Grimshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 17th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A lightweight infrastructure for graph analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The Most Advanced Datacenter Accelerator Ever Built Featuring Pascal GP100, the World&apos;s Fastest GPU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Tesla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>P100</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>White paper, NVIDIA</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><forename type="middle">V100</forename><surname>Tesla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V100</forename><surname>Tesla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gpu Architecture</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The World&amp;apos;s Most Advanced Data</forename><surname>Whitepaper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nvidia</forename><surname>Center Gpu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A compiler for throughput optimization of graph algorithms on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sreepathi</forename><surname>Pai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graphphi: efficient parallel graph processing on emerging throughput-oriented architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tekin</forename><surname>Bicer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<meeting>the 27th International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chaos: Scale-out graph processing from secondary storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Bindschaedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasmina</forename><surname>Malicevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 25th Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="410" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">X-stream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 24th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graphreduce: processing large-scale graphs on accelerator-based systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Shuaiwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</title>
		<meeting>the 27th International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Trinity: A distributed graph engine on a memory cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Management of Data (SIGMOD)</title>
		<meeting>the 2013 International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="505" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast and concurrent rdf queries with rdmabased distributed graph exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youyang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th USENIX Conference on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph processing on gpus: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongluan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Arabesque: a system for distributed graph mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><forename type="middle">J</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgos</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">J</forename><surname>Siganos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashraf</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aboulnaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 25th Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="425" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Lumos: Dependency-driven disk-based graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Conference on Usenix Annual Technical Conference (USENIX ATC)</title>
		<meeting>the USENIX Conference on Usenix Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="429" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Aspire: Exploiting asynchronous parallelism in iterative algorithms using a relaxed consistency based dsm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Sai Charan Koduru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications (OOPSLA)</title>
		<meeting>the ACM International Conference on Object Oriented Programming Systems Languages and Applications (OOPSLA)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="861" to="878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Load the edges you need: A generic i/o optimization for diskbased graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Annual Technical Conference (USENIX ATC)</title>
		<meeting>the 2016 USENIX Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Replication-based fault-tolerance for large-scale graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</title>
		<meeting>the 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="562" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gunrock: A high-performance graph processing library on the gpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuechao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuduo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Riffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Gram: scaling graph computation to the trillions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 6th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for inmemory cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI)</title>
		<meeting>the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Three fastest shortest path algorithms on real road networks: Data structures and procedures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Benjamin Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Geographic Information and Decision Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="82" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Boosting the performance of fpga-based graph processor using hybrid memory cube: A case for breadth first search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroosh</forename><surname>Khoram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
		<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays (FPGA)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Numaaware graph-structured analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</title>
		<meeting>the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Wonderland: A novel abstraction-based out-of-core graph processing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengying</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS)</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems (ASP-LOS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="608" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cgraph: a correlationsaware approach for efficient concurrent iterative graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference (USENIX ATC)</title>
		<meeting>the USENIX Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="441" to="452" />
		</imprint>
	</monogr>
	<note>Ligang He, Bingsheng He, and Haikun Liu</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Flashgraph: Processing billion-node graphs on an array of commodity ssds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Disa</forename><surname>Mhembere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randal</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Szalay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th USENIX Conference on File and Storage Technologies (USENIX FAST)</title>
		<meeting>13th USENIX Conference on File and Storage Technologies (USENIX FAST)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Medusa: Simplified graph processing on gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Gemini: A computation-centric distributed graph processing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</title>
		<meeting>the 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="301" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gridgraph: Large-scale graph processing on a single machine using 2-level hierarchical partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 USENIX Annual Technical Conference (USENIX ATC)</title>
		<meeting>the 2015 USENIX Annual Technical Conference (USENIX ATC)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">GraphQ: Scalable pim-based graph processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="712" to="725" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
