<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-01T14:47+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fault-Tolerance in the Borealis Distributed Stream Processing System</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magdalena</forename><surname>Balazinska</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory The Stata Center</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Balakrishnan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory The Stata Center</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
							<email>madden@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory The Stata Center</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
							<email>stonebraker@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory The Stata Center</orgName>
								<address>
									<addrLine>32 Vassar Street</addrLine>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fault-Tolerance in the Borealis Distributed Stream Processing System</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>We present a replication-based approach to fault-tolerant distributed stream processing in the face of node failures, network failures, and network partitions. Our approach aims to reduce the degree of inconsistency in the system while guaranteeing that available inputs capable of being processed are processed within a specified time threshold. This threshold allows a user to trade availability for consistency: a larger time threshold decreases availability but limits inconsistency , while a smaller threshold increases availability but produces more inconsistent results based on partial data. In addition, when failures heal, our scheme corrects previously produced results, ensuring eventual consistency. Our scheme uses a data-serializing operator to ensure that all replicas process data in the same order, and thus remain consistent in the absence of failures. To regain consistency after a failure heals, we experimentally compare approaches based on checkpoint/redo and undo/redo techniques and illustrate the performance trade-offs between these schemes.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In recent years, a new class of data-intensive applications requiring near real-time processing of large volumes of streaming data has emerged. These stream processing applications arise in several different domains, including computer networks (e.g., intrusion detection), financial services (e.g., market feed processing), medical information systems (e.g., sensor-based patient monitoring), civil engineering (e.g., highway monitoring, pipeline health monitoring), and military systems (e.g., platoon tracking, target detection).</p><p>In all these domains, stream processing entails the composition of a relatively small set of operators (e.g., filters, aggregates, and correlations) that perform their computations on windows of data that move with time. Most stream processing applications require results to be continually produced at low latency, even in the face of high and variable input data rates. As has been widely noted <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref>, traditional data base management systems (DBMSs) based on Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. <ref type="bibr">SIGMOD 2005</ref><ref type="bibr">June 14-16, 2005</ref>, Baltimore, Maryland, USA.</p><p>Copyright 2005 ACM 1-59593-060-4/05/06 $5.00.</p><p>the "store-then-process" model are inadequate for such highrate, low-latency stream processing.</p><p>Stream processing engines (SPEs) (also known as data stream managers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30]</ref> or continuous query processors <ref type="bibr" target="#b13">[14]</ref>) are a class of software systems that handle the data processing requirements mentioned above. Much work has been done on data models and operators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>, efficient processing <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30]</ref>, and resource management <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref> for SPEs. Stream processing applications are inherently distributed, both because input streams often arrive from geographically distributed data sources, and because running SPEs on multiple processing nodes enables better performance under high load <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35]</ref>. In a distributed SPE, each node produces result streams that are either sent to applications or to other nodes for additional processing. When a stream goes from one node to another, the nodes are called upstream and downstream neighbors.</p><p>In this paper, we add to the body of work on SPEs by addressing fault-tolerant stream processing, presenting a faulttolerance protocol, implementation details, and experiments. Our approach enables a distributed SPE to cope with a variety of network and system failures. It differs from previous work on high availability in streaming systems by offering a configurable trade-off between availability and consistency. Previous schemes either do not address network failures <ref type="bibr" target="#b24">[25]</ref> or strictly favor consistency over availability, by requiring at least one fully connected copy of the query network to exist to continue processing at any time <ref type="bibr" target="#b34">[35]</ref>. As such, our scheme is particularly well-suited for applications where it is possible to make significant progress even when some of the inputs are unavailable.</p><p>As in most previous work on masking software failures, we use replication <ref type="bibr" target="#b21">[22]</ref>, running multiple copies of the same query network on distinct processing nodes. In our approach, when a node stops receiving data (or "heartbeat" messages signifying liveness) from one of its upstream neighbors, it requests the missing input streams from a replica of that neighbor (if it can find one). For a node to be able to correctly continue processing after such a switch, all replicas of the same processing node must be consistent with each other. They must process their inputs in the same order, progress at roughly the same pace, and their internal computational state must be the same. To ensure replica consistency, we define a simple data-serializing operator, called SUnion, that takes multiple streams as input and produces one output stream with deterministically ordered tuples.</p><p>At the same time, if a node is unable to find a new upstream neighbor for an input stream, it must decide whether to continue processing with the remaining (partial) inputs, or block until the failure heals. If it chooses to continue, a number of possibly incorrect results will be produced, while blocking makes the system unavailable.</p><p>Our approach gives the user explicit control of trade-offs between consistency and availability in the face of network failures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>. We also ensure eventual consistency: i.e., clients eventually see the complete correct results. We introduce an enhanced streaming data model in which results based on partial inputs are marked as tentative, with the understanding that they may subsequently be modified; all other results are considered stable and immutable.</p><p>To provide high availability, each SPE processes input data and forwards results within a user-specified time threshold of arrival, even if other inputs are currently unavailable. At the same time, to prevent downstream nodes from unnecessarily having to react to tentative data, an SPE tries to avoid or limit the number of tentative tuples it produces.</p><p>When a failure heals, each SPE that processed tentative data reconciles its state by re-running its computation on the correct input streams. While correcting its internal state, the replica also stabilizes its output by replacing the previously tentative output with stable data tuples, allowing downstream neighbors to reconcile in turn. We argue that traditional approaches to record reconciliation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref> are illsuited for streaming systems, and adapt two approaches similar to known checkpoint/redo and undo/redo schemes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref> to allow SPEs to reconcile their states.</p><p>Our fault-tolerance protocol addresses the problem of minimizing the number of tentative tuples while guaranteeing that the results corresponding to any new tuple are sent downstream within a specified time threshold. The ability to trade availability (via a user-specified threshold) for consistency (measured by the number of tentative result tuples, since that is often a reasonable proxy for replica inconsistency) is useful in many streaming applications where having perfect answers at all times is not essential (see Section 2). Our approach also performs well in the face of the non-uniform failure durations observed in empirical measurements of system failures: most failures are short, but most of the downtime of a system component is due to longduration failures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>We have implemented our approach in Borealis <ref type="bibr" target="#b1">[2]</ref>. Through experiments, we show that Borealis meets the required availability/consistency trade-offs for failures of variable duration, even when query networks span multiple nodes. We show that it is necessary to process new tuples both during failure and reconciliation to meet the availability requirement for long failures. We find that reconciliation based on checkpoint/redo outperforms reconciliation based on undo/redo because it incurs lower overhead and achieves faster recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL, ASSUMPTIONS, AND GOALS</head><p>This section describes our distributed stream processing model, failure assumptions, and design goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Query and Failure Model</head><p>A loop-free, directed graph of operators that process data arriving on streams forms a query network. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates a query network distributed across four nodes. In many stream processing applications, input streams arrive from multiple sources across the network, and are processed by a Union operator that produces a FIFO order of the in- puts before further processing. These inputs may come directly from data sources, such as network monitors sending synopses of connection information or other activity, or may be the results of processing at upstream SPE nodes.</p><p>To avoid blocking in face of infinite input streams, operators perform their computations over windows of tuples. Some operators, such as Join, still block when some of their inputs are missing. In contrast, a Union is an example of a non-blocking operator because it can perform meaningful processing even when some of its input streams are missing. In <ref type="figure" target="#fig_0">Figure 1</ref>, the failure of a data source does not prevent the system from processing the remaining streams. Failure of node 1 or 2 does not block node 3 but blocks node 4.</p><p>Because many stream processing applications are geared toward monitoring tasks, when a failure occurs upstream from a non-blocking operator and causes some (but not all) of its input streams to be unavailable, it is often useful to continue processing the inputs that remain available. For example, in a network monitoring application, even if only a subset of monitors are available, processing their data might suffice to identify some potential attackers or other network anomalies. In this application, low latency processing is critical to mitigate attacks. However, some events might go undetected because a subset of the information is missing, and some aggregate results may be incorrect. Furthermore, the state of replicas diverges as they process different inputs.</p><p>After a failure heals, previously unavailable data streams are made available again. To ensure that replicas become once again consistent with one another and that client applications eventually receive the complete correct streams, it is important to arrange for each node to correct its internal state and the output it produced during the failure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Failure Assumptions</head><p>Our approach handles fail-stop failures (e.g., software crashes) of processing nodes, network failures, and network partitions where any subset of nodes lose connectivity to one another. When each node has N replicas (including itself), we tolerate up to N − 1 simultaneous node failures. We consider long delays as network failures.</p><p>We assume that data sources and clients implement the fault-tolerance protocols described in the next section. This can be achieved by having clients and data sources use a fault-tolerant library or by having them communicate with the system through proxies (or nearby processing nodes) that implement the required functionality. We also assume that data sources, or proxies acting on their behalf, log input tuples persistently (e.g., in a transactional queue <ref type="bibr" target="#b9">[10]</ref>) before transmitting them to all replicas that process the corresponding streams. A persistent log ensures that all replicas eventually see the same input tuples, in spite of proxy or data source failures. The fail-stop failure of a data source, however, causes the permanent loss of input tuples that would have otherwise been produced by the data source.</p><p>Our scheme is designed for a low level of replication and a low failure frequency. We assume that replicas have spare processing and bandwidth capacity and that they communicate using a reliable, in-order protocol like TCP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Design Goals</head><p>Our goal is to ensure, for each node, that any data tuple on an input stream is processed within a specified time bound, regardless of whether failures occur on other input streams or not. Among possible ways to achieve this goal, we seek methods that produce the fewest tentative tuples. If Ntentative is the number of tentative tuples produced by a node and Delay new , the maximum delay for that node to process an input tuple and produce a result, our goal is for each node to minimize Ntentative, subject to Delay new &lt; X.</p><p>X is a measure of the maximum processing latency that an application or user can tolerate to avoid inconsistency. Different algorithms are possible to convert an end-to-end latency into a per-node delay. We do not discuss this assignment in this paper and assume each node is given X. The constraint on Delay new implies that a node cannot buffer inputs longer than αX, where αX &lt; X − P and P is the normal processing delay. Alternatively, X could express an added delay, but we use the former definition in this paper.</p><p>Reducing Ntentative reduces the amount of resources consumed by downstream nodes in processing tentative tuples. Ntentative may also be thought of as a (crude) substitute for the degree of divergence between replicas when the set of input streams is not the same at the replicas.</p><p>Our approach ensures that as long as some path of nonblocking operators is available between one or more data sources and a client application, the client receives results. Furthermore, our approach favors stable results over tentative results when both are available. Once failures heal, we ensure that clients receive stable versions of all results, and that all replicas converge to a consistent state. We handle single failures and multiple overlapping (in time) failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPROACH</head><p>This section describes our replication scheme and underlying algorithms. Each node implements the state machine shown in <ref type="figure" target="#fig_1">Figure 2</ref> that has three states: STABLE, UP-STREAM FAILURE (UP FAILURE), and STABILIZATION.</p><p>As long as all upstream neighbors of a node are producing stable tuples, the node is in the STABLE state. In this state, it processes tuples as they arrive and passes stable results to downstream neighbors. To maintain consistency between replicas that may receive inputs in different orders, we define a data-serializing operator, SUnion. Section 3.2 discusses the STABLE state and the SUnion operator.</p><p>If one input stream becomes unavailable or starts carrying tentative tuples, a node goes into the UP FAILURE state, where it tries to find another stable source for the input stream. If no such source is available, the node has three choices to process the remaining available input tuples:</p><p>1. Suspend processing until the failure heals and the failed upstream neighbors start producing stable data again. 2. Delay new tuples for a short period of time before processing. 3. Process each new tuple without any delay. The first option favors consistency. It does not produce any tentative tuples and may be used only for short failures given our goal to process new tuples with bounded delay. The latter two options both produce result tuples that are marked "tentative;" the difference between the options is in the latency of results and the number of tentative tuples produced. Section 3.3 discusses the UP FAILURE state.</p><p>A failure heals when a previously unavailable upstream neighbor starts producing stable tuples again or when a node finds another replica of the upstream neighbor that can provide the stable version of the stream. Once a node receives the stable versions of all previously missing or tentative input tuples, it transitions into the STABILIZATION state. In this state, if the node processed any tentative tuples during UP FAILURE it must now reconcile its state and stabilize its outputs. We explore two approaches for state reconciliation: a checkpoint/redo scheme and an undo/redo scheme. While reconciling, new input tuples are likely to continue to arrive. The node has the same three options mentioned above for processing these tuples: suspend, delay, or process without delay. Our approach enables a node to reconcile its state and correct its outputs, while ensuring that new tuples continue to be processed. We discuss the STABILIZATION state in Section 3.4.</p><p>Once stabilization completes, the node transitions to the STABLE state if there are no other current failures, or back to the UP FAILURE state otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data Model</head><p>With our approach, nodes and applications must distinguish between stable and tentative results. Stable tuples produced after stabilization may override previous tentative ones, requiring a node to correctly process these amendments. Traditionally, a stream is an append-only sequence of tuples of the form: (t, a1, . . . , am), where t is a timestamp value and a1, . . . , am are attribute values <ref type="bibr" target="#b0">[1]</ref>. To accommodate our new tuple semantics, we adopt and extend the Borealis data model <ref type="bibr" target="#b2">[3]</ref>. In Borealis, tuples take the form:</p><p>(tuple type, tuple id, tuple time, a1, . . . , am)</p><p>1. tuple type indicates the type of the tuple. 2. tuple id uniquely identifies the tuple in the stream. 3. tuple time is the tuple timestamp. We discuss these timestamps further in Section 3.2. Traditionally, all tuples are immutable stable insertions. We introduce two new types of tuples: TENTATIVE and UNDO. A tentative tuple is one that results from processing a subset of inputs and may subsequently be amended with a stable version. An undo tuple indicates that a suffix of tuples on a stream should be deleted and the associated state of any operators rolled back. As illustrated in <ref type="figure" target="#fig_3">Figure 3</ref>, the undo tuple indicates the suffix with the tuple id of the last tuple not to be undone. Stable tuples that follow an undo replace the undone tentative tuples. Applications that do not tolerate inconsistency may thus simply drop tentative and    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Stable State</head><p>An operator is deterministic if its results do not depend on the times at which its inputs arrive (e.g., the operator does not use timeouts); of course, the results will usually depend on the input data order. If all operators are deterministic, we only need to ensure that replicas of the same operator process data in the same order to maintain consistency; otherwise, the replicas will diverge even without failures.</p><p>Since nodes communicate with TCP, tuples never get reordered within a stream and the problem affects only operators with more than one input stream (e.g., Union and Join). We thus need a way to order tuples deterministically across multiple input streams that feed the same operator. The challenge is that tuples on streams may not be sorted on any attribute and they may arrive at significantly-different rates. To compute an order without the overhead of interreplica communication, we propose a simple data-serializing operator, SUnion. SUnion takes multiple streams as input and applies a deterministic sort function on buckets of tuples.</p><p>SUnion uses tuple time values to place tuples in buckets of statically defined sizes. The sort function later typically orders tuples by increasing tuple time values, but other functions are possible. To distinguish between failures and lack of data, data sources send periodic heartbeats in the form of boundary tuples. These tuples have tuple type = BOUND-ARY and each data source guarantees that no tuples with tuple time smaller than the boundary's tuple time will be sent after the boundary 1 . Boundary tuples are similar to punctuation tuples <ref type="bibr" target="#b40">[41]</ref> or heartbeats <ref type="bibr" target="#b35">[36]</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the serialization of three streams. Tuples in bucket i can be sorted and forwarded as stable because boundary tuples with timestamps greater than the bucket boundary have arrived (in bucket i+1). These bound- ary tuples make the bucket stable as they guarantee that no tuples are missing from the bucket. Neither of the other buckets can be processed, since both buckets are missing boundary tuples and bucket i + 2 contains tentative tuples. SUnion operators may appear at any location in a query network. Operators must thus set tuple time values on their output tuples deterministically as these values will affect tuple order at downstream SUnions. Operators must also produce periodic boundary tuples and tuple time values in boundary tuples must be monotonically increasing. If output tuples are not ordered on tuple time values, boundary tuples must propagate through the query network to enable downstream operators to produce correct boundary tuples.</p><p>SUnion is similar to the Input Manager in STREAM <ref type="bibr" target="#b35">[36]</ref>, which sorts tuples by increasing timestamp order and deduces heartbeats if applications do not provide them. SUnion, in contrast, ensures that replicas process tuples in the same order, distinguishes failures from delays, offers a flexible availability/consistency trade-off (as we discuss in the next section), and corrects input streams after failures heal. The Input Manager does not make such distinctions. It assumes that delays are bounded.</p><p>A natural choice for tuple time is to use wall clock time. By synchronizing clocks at the data sources, tuples will get processed approximately in the order they are produced. The NTP (Network Time Protocol) <ref type="bibr" target="#b39">[40]</ref> is standard today and implemented on most computers and essentially all servers. NTP synchronizes clocks to within 10 ms. Wall-clock time is not the only possible choice, though. In Borealis, any integer attribute can serve to define the windows that delimit operator computations. When this is the case, operators also assume that input tuples are sorted on that attribute and tolerate only limited re-ordering <ref type="bibr" target="#b0">[1]</ref>. Hence, using the same attribute for tuple time as for windows helps enforce the ordering requirement.</p><p>SUnion operators delay tuples because they buffer and sort them. This delay depends on three properties of boundary tuples. First, the interval between boundary tuples with increasing tuple time values as well as the bucket size determine the average buffering delay. Second, the buffering delay further increases with disorder. The increase is bounded above by the maximum delay between a tuple with a tuple time, t, and a boundary tuple with a tuple time &gt; t. Third, a bucket is stable only when boundary tuples with sufficiently high tuple time values appear on all streams input to the same SUnion. The maximum differences in tuple time values across these streams bounds the added delay. Because the query network typically assumes tuples are ordered on the attribute selected for tuple time, we can expect serialization delays to be small in practice. In particular, these delays should be significantly smaller than the maximum processing delay, X. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Upstream Failure</head><p>Each node monitors the availability and consistency of its input streams by periodically requesting heartbeat responses from each replica of each upstream neighbor. These responses not only indicate if a replica is reachable but include the states (STABLE, UP FAILURE, or STABILIZATION) of its output streams. Even though a node is in UP FAILURE, a subset of its outputs may be unaffected by the failure and may remain in the STABLE state. Additionally, a node monitors the data it receives, namely the identifiers of the last stable and tentative input tuples on each input stream.</p><p>With the above information, if an upstream neighbor is no longer in the STABLE state or is unreachable, the node can switch to another STABLE replica of that neighbor and continue receiving data from the correct point in the stream. If no STABLE replica is reachable, the node will try to continue from a replica in the UP FAILURE state to ensure the required availability. The result of these switches is that any replica can forward data streams to any downstream replica or client and the outputs of some replicas may not be used, as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>. We further discuss switching between upstream neighbors in various consistency states in Section 3.5.</p><p>To enable such switches, every node buffers its output tuples. We assume that these buffers can hold more tuples than the maximum number that can be delivered during a single failure and recovery; we further discuss buffer management in Section 5.4.</p><p>If a node fails to find a STABLE replica to replace an upstream neighbor it can either block or continue processing the available tentative tuples or even continue with a missing input stream. Blocking avoids inconsistency and is thus the best approach for failures shorter than αX. For longer failures, the node must eventually stop blocking new tuples to ensure the required availability. When this occurs, SUnions serialize the available tuples, labelling them as tentative, and buffering them in preparation for future reconciliation (SUnions monitor all input streams). In the example from <ref type="figure" target="#fig_4">Figure 4</ref> if the boundary for stream s2 does not arrive within αX of the time the first tuple entered bucket i + 1 or bucket i + 2 still contains tentative tuples αX time units after the first tuple entered that bucket, SUnion will store and forward the remaining tuples as tentative.</p><p>As a node processes tentative tuples, its state may start to diverge. The node can do one of two things: delay new tuples as much as possible or process them without delay. Continuously delaying new tuples reduces the number of tentative tuples produced during failure but it constrains what the node can do during stabilization, as we discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Stabilization</head><p>A node determines that a failure healed when it is able to communicate with a stable upstream neighbor and receives corrections to previously-tentative tuples (or a replay of previously missing inputs). To ensure eventual consistency, the node must then reconcile its state and stabilize its outputs. This means that the node replaces previously tentative result tuples with stable ones, thus allowing downstream neighbors to reconcile their states in turn. To avoid correcting tentative tuples with other tentative ones, a node reconciles its state only after correcting all its input streams. We present state reconciliation and output stabilization techniques in this section. We also present a technique that enables each node to maintain availability (meet the Delay new &lt; X requirement) while reconciling its state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">State Reconciliation</head><p>Because no replica may have the correct state after a failure and because the state of a node depends on the exact sequence of tuples it processed, we propose that a node reconcile its state by reverting it to a pre-failure state and reprocessing all input tuples since then. To revert to an earlier state, we explore two approaches: reverting to a checkpointed state or undoing the effects of tentative tuples. Both approaches require that the node suspends processing new input tuples while reconciling its state.</p><p>Checkpoint/redo reconciliation. In this approach, a node periodically checkpoints the state of its query network when it is in STABLE state. SUnions on input streams buffer input tuples between checkpoints and they continue to do so during UP FAILURE. These input tuples must be buffered because they will be replayed if the node restarts from the checkpoint. When a checkpoint occurs, however, SUnion operators truncate all buckets that were processed before that checkpoint.</p><p>To perform a checkpoint, a node suspends all processing and iterates through operators and intermediate queues to make a copy of their states. Checkpoints could be optimized to copy only differences in states since the last checkpoint. We do not investigate this optimization and show, in Section 5.3, that it is actually not needed. To reconcile its state, a node re-initializes operator and queue states from the checkpoint and reprocesses all buffered input tuples. To enable this approach, operators must thus be modified to include a method to take a snapshot of their state or reinitialize their state from a snapshot.</p><p>Undo/redo reconciliation. To avoid the CPU overhead of checkpointing and to recover at a finer granularity by rolling back only the state on paths affected by the failure, another approach is to reconcile by undoing the processing of tentative tuples and redoing that of their stable counterparts. With undo/redo, SUnions on input streams only need to buffer tentative buckets, truncating stable ones as soon as they process them.</p><p>To support such an approach, all operators should implement an "undo" method, where they remove a tuple from their state and, if necessary, bring some tuples previously evicted from the state back into the current window. Supporting undo in operators may not be straightforward-for example, suppose an input tuple, p, caused an aggregate operator to close a window and output a value. To undo p, the aggregate must undo its output but must also bring back all the evicted tuples and reopen the window.</p><p>Instead, we propose that operators buffer their input tuples and undo by rebuilding the state that existed right before they processed the tuple that must now be undone. To determine how far back in history to restart processing from, operators maintain a set of stream markers for each input tuple. The stream markers for a tuple p in operator u are identifiers of the oldest tuples on each input stream that still contribute to the operator's state when u processes p. To undo the effects of processing all tuples following p, u looks up the stream markers for p, scans its input buffer until it finds that bound, and reprocesses its input buffer since then, stopping right after processing p. A stream marker is typically the beginning of the window of tuples to which p belongs. Stream markers do not hold any state. They are pointers to some location in the input buffer. To produce the appropriate undo tuple, operators must store the last tuple they produced with each set of stream markers.</p><p>Operators that keep their state in aggregate form must explicitly remember the first tuple on each input stream that begins the current aggregate computation(s). In the worst case, determining the stream markers may require a linear scan of all tuples in the operator's state. To reduce the runtime overhead, rather than compute stream markers for every tuple, operators may set stream markers periodically. This will increase reconciliation time, however, as re-processing will restart from an inexact marker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Stabilizing Output Streams</head><p>Independently of the approach chosen to reconcile the state, a node stabilizes each output stream by deleting a suffix of the stream (normally all tentative tuples) with a single undo tuple and forwarding corrections in the form of stable tuples. When it receives an undo tuple, an SUnion at a downstream node stabilizes the corresponding input stream by replacing, in its buffer, undone tuples with their stable counterparts. Once all input streams are corrected, SUnions trigger a state reconciliation.</p><p>With undo/redo, operators process and produce undo tuples, which simply propagate to downstream nodes. To generate an undo tuple with checkpoint/redo, we introduce a new operator, SOutput, that we place on each output stream that crosses node boundary. At runtime, SOutput acts as a pass-through filter that also remembers the last stable tuple it produced. During checkpoint recovery, SOutput drops duplicate stable tuples and produces the undo tuple.</p><p>Stabilization completes when one of two situations occurs. The node re-processes all previously tentative input tuples and catches up with normal execution (i.e., it clears its queues) or another failure occurs and the node goes back into UP FAILURE. Once stabilization completes, a node transmits a REC DONE tuple to its downstream neighbors. SOutput operators generate and forward the REC DONE tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Processing New Tuples During Reconciliation</head><p>After long failures, the reconciliation itself may take longer than X. A node then cannot suspend new tuples while reconciling. It must produce both corrected stable tuples and new tentative tuples. We propose to achieve this by using two replicas of a query network: one replica remains in UP FAILURE state and continues processing new input tuples while the other replica performs the reconciliation. A node could run both versions locally but because we already use replication, we propose that replicas use each other as the two versions, when possible. By doing so, we never create new replicas in the system. Hence, to ensure availability, before reconciling its state, a node must find another replica and request that it postpone its own reconciliation.</p><p>It is up to each downstream node to detect when any one of its upstream neighbors goes into the STABILIZATION state and stops producing recent tuples in order to produce corrections. The downstream node then remains connected to that replica to correct its input stream while at the same time, connecting to another replica that is still in UP FAILURE state (if possible). The downstream node processes both streams in parallel, until it receives a REC DONE tuple on the corrected stream. At this point, it enters the STABILIZATION state, in turn. SUnion considers that tentative tuples between an UNDO and a REC DONE correspond to the old failure while tentative tuples that appear after the REC DONE correspond to a new failure. We discuss how a node produces the correct REC DONE tuple in spite of failures during its recovery in Section 3.5.</p><p>Once again, we have a trade-off between availability and consistency. Suspending new tuples during reconciliation reduces the number of tentative tuples but may eventually break the availability requirement. Processing new tuples during reconciliation increases the number of tentative tuples but a node may still attempt to reduce their number by delaying new tuples as long as possible. We compare these alternatives in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Failed Node Recovery</head><p>A failed node restarts from an empty state and refuses new clients until it processes sufficiently many tuples to reach a consistent state. This approach is possible when operators are convergent capable <ref type="bibr" target="#b24">[25]</ref>: i.e., they keep a finite state that is also updated in a manner that always converges back to a consistent state. Our schemes could be extended to other types of operators by recovering using a combination of persistent checkpoints and logging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis</head><p>We now discuss the main properties of our approach. To help us state these properties, we start with a few definitions.</p><p>A data source contributes to a stream, s, if it produces a stream that becomes s after traversing some sequence of operators, called a path. The union of paths that connect a set of sources to a destination (a client or an operator), forms a tree. A tree is valid if paths that traverse the same operator also traverse the same replica of that operator. A valid tree is stable if it contains all data sources that contribute to the stream received by the destination. A stable tree produces stable tuples during execution. If any of the missing sources from a tree would connect to it through non-blocking operators, the tree is tentative. Otherwise, the tree is blocking. <ref type="figure">Figure 6</ref> illustrates each type of tree. Property 1. In a static failure state, if there exists a stable tree, a destination receives stable tuples. If only tentative trees exist, the destination receives tentative tuples from one of the tentative trees. In both cases, the destination receives results within at most a kX time-unit delay, where X is the delay assigned to each SUnion operator and k is the number of SUnions on the longest path in the tree. In other cases, the destination may block.</p><p>The above property comes from the ability of downstream nodes to monitor and switch upstream neighbors, preferring stable ones over those in UP FAILURE state and those in UP FAILURE state over no input at all. We study the delay properties in Section 5, where we assume that the number Not valid <ref type="figure">Figure 6</ref>: Example trees for a query network with three sources, one client, a Union, and a Join. {s1, s2, s3} contributes to the stream received by c. Each operator has two replicas.</p><p>of SUnions is equal to the number of nodes. If this is not the case, the delay assigned to a node must be divided among the sequence of SUnions at the node.</p><p>Property 2. Switching between trees never causes duplicate results and may only lose tentative tuples.</p><p>We discuss this property by examining each possible neighbor-switching scenario:</p><p>1) Switching between stable upstream neighbors: Because the downstream node indicates the identifier of the last stable tuple it received, a new stable replica can continue from that point in the stream either by waiting to produce that tuple or replaying its output buffer.</p><p>2) Switching from a neighbor in UP FAILURE state to a stable upstream neighbor: In this situation, the downstream node indicates the identifiers of the last stable and tentative tuples it received. This allows the new upstream neighbor to stabilize the stream and continue with stable tuples.</p><p>3) Switching to an upstream neighbor in UP FAILURE state: Because nodes cannot undo stable tuples, the new upstream and downstream pair may have to continue processing tuples while in mutually inconsistent states, which can lead to duplicate or missing results. We choose to avoid duplications as this leads to fewer tentative tuples. We add a second timestamp, t max to tuples. t max of a tuple p is the tuple time of the most recent input tuple that affected p. The new upstream node forwards only output tuples that have a t max greater than the highest t max that the downstream node previously received. These tuples necessarily result from processing at least a partially non-overlapping sequence of input tuples. Other techniques are possible.</p><p>4) If an upstream neighbor is in the STABILIZATION state, a node treats the incoming stream as redundant information that serves to correct input streams in the background.</p><p>Property 3. As long as one replica of each processing node never fails, assuming all tuples produced during a failure are buffered, when all failures heal, the destination receives the complete stable stream.</p><p>After a failure heals, each node reconciles its state and stabilizes its output, letting its downstream neighbors correct their inputs and reconcile in turn. This process propagates all the way to the clients. Property 4. Stable tuples are never undone. We show that our approach handles failures during failures and recovery without the risk of undoing stable tuples.</p><p>Undo/redo reconciliation: As soon as an operator receives a tentative tuple, it starts labeling its output tuples as tentative. Therefore, undoing tentative tuples can never cause a stable output to be undone. When reconciling, SUnions produce undo tuples followed by the stable versions of tuples processed during the failure. Any new tentative input tuples will thus be processed after the undo and stable tuples such that any new failure will follow the reconciliation, without affecting it. While an undo tuple propagates on a stream, if a different input stream becomes tentative, and both streams merge at an operator, the operator could see the new tentative tuples before the undo tuple. In this case, when the operator finally processes the undo tuple, it rebuilds the state it had before the first failure and processes all tuples that it processed during that failure before going back to processing the new tentative tuples. The operator thus produces an undo tuple followed by stable tuples that correct the first failure, followed by the tentative tuples from the new failure. Once again, the new failure appears to occur after stabilization.</p><p>Checkpoint/redo: SOutput guarantees that stable tuples are never undone. When restarting from a checkpoint, SOutput enters a "duplicate elimination" mode. It remains in that state and continues waiting for the same last duplicate tuple until it produces the undo tuple, even if another checkpoint or recovery occurs. After producing the undo, SOutput goes back to its normal state, where it remembers the last stable tuple that it sees and saves the identifier of that tuple during checkpoints.</p><p>In both cases, if a new failure occurs before the node had time to catch up and produce a REC DONE tuple, SOutput forces a REC DONE tuple between the last stable and first tentative tuples that it sees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">IMPLEMENTATION</head><p>To implement our scheme in Borealis, in addition to inserting SUnion and SOutput operators into query networks, we add a Consistency Manager and an HA ("high availability") component to each SPE node. HA monitors all the replicas of a node and those of its upstream neighbors. It informs the query processor of changes in the states of their outputs. To modify the data path, nodes send each other subscribe and unsubscribe messages.</p><p>The Consistency Manager makes all decisions related to failure handling. In STABLE state, it periodically requests that the SPE checkpoints the state of the query network. When the node must reconcile its state, the Consistency Manager asks a partner to suspend its own reconciliation and chooses whether to use undo/redo or checkpoint/redo. For undo/redo, the Consistency Manager injects UNDO START tuples on input streams of affected SUnion operators. For checkpoint/redo, the Consistency Manager requests that the SPE performs checkpoint recovery.</p><p>In addition to their tasks described in previous sections, SUnion and SOutput communicate with the Consistency Manager through extra control output streams. When an SUnion can no longer delay tuples, it informs the Consistency Manager about the UP FAILURE, by producing an UP FAILURE tuple on its control stream. Similarly, when input streams are corrected and the node can reconcile its state, SUnion produces a REC REQUEST tuple. Once reconciliation finishes, SOutput forwards a REC DONE tuple on its control and output streams.</p><p>We also require operators to implement a simple API. For checkpoint/redo, operators need the ability to take snapshots and recover their state ((un)packState methods). For undo/redo, operators must be able to correctly process undo tuples. At runtime, they must compute stream markers and remember the last tuple they output. This functionality can be implemented with a wrapper, requiring that the operator itself only implements two methods: clear() clears the operator's state and findOldestTuple(int stream id) returns the oldest tuple from input stream, stream id, that is currently in the operator's state. To propagate boundary tuples, operators must implement the method findOldestTimestamp() that returns the oldest timestamp that the operator can still produce. This value is typically the smaller of the oldest timestamp present in the operator's state and the oldest timestamp in the boundary tuples received on all input streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>In this section, we evaluate the performance of our faulttolerance protocol through experiments with our prototype implementation. All single-node experiments were performed on a 3 GHz Pentium IV with 2 GB of memory running Linux (Fedora Core 2). Multi-node experiments were performed by running each pair of node replicas on a different machine. All machines were 1.8 GHz Pentium IV's or faster with greater than 1 GB of memory.</p><p>Our basic experimental setup is the following. We run a query network composed of three input streams, an SUnion that merges these streams into one, a Join that serves as a generic query network with a 100 tuple state size, and an SOutput. The aggregate input rate is 3000 tuples/s. We create a failure by temporarily disconnecting one of the input streams without stopping the data source. After the failure, we send all missing tuples while continuing to stream new tuples. X is 3 s. α is 0.9 (so αX is 2.7 s). Each result is an average of at least three experiments.</p><p>We first examine the performance of a single Borealis node in the face of temporary failures of its input streams. In particular, we compare in terms of Delay new and Ntentative different strategies regarding suspending, delaying, and processing new tuples during UP FAILURE and STABILIZATION. As we point out, some combinations are unviable as they break the availability requirement for sufficiently long failures. In these experiments, the node uses checkpoint/redo to reconcile its state. Second, we examine the performance of our approach when failures and reconciliation propagate through a sequence of processing nodes. Third, we compare the undo/redo and checkpoint/redo reconciliation tech- niques. We finally discuss the overhead of our approach.</p><p>In our prototype, it takes a node approximately 40 ms to switch between upstream neighbors. Given that this value is small compared with αX, our system masks node failures within the required availability constraints. We thus focus the evaluation on failures of input streams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single-Node Performance</head><p>The optimal approach to handling failures shorter than αX is to delay processing new tuples until the failure heals. This is therefore always our first line of defense. When a failure exceeds αX, however, a node must restart processing new tuples to satisfy the availability requirement. It can either continuously delay new tuples by αX or catch-up and process new tuples almost as they arrive. We call these alternatives Delay and Process and examine their impact on Delay new and Ntentative.</p><p>We cause a 5 s failure, vary αX from 500 ms to 6 s, and observe Delay new and Ntentative until after STABILIZATION completes. <ref type="figure" target="#fig_9">Figure 9</ref> shows the results. From the perspective of our optimization, Delay appears better than Process as it leads to fewer tentative tuples. Indeed, with Process, as soon as the initial delay is small compared with the failure duration (αX ≤ 4 s for a 5 s failure), the node has time to catch-up and produces a number of tentative tuples almost proportional to the failure duration. The Ntentative graph approximates a step function. In contrast, Delay reduces the number of tentative tuples proportionally to αX. With both approaches, Delay new increases linearly with αX.</p><p>For sufficiently long failures, however, reconciliation itself may last longer than X. To avoid breaking the availability requirement, a node must thus continue processing new tuples while reconciling. It can do so in one of several ways. During the failure, the node can either delay new tuples (Delay) or process them without delay (Process). During STABI-LIZATION the node can either suspend new tuples (Suspend), or have a second version of the SPE continue processing them with or without delay (Delay or Process). Our goal is to examine all six combinations and determine the failure durations when each one produces the fewest tentative tuples without breaking the availability requirement. <ref type="figure" target="#fig_0">Figure 10</ref> shows Delay new and Ntentative for each combination and for increasing failure durations. We only show results for failures up to 1 minute. Longer experiments continue the same trends. In this experiment, we increase the input rate to 4500 tuples/s to emphasize differences between approaches.   Because blocking is optimal for short failures, all approaches block for αX = 2.7 s and produce no tentative tuples for failures below this threshold. Delaying tuples in UP FAILURE and suspending them during STABILIZATION (Delay &amp; Suspend) is unviable for failures longer than 3 s because it breaks the Delay new &lt; X requirement as reconciliation last longer than 300 ms. <ref type="figure" target="#fig_0">(Figure 10(top)</ref>). Therefore, this combination is of no interest because it never wins and cannot be used for long failures.</p><p>Continuously processing new tuples during both UP FAILURE and STABILIZATION <ref type="bibr">(Process &amp; Process)</ref> ensures that the maximum delay always remains below αX independently of failure duration. This combination, however, produces the most tentative tuples as it produces them for the duration of the whole failure and reconciliation. We can reduce the number of tentative tuples without hurting Delay new , by delaying new tuples during STABILIZATION (Process &amp; Delay), during UP FAILURE, or in both states <ref type="bibr">(Delay &amp; Delay)</ref>. Hence to meet the availability requirement for longer failures, nodes must process new tuples not only during UP FAILURE but also during STABILIZATION. Nodes can produce fewer tentative tuples, however, by always running on the verge of breaking that requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multiple Nodes</head><p>We now examine which of the above combinations meets the required availability while producing the fewest tentative tuples in a distributed SPE. We cause a 15 second failure at the input of a chain of 1 to 4 SPEs. Once the failure heals, the nodes reconcile their states in sequence: a node produces boundary tuples only after it goes back into STABLE state, while its downstream neighbors can start reconciling only after receiving these boundary tuples. We reduce the state of the joins to 50 tuples to speed-up the experiments. <ref type="figure" target="#fig_0">Figure 11</ref>(top) shows the maximum end-to-end processing delay for new tuples. The Process &amp; Process combination has the lowest Delay new . The delay is equal to only αX plus the normal processing delay through the chain. Delay &amp; Delay leads to a slightly worse availability as Delay new increases by αX for each node in the sequence. Both combinations, however, keep the end-to-end delay within the required kX, where k is the number of nodes in the chain. Process &amp; Suspend once again is clearly unviable. Delay new is the sum of the stabilization delays of all nodes in the chain. This delay increases for each consecutive node as it undoes and redoes more tuples than its upstream neighbor. <ref type="figure" target="#fig_0">Figure 11</ref>(bottom) shows Ntentative received by the client application. With Process &amp; Process, Ntentative increases with the length of the chain because all nodes produce tentative tuples during STABILIZATION, which occurs in sequence at each node. Interestingly, Delay &amp; Delay not only does not provide any benefit but can even hurt when compared with no delay. Indeed, when STABILIZATION starts, each consecutive node in the sequence runs behind by αX more than its upstream neighbor. When that neighbor stabilizes, both downstream replicas receive all tuples until the most recent ones. Because the replica that continues processing new tuples is only supposed to delay new tuples by αX, it catches up and it does so while processing significantly more tuples than the savings during UP FAILURE.</p><p>Overall, for a chain of nodes, Process &amp; Process is clearly the best approach as it maximize availability and produces the fewest tentative tuples.   </p><formula xml:id="formula_0">S + (l + D)λ in Undo P + S(pcomp + pproc) + (D + 0.5l)λ(pcomp + pproc) Spcomp l S + (l + D)λ stateful</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Reconciliation</head><p>We now compare the overhead and performance of checkpoint/redo and undo/redo reconciliation. Overheads due to SUnion operators are examined in the next section. <ref type="table" target="#tab_3">Table 2</ref> summarizes the analytical results. P is the per-node processing delay. pcomp is the time to read and compare a tuple. pcopy is the time to copy a tuple. pproc is the time an operator takes to process a tuple. We assume pproc is constant but it may increase with operators' state sizes.</p><p>Delay new is the normal processing delay, P , plus the reconciliation time. For checkpoint/redo, the reconciliation time is the sum of Spcopy, the time to copy the state with size S, and (D + 0.5l)λpproc, the average time to reprocess all tuples since the last checkpoint before failure. D is the failure duration, l is the interval between checkpoints, and λ is the aggregate tuple rate on all input and intermediate streams.</p><p>For undo/redo, reconciliation consists of processing the undo history up to the correct stream markers and reprocessing all tuples since then. Producing an undo message takes a negligible time. We assume that the number of tuples necessary to rebuild an operator state is equal to the state size and that stream markers are computed ever l time units. The number of tuples in the undo log that must be processed backward then forward is thus: (D + 0.5l)λ + S. Hence, we expect checkpoint/redo to perform better but the difference should appear only for a large query network state size. <ref type="figure" target="#fig_0">Figure 12</ref> shows the experimental Delay new as we increase the state size, S, of the query network (left) or the number of tuples to re-process i.e., Dλ (middle). In this experiment, D is 5 seconds and we vary λ. For both approaches, the time to reconcile increases linearly with S and Dλ. When we vary the state size, we keep the tuple rate low at 1000 tuples/s. When we vary the tuple rate, we keep the state size at only 20 tuples.</p><p>Undo/redo takes longer to reconcile primarily because it must rebuild the state of the query network (Spproc) rather than recopy it (Spcopy), as shown in <ref type="figure" target="#fig_0">Figure 12(left)</ref>. Interestingly, even when we keep the state size small and vary the number of tuples to reprocess <ref type="figure" target="#fig_0">(Figure 12(middle)</ref>), checkpoint/redo beats undo/redo, while we would expect the approaches to perform the same (∝ (D + 0.5l)λpproc). The difference is not due to the undo history because when we do not buffer any tentative tuples in the undo buffer (Undo "limited history" curve), the difference remains. In fact, an SPE always blocks for αX (1 s in this experiment) before going into UP FAILURE. For checkpoint/redo, because we checkpoint every 200 ms, we always checkpoint the prefailure state and avoid reprocessing on average 0.5lλ tuples, which corresponds to tuples that accumulate between the checkpoint and the beginning of the failure. Undo/redo always pays this penalty, as stream markers are computed only when the join processes new tuples.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 12</ref>(left and middle), for both approaches, splitting the state across two operators in series (curves labeled "2 boxes"), simply doubles λ and increases curve slopes.</p><p>In theory, checkpoint/redo has higher CPU overhead than undo/redo because checkpoints are more expensive than scanning the state of an operator to compute stream markers <ref type="figure" target="#fig_0">(Figure 12(right)</ref>). However, because a node has time to checkpoint its state when going into UP FAILURE state, it can perform checkpoints only at that point and avoid the  <ref type="table">Table 3</ref>: Latency overhead of serialization. overhead of periodic checkpoints at runtime. Stream markers can also be computed only once a failure occurs. Hence both schemes can avoid overhead in the absence of failures.</p><p>Given that we checkpoint the state when entering UP FAILURE, l = 0. Hence, the memory overhead for checkpoint/redo is only S + Dλin, the state size plus the input tuples that accumulate during the failure (λin is the aggregate input rate). Even if we assume that we need no more than S tuples to rebuild the state, the memory overhead for undo/redo is higher because we need to buffer tuples on all streams that feed stateful operators. λ stateful will most frequently be significantly greater than λin.</p><p>Checkpoint/redo thus appears superior to undo/redo both in terms of reconciliation time and memory overhead. The main advantage of the undo-based approach, however, is the flexibility to undo any suffix of the input streams and propagate reconciliation only on paths affected by failures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Overhead and Scalability</head><p>In addition to undo and checkpoint overheads, SUnions are the main cause of overhead. If the sorting function requires the operator to wait until a bucket is stable before processing tuples, the processing delay increases linearly with the boundary tuple interval (we assume this interval is equal to the bucket size). <ref type="table">Table 3</ref> shows the average end-to-end delay from nine 20 s experiments and increasing bucket sizes. The memory overhead increases proportionally to the number of SUnion operators, bucket sizes, and the rate of tuples that arrive into each SUnion.</p><p>Other overheads imposed by our scheme are negligible. Operators must check tuple types and must process boundary tuples. The former is negligible while the latter is equivalent to the overhead of computing stream markers. SOutput must also save the last stable tuple that it sees in every burst of tuples that it processes.</p><p>Our approach relies on replication. It increases resource utilization proportionally to the number of replicas. These replicas, however, can actually improve runtime performance by forming a content distribution network, where clients and nodes connect to nearby upstream neighbors rather than a single, possibly remote, location.</p><p>In this paper, we assume that tuples produced during failure and recovery are logged in output buffers and inside SUnions on input streams. Under normal operation, a node can truncate its output buffers once all replicas of all downstream neighbors acknowledge either receiving or fully processing a prefix of tuples. Both techniques are acceptable. As discussed in <ref type="bibr" target="#b24">[25]</ref>, acknowledging only processed tuples has the advantage that input tuples necessary to rebuild the latest consistent state are stored at upstream neighbors, which speeds-up recovery of failed nodes. A similar approach can be used to truncate buffers during failures, preserving only enough tuples to rebuild the latest consistent state and correct the most recent tentative tuples. To truncate buffers, a node must hear at least from one downstream replica during a failure. Otherwise, a node may have to use conservative estimates to truncate its buffers.</p><p>In this paper, we assume that operators are convergentcapable but our techniques can be extended to support arbitrary operators. For such operators, however, when sufficiently long failures occur, the system must either drop tuples at system input, or replicas must communicate with each other to reach a mutually consistent state after failures heal. We plan to explore such extensions in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Until now, work on high availability in stream processing systems has focused on fail-stop failures of processing nodes <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35]</ref>. These techniques either do not address network failures <ref type="bibr" target="#b24">[25]</ref> or strictly favor consistency by requiring at least one fully connected copy of the query network to exist to continue processing <ref type="bibr" target="#b34">[35]</ref>. Some techniques use punctuation <ref type="bibr" target="#b40">[41]</ref>, heartbeats <ref type="bibr" target="#b35">[36]</ref>, or statically defined slack <ref type="bibr" target="#b0">[1]</ref> to tolerate bounded disorder and delays. These approaches, however, block or drop tuples when disorder or delay exceed expected bounds. Another approach, developed for publishsubscribe systems tolerates failures by restricting all processing to "incremental monotonic transforms" <ref type="bibr" target="#b36">[37]</ref>.</p><p>Traditional query processing also addresses trade-offs between result speed and consistency, materializing query outputs one row or even one cell at the time <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b33">34]</ref>. In contrast to these schemes, our approach supports possibly infinite data streams and ensures that once failures heal all replicas produce the same final output streams in the same order.</p><p>Fault-tolerance through replication is widely studied and it is well known that it is not possible to provide both consistency and availability in the presence of network partitions <ref type="bibr" target="#b10">[11]</ref>. Eager replication favors consistency by having a majority of replicas perform every update as part of a single transaction <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> but it forces minority partitions to block. With lazy replication all replicas process possibly conflicting updates even when disconnected and must later reconcile their state. They typically do so by applying system-or user-defined reconciliation rules <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref>, such as preserving only the most recent version of a record <ref type="bibr" target="#b21">[22]</ref>. It is unclear how one could define such rules for an SPE and reach a consistent state. Other replication approaches use tentative transactions during partitions and reprocess transactions possibly in a different order during reconciliation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. With these approaches, all replicas eventually have the same state and that state corresponds to a single-node serializable execution. Our approach applies the ideas of tentative data to stream processing.</p><p>Some schemes offer users fine-grained control over the trade-off between precision (or consistency) of query results and performance (i.e., resource utilization) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. In contrast, we explore consistency/availability trade-offs in the face of failures and ensure eventual consistency.</p><p>Workflow management systems (WFMS) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24]</ref> share similarities with stream processing engines. Existing WFMSs, however, typically commit the results of each execution step (or messages these steps exchange) in a central highly-available storage server <ref type="bibr" target="#b25">[26]</ref> or in persistent queues <ref type="bibr" target="#b3">[4]</ref>. Some approaches allow replication of the central data server using standard lazy replication <ref type="bibr" target="#b3">[4]</ref>. They support disconnection by locking activities prior to disconnection <ref type="bibr" target="#b4">[5]</ref>.</p><p>Approaches that reconcile state after a failure using combinations of checkpoints, undo, and redo are well known <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b38">39]</ref>. We adapt and use these techniques in the context of fault-tolerance and state reconciliation in an SPE and comparatively evaluate their overhead and performance in these environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We presented a replication-based approach to faulttolerant stream processing that handles node failures, network failures, and network partitions. Our approach uses a new data model that distinguishes between stable tuples and tentative tuples, which result from processing partial inputs and may later be corrected. Our approach favors availability but guarantees eventual consistency. Additionally, while ensuring that each node processes new tuples within a predefined delay, X, our approach reduces the number of tentative tuples, when possible. To ensure consistency at runtime, we introduce a data-serializing operator called SUnion. To regain consistency after failures heal, nodes reconcile their states using either checkpoint/redo or undo/redo.</p><p>We implemented the approach in Borealis and showed several experimental results. For short failures, SPE nodes can avoid inconsistency by blocking and looking for a stable upstream neighbor. For long failures, nodes need to process new inputs both during failure and stabilization to ensure the required availability. Checkpoint/redo leads to a faster reconciliation at a lower cost compared with undo/redo.</p><p>Many stream processing applications prefer approximate results to long delays but eventually need to see the correct output streams. It is important that failure-handling schemes meet this requirement. We view this work as an important first step in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>We thank Mehul Shah and Jeong-Hyon Hwang for helpful discussions. This material is based upon work supported by the National Science Foundation under Grant No. 0205445. M. Balazinska is supported by a Microsoft Fellowship.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Query network in a distributed SPE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Borealis state machine. The latter two options both produce result tuples that are marked "tentative;" the difference between the options is in the latency of results and the number of tentative tuples produced. Section 3.3 discusses the UP FAILURE state. A failure heals when a previously unavailable upstream neighbor starts producing stable tuples again or when a node finds another replica of the upstream neighbor that can provide the stable version of the stream. Once a node receives the stable versions of all previously missing or tentative input tuples, it transitions into the STABILIZATION state. In this state, if the node processed any tentative tuples during UP FAILURE it must now reconcile its state and stabilize its outputs. We explore two approaches for state reconciliation: a checkpoint/redo scheme and an undo/redo scheme. While reconciling, new input tuples are likely to continue to arrive. The node has the same three options mentioned above for processing these tuples: suspend, delay, or process without delay. Our approach enables a node to reconcile its state and correct its outputs, while ensuring that new tuples continue to be processed. We discuss the STABILIZATION state in Section 3.4. Once stabilization completes, the node transitions to the STABLE state if there are no other current failures, or back to the UP FAILURE state otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of using tentative and undo tuples. U2 indicates that all tuples following tuple with tuple id 2 (S2 in this case) should be undone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of serialization of streams s1, s2, and s3 with boundary interval d. The t's denote tentative inserts and b's denote boundary tuples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of replicated SPEs. Rij is the j'th replica of processing node i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Figures 7 and 8 il- lustrate these modifications (arrows indicate communication between components).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 7: Modified query network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Delaying tuples during UP FAILURE reduces Ntentative. Y-Axes: left Delay new , right Ntentative. Failure duration: 5 s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Delay new (top) and Ntentative (bottom) for each combination of delaying, processing, and suspending during UP FAILURE and STABILIZATION. Each approach offers a different consistency-availability trade-off. X-axis starts at 2 s. Graphs on the right show results for longer failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>For short failures, however, Process &amp; Suspend may win over Delay &amp; Delay. If reconciliation is longer than αX (for D &gt; 6 s in the experiment), Process &amp; Suspend produces fewer tentative tuples. It is thus better for such failures to process tuples during the failure in order to suspend new tuples during reconciliation. Once reconciliation becomes longer than X, though (for D &gt; 9 s), Process &amp; Suspend causes Delay new to exceed X. Hence Process &amp; Suspend outperforms Delay &amp; Delay only for failures between 6 and 9 s, which is a small, barely significant window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :Figure 11 :</head><label>1211</label><figDesc>Figure 12: Performance and overhead of checkpoint/redo and undo/redo reconciliations. Delay new for increasing state size (left). Delay new for increasing failure size starting at 5000 tuples (middle). CPU Overhead (right). Checkpoint/redo is faster than undo/redo but checkpoints can be expensive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Types of tuples undo tuples. We use a few additional tuples types in our approach but they do not fundamentally change the data model. Table 1 summarizes the new tuple types.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 : Performance and overhead of checkpoint/redo and undo/redo reconciliations.</head><label>2</label><figDesc></figDesc><table>0 

1 

2 

3 

4 

0 
0.5 
1 
1.5 
2 
2.5 
3 
3.5 
4 
Max. proc. delay (seconds) 

State size (thousand tuples) 

Undo (2 boxes) 
Checkpoint (2 boxes) 
Undo 
Checkpoint 

0 

1 

2 

3 

4 

5 

5 
10 
15 
20 
25 
30 
35 
40 
45 
Max. proc. delay (seconds) 

Failure size (thousand tuples) 

Undo (2 boxes) 
Checkpoint (2 boxes) 
Undo 
Undo (limited history) 
Checkpoint 

0 

100 

200 

300 

400 

500 

600 

0 
0.5 
1 
1.5 
2 
2.5 
Duration (microseconds) 

State size (thousand tuples) 

Checkpoint 
Copy state 
Compute stream markers 

</table></figure>

			<note place="foot" n="1"> If a data source cannot set these values, the first processing node to see the data can act as a proxy for the data source, setting tuple headers and producing boundary tuples.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new model and architecture for data stream management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The design of the Borealis stream processing engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The design of the Borealis stream processing engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abadi</surname></persName>
		</author>
		<idno>CS-04-08</idno>
		<imprint>
			<date type="published" when="2005-01" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Brown University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">WFMS: The next generation of distributed processing tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Transaction Models and Architectures</title>
		<editor>S. Jajodia and L. Kerschberg</editor>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FMQM: A persistent message-based architecture for distributed workflow management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alonso</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IFIP WG8.1 Working Conf. on Information Systems for Decentralized Organizations</title>
		<meeting>of IFIP WG8.1 Working Conf. on Information Systems for Decentralized Organizations</meeting>
		<imprint>
			<date type="published" when="1995-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The CQL continuous query language: Semantic foundations and query execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
		<idno>2003-67</idno>
		<imprint>
			<date type="published" when="2003-10" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eddies: continuously adaptive query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Avnur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2000-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chain : Operator scheduling for memory minimization in data stream systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Models and issues in data stream systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Implementing recoverable requests using queues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1990-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lessons from giant-scale services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Operator scheduling in a data stream manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th VLDB</title>
		<imprint>
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Remembrance of streams past: Overload-sensitive management of archived streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th VLDB</title>
		<imprint>
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TelegraphCQ: Continuous dataflow processing for an uncertain world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable distributed stream processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cherniack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gigascope: A stream database for network applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cranor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shkapenyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Spatscheck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Approximate join processing over data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey of rollback-recovery protocols in message-passing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">N M</forename><surname>Elnozahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Alvisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="408" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Measuring the Effects of Internet Path Faults on Reactive Routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feamster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SigmetricsPerformance</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">How to assign votes in a distributed system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barbara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="841" to="860" />
			<date type="published" when="1985-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weighted voting for replicated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Gifford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th SOSP</title>
		<imprint>
			<date type="published" when="1979-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The dangers of replication and a solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Helland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="1996-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Transaction processing: concepts and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reuters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Special issue on workflow systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bulletin</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1995-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">High-availability algorithms for distributed stream processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balazinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
	<note>In 21st ICDE</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Providing high availability in very large workflow management systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guenthor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Int. Conf. on Extending Database Technology</title>
		<imprint>
			<date type="published" when="1996-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Replicated document management in a group communication system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kawell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second CSCW</title>
		<imprint>
			<date type="published" when="1988-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Query languages and data models for database sequences and data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th VLDB</title>
		<imprint>
			<date type="published" when="2004-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A theory of redo recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tuttle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Query processing, approximation, and resource management in a data stream management system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2003-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Niagara Internet query system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Naughton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bulletin</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2001-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Approximate Replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive filters for continuous queries over distributed data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Partial results for online query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2002-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Highly-available, fault-tolerant, parallel dataflows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Flexible time management in data stream systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd PODS</title>
		<imprint>
			<date type="published" when="2004-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fault-tolerance in the SMILE stateful publish-subscribe system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DEBS</title>
		<imprint>
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Load shedding in a data stream manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zdonik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cherniack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stonebraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th VLDB</title>
		<imprint>
			<date type="published" when="2003-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Managing update conflicts in Bayou, a weakly connected replicated storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th SOSP</title>
		<imprint>
			<date type="published" when="1995-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">NTP: The Network Time Protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ntp</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Project</surname></persName>
		</author>
		<ptr target="http://www.ntp.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dealing with disorder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MPDS</title>
		<imprint>
			<date type="published" when="2003-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Oracle Streams Replication Administrator&apos;s Guide, 10g Release 1 (10.1). Oracle Corporation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urbano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
