<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effectively Mitigating I/O Inactivity in vCPU Scheduling Effectively Mitigating I/O Inactivity in vCPU Scheduling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 11-13. 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xusheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchen</forename><surname>Shan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Shang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Facebook;</roleName><forename type="first">Luwei</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexuan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of HongKong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xusheng</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchen</forename><surname>Shan</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of HongKong</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<addrLine>3 Facebook 4 Huawei</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Shang</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<addrLine>3 Facebook 4 Huawei</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Cui</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of HongKong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoning</forename><surname>Ding</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">New Jersey Institute of Technology</orgName>
								<address>
									<addrLine>3 Facebook 4 Huawei</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luwei</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">C M</forename><surname>Lau</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of HongKong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of HongKong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuangang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Technology; Heming Cui</orgName>
								<orgName type="institution" key="instit1">New Jersey Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
								<address>
									<country key="JE">Jersey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">The University of Hong</orgName>
								<address>
									<addrLine>Kong; Xiaoning Ding</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">New Jersey Institute of Technology</orgName>
								<orgName type="institution" key="instit2">The University of Hong Kong</orgName>
								<address>
									<addrLine>Yuangang Wang</addrLine>
									<settlement>Huawei</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effectively Mitigating I/O Inactivity in vCPU Scheduling Effectively Mitigating I/O Inactivity in vCPU Scheduling</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18)</title>
						<meeting>the 2018 USENIX Annual Technical Conference (USENIX ATC &apos;18) <address><addrLine>Boston, MA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">July 11-13. 2018</date>
						</imprint>
					</monogr>
					<note>Open access to the Proceedings of the 2018 USENIX Annual Technical Conference is sponsored by USENIX. This paper is included in the</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In clouds where CPU cores are time-shared by virtual CPUs (vCPU), vCPUs are scheduled and de-scheduled by the virtual machine monitor (VMM) periodically. In each virtual machine (VM), when its vCPUs running I/O bound tasks are desched-uled, no I/O requests can be made until the vC-PUs are rescheduled. These inactivity periods of I/O tasks cause severe performance issues, one of them being the utilization of I/O resources in the guest OS tends to be low during I/O inactivity periods. Worse, the I/O scheduler in the host OS could suffer from low performance because the I/O sched-uler assumes that I/O tasks make I/O requests constantly. Fairness among the VMs within a host can also be at stake. Existing works typically would adjust the time slices of vCPUs running I/O tasks, but vCPUs are still descheduled frequently and cause I/O inactivity. Our idea is that since each VM often has active vCPUs, we can migrate I/O tasks to active vC-PUs, thus mitigating the I/O inactivity periods and maintaining the fairness. We present VMIGRATER, which runs in the user level of each VM. It incorporates new mechanisms to efficiently monitor active vCPUs and to accurately detect I/O bound tasks. Evaluation on diverse real-world applications shows that VMIGRATER can improve I/O performance by up to 4.42X compared with default Linux KVM. VMIGRATER can also improve I/O performance by 1.84X to 3.64X compared with two related systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To ease management and save energy in clouds, multiple VMs are often consolidated on a physical host. In each VM, multiple vCPUs often time-share a physical CPU core (aka. pCPU). The VMM controls the sharing by scheduling and descheduling the vCPUs periodically. When a vCPU is scheduled, tasks running on it become active and make progress. When a vCPU depletes its time slice, it is descheduled, and tasks on it become inactive and stop making progress.</p><p>vCPU inactivity leads to a severe I/O inactivity problem. After the vCPU is descheduled, the I/O tasks on it become inactive and cannot generate I/O requests, as shown in the first two curves in Figure 1. The inactive periods can be much longer than the latencies of storage devices. Typical time slices can be tens of milliseconds; the storage device latencies are a few milliseconds for HDDs and microseconds for SSDs. Thus, during the I/O inactive periods, I/O devices (both physical and virtual devices) may be under-utilized. The underutilization becomes more serious with a higher consolidation rate (i.e., the number of vCPUs shared on each pCPU), because a vCPU may need to wait for multiple time slices before being rescheduled. The I/O throughput of a VM drops significantly with a consolidation rate of 8 as recommended by VMware <ref type="bibr" target="#b33">[47]</ref>, based on our evaluation in Section 5.</p><p>The I/O inactivity problem becomes even more pronounced when I/O requests are supposed to be processed by fast storage devices (e.g., SSDs). Usually, a vCPU remains active during I/O requests, so it can quickly process them. A similar situation is when a computation task and an I/O task run on the same vCPU. When the I/O task issues a read request and then waits for the request to be satisfied, the computation task is switched on. At this moment, the vCPU is still running; thus, when the read request is satisfied, the vCPU can quickly respond to the event and switch the I/O task back. However, if the time slice of the vCPU is used up by the computation task in one scheduling period, the I/O task cannot proceed until the next period, causing the I/O task to be slowed down significantly.</p><p>Worse, the I/O inactivity problem causes the I/O scheduler running in the host OS to work extremely ineffectively. To fully utilize the storage devices, based on the latencies of I/O devices, system designs would carefully control the factors affecting the latencies experienced by I/O workloads (e.g., wake-up latencies and priorities). Thus, I/O workloads running on bare-metal can issue the next request after the previous request is finished. I/O inactive periods make these mechanisms ineffective. Moreover, non-work-conserving I/O schedulers <ref type="bibr" target="#b29">[40]</ref> would often hold an I/O request until the next request from the same I/O task comes in (refer to §2.2). By serving the requests from the same task continuously, which have better locality than requests from different tasks, such I/O schedulers <ref type="bibr" target="#b29">[40]</ref> can improve I/O throughputs. However, since an I/O workload cannot continue to issue I/O requests after its vCPU becomes inactive, the I/O scheduler in the host OS must switch to serve the requests from other I/O tasks, which greatly reduces locality and I/O throughput, as we will show in our evaluation.</p><p>Last but not least, the I/O throughput of a VM can be "capped" by its amount of CPU resources. If the vCPUs in a VM (V M a ) are assigned with smaller proportions of CPU time on each pCPU than the vCPUs on another VM (V M b ), the I/O workloads on V M a will get less time to issue I/O requests and may only be able to occupy a smaller proportion of the available I/O bandwidth. Since the actual I/O throughputs of the VMs are affected by both I/O scheduling and vCPU scheduling, it is difficult for the I/O scheduler to ensure fairness between the VMs.</p><p>All the above problems share the same root cause, I/O inactivity, and existing works mainly try to curb vCPU inactivity but ignore this root cause. Existing works primarily follow two approaches: 1) shortening vCPU time slices <ref type="bibr">(vSlicer [49]</ref>); and 2) assigning higher priority to I/O tasks running on active vCPUs (xBalloon <ref type="bibr" target="#b31">[44]</ref>). Unfortunately, vCPUs with either approach are still descheduled frequently and cause I/O inactivity.</p><p>Since a VM often has active vCPUs, our idea to mitigate I/O inactivity is to try to efficiently migrate I/O tasks to active vCPUs. By evenly redistributing I/O tasks to active vCPUs in a VM, I/O inactivity can be greatly mitigated and I/O tasks can make progress constantly. This maintains both performance and fairness for I/O tasks as they are running on bare-metal. The fairness of I/O bandwidth among VMs on the same host is also maintained.</p><p>We implement our idea in VMIGRATER, a user level tool working in each VM. It is transparent as it does not need to modify application, OS in VM, or VMM. VMIGRATER carries simple and efficient mechanisms to predict whether a vCPU will be descheduled and to migrate the I/O tasks on this vCPU to another active vCPU.</p><p>VMIGRATER adds only small overhead to applications for two reasons. First, I/O bound tasks use little CPU time, so the I/O tasks migrated by VMIGRATER hardly affect the co-running tasks on the active vCPUs. Second, VMIGRATER migrates more I/O bound tasks to the active vCPUs with more remaining time slices, so all vCPUs' loads in the same VM are well balanced. By reducing I/O inactivity with low overhead, VMIGRATER makes applications run in a fashion similar to what they do on bare-metal, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. VMIGRATER has to address three practical issues. First, it needs to identify I/O tasks. To address this issue, VMIGRATER uses an event-driven model to collect I/O statistics and to detect I/O bound tasks quickly. Second, VMIGRATER needs to determine when an I/O bound task should be migrated. To minimize overhead, VMIGRATER only migrates an I/O bound task when the vCPU running this task is about to be descheduled. VMIGRATER monitors each vCPU's time slice and uses the length of the previous time slice to predict the length of the current time slice. Third, VMIGRATER needs to decide where a task should be migrated to keep it active. Based on the collected time slice and I/O task information, VMIGRATER migrates I/O tasks from to-bedescheduled vCPUs to the active vCPUs with light workload.</p><p>We implemented VMIGRATER in Linux and evaluated it on KVM <ref type="bibr">[30]</ref> with a collection of microbenchmarks and 7 widely used or studied programs, including small programs (sequential, random and bursty read) from SysBench <ref type="bibr" target="#b5">[7]</ref>, a distributed file system HDFS <ref type="bibr" target="#b3">[5]</ref>, a distributed database Hbase <ref type="bibr" target="#b1">[2]</ref>, a mail server benchmark PostMark <ref type="bibr" target="#b4">[6]</ref>, a database management system LevelDB <ref type="bibr" target="#b2">[3]</ref>, and a documentoriented database program MongoDB <ref type="bibr" target="#b26">[35]</ref>. Our evaluation shows that:</p><p>1. <ref type="bibr">VMIGRATER</ref>  The paper makes the following contributions. First, the paper identifies I/O inactivity as a major factor degrading I/O throughputs in VMs, and quantifies the severity of the problem. Second, it designs VMIGRATER, a simple and practical user-level solution, which greatly improves the throughput of I/O applications in VMs. Third, VMIGRATER is implemented in Linux, and is evaluated extensively to demonstrate its effectiveness.</p><p>The remainder of this paper is organized as follows. §2 introduces the background and motivation of VMIGRATER. §3 presents the design principles, architecture, and other design details of VMIGRATER. §4 describes implementation details. §5 presents evaluation results. §6 introduces related work, and §7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Motivation</head><p>This section first introduces vCPU scheduling ( §2.1) and I/O request scheduling ( §2.2) as the background. Then it explains three performance problems caused by I/O inactivity in virtualized systems ( §2.3) to motivate our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">vCPU Scheduling</head><p>To improve resource utilization in virtualized systems, a pCPU is usually time-shared by multiple vCPUs. A vCPU scheduler is used to periodically deschedule a vCPU and schedule another vCPU. For instance, KVM uses completely fair scheduler (CFS) <ref type="bibr">[13,</ref><ref type="bibr" target="#b31">44]</ref> to schedule vCPUs onto pCPUs. CFS uses virtual runtime (vruntime) to keep track of the CPU time used by each vCPU and to make scheduling decisions. With a red-black tree, it sorts vCPUs based on their vruntime values, and periodically schedules the vCPU with the smallest vruntime value. In this way, CFS distributes time slices to vCPUs in a fair way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">I/O Request Scheduling</head><p>I/O requests are scheduled by the I/O scheduler in the VMM. There are two types of I/O schedulers: work-conserving schedulers <ref type="bibr">[19,</ref><ref type="bibr">38]</ref> and non-workconserving schedulers <ref type="bibr" target="#b37">[51,</ref><ref type="bibr" target="#b18">27]</ref>. A work-conserving I/O scheduler always keeps the I/O device busy by scheduling pending I/O requests as soon as possible.</p><p>Non-work-conserving I/O schedulers, such as anticipatory scheduler (AS) <ref type="bibr" target="#b18">[27]</ref> and Completely Fair Queuing (CFQ) <ref type="bibr">[12]</ref>, are now widely used. A nonwork-conserving scheduler waits for a short period after scheduling a request from a task, expecting that other requests from the same task may arrive. Because requests from the same task usually show good locality (i.e., requesting the data at the locations close to each other on the disk), if there are requests from the same task arriving, the scheduler may choose to schedule these requests, even when there are requests from other tasks arriving earlier. It switches to serve the requests from other tasks when the waiting period expires and there are not requests from the same task. Compared to work-conserving I/O schedulers, non-work-conserving schedulers can improve I/O throughput by exploiting locality. The length of waiting periods is selected to balance improved locality and the utilization of I/O devices. To enforce fairness between I/O tasks, an I/O request scheduler controls the distribution of disk time among the tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Performance Issues Caused by I/O Inactivity</head><p>We use experiments to show that serious performance issues will be caused by I/O inactivity. Specifically, we use SysBench <ref type="bibr" target="#b5">[7]</ref> to test I/O throughput in three settings. In the Bare-metal setting, we run SysBench on the host. In the No sharing setting, we run SysBench in a VM; the VM is the only VM in the host; In the Vanilla setting, we consolidate 2 VMs on the same host. In the experiments, each VM has 4 vCPUs, and the host has 4 cores. Thus, in the No-sharing setting, there is one vCPU on each core, and in the Vanilla setting, each core is time-shared by 2 vCPUs. The VMs are configured to have the same I/O bandwidth quota in KVM <ref type="bibr">[30]</ref>. In each VM, the CPU workload in SysBench <ref type="bibr" target="#b5">[7]</ref> is run as a compute-bound task, and keep the vCPUs always busy. Note that we select these workloads and settings mainly to ease the demonstration and analysis of the performance issues. Our evaluation with real workloads and normal settings ( §5) show that these performance issues can actually be more severe. , we run only one instance of I/O bound task (i.e., I/O workload of SysBench). Among the three settings, No sharing has roughly the same I/O throughput as Bare-metal; but the I/O throughput in the vanilla setting is about half of those of the other two settings. This is because the VM running the I/O bound task only obtains 50% of CPU time on each core. Thus, the I/O bound task is only active for 50% of the time, as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>In the experiment shown in <ref type="figure" target="#fig_2">Figure 2</ref> (b), we run two instances of I/O bound task, one in each VM. For brevity, we refer to the I/O bound task in the first VM as I/O bound task 1, and refer to the task in the other VM as I/O bound task 2. The bars in .1% compared to bare-metal and no sharing, which is more than 50%. <ref type="figure">Figure 3</ref> explains the reason. The non-workconserving I/O scheduler in the VMM serves an I/O bound task in a VM for a short period before the vCPU running the task is descheduled. Then, it waits for 8ms without seeing any requests from I/O bound task 1. Thus, it has to switch and start to serve the I/O-bound task in the other VM (i.e., I/O-bound task 2). The changes between tasks are caused by I/O inactivity. They incur costly disk seeks. The wasted waiting time further reduces I/O throughput. In the experiments, for the Vanilla setting, we launch two VMs with the same I/O priority, and run an instance of I/O bound task on each of the VMs. We assign to the VMs with 20% and 80% of CPU time, respectively. For the Bare-metal setting and No sharing setting, we launch two instances of I/O bound task on the host and the VM, respectively. The two instances of I/O bound task are assigned with the same I/O priority but different CPU time shares (20% and 80%, respectively).</p><p>As shown in the figure, the two I/O bound tasks achieve similar I/O throughputs in the Bare-metal and No sharing settings. However, in the Vanilla setting, the I/O bound task in the VM with a larger CPU time share achieves a much higher (5.8x) throughput than that of the I/O bound task in the other VM. <ref type="figure">Figure 4</ref> explains the cause of this fairness issue. Since VM1 is allocated much less CPU time than VM2, it experiences much longer I/O inactivity periods. As a result, the I/O scheduler serves VM2 for much longer time than VM1.</p><p>There are two approaches that may be used to improve I/O throughput. One approach <ref type="bibr" target="#b34">[48,</ref><ref type="bibr" target="#b8">10,</ref><ref type="bibr" target="#b35">49]</ref> uses smaller time slices (e.g., vSlicer), such that vCPUs are scheduled more frequently, and thus become more responsive to I/O events. As shown in <ref type="figure">Figure 5</ref> with the curve labeled with vSlicer, this approach reduces the length of each vCPU inactivity period. But I/O inactivity periods become more frequent, and the portion of time in which an I/O task is inactive may not be reduced. Moreover, vSlicer incurs frequent context switches between vCPUs and increases the associated overhead. The other approach <ref type="bibr" target="#b21">[31,</ref><ref type="bibr" target="#b31">44]</ref> lifts the priority of I/O tasks. For example, xBalloon controls how vCPUs consume time slices such that more CPU time can be reserved for the execution of I/O bound tasks on the vCPUs. While this actually lengthens I/O active periods, as shown in <ref type="figure">Figure 5</ref> with the curve labeled with xBalloon, vCPUs still must be descheduled when they run out of time slices, and I/O inactivity problems are still incurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VMIGRATER Design</head><p>In this section, we first introduce the design principles and overall architecture of VMIGRATER. Then, we present the design details of each key component, focusing on how VMIGRATER monitors the scheduling and descheduling of vCPUs to keep track of their time slices ( §3.2), quickly detects I/Obound tasks ( §3.3), and migrates I/O-bound tasks with low overhead <ref type="figure">( §3.4)</ref>. Finally, we analyze the performance potential of VMIGRATER ( §3.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Principles and Overall Architecture</head><p>The design of VMIGRATER follows three principles:</p><p>• . . . should be transparent to applications. Thus, we choose to design VMIGRATER in the user space of guest OSs. This also helps maintain the original vCPU scheduling and I/O scheduling decisions of the VMM. However, this poses a few challenges, since the migration of I/O bound tasks relies on some key information about vCPU scheduling (e.g., remaining time slice of a vCPU), which is not easy to obtain in the user space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Descheduled vCPU vCPU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scheduled vCPU vCPU</head><p>• Low overhead: VMIGRATER needs to migrate I/O bound tasks. Frequent migrations may incur high overhead. The design of VMIGRATER must effectively control the frequency of migrations. <ref type="figure" target="#fig_5">Figure 6</ref> shows the overall architecture of VMIGRATER and its position in the software stack. VMIGRATER resides in each VM, and runs at the user level. Following the above principles, three key components are designed as follows.</p><p>vCPU Monitor ( §3.2) monitors the scheduling and descheduling of vCPUs. The objective is to measure time slice lengths for each vCPU and use the lengths to predict whether a vCPU is about to be preempted. The prediction is then used to make decisions on when an I/O bound task should be migrated and where it should be migrated.</p><p>Task Detector ( §3.3) detects I/O activities to quickly determine whether a task is I/O-bound.</p><p>Task Migrater ( §3.4) makes migration decisions and actually migrates I/O-bound tasks. It makes migration decisions based on the vCPU scheduling information from the Task Migrater and I/O activities of the tasks from the Task Detector. Specifically, it tries to migrate an I/O bound task detected in the Task Detector when the vCPU running the task is about to be descheduled. It migrates the task to another vCPU which may not be descheduled in near future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">vCPU Monitor Design</head><p>The vCPU Monitor uses a heartbeat-like mechanism to detect whether a vCPU is running or has been descheduled, with timer events being heartbeats. The idea is that, when a vCPU is descheduled, it cannot process timer events, and the heartbeat pauses. Specifically, vCPU Monitor runs a sleeping thread, namely vCPU Monitor thread, on each vCPU. The sleeping thread is woken up by a timer periodically. When it is woken up, it checks the current clock time, and compare the time with the time it observes last time. A time difference longer than the period for waking up the thread indicates that the vCPU was descheduled earlier, and has just been rescheduled.</p><p>This mechanism is as shown in <ref type="figure">Figure 7</ref>. The vCPU Monitor thread can detect that the vCPU is rescheduled at time t 2 and time t 6 . The thread keeps track of the timestamps when the vCPU is rescheduled (e.g., t 2 and t 6 ) and the timestamps immediately before them (e.g., t 1 and t 5 ). The time slice lengths can be estimated from these timestamps (e.g., t 5 − t 2 ).</p><p>Note that, since a vCPU may be scheduled or descheduled while its vCPU Monitor thread is sleeping, the exact time of the vCPU being rescheduled/descheduled cannot be obtained, and thus accurate time slice lengths cannot be measured with this method. Waking up the vCPU Monitor thread more frequently improves the accuracy of estimation; but it increases the overhead at the same time. Considering that typical time slice lengths are tens of miliseconds, VMIGRATER sets the length of the periods for waking up vCPU Monitor threads to 300 µs to make a trade-off between accuracy and overhead.  <ref type="figure">Figure 7</ref>: vCPU Monitor workflow. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Detecting I/O bound Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Migrating I/O bound Tasks</head><p>Task Migrater relies the information from vCPU Monitor and Task Detector to make migration decisions. It first needs to decide which I/O tasks should be migrated. To minimize the overhead, Task Migrater only migrates I/O bound tasks when vCPUs running them are to be descheduled shortly. To find these tasks, Task Migrater estimates the remaining time slice for each vCPU <ref type="bibr" target="#b0">1</ref> . If the remaining time slice is shorter than the length of two periods for waking up vCPU Monitor threads (i.e., 600 µs) 2 , Task Migrater determines that the vCPU is about to be descheduled. Task Migrater then checks the tasks scheduled on the vCPU. If there is an I/O bound task reported by Task Detector, Task Migrater migrates the task.</p><p>Second, Task Migrater needs to decide which vCPU the I/O bound tasks should be migrated to. A na¨ıvena¨ıve approach is to migrate I/O tasks to the vCPU with the longest remaining time slice. However, this method has two problems if Task Migrater needs to migrate multiple I/O bound tasks: (1) the I/O bound tasks are migrated to the same vCPU and cannot make progress concurrently; (2) the vCPU might be overloaded by accepting all these tasks, and the performance of its existing tasks is degraded.</p><p>Task </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Performance Analysis</head><p>We use Equation (1) to show the performance potential of VMIGRATER. For simplicity, we assume each VM has at least one active vCPU at any given time. Thus, an I/O application can be kept active with VMIGRATER, except when it is being migrated.</p><formula xml:id="formula_0">Speedup vMigrater = T ns × N T ns + N migrate ×C avg = N 1 + N migrate ×C avg T ns<label>(1)</label></formula><p>Equation <ref type="formula" target="#formula_0">(1)</ref> calculates the speedup of an I/O application with VMIGRATER relative to its execution without VMIGRATER on a VM. N is the number of vCPUs consolidated on each pCPU (i.e., consolidation rate). T ns is execution time of the I/O application on a VM when its execution is not affected by I/O inactivity problem. This can be achieved by running the application on a vCPU with a dedicated pCPU. It reflects the best performance that an I/O application can achieve on a VM. N migrate is the number of migrations conducted by VMIGRATER. C avg is the average time cost incurred by each migration.</p><p>The numerator of equation <ref type="formula" target="#formula_0">(1)</ref> is the execution time of an I/O application on a VM without VMIGRATER. With N vCPUs consolidated on a pCPU, in each period of N time slices, the I/O application can be active only for a period of one time slice. Thus, its execution time is roughly N × T ns . The denominator is the execution time with VMIGRATER, which is determined by the time spent on application execution and the time spent on migration.</p><p>Equation <ref type="formula" target="#formula_0">(1)</ref> shows that N migrate must be reduced to improve the performance of VMIGRATER. Suppose VMIGRATER migrates the I/O application by a minimum number N min of times in an optimal scenario. Thus, N min = T ns /T ts , where T ts is the length of a time slice allocated to a vCPU. In this optimal scenario, the I/O application is moved to a vCPU when the vCPU is just rescheduled; it stays there until the timeslice of the vCPU is used up; it is then moved to another vCPU which is newly rescheduled.</p><p>Replacing T ns with T ts × N min in equation <ref type="formula" target="#formula_0">(1)</ref>, we get: as P vMigrater , which has a value greater than 1. The speedup is mainly determined by P vMigrater . When P vMigrater approaches to 1, the speedup approaches to N. Our experiments show that the speedup with VMIGRATER matches the speedup calculated by Equation 1.</p><formula xml:id="formula_1">Speedup vMigrater = N 1 + N migrate ×C avg N min ×T ts<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Details</head><p>We have implemented VMIGRATER on Linux. The implementation of vCPU Monitor relies on a reliable and accurate clock source to generate timer events. The traditional system time clock cannot satisfy this need when vCPUs time-share a pCPU <ref type="bibr" target="#b31">[44]</ref>. Instead, we use the clock source CLOCK MONOTONIC <ref type="bibr">[18]</ref>, which is more reliable and can provide more accurate time measurement. The implementation of Task Detector leverages BCC <ref type="bibr">[11,</ref><ref type="bibr" target="#b16">24]</ref> to monitor I/O requests. BCC is a toolkit supported by Linux kernel for creating efficient kernel tracing and manipulation programs.</p><p>The implementation of Task Migrater uses two mechanisms, PUSH and PULL, to migrate tasks. A PUSH operation is conducted by the source vCPU of a task to move the task to the destination vCPU, while a PULL operation is initiated by the destination vCPU to move a task to it from the source vCPU. Usually PUSH operations are used. PULL operations are only used when source vCPUs are descheduled and cannot conduct PUSH operations. VMIGRATER's source codes are available on github.com/hku-systems/vMigrater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Our evaluation is done on a DELL TM PowerEdge TM R430 server with 64GB of DRAM, one 2.60GHz Intel R Xeon R E5-2690 processor with 12 cores, a 1TB HDD, and a 1TB SSD. All VMs (unless specified) have 12 vCPUs and 4GB memory. The VMM is KVM <ref type="bibr">[30]</ref>   <ref type="bibr">[42,</ref><ref type="bibr" target="#b29">40]</ref>.</p><p>We evaluate VMIGRATER using a collection of micro-benchmarks and 7 widely used applications. Micro-benchmarks include SysBench <ref type="bibr" target="#b5">[7]</ref> sequential read, SysBench random read, and bursty read implemented by us. As summarized in Ta- <ref type="bibr">Nginx [37]</ref>, and MongoDB <ref type="bibr" target="#b26">[35]</ref>. To be close to realworld deployments, PostMark is run with ClamAV (antivirus program) <ref type="bibr">[17]</ref> to generate the workload of a complete mail server with antivirus support; LevelDB and MongoDB are deployed as the backend storage of a Spark <ref type="bibr" target="#b38">[52]</ref> system.</p><formula xml:id="formula_2">ble 1, applications include HDFS [5], LevelDB [3], MediaTomb [9], HBase [2], PostMark [6],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application Workload HDFS</head><p>Sequential read 16GB with HDFS TestDFSIO <ref type="bibr" target="#b17">[25]</ref>. LevelDB Random scan table with db bench <ref type="bibr">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MediaT</head><p>Concurrent requests on transcoding a 1.1GB video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HBase</head><p>Random read 1GB with HBase PerfEval <ref type="bibr" target="#b17">[25]</ref>. PostMark Concurrent requests on a mail server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nginx</head><p>Concurrent requests on watermarking images <ref type="bibr" target="#b0">[1]</ref>. MongoDB Sequential scan records with YCSB <ref type="bibr" target="#b6">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ease of Use</head><p>With VMIGRATER, all 7 real applications we evaluated could run smoothly without any modification. When we evaluate these applications, VMIGRATER runs in the user-level of the guest OS. There is no need to change any parts of the VM or the VMM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Improvements</head><p>We first demonstrate that VMIGRATER can greatly improve the throughput of I/O intensive applications in each VM. For this purpose, we vary the number of VMs hosted on the server from 1 to 8, and run the workload with the micro-benchmarks and the workloads with the real applications summarized in <ref type="table" target="#tab_6">Table 1</ref>. In each experiment, we run one instance of the workload in each VM. So the co-located VMs have the same workload. We measure the throughputs of the benchmarks and real applications. When only one VM is hosted on the server, the I/O inactivity problem does not happen; the benchmarks and applications achieve the highest performance. We refer to this setting as No sharing, and use the performance under this setting as reference performance. We normalize the performance under other settings (i.e., 2/4/8 VMs consolidated on the server) against the reference performance, and show the normalized performance. Thus, the normalized performance of 1 is the best performance that can be achieved. The closer the normalized performance is, the better the performance is. <ref type="figure" target="#fig_8">Figure 8</ref> shows the normalized throughputs for micro-benchmarks when the number of consolidated VMs is varied from 2 to 8. With VMIGRATER, the benchmarks consistently achieve better performance than they do on vanilla KVM. At the same time, the performance advantage with VMIGRATER becomes more prominent when more VMs are consolidated. On average, with VMIGRATER, the throughputs of these benchmarks are improved by 97%, 225%, and 431% than those on vanilla KVM for the settings with 2 VMs, 4 VMs, and 8VMs, respectively.</p><p>Similar performance improvements are also observed with real applications, as shown in <ref type="figure">Figure 9</ref>. On average, with VMIGRATER, the throughputs of these applications are improved by 72%, 192%, and 342% than those on vanilla KVM for the settings with 2 VMs, 4 VMs, and 8VMs, respectively.</p><p>Compared to vSlicer and xBalloon, the applications can also achieve better performance with VMIGRATER. As shown in <ref type="figure">Figure 9</ref>, On average, with VMIGRATER, the throughputs of these applications are improved by 88.41%, 74.86%, and 121.22% than those with vSlicer for the settings with 2 VMs, 4 VMs, and 8VMs, respectively; and the throughputs are improved by 3.29%, 83.78%, and 175.37% than those with xBalloon under these three settings.</p><p>While VMIGRATER can significantly improve the throughput of I/O applications when all the consolidated VMs are equipped with VMIGRATER. Since VMIGRATER is designed at the user space, it is possible that not all the VMs have VMIGRATER deployed. We wonder whether VMIGRATER can still effectively improve I/O throughput in this scenario. To answer this question, we run the workloads with HDFS and LevelDB in one VM and enables VMIGRATER in this VM; in other colocated VM(s), we run the IS benchmark in NPB benchmark suite <ref type="bibr">[36]</ref>, and disable VMIGRATER in the VM(s). <ref type="figure" target="#fig_0">Figure 10</ref> shows that the effectiveness of VMIGRATER is not affected. On average, with    VMIGRATER, the throughputs of these applications are improved by 62.72%, 176.92%, and 218.75% than those on vanilla KVM for the settings with 2 VMs, 4 VMs, and 8VMs, respectively.</p><p>To understand how the performance improvements are achieved with VMIGRATER, we profile the executions of the real applications. We collect the number of migrations and the time during which I/O bound tasks "run" on descheduled vCPUs (i.e., I/O inactivity time). We show the data in <ref type="table" target="#tab_4">Table 2</ref> and <ref type="table" target="#tab_10">Table 3.</ref> VMIGRATER greatly improves application performance by first dramatically reducing I/O inactivity time. As shown in <ref type="table" target="#tab_4">Table 2</ref>, on average, for the applications, VMIGRATER reduces I/O inactivity time by 860.27%, 657.87%, 562.92%, respectively, relative to vanilla KVM, vSlicer, and xBalloon.</p><p>When I/O inactivity time has been dramatically reduced, as we have analyzed in Section 3.5, VMIGRATER maintains high throughputs by minimizing the time spent on migrating tasks, which is determined by the number of migrations and the time to finish each migration. As shown in Table 3, for most applications, the P vMigrater values are very close to 1. This confirms that the migration mechanisms in VMIGRATER are well designed. On one hand, they have effectively migrated I/O bound tasks to keep them active and minimize I/O inactivity. On the other hand, they only migrate the tasks for close-to-minimal times, so as to keep the time spent on migration low. We notice that the P vMigrater value is the highest (1.34) for MediaTomb among these applications, and its Speedup is the lowest <ref type="bibr">(1.41)</ref>. This confirms our performance analysis in Section 3.5.</p><p>We also notice that the effectiveness of VMIGRATER slightly reduces when the consolidation rate increases. This is caused by the special design with the vCPU scheduler in KVM (i.e., CFS in Linux), which allocate smaller time slices with higher consolidation rates. This reduces the opportunity to migrate I/O bound tasks. This problem can be mitigated by waking up Task Detector threads more frequently. <ref type="figure" target="#fig_0">Figure 11</ref> shows the response time of the three systems normalized to no sharing. For 7 applications, the response times of VMIGRATER and xBalloon are almost the same. Since each VM has 50% CPU resource, xBalloon has good performance (mentioned above). For MediaTomb, all three systems incur high response time because MediaTomb combines I/O and compute in one task. <ref type="figure" target="#fig_0">Figure 12</ref> shows three systems' overhead to co-      running compute-bound applications in the same VM. xBalloon's overhead is much higher than VMIGRATER and vSlicer because xBalloon prioritizes I/O-bound tasks and delays computebound tasks. vSlicer's overhead is higher than VMIGRATER because it incurs much more context switching overhead for compute-bound tasks. Unlike xBalloon and vSlicer, VMIGRATER almost would not delay co-running applications ( §3).  <ref type="figure" target="#fig_0">Figure 13</ref>: Throughput scalability on the loads of VMs, normalized to vanilla. Each client consumes around 20% CPU resources; two 2-vCPU VMs share two pCPUs; the more concurrent clients, the more faster VMIGRATER than vSlicer and xBalloon. <ref type="figure" target="#fig_0">Figure 13</ref> shows the three systems' throughput under varing workloads. When the number of clients is lower than 10, the throughput of VMIGRATER is almost the same as vSlicer and xBalloon because VMs are not shared. VMIGRATER is not started when there is no sharing. As the number of clients increased to 40, VMIGRATER outperforms the other two systems significantly because VMIGRATER can efficiently avoid I/O inactivity periods by migrating I/O tasks to scheduled vCPUs. vSlicer and xBalloon do not work when vCPU is inactive. xBalloon has almost the same performance as VMIGRATER for around 20 clients (each VM has 50% CPU resource). However, VMIGRATER is much more scalable than vSlicer and xBalloon when workloads increase.  <ref type="figure" target="#fig_0">Figure 14</ref> shows the robustness of VMIGRATER in the face of suddenly changing workloads. There are 8 clients at 0s, and the VMs are not overloaded. At around 4s, 8s and 11s , 4, 8 and 16 more clients are added, VMIGRATER's throughput decreases to around 240MB/s, but it becomes stable (peak, around 430MB/s) again after a short period because VMIGRATER needs some time to precisely re-estimate the time slices of vCPUs and then migrate the I/O bound tasks ( §7).   <ref type="figure" target="#fig_0">Figure 15 (a)</ref>, (b), (c) and (d), VM1's CPU resource decreases from 90% to 60%, and VM2's CPU resource increases from 10% to 40%. Each VM runs TestDFSIO (I/O-bound task) and TeraSort (compute-bound task) concurrently, and each VM is allocated with the same I/O bandwidth. Without VMIGRATER, TestDFSIO throughput is related to the CPU resource allocated to the VM, which shows that vanilla hurts the fairness of the I/O scheduler in the host OS. With VMIGRATER, two VMs in each figure achieve roughly the same TestDFSIO throughput, which implies VMIGRATER maintains fairness (roughly the same I/O bandwidth) for the two VMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Robustness to Varing Workloads</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Fairness for I/O Scheduler</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Shortening time slices. Many efforts have focused on shortening the time slices of vCPUs <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b35">49,</ref><ref type="bibr" target="#b34">48]</ref> for vCPUs to process I/O requests more frequently. This solution has two drawbacks: (1) the I/O inactivity period still exits and could degrade I/O performance; (2) it suffers from performance degradation because of frequent context switches <ref type="bibr" target="#b13">[21,</ref><ref type="bibr" target="#b32">46,</ref><ref type="bibr" target="#b23">32]</ref>. <ref type="bibr" target="#b34">[48]</ref> uses the same idea to reduce the delay of IRQ processing. These solutions require intensive modifications to both the VMM and guest OS kernel. Dedicating CPUs. Dedicating CPUs <ref type="bibr" target="#b30">[43,</ref><ref type="bibr" target="#b9">14]</ref> aims to solve the resource contention problem. This solution makes fewer vCPUs share one pCPU in order to reduce contention. These systems are complementary to VMIGRATER because they focus on reducing the vCPU sharing, while VMIGRATER focuses on improving performance in the shared setting.</p><p>Task-aware Priority Boosting. Existing systems <ref type="bibr" target="#b13">[21,</ref><ref type="bibr" target="#b21">31,</ref><ref type="bibr" target="#b10">15,</ref><ref type="bibr" target="#b31">44,</ref><ref type="bibr" target="#b28">39,</ref><ref type="bibr" target="#b12">20,</ref><ref type="bibr" target="#b20">29,</ref><ref type="bibr" target="#b36">50,</ref><ref type="bibr" target="#b15">23,</ref><ref type="bibr" target="#b25">34,</ref><ref type="bibr" target="#b14">22,</ref><ref type="bibr" target="#b24">33,</ref><ref type="bibr" target="#b11">16,</ref><ref type="bibr" target="#b19">28]</ref> focus on prioritizing latency-sensitive tasks to improve overall performance. Task aware VM scheduling <ref type="bibr" target="#b21">[31]</ref> improves the performance of workloads by prioritizing I/O bound VMs. <ref type="bibr" target="#b21">[31]</ref> works in the VMM layer and may require changing the source codes of the host OS. xBalloon preserves the priority of I/O tasks by preserving CPU resource for I/O tasks. However, the vCPUs are still descheduled so the I/O inactivity periods still exist. xBalloon works best for VMs with single vCPUs, while VMIGRATER is designed for multi-vCPU VMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Future Work</head><p>This paper identifies I/O inactivity problem in VMs which has not been adequately studied before. It presents VMIGRATER, a simple, fast and transparent system that can greatly mitigate I/O inactivity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: I/O inactivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Three performance issues caused by I/O inactivity. "Bare-metal" means physical server; "No sharing" means only one VM running on the host; "Vanilla" means two VMs consolidated and managed by vanilla KVM on one host. I/O-bound Task 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>a) and Figure 2 (b) show that I/O in- activity significantly reduces I/O throughput in two different ways. In the experiment shown in Fig- ure 2 (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: I/O inactivity causes unfairness issue. The I/O task in a VM with more CPU time gets more I/O bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 (</head><label>2</label><figDesc>c) illustrates the unfairness issue caused by I/O inactivity. It shows that two I/O bound tasks on two VMs with the same I/O priority achieve quite different I/O throughputs because the two VMs are assigned with different CPU time shares.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Overall Architecture of VMIGRATER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Migrater migrates I/O tasks to vCPUs in a globally balanced way. Specifically, Task Mi- grater ranks active vCPUs based on the lengths of their remaining time slices, and ranks the I/O bound tasks to be migrated based on their I/O load lev- els. It migrates the I/O bound tasks with heavier I/O load levels to the vCPUs with longer remain- ing time slices. This migration mechanism can pre- vent the above problems because it distributes I/O bound tasks among active vCPUs. At the same time, it helps maintain high I/O throughput because the tasks with the most I/O activities are scheduled on the vCPUs that are least likely to be descheduled shortly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Normalized throughputs of micro-benchmarks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Normalized throughput of HDFS and LevelDB when VMIGRATER is enabled in one of the consolidated VMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Response time normalized to "no vCPU sharing". Two 12-vCPU VMs share 12 pCPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Execution time normalized to "Vanilla". "Hadoop" means each VM is running Hadoop standard TeraSort workload; "Spark" means each VM is running standard WordCount workload; "ClamAV" means each VM is scanning virus for the whole OS. Each VM has 12 vCPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: VMIGRATER's performance on handling the load change of vCPUs by adding clients dynamically. 8 clients at the time 0; each client exhausts 20% CPU resource; two 2-vCPU VMs share two pCPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: VMIGRATER improves the fairness of I/O Scheduler. Two 12-vCPU VMs share 12 pCPUs; each VM is allocated different CPU resources but the same I/O bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15</head><label>15</label><figDesc>Figure 15 shows the fairness of the VMM I/O scheduler among VMs. In Figure 15 (a), (b), (c) and (d), VM1's CPU resource decreases from 90% to 60%, and VM2's CPU resource increases from 10% to 40%. Each VM runs TestDFSIO (I/O-bound task) and TeraSort (compute-bound task) concurrently, and each VM is allocated with the same I/O bandwidth. Without VMIGRATER, TestDFSIO throughput is related to the CPU resource allocated to the VM, which shows that vanilla hurts the fairness of the I/O scheduler in the host OS. With VMIGRATER, two VMs in each figure achieve roughly the same TestDFSIO throughput, which implies VMIGRATER maintains fairness (roughly the same I/O bandwidth) for the two VMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Equation 2 shows that the speedup is determined by N and N migrate ×C avg N min ×T ts ; N, C arg , and T ts are con- stants for an application. We denote</head><label>2</label><figDesc></figDesc><table>N migrate 
N min 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>in Ubuntu 16.04. The guest OS in each VM is also Ubuntu 16.04. The length of a vCPU time slice is 11ms, as recommended by Red Hat [41]. The I/O scheduler in VMM is CFQ with wait time set to 8ms, as recommended by Red Hat</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 1 :</head><label>1</label><figDesc>7 applications and workloads. Most of the experiments are conducted with the SSD. Only the experiments in §5.4 (fairness of I/O scheduler) use the HDD, because they need a non-work-conserving I/O scheduler (e.g., CFQ) and CFQ is used in Linux to schedule HDD requests. We compare VMIGRATER with two related so- lutions: xBalloon [44] and vSlicer [49]. Because they do not have open-source implementations, we implemented them based on the description in their papers.</figDesc><table>Our evaluation aims to answer the following 
questions: 
 §5.1: Is VMIGRATER easy to use? 
 §5.2: How much performance improvement can be 
achieved with VMIGRATER, compared with 
vanilla KVM and two related solutions? What 
is the overhead incurred by VMIGRATER. 
 §5.3: What is VMIGRATER's performance when the 
workload in a VM varies over time? 
 §5.4: Can VMIGRATER help the I/O scheduler in the 
VMM to achieve fairness between VMs? 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>100</head><label>100</label><figDesc></figDesc><table>Se 
q 
Re 
ad 
1 Ra 

n 
Re 
ad 
1 Se 

q 
Bu 
rs 
ty 
Re 
ad 
1 

Ra 
n 
Bu 
rs 
ty 
Re 
ad 
1 

Se 
q 
Re 
ad 
2 Ra nd 

Re 
ad 
2 

Se q Bu rs 

ty 
Re 
ad 
2 

Ra n Bu rs ty Re 

ad 
2 

Se q Re ad 4 Ra nd Re ad 4 
Se q Bu rs ty Re ad 4 
Ra n Bu rs ty Re ad 4 
Se q Re ad 8 Ra n Re ad 8 Ra n Bu rs ty Re ad 8 
Se q Bu rs ty Re ad 8 
Se q Re ad 1 Ra n Re ad 1 Se q Bu rs ty Re ad 1 
Ra n Bu rs ty Re ad 1 
Se q Re ad 2 Ra n Re ad 2 Se q Bu rs ty Re ad 2 
Ra n Bu rs ty Re ad 2 
Se q Re ad 4 Ra n Re ad 4 Se q Bu rs ty Re ad 4 
Ra n Bu rs ty Re ad 4 
Se q Re ad 8 Ra n Re ad 8 Ra n Bu rs ty Re ad 8 
Se q Bu rs ty Re ad 8 
Se q Re ad 1 Ra n Re ad 1 Se q Bu rs ty Re ad 1 
Ra n Bu rs ty Re ad 1 
Se q Re ad 2 Ra n Re ad 2 Se q Bu rs ty Re ad 2 
Ra n Bu rs ty Re ad 2 
Se q Re ad 4 Ra n Re ad 4 Se q Bu rs ty Re ad 4 
Ra n Bu rs ty Re ad 4 
Se q Re ad 8 Ra n Re ad 8 Ra n Bu rs ty Re ad 8 
Se q Bu rs ty Re ad 8 
Normalized Throughput (%) 

2 VMs 
4 VMs 
8 VMs 

Vanilla 
vMigrater 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>I/O inactivity time (seconds) of 7 applica-
tions. Four VMs are used. The last column is the ratio be-

tween the I/O inactive time with vanilla KVM and that with 

VMIGRATER. 

Application N migrate N min P vMigrater Speedup 
HDFS 
3363 3181 
1.05 
1.86 
LevelDB 
2154 2003 
1.07 
1.75 
HBase 
3454 3181 
1.08 
1.76 
MongoDB 
1545 1363 
1.13 
1.70 
PostMark 
5181 4818 
1.07 
1.82 
MediaTomb 
2454 1818 
1.34 
1.41 
Nginx 
4181 4090 
1.02 
1.73 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>VMIGRATER only migrates I/O bound tasks 
for close-to-minimal times. Two VMs are used. 

0 

50 

100 

150 

200 

250 

HD 
FS 

HB 
as 
e 

Po 
st 
M 
ar 
k 

Le 
ve 
lD 
B 

M 
on 
go 
DB 

M 
ed 
ia 
T 

Ng 
in 
x 
Normalized Latency (%) 

Shared vCPU 
vSlicer 
xBalloon 
vMigrater 

</table></figure>

			<note place="foot" n="1"> The remaining time slice of a vCPU at a moment (e.g., t 7 in Figure 7) is estimated using the length of time slice assigned to the vCPU before the most recent descheduling of the vCPU (e.g., t 5 − t 2 ) and the CPU time that has already been consumed by the vCPU after the most recent rescheduling of the vCPU (e.g., t 7 − t 6 ). 2 This is to tolerate the inaccuracy in the estimation of time slices and remaining time slices.</note>

			<note place="foot" n="274"> 2018 USENIX Annual Technical Conference USENIX Association</note>

			<note place="foot">VMIGRATER has two limitations, and we leave them as future work. First, when VMIGRATER runs in a VM, the performance of an application in the VM may drop temporarily when the application&apos;s workload changes suddenly. Our evaluation (see §5.3) shows that, when the number of clients for HDFS increased from 16 to 32, HDFS&apos;s throughput dropped by 45.6% for 1.3 seconds and then went back to the peak throughput immediately. The reason is that the sudden changing workload makes time slices of some vCPUs not stable, and VMIGRATER needs some time to precisely reestimate the time slices of vCPUs and then migrate the I/O bound tasks. Second, VMIGRATER mainly aims to mitigate the performance degradation caused by disk (HDD or SSD) I/O inactivity periods in VMs, and it is not designed to handle network I/O. Comparing to disk I/O, network I/O is much more sparse, and we have not come across any situation where VMIGRATER affects the performance of network I/O in our evaluation.</note>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adding watermarks to images using alpha channels</title>
		<ptr target="http://php.net/manual/en/image.examples-watermark.php" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hbase</surname></persName>
		</author>
		<ptr target="https://hbase.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leveldb</surname></persName>
		</author>
		<ptr target="https://github.com/google/leveldb" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://hadoop.apache.org/hdfs/" />
		<title level="m">The Hadoop Distributed File System</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://www.filesystems.org/docs/auto-pilot/Postmark.html" />
		<title level="m">The PostMark Benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sysbench</surname></persName>
		</author>
		<ptr target="http://sysbench.sourceforge.net" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yahoo! Cloud Serving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benchmark</surname></persName>
		</author>
		<ptr target="https://github.com/brianfrankcooper/YCSB" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mediatomb -Free Upnp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mediaserver</surname></persName>
		</author>
		<ptr target="http://mediatomb.cc/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Micro-sliced virtual processors to hide the effect of discontinuous cpu availability for consolidated systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="394" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">vscale: automatic and efficient processor scaling for smp virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh European Conference on Computer Systems</title>
		<meeting>the Eleventh European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">vbalance: using interrupt load balance to improve i/o performance for smp virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third ACM Symposium on Cloud Computing</title>
		<meeting>the Third ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tracon: Interference-aware scheduling for data-intensive applications in virtualized environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>2011 International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A hidden cost of virtualization when scaling multicore applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<editor>HotCloud</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gleaner: Mitigating the blocked-waiter wakeup problem for virtualized multicore applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference (USENIX &apos;14)</title>
		<meeting>the USENIX Annual Technical Conference (USENIX &apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Opportunistic flooding to improve tcp transmit performance in virtualized clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kangarlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM Symposium on Cloud Computing</title>
		<meeting>the 2nd ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">vpipe: Piped i/o offloading for efficient data movement in virtualized clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Cloud Computing</title>
		<meeting>the ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Performance superpowers with enhanced BPF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gregg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>USENIX Association</publisher>
			<pubPlace>Santa Clara, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/core/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anticipatory scheduling: A disk scheduling framework to overcome deceptive idleness in synchronous i/o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Druschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking the scalability of multicore applications on big virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Parallel and Distributed Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">vsnoop: Improving tcp throughput in virtualized environments via acknowledgement offload</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kangarlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing, Networking, Storage and Analysis (SC), 2010 International Conference for</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Task-aware virtual machine scheduling for i/o performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<title level="m">SIGPLAN/SIGOPS international conference on Virtual execution environments</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quantifying the cost of context switch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 workshop on Experimental computer science</title>
		<meeting>the 2007 workshop on Experimental computer science</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">vfair: Latency-aware fair storage scheduling via perio cost-based differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saltaformaggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Symposium on Cloud Computing</title>
		<meeting>the Sixth ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">vhaul: Towards optimal scheduling of live multi-vm migration for multi-tier applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 8th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
	<note>Cloud Computing (CLOUD)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mongodb</surname></persName>
		</author>
		<ptr target="http://www.mongodb.org" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Server</forename><surname>Nginx Web</surname></persName>
		</author>
		<ptr target="https://nginx.org/" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scheduling i/o in virtual machine monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments</title>
		<meeting>the fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">What is the suggested I/O scheduler to improve disk performance</title>
		<ptr target="https://access.redhat.com/solutions/5427" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Schedule processes, not vcpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th AsiaPacific Workshop on Systems</title>
		<meeting>the 4th AsiaPacific Workshop on Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Preserving i/o prioritization in virtualized oses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Symposium on Cloud Computing</title>
		<meeting>the 2017 Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="269" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The context-switch overhead inflicted by hardware interrupts (and the enigma of do-nothing loops)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 workshop on Experimental computer science</title>
		<meeting>the 2007 workshop on Experimental computer science</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note>page 4.</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vmware horizon view architecture planning 6.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vmware</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VMware Technical White Paper</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accelerating virtual machine i/o processing using designated turbo-sliced core</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vturbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">vslicer: latency-aware virtual machine scheduling via differentiated-frequency cpu slicing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kangarlou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international symposium on High-Performance Parallel and Distributed Computing</title>
		<meeting>the 21st international symposium on High-Performance Parallel and Distributed Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">vread: Efficient data access for hadoop in virtualized clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saltaformaggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gamage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Kompella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Middleware Conference</title>
		<meeting>the 16th Annual Middleware Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="125" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A scheduling framework that makes any disk schedulers non-work-conserving solely based on request characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="119" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation</title>
		<meeting>the 9th USENIX conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
