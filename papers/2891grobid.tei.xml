<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Can Data Center Become Water Self-Sufficient? *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishwar</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Florida International University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">A</forename><surname>Islam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Florida International University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaolei</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Florida International University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Quan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Florida International University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Can Data Center Become Water Self-Sufficient? *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>To curtail data centers&apos; huge cooling power consumption and water demand (for cooling), air-side economizer has been increasingly adopted to cool down servers. Recently, another sustainability practice, rainwater harvesting , has also seen a growing adoption in data centers , potentially leading to water self-sufficiency without connecting to water utilities to supply cooling water. Nonetheless, various factors, e.g., unpredictable rain falls and limitations on water harvesting area, make water self-sufficiency challenging. In this paper, we present a first-of-its-kind study to evaluate whether it is feasible to achieve water self-sufficiency in data centers. We find that although water self-sufficiency depends on non-controllable factors such as weather; improving power proportionality (via power management) and increasing water tank size will increase the feasibility, relieving the requirement on water harvesting area and making water self-sufficiency a reality in different locations.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As critical assets for digital economy, data centers have collectively accounted for over 2% of global electricity usage <ref type="bibr" target="#b10">[13]</ref>. Meanwhile, data centers are also very "thirsty", consuming an enormous amount of water (e.g., through evaporation in cooling towers to exhaust server heat into the environment) <ref type="bibr" target="#b15">[18]</ref>. For example, U.S. National Security Agency (NSA) data center in Utah, when fully operational, would consume up to 1.7 million gallons of fresh water <ref type="bibr" target="#b11">[14]</ref>. Amid the anticipation that water demand worldwide exceeds the supply by 40% in 2030 and also urged by prevalent severe droughts <ref type="bibr" target="#b12">[15]</ref>, reducing data centers' water consumption has become increasingly imperative, as attested by LBNL's recent guideline which places water and energy as two major considerations for U.S. federal data center consolidation <ref type="bibr" target="#b17">[20]</ref>. * This work was supported in part by NSF CNS-1423137.</p><p>While there have been numerous efforts to improve data center energy efficiency <ref type="bibr" target="#b9">[12]</ref>, attention has also been recently paid to reduce water consumption in data centers. For example, Google is using recycled/industry water or seawater instead of potable water in some of its data centers <ref type="bibr" target="#b7">[10]</ref>, AT&amp;T is partnering with EDF to improve water efficiency in cooling towers through water treatment <ref type="bibr" target="#b4">[5]</ref>, and recent research also investigated saving water by routing more workloads to data centers with higher water efficiency <ref type="bibr" target="#b15">[18]</ref>.</p><p>Among various water-saving approaches for data center, a very appealing, and perhaps the most attractive one is using air-side economizer (a.k.a. "free" outside air cooling), which is highly power efficient compared to traditional cooling systems and has a significantly lower water footprint compared to systems with cooling towers <ref type="bibr" target="#b14">[17]</ref>. In this paper, we consider a data center using air-side economizer with evaporation-assist as the main cooling method, where evaporation-assist is employed to reduce the outside air temperature and extend the operating hours for economization. While sustainability leaders (e.g., Google and Apple <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">11]</ref>) have pledged to become "net-zero" (i.e., electricity usage completely offset by renewable energy) and/or power their data centers entirely by renewable energy without grid power, our paper investigates another distinctly different sustainability practice: Can a data center become water selfsufficient? Here, the data center cooling system does not rely on any fresh water tapped from water utilities; 1 instead, all the cooling water (in evaporation assist <ref type="bibr" target="#b14">[17]</ref>) is harvested from available natural resources -rainwater. While the limited rainfall cannot possibly sustain water self-sufficiency for conventional data centers that have huge water footprints due to water-consuming cooling towers, the low water footprint of increasingly popular air-side economizers could potentially make water self-sufficiency a reality and hence motivate our study.</p><p>What is rainwater harvesting and why does it matter for data centers? Rainwater harvesting is a clean and natural water resource processing, providing various benefits such as reduced rainwater pollution (e.g., runny rainwater carries significant amount of debris) and decreased pressure on sewage systems. With increasingly frequent droughts and water scarcity, many states in the U.S. are encouraging rainwater harvesting through programs such as tax rebates or incentives on rainwater harvesting equipment <ref type="bibr" target="#b19">[22]</ref>. In addition, there are other benefits that motivate data center operators to harvest rainwater including, among others: green certification (e.g., LEED program), social responsibility, lowering the rapidly-increasing water cost, and business risk concerns (e.g., in drought-prone areas where compliance codes on water usage are tightening <ref type="bibr" target="#b4">[5]</ref>). Further, rainwater harvesting is cost-effective and easy to maintain, since rainwater only needs basic processing (e.g., filtration) before it can be used for evaporation-assist in air-side economizer. Industry leaders, such as Google and Equinix (a global colocation data center provider), have all implemented rainwater harvesting in (some of) their data centers.</p><p>Water self-sufficiency, without tapping into water utilities, naturally represents the ultimate goal of rainwater harvesting for sustainability, which brightens corporate image and brings tax benefits. Nonetheless, it is very challenging to achieve water self-sufficiency. First, and foremost, rainwater is scarce in supply and demonstrates a high unpredictability. Second, water harvesting area is often limited (due to physical area constraint and/or government regulations <ref type="bibr" target="#b13">[16]</ref>), and so is water tank size for storing rainwater. In the following, we present an evaluation model for water self-sufficiency. We present a first-of-its-kind study on whether water self-sufficiency is feasible, via trace-based evaluations for data centers in six different places in the U.S. Our results show that water self-sufficiency is indeed highly feasible, under a variety of weather conditions. As a stepping stone, understanding the feasibility at the first place can help data center operators decide whether and what additional efforts are needed for water self-sufficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Rainwater Harvest and Cooling Model</head><p>This section provides some background information on rainwater harvesting in the context of data centers, and also presents the operating modes and water consumption for air-side economizer with evaporation-assist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Rainwater harvesting</head><p>Rainwater harvesting area is surface area in direct contact to rainfall (e.g., rooftop, parking area), while storage tank/cistern represents the place where rainwater is stored. With large space, data centers can harvest rainwater from rooftop and/or other plane surfaces (e.g., parking lot, terraces). Besides, applicable area for data center water storage can also be explored for rainwater harvesting (e.g., Google's rainwater retention pond in its South Carolina data center <ref type="bibr" target="#b7">[10]</ref>). In Section 3, we demonstrate optimal harvesting area and water tank size for data centers located in places with volatile weather pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cooling system</head><p>Air-side economizer directs outside cold air into the data center to cool down servers and is mostly suitable in regions with cool/cold climate. This type of cooling systems is becoming popular for energy savings over traditional cooling systems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">7,</ref><ref type="bibr" target="#b14">17]</ref>. In our study, we consider the data center uses air-side economizer with evaporation-assist as the cooling system. In this cooling system, outside air first enters the air-mixing chamber, where it is mixed with the return air from the server room. Controllable openings are used to control the amount of outside air and return air that enters the airmixing chamber. The air from the mixing chamber then passes through the evaporation chamber where water is sprayed from sprinklers onto the air. The air gets cooled down through water evaporation. The cold air (i.e., supply air) is then directed into the server room to cool down the servers. Humidity and temperature of the supply air are conditioned by controlling the flow rate of air and evaporation (amount of water spray), such that the supply air falls within the recommended operating range by ASHRAE: 64.4 o F(18 o C) to 80.6 o F(27 o C), and relative humidity of 40% to 60% <ref type="bibr" target="#b1">[2]</ref>.</p><p>Based on the environmental condition, the cooling system may run in different operating modes. Here, we briefly present the operating conditions of these modes.</p><p>Direct-air: The cooling system runs in this mode when the outside air temperature and humidity falls within the ASHRAE specified range <ref type="bibr" target="#b1">[2]</ref>. In this mode, the outside air is directly sent to the server room without any conditioning other than filtering for particles. This operating mode has no water footprint and a minimum/negligible power consumption.</p><p>Evaporative: When the outside air temperature is above the allowed range but the humidity is low, evaporation is used to cool down the outside air. The temperature drop during evaporation depends on the difference between dry-bulb and wet-bulb temperatures of the outside air. Humidity of the air increases during the evaporation process, and therefore the evaporation rate is controlled to keep the supply air humidity within specification. This operating mode has a high water footprint because of the evaporation, but still low power consumption.</p><p>Mechanical: The cooling system uses mechanical cooling when the outside air cannot be conditioned for the data center. This happens when the temperature and humidity of the outside air are both higher than the allowed range. In this mode, the return air is cooled down using mechanical cooling and then used in the server room. This operating mode has no water footprint, but has the highest power consumption due to the high power requirement of the mechanical cooling.</p><p>MIX: When the outside air temperature is low but the humidity is high, this operating mode is used to reduce the humidity of the outside air by mixing it with the server room return air which has low humidity. In this process, the temperature of the mixed air also increases. The supply air temperature and humidity is controlled by adjusting mixing ratio of the outside air and return air. This operating mode also has no water footprint, and has low power consumption.</p><p>MIX+Evap.: In this operation mode, both mix and evaporation is used to condition the outside air. This mode of operation is required for very cold outside temperature, where the amount of water present in the air is also low. De-humidification process increases the temperature, and evaporation adds water to the air to get it into allowed range of operation. This operating mode has a moderate water footprint and low power consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Water consumption</head><p>The cooling system operation modes that involve evaporation have water footprint. We calculate the water usage effectiveness (WUE) of the cooling system at different weather conditions using the formula</p><formula xml:id="formula_0">WUE = (X s − X a ) × 3.6 h e − h s ,<label>(1)</label></formula><p>where WUE has unit L/kWh, X a and X s are the humidity ratios of outside air and supply air, respectively, with unit gm w /kg a (grams of water/moisture per kg of air), h s and h e are the enthalpy of the data center supply air and return air, respectively, with unit kJ/kg. In Equation (1), h e − h s represents the amount of heat each kg of air carries out of the data center, while X s − X a measures the amount of water added to each kg of air during evaporation to condition the outside air to be used in the server room. We can determine the humidity ratios and enthalpy from the temperature, humidity and pressure of the air at different points (e.g., outside, supply, exhaust, etc.). We consider that there is no water condensation inside the data center, i.e., the amount of water in the air does not change when it passes through the sever room (i.e., humidity ratio remains constant). Although WUE is also affected by other factors (e.g., water leakage), Equation (1) gives us a reasonable estimate of water usage for air-side economizer with evaporation-assist. For example, the WUE calculated for Prineville, OR, using (1) matches well with that of Facebook's data center with a similar cooling system <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Case Study</head><p>In this section, we present trace-based case studies of water self-sufficiency feasibility for data centers located at six locations in the U.S. After introducing our data sets, we present experimental results and observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data sets</head><p>We consider six geographically distributed data centers located at: Prinevile (OR), Forest City (NC), Los Angeles (CA), Ashburn (VA), Chicago (IL), and New York (NY), which all have large (colocation) data centers <ref type="bibr" target="#b6">[9]</ref>, while other places can be studied using the same methodology. We conduct our simulation with a data center with 6MW static/idle power and 10MW peak IT power that has a air-side economizer.</p><p>• IT load: We evaluate four different workload traces, which we denote as "Workload #1", "Workload #2", "Workload #3" and "Workload #4". These workloads include web service traces (e.g., Hotmail, Wikipedia), I/O log (Microsoft Research), and colocation data center UPS load (Terremark in Miami, FL). Traces are taken from prior publications <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b18">21]</ref> and our direct collaboration. We use Workload #1 as our default workload. Due to limited availability of trace data throughout the year, we add 30% random variation and extend the available traces to get one-year trace, and scale the traces to have a 30% average server utilization. <ref type="figure" target="#fig_0">Fig. 1</ref> shows workload traces (normalized based on data center capacity) for four different workloads for three days.</p><p>• Weather data: We obtain hourly weather data from <ref type="bibr">[8]</ref> at the six different cities where we consider the data centers' locations for the year of 2013. The operation mode of the cooling system is determined based on the dry-bulb temperature and relative humidity. We consider that the recommended range of operation follows ASHRAE <ref type="bibr" target="#b1">[2]</ref> • Others: Default size of water tank is considered to be one million gallons, with an initial storage of 50%. We set the minimum water level at 5% of the total water capacity for emergency uses. Note that one-million-gallon water tank is not restrictive for a data center with 10MW IT power (e.g., Google has several 240, 000 gallon water tanks in its SC data center <ref type="bibr" target="#b7">[10]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Now, we present our results on case studies using the aforementioned data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cooling modes and WUE</head><p>We first study the cooling system operation modes and resulting WUE. In <ref type="figure">Fig. 2(a)</ref>, we show the operation time of different cooling modes. We see that the MIX+Evap. mode is the most prominent mode of operation for airside cooling system with evaporation-assist. In general, MIX is the second most frequently used mode. The direct-air mode is used less than 10% of the time for all locations except CA. Also, we observe that mechanical cooling is not used at all for OR and CA data centers. All the cooling modes, except the mechanical cooling, take advantage of the outside air and have low power consumption, thus demonstrating the potential of using aireconomizer in these locations. <ref type="figure">Fig. 2(b)</ref> shows the resulting average annualized WUE of the cooling system. We see that, OR data center has the highest WUE, which is the result of the cooling system running in MIX+Evap. mode almost 70% of the time. Data center at CA has the lowest WUE because it operates in the water-less MIX mode most of the time. We also show the annual total rainfall of 2013 in the six data center locations in <ref type="figure">Fig. 2(c)</ref>. It shows that OR and CA have very low rainfall in 2013 (less than 5 inches during the entire year), where other locations have more than 30 inches of rainfall, demonstrating that WUE is not correlated with the amount of rainfalls. Thus, geographic load balancing could potentially help achieve water self-sufficiency in multiple locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Harvesting area</head><p>The harvesting area determines how much water is added to the water tanks when there is rainfall. For the same amount of rainfall, a larger harvesting area thus can collect more water. In <ref type="figure">Fig. 3</ref>, we study the harvesting area required for different locations as well as for different workload types. We show in <ref type="figure">Fig. 3(a)</ref> that, except for OR, all the other locations require a harvesting area of less than 500,000 sqft to become water self-sufficient. NC data center has the lowest surface requirement because it has a high rainfall. For CA, despite having very low amount of annual rainfall, it still can be water selfsufficient at harvesting area greater than 200, 000 sqft,  because of the very low average WUE. OR data center requires a harvesting area of more than 1.5 million sqft to become water self-sufficient, and thus is considered infeasible for water self-sufficiency. <ref type="figure">Fig. 3(b)</ref> shows the effect of different workload types on the harvesting area requirement. We see that despite the variety in workloads, harvesting area requirement does not vary much with workload pattern because the average workload (and hence, power consumption) is kept the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Impact of different factors</head><p>Here, we study the impact of different factors that affects water self-sufficiency.</p><p>Power proportionality. Power proportionality refers to data center's power consumption in proportion to its workload. In the default setting, we do not consider advanced power management (e.g., dynamically turning off unused servers to reduce power) in the data center, and hence there is a low power proportionality (e.g., data center uses 6MW power even for zero workload). <ref type="figure" target="#fig_3">Fig. 4(a)</ref> shows the impact of improved power proportionality on the harvesting area requirement. We see that, when perfect power proportionality is achieved (zero static power), the harvesting area requirement becomes less than 200,000 sqft for all the locations (except for OR, which is omitted in these figures for its very high water consumption and also harvesting area requirement).</p><p>Water tank size. In <ref type="figure" target="#fig_3">Fig. 4(b)</ref>, we show the impact of water tank size on the minimum harvesting area requirement. We see that the harvesting area requirement rises sharply when the tank size falls short of a certain value for all the locations. This indicates that there is a constraint on the minimum tank size for a feasible harvesting area. We also observe that, the harvesting area slowly decreases with increasing tank size, showing a saturation of benefit from larger water tanks. This is due in part to inconstant rain falls, i.e., rainwater cannot be collected continuously even with very large water tanks.</p><p>Initial water storage. <ref type="figure" target="#fig_3">Fig. 4(c)</ref> shows the impact of initial storage size on harvesting area. We determine the minimum harvesting area with the constraint that the amount of water present in the water tank at the end of the year is not less than the initial water storage. This ensures water self-sufficiency is sustainable for subsequent years. We see that no locations can achieve waterself-sufficiency when the initial storage is too low (below 10%) or too high (above 90%). Data centers at NC, CA and VA have a wider range of initial water level for which the minimum harvesting area is not affected, while for data centers at IL and NY have the optimal initial water level around 40%.</p><p>Rainfall. <ref type="figure" target="#fig_3">Fig. 4(d)</ref> shows the impact of precipitation on harvesting area requirement, where the yearly average precipitation is varied by scaling the 2013-year precipitation from −30% to +30%. We see that the harvesting area requirement demonstrates a natural trend of increased harvesting area for lower precipitation, but with only a small variation with precipitation changes. Hence, once rainwater harvesting system is built, it is unlikely to be invalidated by normal changes in precipitation. Figure 5: Summary of water self-sufficiency feasibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Summary</head><p>Now, we summarize our findings. Considering that a 10MW data center requires 100,000 sqft for the servers alone (with the industry average power density of 100 watts/sqft <ref type="bibr">[6]</ref>), we set different levels of water selfsufficiency feasibility based on the harvesting area requirement as shown in <ref type="figure">Fig. 5</ref>. For example, a harvesting area requirement of less than 100,000 sqft is highly feasible, whereas greater than 500,000 sqft is deemed as infeasible. We can see that, with 60% static/idle power (i.e., no advanced power management for energy saving) and 1 million gallons of water tank size, data center in NC achieves medium feasibility (i.e., requirement for harvesting area is feasible), while it is difficult to achieve water self-sufficiency for data centers in OR and NY. However, by reducing static/idle power (i.e., improved power proportionality), feasibility of all data centers can be improved. For example, with perfect power proportionality (0% static power), data centers at NC, CA and VA become highly feasible for achieving water self-sufficiency (i.e., low harvesting area requirement). Moreover, increasing water tank size further assists with achieving water self-sufficiency: for example, increasing water tank size to 2 million gallons improves feasibility of data centers in all locations. "zero" surface area for data centers at CA and NC indicate that, the data centers can sustain on the initial water storage (default of 50% of the tank) when the water tank size is two million gallons. In sum, although water self-sufficiency depends on non-controllable factors such as weather; improving power proportionality (via effective IT power management) and increasing water tank size will significantly increase the feasibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This paper studies if it is feasible to achieve water selfsufficiency in data centers, through rainwater harvesting area and choice of air-side economizers. We show that the feasibility is strengthened by improving power proportionality and increasing water tank sizes. Further, "uncorrelation" of water efficiency and amount of rainfalls suggests that geographic load balancing may help achieve water self-sufficiency in multiple locations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Workload trace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: (a) Annual operation percentage of different cooling modes. (b) WUE of different locations. (c) Annual rainfall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impact of (a) power proportionality, (b) water tank size, (c) initial water storage, and (d) annual rainfall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>: 64 .4 o F(18 o C) to 80.6 o F(27 o C), and rel</head><label>64</label><figDesc>- ative humidity of 40% to 60%. The exhaust air tempera- ture is considered to be at 96.8 o F(36 o C).</figDesc><table></table></figure>

			<note place="foot" n="1"> Data center also needs water for other purposes such as employees&apos; drinking, but such non-cooling water comes from utilities through a different pipe and is out of the scope of this paper [10].</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://www.apple.com/environment/" />
	</analytic>
	<monogr>
		<title level="j">Apple and the environment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">thermal guidelines for data processing environmentsexpanded data center classes and usage guidance</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note>Whitepaper prepared by ASHRAE technical committee (TC</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Dashboard</surname></persName>
		</author>
		<ptr target="https://www.facebook.com/PrinevilleDataCenter/app_" />
		<imprint>
			<publisher>PUE &amp; WUE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Inside facebook&apos;s forest city data center</title>
		<ptr target="https://www.facebook.com/video.php?v=4638122307624" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">At&amp;amp;t</forename><surname>Sustainability</surname></persName>
		</author>
		<ptr target="http://www.att.com/gen/landing-pages?pid=24188" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The evolution of facebook&apos;s data center cooling</title>
		<ptr target="http://www.datacenterknowledge.com/archives/2012/12/04/evolution-of-facebooks-cooling/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<ptr target="http://www.datacentermap.com/usa/" />
	</analytic>
	<monogr>
		<title level="j">DATACENTERMAP. Colocation USA</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<ptr target="http://www.google.com/about/datacenters/" />
	</analytic>
	<monogr>
		<title level="j">GOOGLE. Data center efficiency</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://static.googleusercontent.com/external_content/untrusted_dlcp/www.google.com/en/us/green/pdfs/renewable-energy.pdf" />
		<title level="m">GOOGLE. Google&apos;s green PPAs: What, how, and why</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoelzle</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barroso</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan and Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Growth in data center electricity use</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koomey</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<ptr target="http://www.ksl.com/?sid=25978926" />
		<title level="m">Utah NSA center requires 1.7M gallons of water daily to operate</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Charting our water future: Economic frameworks to inform decision-making</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mckinsey</forename><surname>Report</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<title level="m">NATIONAL CONFERENCE OF STATE LEGISLATURES. State rainwater harvesting and graywater laws and programs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Economizer modes of data center cooling systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niemann</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avelar</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Schneider Electric Data Center Science Center Whitepaper</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing water efficiency in distributed data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cloud and Green Computing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sierra: a power-proportional, distributed storage system. Microsoft Research Ltd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thereska</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno>MSR-TR-2009-153</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Guideline for water and energy considerations during federal data center consolidations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tschudi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<ptr target="http://www1.eere.energy.gov/femp/pdfs/consolidation_guidelines.pdf" />
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Wikipedia workload analysis for decentralized hosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urdaneta</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Steen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Rainwater harvesting conservation, credit, codes, and cost: Literature review and case studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Epa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
