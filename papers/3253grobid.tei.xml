<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T03:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Re-optimizing Data-Parallel Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bruno</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Chuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microso</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Microso</forename><surname>Bing</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Re-optimizing Data-Parallel Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Performant execution of data-parallel jobs needs good execution plans. Certain properties of the code, the data, and the interaction between them are crucial to generate these plans. Yet, these properties are dif-cult to estimate due to the highly distributed nature of these frameworks, the freedom that allows users to specify arbitrary code as operations on the data, and since jobs in modern clusters have evolved beyond single map and reduce phases to logical graphs of operations. Using xed apriori estimates of these properties to choose execution plans, as modern systems do, leads to poor performance in several instances. We present RoPE, a rst step towards re-optimizing data-parallel jobs. RoPE collects certain code and data properties by piggybacking on job execution. It adapts execution plans by feeding these properties to a query optimizer. We show how this improves the future invocations of the same (and similar) jobs and characterize the scenarios of beneet. Experiments on Bing&apos;s production clusters show up to × improvement across response time for production jobs at the th percentile while using .× fewer resources.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In most production clusters, a majority of data parallel jobs are logical graphs of map, reduce, join and other operations <ref type="bibr">[, , , ]</ref>.</p><p>An execution plan represents a blueprint for the distributed execution of the job. It encodes, among other things, the sequence in which operations are to be done, the columns to partition data on, the degree of parallelism and the implementations to use for each operation.</p><p>While much prior work focuses on executing a given plan well, such as dealing with stragglers at runtime [], placing tasks <ref type="bibr">[, ]</ref> and sharing the network <ref type="bibr">[, ]</ref>, little has been done in choosing appropriate execution plans.</p><p>Execution plan choice can alleviate some runtime concerns. For example, outliers are less bothersome if even the most expensive operation is given enough parallelism.</p><p>But plan choice can do much more-it can avoid needless work (for example, by deferring expensive operations till aer simpler or more selective operations) and it can trade-o one resource type for another to speed up jobs (for example, in certain cases, some extra network trac can avoid a read/write pass on the entire data set). However, plan choice is more challenging because the space of potential plans is large and also because the appropriateness of the plan depends on the interplay between code, data and cluster hardware.</p><p>Early data-parallel systems force developers to specify the execution plan (e.g., Hadoop). To shield developers from coping with these details, some recent proposals raise the level of abstraction. A few use hand-craed rules to generate execution plans (e.g., <ref type="bibr">HiveQL []</ref> A central theme, across all schemes, is the absence of insight into certain properties of the code (such as expected CPU and memory usage), the data (such as the frequency of key values), and the interaction between them (such as the selectivity of an operation). ese properties crucially impact the choice of execution plans.</p><p>Our experience with Bing's production clusters shows that these code and data properties vary widely. Hence, using xed apriori estimates leads to performance inefciency. Even worse, not knowing these properties constrains plan choice to be pessimistic; techniques that provide gains in certain cases but not all cannot be used.</p><p>is paper presents RoPE , a rst step towards reoptimizing data parallel jobs, i.e., adapting execution plans based on estimates of code and data properties. To our knowledge, we are the rst to do so. e new domain brings challenges and opportunities. Accurately estimating code and data properties is hard in a distributed context. Predicting these properties by collecting statistics on the raw data stored in the le-system is not practical due to the prevalence of user-deened operations. But, knowing these properties enables a large space of improvements that is disjoint from prior work and so are the methods to achieve these improvements. e sheer number of jobs indicates that the estimation and use of properties has to be automatic. RoPE piggybacks estimators with job execution. e scale of the data and distributed nature of computation means that no single task can examine all the data. Hence, RoPE collects statistics at many locations and uses novel ways to compose them. To keep overheads small, RoPE's collectors can only keep a small amount of state and work in a single pass over data. Collecting meaningful properties, such as the number of distinct values or heavy hitters, under these</p><p>Reoptimizer for Parallel Executions constraints precludes traditional data structures and leads to some interesting designs. e exibility allowed for users to deene arbitrary code leads to a much tighter coupling between data and computation in data parallel clusters. As long as they conform to well-deened interfaces, users can submit jobs with binary implementations of operations. In this context, predicting code properties becomes even more diicult. Traditional database techniques can project statistics on the raw data past some simple operations with alphanumeric expressions but doing so through multiple operations, more complex expressions, potentially dependent columns and user-deened operations introduces impractically large error []. Rather than predicting, RoPE instruments job execution to measure properties directly.</p><p>We nd traditional work on adaptive query optimization to be speciic to the environment of one database server <ref type="bibr">[, , ]</ref> and the resulting space of optimizations. For example, a target scenario minimizes the reads from disk by keeping one side of the join in the server's memory. RoPE translates these ideas to the context of distributed systems and parallel plans. In doing so, RoPE uses a few aspects of the distributed environment that make it a better t for adaptive optimization. Unlike the case of a database server where most queries nish quickly and the server has to decide whether to use its constrained resources to run the query or to re-optimize it, map-reduce jobs last much longer and the resources to re-optimize are only a small fraction of those used by the job. Further, if a better plan becomes available, transitioning from the current plan to the better plan is tricky in databases [] whereas data parallel jobs have many inherent barriers at which execution plans can be switched.</p><p>In Bing's production clusters, we observe that many key jobs are re-run periodically to process newly arriving data. Such recurring jobs contribute . of all jobs, . of all cluster hours and . of the intermediate data produced. We also observe that the code and data properties are remarkably stable across recurring jobs despite the fact that each job processes new data. ese jobs are RoPE's primary use-case.</p><p>RoPE adapts the execution plans for future invocations of such jobs by feeding the observed properties into a cost-based query optimizer (QO). Our prototype is built atop SCOPE [], the default engine for all of Bing's mapreduce clusters, but our techniques can be applied to other systems. e optimizer evaluates various alternatives and chooses the plan with the least expected job latency. Additionally, we modiied the optimizer to use the actual code and data properties while estimating costs. Working with a QO enables RoPE to not only perform local changes such as changing the degree of parallelism of operations, but also changes that require a global context, such as reordering operations, choosing appropriate operation implementations and grouping operations that have little work to do into a single physical task.</p><p>We nd jobs that are not completely identical oen have common parts. Further, during the execution of a job, while some of the global changes are logically impossible due to operations that have already executed, other changes remain feasible. RoPE can help in both these cases since (a) the query optimizer can work with incomplete estimates and (b) the code and data properties are linked to the sub-graph at which they were collected and can be matched to other jobs with an identical sub-graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions include:</head><p>Based on experiences and measurements on Bing's production clusters, we describe scenarios where knowledge of code and data properties can improve performance of data-parallel jobs. A few of these scenarios are novel ( §). Design of the rst re-optimizer for data-parallel clusters, which involves collecting statistics in a distributed context, matching statistics across subgraphs and adapting execution plans by interfacing with a query optimizer ( §). Results from a partial prototype, deployed on production clusters, which show RoPE to be eeective at reoptimizing jobs ( §, §). Production jobs speed up by over × at the th percentile while using .× fewer resources. RoPE achieves these gains by designing better execution plans that avoid wasteful work (reads, writes, network shuues) and balance operations that run in parallel.</p><p>A user would expect her data-parallel jobs to run quickly. She would expect this even though the code is unknown, even though the data properties are hard to estimate, even though the code and data interact in unpredictable ways, and even though the code, the data and the cluster hardware and soware keep evolving. RoPE is a rst step towards reoptimizing data-parallel computing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">COST OF IGNORING CONTEXT</head><p>It is not uncommon for data-parallel computing frameworks such as Dryad and MapReduce to process petabytes of data each day. However, their inability to leverage data and computation statistics renders them unable to generate execution plans that are better suited for the jobs they run and prevents them from utilizing historical context to improve future executions. Here, we describe axiomatic scenarios of such ineeciency and quantify both their impact and frequency of occurrence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Our experience is rooted in Bing's production clusters consisting of thousands of multi-core servers participating in a distributed le system that supports both structured and unstructured data. Jobs are written in SCOPE [], a SQL-like mashup language with support for arbitrary user-deened operators. at is, users specify their data parallel jobs within a declarative framework (e.g., select, join, group by) but are allowed to declare their own implementations of operators as long as they t the templates provided (e.g., extractor, processor, combiner, reducer). A compiler translates the query into an execution plan which is then executed in parallel on a Dryad-like [] runtime engine. Plans are directed acyclic graphs where edges represent dataow and nodes represent work that can be executed in parallel by many tasks. A task can consist of multiple operations. A job manager orchestrates the execution of the job's plan by issuing tasks when their inputs are ready, choosing where tasks run on the cluster, and reacting to outliers and failures. To facilitate better resource allocation across concurrent jobs, individual job managers work in close contact with a per-cluster global manager.</p><p>Unless otherwise speciied, our results here use a dataset that contains all the events from a large production cluster in Bing. e events encode for each entity (job/ operation/ task/ or network transfer), the start and end times of the entity, the resources used, the completion status, and its dependencies with other entities. Our experiments are based on examining all events during the month of September on a cluster consisting of tens of thousands of servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Little Data</head><p>We notice that while most map-reduce programs start o by reading a large amount of data, each successive operation ((lters, reduces, etc.) produces considerably fewer output compared to its input. Hence, more oen than not, just aer a small number of these consecutive operations there is very little data lee to process. We call this the little data case. e little-data observation can be used to optimize the tail of most jobs. In some cases, the degree of parallelism on many operations in the tail can be reduced. is saves scheduling overhead on the un-necessary tasks. In other cases, multiple operations in the tail can be coalesced into a single physical opera- </p><formula xml:id="formula_0">(a) CDF Range Fraction ... (, .) ... [., .) ... [., .) ... [., .) ... [., ] ... &gt; ... &gt; .. &gt; ..</formula><p>(b) readout</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure : Variation in selectivity across tasks</head><p>tion, i.e., one group of tasks executes these operations in parallel. is avoids needless checkpoints to disk. In yet other cases, broadcast joins can be used instead of pairwise joins (see §..) thereby saving on network shuue and disk accesses. e challenge however is that the reduction factors are unknown apriori and vary by several orders of magnitude. <ref type="figure">Fig. plots</ref> the fraction of input data that remains in ight aer jobs have been running for (and ) of their runtime. We compute the data in--ight at any time by taking a cut of the job's execution graph at that time and adding up the data exchanged between tasks that are on either side of this cut. For convenience, we place tasks running at that time to the lee of the cut. We see that while some jobs have more data in ight than their input (above y = line), most of the jobs have much fewer. In fact for over of jobs, the data in ight reduces to less than of their input within a quarter of the job's running time and over of jobs have less than a tenth of data in ight aer three quarters of their running time. is means that little data, and the above optimizations, can be brought to bear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Varying Selectivity, Reordering</head><p>Consider a pair of commutative operations. Ordering them, so that the more selective operation (one with a lower output to input ratio) runs rst will avoid work thereby saving compute hours, disk accesses and network shuue. See <ref type="figure">Fig. (a)</ref>, where the width of block arrows represent the data ow between a pair of commutative operators (indicated by the circles). Evidently, the plan on the right avoids processing unnecessary data and potentially saves signiicant cluster cycles by appropriately ordering the operators based on their selectivity. ese pairs hap- pen oen, due to operations that are independent of each other (e.g. operations on diierent columns) or are commutative. Identifying these pairs can be hard in general but SCOPE's declarative syntax allows the use of standard database techniques to discover such pairs. Finding the selectivities of operations remains a challenge.</p><p>Standard database techniques to predict operator selectivity are hard to translate to map-reduce like frameworks due to the complexity of expressions and long sequences of operations. e selectivity of alphanumeric expressions (e.g. select on X=30) can be predicted by using clever histograms on the raw data (e.g. equi-depth) but creating these histograms requires many passes over the data. Predicting the selectivity for user-deened operations (e.g. select when columnvalue.BeginsWith("http://bing")) is an open problem []. We see such code in a majority of jobs. Moreover, the prediction errors grow exponentially with the length of the sequence of operations []. Computing more detailed synopsis on a random sample is oen of only marginal beneet []. Finally, correlations between sets of columns, as is common, increases prediction error (e.g. select on X=30 and Y=10 can produce just as much data as the select on X=30 or much less). RoPE estimates selectivity by direct instrumentation. <ref type="figure">Fig. (a)</ref> plots a CDF of the selectivity (ratio of output to input) of operations in our dataset. Note the y axis is in log scale. About of operations produce more data than they consume (above y = ). ese are typically (outer) joins. About have output roughly equalling input. e remaining operations produce less output than their input but the selectivity varies widelyproduce fewer than 'th of their input, and the coeecient of variation ( std ev me an ) is .. with a range from to . is means that if these selectivities were available, there is substantial room to reorder operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Varying Costs, Balance</head><p>Suppose we gured out selectivities and picked the right order. Uniquely for data parallel computing, we need to choose the number of parallel instances for each operation. Choosing too few instances will cause that operation to become a bottleneck as per Amdahl's law. On the other hand, choosing too many leads to needless per-task scheduling, queuing and other startup overhead. Balance, i.e., ensuring that each operation has enough parallelism and takes roughly the same amount of time, can improve performance signiicantly []. See <ref type="figure">Fig. (b)</ref> where block arrows again represent the dataow and the thin arrows now represent the tasks in each of the two operations. Here, using one less task for the upstream operation and one more for the downstream operation reduces job latency by over .</p><p>Achieving balance in the context of general data parallel computing is hard because the costs (runtime, memory etc.) of the operators are unknown apriori. ese costs depend on the amounts of data processed, the types of computation performed and also on the type of data. For example, a complex sentence can take longer to translate than a simpler one of the same length. Even worse, latebinding, i.e., deferring the choice of the amount of parallelism to the runtime is hard because local changes have global impact; for example, the number of partitions output by the map phase restricts the maximum number of reduce tasks at the next stage. RoPE estimates these costs to generate balanced execution plans. <ref type="figure">Fig.</ref> (a) plots a CDF of the runtime per unit byte read or written by operations in the dataset. e analogous plot for memory used by tasks is in <ref type="figure">Fig. (b)</ref>. Note again that the y axes are in log scale. While as variable as their selectivity-the middle th percent of operators have costs spread over two orders of magnitude-we nd that operator costs skew more towards higher values. Per unit data processed, of operations take over X more time and memory than the average over the remaining operations. It is crucial to identify these heavy operations, to oset their costs by increasing the parallelism and for the case of memory, to place tasks so that they do not compete with other memory hungry tasks.</p><p>A consequence of unpredictable data selectivity and operator costs is the lack of balance. We nd that our compiler both underestimates and overestimates an operation's work. <ref type="figure">Fig. plots</ref> a CDF of the runtime of the median task in each stage. e y axis is in log scale. e median task in a stage is unlikely to be impacted by failures or be an outlier. So, if the compiler apportions parallelism well, the median task in each stage should take about the same time. is duration could be chosen to trade-o fault-tolerance vs. the cost of checkpointing to disk. However, we see that while roughly of all tasks nish within seconds, take over s with the last taking over s. We found the tail dominated by tasks with user deened operators. Setup and scheduling overheads outweigh the useful work in short-lived tasks whereas the long-lived tasks are bottlenecks in the job.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Partition Skew and Repartitioning</head><p>A key to eecient data-parallel computing is to avoid skews in partition and to re-partition only when needed. Consider the example in <ref type="figure">Fig. (c)</ref>. e naive implementation would partition the data twice-once on A, B, C (at P ), followed by a network shuue and a reduce to compute the sum, and then again on A, B (at P ) followed by another shuue for the join. It is tempting to just partition the data once, say on A, B to avoid the network shuue and the pass over data. Note that partitioning on fewer keys does not violate the correctness of the reduce that computes SUM(D). Each reduce task will now compute many rows, one per distinct value of C, rather than just the one row they would have produced were the map to partition on all three columns. However, if there is not enough entropy on A and B, i.e., only a few distinct values have many records, then partitioning on the sub-group can make things worse. A few reduce tasks might receive a lot of data to process while other reduce tasks have none, and the overall parallel execution can slow down.</p><p>Fig. estimates how skewed the partitions can be in our cluster. Note that our compiler is conservative and does not partition on sub-groups to avoid re-partitioning. Yet signiicant skew happens. We deene skew as the ratio of the maximum data processed by a reduce task to the average over other tasks in that reduce phase. We see that in of the reduce stages, the largest partition is twice as large as the average and in about of the stages the largest partition is over ten times larger than the average. Such skew causes unequal division of work and bottlenecks but can be avoided if the data properties are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">From operations to implementations</head><p>OOen, the same operation can be implemented in several ways. While choosing the appropriate implementation can result in signiicant improvements, doing so requires appropriate context that is not available in today's</p><formula xml:id="formula_1">!" # $% &amp; ' (%)*+</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure : Stability of data properties across recurring jobs</head><p>systems. For example, consider Join. e default implementation PairJoin, involves a map-reduce operation on each side that partitions data on the join columns. is causes three read/write passes on each of the sides and at least one shuue each across the network. However, if one of the sides is smaller, perhaps due to the little data case ( §..), one could avoid shuuing the larger side and complete the join in one pass on that side. e trick is to broadcast the smaller side to each of the tasks that is operating in parallel on the larger side. e problem though is that when used inappropriately, a BroadcastJoin can be even more expensive than a PairJoin.</p><p>Fig. plots the potential beneets of replacing PairJoins with BroadcastJoins. It shows the amount of data shufed in either case. We see that about of joins in the dataset would see no beneet. is can happen if both join inputs are considerably large and/or when the parallelism on the larger side is so much that broadcasting the smaller input dataset to too many locations becomes a bottleneck. However, o the remaining joins, the median join shuues less data when using broadcast joins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7">Recurring Jobs</head><p>We nd that many jobs in the examined cluster repeat and are re-run periodically to execute on the newly arriving data. Such recurring jobs contribute to . of all jobs, . of all cluster hours and . of intermediate data produced. If the extracted statistics are stable per recurring job, i.e., the operations behave statistically similar to the previous execution when running on newer data of the same stream, then RoPE's instrumentation would suuce to re-optimize future invocations.</p><p>Fig. plots the average diierence between statistics collected at the same location in the execution plan across recurring jobs. We picked all of the recurring jobs from one business group and instrumented ve diierent runs of each job. While most of these jobs repeated daily, a few repeated more frequently. e gure has four distributions, one per data property that RoPE measures. We defer details on the speciics of the properties (see <ref type="table">Table ,</ref> §..) but note that while some properties, such as row length, are more predictable than others, the overall statistics are similar across jobs-the ratio std ev me an is less than . for of the locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.8">Current Approaches, Alternatives</head><p>To the best of our knowledge, we are the rst to reoptimize data parallel jobs by leveraging data and computation statistics. Current frameworks use best-guess estimates on the selectivity and costs of operations. Such rules-of-thumb, as we saw in §..- §.., perform poorly. Hadoop and the public description of MapReduce leave the choice of execution plans, the number of tasks per machine and even low-level system parameters such as buuer sizes to the purview of the developer. HiveQL [] uses a rule-based optimizer to translate scripts written in a SQL-like language to sequences of map-reduce operations. Starsh [] provides guidance on system parameters for Hadoop jobs. To do so, it builds a machine learning classiier that projects from the multi-dimensional parameter space to expected performance but does not explore semantic changes such as reordering. <ref type="bibr">Ke et. al.</ref> [] propose choosing the number of partitions based on operator costs. In contrast, RoPE can perform more significant changes to the execution plan, similar to FlumeJava [] and Pig [], but additionally does so based on actual properties of the code and data.</p><p>It is tempting to ask end-users to specify the necessary context, for e.g., tag operations with cost and selectivity estimates. We found this to be of limited use for a few reasons. First, considerable expertise and time is required to hand-tune each query. Users oen miss opportunities to improve or make mistakes. Second, exposing important knobs to a wide set of users is risky. Unknowingly, or greedily, a user can hog the cluster and deny service to others; for instance, by tricking the system to give her job a high degree of parallelism. Finally, changes in the script, the cluster characteristics, the resources available at runtime or the nature of data being processed can require re-tuning the plan. Hence, RoPE re-optimizes execution plans by automatically inferring these statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.9">Experience Summary, Takeaways</head><p>Note that all of the problems described here happen deterministically. ey are not due to heterogeneity or runtime variations [] or due to poor placement of tasks <ref type="bibr">[, ]</ref> or due to sharing the cluster <ref type="bibr">[, ]</ref>. We believe that the choice of execution plan is orthogonal to these problems that arise during the execution of a plan. e underlying cause is that predicting relevant data and computation statistics deep into a job's execution is necessary to nd a good plan but is diicult due to the intricate coupling between data and computation. When employed together, these improvements add up to more than their sum. Promoting a more selective operator closer to the input, can reduce the data owing in so much that a subsequent join may be implemented with broadcast join. We found in practice that such global changes to the execution plan accrue more beneets than making singleton changes. RoPE achieves both types of re-optimizations as we will see in §.</p><p>A few key takeaways follow.</p><p>Being un-aware of data and computation context leads to slower responses and wasted resources. Unlike the case of singleton database servers, dataparallel computation provides diierent space for improvements and has new challenges such as coping with arbitrary user deened operations and expressions. Global changes to the execution plan add more value than local ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DESIGN</head><p>RoPE enables re-optimization of data parallel computing. To obtain data-and computation-context, RoPE interposes instrumentation code into the job's dataow. An operation can be instrumented with collectors on its input(s), output(s) or both. We describe how RoPE chooses what information to collect, avoids redundancy in the locations from which stats are collected, and the algorithms to compose statistics from distributed locations in §... ese statistics are funneled to the job manager and are used to improve execution plans in a few diierent ways, each diiering in the scope of possible changes and the complexity to achieve those changes.</p><p>Even though an execution plan is already chosen for a running job, RoPE uses the statistics collected during the run to improve some aspects of the job. Descendant stages that are at least a barrier away from the stages where datastats are being collected will not have begun execution. e implementation of these stages can be changed on the y. Stages that are pipelined with the currently executing stage can be changed, since inter-task data ow happens through the disk. Some plan changes may be constrained by stages (or parts of stages) that have already run. Hence RoPE performs changes that only impact the un-executed parts of the plan, such as altering the degree of parallelism of non-reduce stages.</p><p>RoPE uses the collected statistics to generate better execution plans for new jobs. Here, RoPE can perform more comprehensive changes. Recall from §.. that many jobs in the examined cluster recur because they periodically execute on new data and that the extracted datastats are stable across runs of these jobs. In this case, upon a new run of a recurrent job, datastats collected from previous runs are used as additional inputs to the plan optimizer. We describe how statistics are stored so they can be matched with expressions from subsequent jobs in §... We also note that partial overlaps are common among jobs [] and our matching framework extends to cover this case. e methodology of how the statistics are used is described in §... An illustrative case study of the changes that RoPE achieves is in §...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Collecting contextual information</head><p>Choosing what to observe and the statistics to collect has to be done with care since the context we collect will determine the improvements that we can make. Broadly, we collect statistics about the resource usage (e.g., CPU, memory) and data properties (e.g., cardinality, number of distinct values). See Table for a summary. e nature of map-reduce jobs leads to a few unique challenges. First, map-reduce jobs examine a lot of data in a distributed fashion. ere is no instrumentation point that observes all the data and even if created, such instrumentation would not scale with the data size. Hence, we require stat collection mechanisms to be composable, i.e., local statistics obtained by looking at parts of the data should be composable into a global statistic over all data. Second, to be applicable to a wide set of jobs, the collection method should have low overhead, i.e., overhead that is only a small fraction of the task that it piggybacks upon. In particular, the memory used should scale sub-linearly with data size and to limit computation cost, the statistics should be collected in a single pass. Together, these constraints are quite strict, so we adapt some pre-existing streaming and approximation algorithms.</p><p>Finally, to be useful, the statistics have to be unambiguous, precise, and have high coverage. By unambiguous, we mean that the statistics should contain metadata describing the location in the query tree that these   statistics were collected at. Global changes to the plan, during re-optimization for subsequent jobs for example, can alter the plan so much that previously observed subtrees no longer occur. Precision is an accuracy metric; we try to match the accuracy requirements of the improvements that we would like to make with the properties of the algorithmic techniques that we use to collect statistics. For coverage, we would like to observe as many diierent points in the job execution as possible. However, to keep costs low, we ignore instrumenting operations whose impact on the data is predictable (e.g., the input size of a sort operation is the same as its output). Further, we only look at the interesting columns. at is, we collect columnspeciic statistics only on columns whose values will, at some later point in the job, be used in expressions (e.g. record stats for col if select col=. . ., join by col=. . ., or reduce on col follow). Implementation: RoPE interposes stat-collectors at key points in the job's dataow. Datastat collectors are passthrough operators which keep negligible amounts of state and add little overhead. We also extend the task wrapper to collect the resources used by the task. When a task nishes, all datastats are ferried to the job manager (see <ref type="figure">Fig- ure</ref> ), which then composes the stats. e stats are used right away and also stored with a matching service for use with future jobs (see <ref type="figure">Figure (b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Data Properties</head><p>At each collection point we collect the number of rows and the average row length. is statistic will inform whether data grows or shrinks and by how much as it ows through the query tree. Composing these statistics is easy-for example, the total number of rows aer a select operation is simply the sum of the number of rows seen by the collectors that observe the output of that select across all the tasks that contain that select.</p><p>Further, for each interesting column, we compute the number of distinct values and the heavy hitters, i.e., values that repeat in a large fraction of the rows. ese statistics, as we will see shortly, help avoid skews during partition and also help pick better implementations. Computing these statistics while keeping only a small amount of state in a single pass is challenging, let alone the need to compose across diierent collection points.</p><p>Our solution builds on some state-of-the-art techniques that we carefully chose because we could extend them to be composable. We will only sketch the basic algorithms and focus on how we extended them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lossy Counting to find Heavy Hitters.</head><p>Suppose we want to identify all values that repeat more frequently than a threshold, say of the data size N. Doing this in one pass, naively, requires tracking running counts of all distinct values and uses up to O(N) space.</p><p>Lossy counting [] is an approximate streaming algorithm, with parameters s, ε that has these properties. First, it guarantees that all values with frequency over sN will be output. Further no value with a frequency smaller than (s − ε)N will be output. Second, the worst case space required to do so is (sub-linear) ε log (εN). In practice, we nd that the usage is oen much smaller. ird, the frequency estimated undercounts the true frequency of the elements by at most εN. e key technique is rather elegant; it tracks running frequency counts but aer every ε records, it retires values that do not pass a test on their frequency. For more details, please refer [].</p><p>RoPE uses a distributed form of lossy counting. Each stat collector employs lossy counting on the subset of data that their task observes with parameters s = ε, ε. To compute heavy hitters over all the data, we add up the frequency estimates over all collectors and report distinct values with count greater than εN. Interestingly, composing in this manner retains the properties of lossy counting with slight mods. A proof sketch follows. Proof Sketch: Let N i be the number of records observed by the i'th collector. Note that s−ε = ε and ∑ N i = N First, for the frequency estimation error, if a value is reported by stat collector i, we know that its frequency estimate is no worse o than εN i . If the value is not reported, its frequency estimate is zero; but by the existence constraint, we know that the element did not occur more than εN i times at this collector. Summing up the errors across collectors, we conclude that the global estimate is not o the true frequency by more than εN. Second, for false positives, we note that we only keep values whose cumulative recorded count is greater than εN, that means their true frequency is at least εN. ird, for false negatives, suppose a value has a global frequency greater than f &gt; εN, then it has to occur more than εN i &gt; sN i times at some collector, and so will be reported. Even more, since we just showed that the cumulative error is no worse than εN, its cumulative recorded count will be no worse than f − εN which is larger than εN, hence RoPE will report this value aer composition. Finally, the space used at each collector is ε log (εN i ) meeting our requirements. Implementation: RoPE uses ε = .. <ref type="bibr">Micro-benchmarks</ref> show that frequency estimates are never worse o by more than εN and the space used is small multiples of log( N ε ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hash Sketches to count Distinct Values.</head><p>Counting the number of distinct items in sub-linear state and in a single pass has canonically been a hard problem. Using only O(log N) space, hash sketches [] computes this number approximately. at is, the estimate is a random variable whose mean is the same as the number of distinct values and the standard deviation is small. e key technique involves uniformly random hashing the values. e rst few bits of the hash value are used to choose a bit vector. From the remaining bits, the rst nonzero bit is identiied and the corresponding bit is set in the chosen bit vector. Such a hash sketch estimates the number of distinct values because of all hash-values will be odd and have their rst non-zero bit at position , will do so at position and so on. Hence, the maximum bit set in a bit-vector is proportional to the logarithm of the number of distinct values. Using a few bit-vectors rather than one guards against discretization error. e actual estimator is a bit more complex to correct for additive bias. For more details, please refer [].</p><p>RoPE uses a distributed form of hash sketches. Each stat collector maintains local hash sketches and relays these bit vectors to the job manager. e job manager maintains global bit vectors, such that the i th global bit vector is an OR of all the individual i th bit vectors. By doing so, the global hash sketch is exactly the same as would be computed by one instrumentation point that looks at all data. Hence, RoPE retains all the properties of hash sketches. Implementation: If the hash values are h bits long, where h = O(log N), and the rst m bits choose the bit-vector, then there are m bit vectors and the size of the hash sketch is h * m bits. RoPE uses m = and h = . Hence, each task's hash sketch is B long. Our microbenchmarks show that hash sketches retain precision even as the number of distinct values grows to .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Operation Costs</head><p>We collect the processing time and memory used by each task. A task is a multi-threaded process that reads one or more inputs from the disk (locally or over the network) and writes its output to the local disk. However, popular data-parallel frameworks can combine more than one operation into the same task (e.g. data extraction, decompression, processing). Since such operations run within the same thread and frequently exchange data via shared memory, the per-operator processing and memory costs are not directly available. But, per-operator costs are necessary to reason about alternate plans, e.g., reordering operators. Program analysis of the underlying code could reason about how the operators interact in a task, but this analysis can be hard because the interactions are complex, e.g., pipelining. Also, proling individual operators does not scale to the large number of UDOs. Instead, RoPE uses a simple approach that only estimates the costs of the more costly operations. e approach works as follows. First, tasks containing costly operations are likely to be costly as well. We pick stages with costs in the top tenth percentile as expensive.</p><p>We only use the costs of the median task in each stage to lter the impact from failures and other runtime eeects. Second, not all the operators in expensive tasks are expensive. So, for each operator, we compute a condence score as the fraction of the stages containing the operator that have been picked as expensive. An operator will have a high conndence score only if it exclusively occurs in expensive tasks. ird, we compute the support of an operator as the number of distinct expensive stages that it occurs in. Finally, we estimate the cost of operators, that have high conndence and high support scores, as the average cost of the stages containing that operator.</p><p>To validate this approach, we proled over K randomly chosen stages from the production cluster. It is hard to obtain ground truth at this scale. Hence, we corroborate our results on succinctness -only a few among all the operations should be expensive, and coveragemost of the expensive tasks should contain at least one expensive operation. is method identiied ... and ... of the operators as expensive for CPU and memory costs respectively. is number was higher than expected because there are only a few generic operations but many UDOs. Most of the costly operations were UDOs. Further, . (.) of the tasks picked as expensive based on their memory (cpu) cost contained at least one expensive operator. Hence, their cost can be explained by these operations. Very few contained more than one expensive operation. e succinct set of operations identied as expensive and the high coverage of this set makes us optimistic about this simple method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Leading Statistics</head><p>e ability to predict behavior of future operators is invaluable, especially for on-the--y optimizations. Doing so precisely is hard. Rather than aspiring for precise predictions in all cases, RoPE collects simple leading statistics to help with typical pain points. We collect histograms and random samples on interesting columns (i.e. columns which are involved in where/group by clauses or joins) at the earliest collection points preceding the oper- ator at which those columns will be used.</p><p>We looked at several histogram generators, including equi-width, equi-depth and serial but ended up with a simpler, albeit less useful alternative. is is because none of the others satissed our single pass and bounded memory criterion with a reasonable accuracy. For each interesting column, in an operator, we build a hash-based histogram with B buckets, that is hashed on a given column value (hash[column value] B → bucket) and counts the frequency of all entries in each bucket. RoPE uses B = .</p><p>We also use reservoir sampling to pick a constant sized, random sample of the data owing through the operator. For each interesting column, RoPE collects up to samples but no more than KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Matching context to query expressions</head><p>As metadata to enable matching, with each stat collector we associate a hash-value that captures the location of the collector in the query graph. In particular, location in the query graph refers to a signature of the input(s) along with a topologically sorted chain of all the operators that preceded the stat collector on the execution plan. We colloquially refer to this as the query subgraph. RoPE uses bit hashes to encode these query subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adapting query plans</head><p>We build on top of SCOPE Cloud Query Optimizer (CQO) which is a cost-based optimizer. CQO translates the user-submitted script into an expression tree, generates variants for each expression or group of expressions and nally chooses the query tree with the lowest cost. However, lacking direct knowledge of query context, the CQO uses simple apriori constants to determine the costs and selectivities of various operators as well as the data properties. By providing exactly this context, RoPE helps the CQO make better decisions.</p><p>RoPE imports statistics uses a stat-view matching technique similar to the analogous method in databases <ref type="bibr">[, , ]</ref>. Statistics are matched in during the exploration stage in optimization, i.e. before implementing physical alternatives but aer equivalent rewrites have been explored. e optimizer then propagates the statistics oered at one location to equivalent semantic locations, e.g., cardinality of rows remains the same aer a sort operation and can be propagated. For expressions that have no direct evidence available, the optimizer makes-do by propagating statis- tics from nearby locations with apriori estimates on the behavior of operators that lie along the path. Such uncertain estimates are deemed to be of lower quality and are used only when other estimates are unavailable.</p><p>We extended the optimizer to make use of these statistics. Cardinality, i.e., the number of rows observed at each collection point, helps estimate operator selectivity and compare reordering alternatives. Along with selectivity, the computation costs of operations are used to determine whether an operation is worth doing now or later when there is less work for it. Costs also help determine the degree of parallelism, the number of partitions, and which operations to t within a task. Besides the choice of broadcast join, statistics also help decide when self-join or index-join are appropriate. Most of these optimizations are speciic to the context of parallel executions. We believe that there is more to do with statistics than what RoPE currently does, but our prototype ( §) suuces to provide substantial gains in production ( §).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PROTOTYPE</head><p>e prototype collects all the statistics described in §. Data stats are written to a distributed le system and the matching functionality ( §..) runs as a pre-processing step of the job compiler. e statistics collection code is a few thousand lines of C, which during code generation for each operator, gets placed into the appropriate location in the operator's dataow. Note that not all operators collect statistics, and even when they do, they do not collect all types of statistics. Collected statistics are passed to the C++ task wrapper via reeection which piggybacks them along with task status reports to the job manager. Composing statistics required a few hundred lines of code in the job manager.</p><p>We allow the compiler to specify varying requirements across the columns. e constants also can change, to trade-o costs for improved precision, based on previous statistics or other information that the compiler has such as required accuracy of an estimate.</p><p>Parts of the code are production quality to the extent that all of our results here are from experiments that run in Bing's production clusters. Rather than implementing every optimization possible with the statistics that RoPE collects, we built a subset up to production quality code in order to deploy, run and gain experiences from Bing's production clusters. We acknowledge that our prototype is just that, and there is more beneet to be achieved. Using these statistics required extensive changes in the SCOPE query optimizer involving several hundred lines of code spread over several tens of les.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EVALUATION</head><p>We built and deployed RoPE on a large production cluster that supports the Bing search engine at Microso.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>Cluster: is cluster constitutes of tens of thousands of -bit, multi-core, commodity servers; processes petabytes of data each day and is shared across many business groups. e core of the network is moderately oversubscribed and hence shuuing data across racks remains more expensive than transfers within a rack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Workload Evaluated: We present results from evaluating</head><p>RoPE on all the recurring jobs of a major business group at Bing and randomly chosen jobs from ten other business groups. e dataset has over jobs. We repeat each job over ten times for each variant that we compare. e only modiication we do to the jobs is to pipe the output to another location in the distributed le system so as to not pollute the production data. ough small, since we pick a large set of jobs, our dataset spans a wide range of characteristics. Latency of the unmodiied runs varied from minutes to several hours. Tasks ranged from small tens to hundreds of thousands. e largest job had several tens of stages. User-deened operations are prevalent in the dataset, as is common across our cluster. Compared Variants: For each job, we compare the execution plan generated by RoPE with the execution plan generated without RoPE. e latter is a strong strawman, since it uses apriori xed estimates of operator costs and selectivities and is the current default in our production clusters. Early results from a variant that was equivalent to Hive over Hadoop, i.e., one that translated users' jobs into literal map-reduce execution plans, were significantly worse than our cluster's baseline implementation.</p><p>Here, we omit those results. Metrics: Our evaluation metrics are the reduction in job completion time and resource usage. Resources include cluster slot occupancy computed in machine-hour units, as well as the total amount of data read, written and moved across low bandwidth (inter-rack) network links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">A Case Study</head><p>Many plan changes can be performed given the statistics that RoPE collects. We report a case study that il-  lustrates some of the changes to execution plans that achieved signiicant gains in practice. Aggregate results from applying RoPE to a wide variety of jobs are in §...</p><p>Consider a job that processes four datasets. Let these be requests (R), synonyms (S), documents (D) and classiied URLs (C). e goal is to compute how many requests (of a certain type) access documents (of a certain other type). Doing so, involves the following ve operations:</p><p>. Filter requests (R) by location. is operation has a selectivity of</p><p>x. . Join requests (R) with synonyms (S). ere are many synonyms per word, so the selectivity is x. . Apply a user-deened operation (UDO) to documents (D). e selectivity is</p><p>x but has a very large CPU cost per document. . Join documents (D) with the list of classiied URLs (C). is has a selectivity of x. . Join synonymized requests with documents containing classiied URLs; has a selectivity of x. . Finally, count number of requests per document URL.</p><p>e dataset sizes are R: GB, S: GB, D: GB, C: MB. <ref type="figure">Figure (</ref>b) shows the plan computed by the unmodied optimizer which uses apriori estimates on data properties and costs. Each circle represents a stage, a collection of tasks that execute one or more operations which are listed in the adjacent label (see legend in (a)). Unlabeled stages are aggregates []. e size of the circle denotes the number of tasks allocated to that stage, in logarithmic scale. e color of the circle in linear redscale denotes the average time per task in that stage, darker implies longer. <ref type="figure">Figure (d)((c))</ref> show the plan generated using (a subset of) the statistics provided by RoPE. We explain the gures as we go along. Table shows how well these plans do when executed, the metrics are averaged over ve diierent runs.</p><p>Recall that the compiler sets costs proportional to the data processed and sets selectivity based on the type of the operation-for instance, lters are assumed to be more selective than joins. ese apriori estimates have mixed results. ey pick the right choice for the operation on requests; the more selective lter operation () is done before joining with synonyms () (see <ref type="figure">Fig. (b)</ref> top lee). However, for documents, the plan performs the user-deened lter (UDO) () before the more selective join () leading to a lot of wasted work. As we see in Table , due to the high cost of the UDO, this plan takes over x the time of the next best alternative.</p><p>By providing an estimate of the UDO's selectivity, RoPE lets the compiler join the documents dataset with the classiied dataset rst. <ref type="figure">Figure (</ref>c) depicts such a plan (see change in middle right). From Table , we see that since the UDO is applied on fewer data, the median execution time improves substantially. And, not many more tasks are needed since fewer documents through the UDO means fewer net work, and so the cluster hours decrease as well. But, by performing the UDO later, more data is shuued across the network, since the join with classiieds is done on unnltered documents.</p><p>When comparing these two plans, note that the thickness of the edges represent data volume moving between stages in logarithmic scale (see legend). Also, the color indicates the type of data movement. Dark solid lines denote data ow from a partition stage to an aggregate stage (many to many) which has to move over the network. Light solid lines indicate one-to-one data ow. Here, data-local placement can avoid movement across the network. Dotted lines indicate broadcast, i.e., the source stage's output is read by every task in the destination stage. Double edges indicate two source tasks per destination task, i.e., at least one of the sides has to be moved over the network.</p><p>With the UDO at a better place, the bottleneck moves to two new places. e average task duration of the stage with the UDO is very high (deep red). If the compiler knew the cost of the UDO, it could apportion more tasks to this stage to resolve this bottleneck. Similarly, even though requests (R) becomes small aer applying the very selective lter (), the compiler is unaware and picks a pair-wise join for . is causes the large dataset S to be  <ref type="figure">(Fig. (c)</ref>)</p><p>.  <ref type="figure">Fig. (d)</ref>  partitioned and shuued across the network needlessly. <ref type="figure">Figure (d)</ref> shows the plan where RoPE provides the compiler with the selectivity and costs of all the operations. Intermediate plans have been omitted for brevity. <ref type="table">Table shows</ref> that the median execution of this plan improves by another .x. A few changes are worth noting.</p><p>First, operation is now implemented as a broadcast join. is not only saves cross rack shuue and reads/writes to disk but also avoids partitioning both these datasets. Since pair-wise joins shuue data across the network, such stages are more at risk from congestion induced outliers. We observed this with the default plan.</p><p>Second, given the high cost of the UDO (), the compiler instead of adding more tasks to the stage with operation defers the UDO till even later, i.e., till aer operations and . Both these operations are net data reductive, however is particularly so since it produces one row for each document url that is in the eventual output. e increase in cost from performing other operations on more data was lower than the beneets from performing the UDO on fewer data.</p><p>More optimizations are enabled recursively. e compiler realizes that both sides of the input for the join in operation are small due to the cumulative impact of earlier operations, leading to the third improvement-implement operation also as a broadcast join.</p><p>is leads to a fourth improvement that is subtle. Notice that operation , a reduce operation that needs data to be partitioned by document url to compute the number of times each URL occurs, now lies between operations and thereby eliminating one complete map-reduce! To understand why, note that the join in operation matches words in synonymized requests with words in the classiied documents. Typically this join would require both inputs to be partitioned on words. However, here there is so little data that the lee side (requests) can be broadcast and the right side (documents) can be partitioned in any way. Hence the right side is partitioned on url immediately aer it is read to facilitate operation and never re-partitioned thereby providing for coalescing many more operations into the same stage. e enabler for this optimization is little data-the later parts of most data parallel jobs can beneet from serial plans. Achieving the change, however, requires reasoning about the entire execution plan and not one stage at a time which is possible in RoPE due to the interface with a query optimizer.</p><p>Fiih, to oset the cost of the UDO, the compiler assigns more tasks (larger size) to the stage that now implements the UDO (along with operations and ). Doing so, is again a global change because the earlier operations have to partition the data enough ways. To benet from not re-partitioning, the compiler implements the join in with the same (larger) amount of parallelism.</p><p>Finally, a potential change that was not performed is worth discussing. Since the classiieds dataset is small (C), it is possible to implement operation also as a broadcast join. But the compiler chooses to not do that. e reason is that the documents have to be partitioned by url to facilitate the reduce in op . Either they are partitioned before the join op or aer. If the partition happens before, as is the case with the chosen plan, the large amount of work in the stage doing ops , and needs more parallelism resulting in more partitions, i.e., more tasks in op . But, the network costs of broadcasting C grow linearly with the number of tasks in op osetting the savings which is one additional read/write of C. Partitioning aer op would mean one more map-reduce on the critical path and just before the end of the job. Any outliers here would directly impact the job unlike outliers on the shuue before op which is not on the critical path since more work lies on the lee side of the DAG.</p><p>Overall, we see that signiicant reductions result from optimizations building on top of each other. Unfortunately, the space of optimizations is not monotonic; some times using more tasks is better, whereas at other times pushing a lter aer some operators but before some others is the best choice. By providing accurate estimates of code and data properties, RoPE is a crucial rst step towards picking appropriate execution plans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Aggregate results</head><p>Here, we present aggregated results across jobs in the dataset. ese are runs of production jobs in a production cluster, so the noise from other jobs and other trac on the cluster is realistic.</p><p>Figure (a) plots a CDF of the ratio between the job runtimes without and with RoPE. So, y = implies no beneet. Samples below that line are regressions while those above indicate improvement. To compute a single metric from a disparate set of jobs, we weight job by the total cluster hours that it occupies. All the results in this section share this format. <ref type="figure">Figure (a)</ref> shows that () of the jobs speed up by over x (.x). Several signiicant jobs see over x reduction in their latencies. Upon inspection we nd that all the samples under the y = line are from two jobs. Both jobs have a small number of stages (and tasks). RoPE does not change their execution plans. Noise due to the cluster is the likely cause for lower performance. e vast majority of jobs see performance improvements. e reason is due to one or more of the optimizations described in the case study above.</p><p>Figure (b) shows that the latency savings due to RoPE are not from simply using more resources. In fact by avoiding wasteful work, RoPE speeds up jobs while reducing cluster usage. At the th ((th) percentile, jobs use .x (.x) fewer cluster hours with RoPE.</p><p>Figure (c) shows that while the volume of cross rack shuue is lower for some jobs, it stays the same for many and increases for only a few. is is expected since while some of the optimizations enabled by RoPE lower cross rack shuue, others can increase it. Our optimization goal is to improve job latency and hence, on all the other metrics, we only indirectly constrain RoPE. Yet, we nd that RoPE mostly achieves its gains by shuuing fewer amounts of data across the network. <ref type="figure">Figure (b)</ref> shows a similar pattern for the data read and written to disk. <ref type="figure">Figures (a)</ref> and (c) show that RoPE's plans mostly use fewer tasks and stages, though sometimes, for e.g., when osetting the cost of UDOs, RoPE can use more tasks.</p><p>In summary, RoPE judiciously uses resources to improve job latency. Some gains accrue from avoiding wasted work, others from trading a little more of one type of resource for large savings on another while still others accrue from balancing the parallel plans. Orthogonally, RoPE generates better execution plans by leveraging data and execution insights. e AutoAdmin project examined adapting physical database design, e.g., choosing which indices to build and which views to materialize, based on the data and queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Closer to us, is the work that adapts query plans based on data. <ref type="bibr">Kabra and DeWitt []</ref> were one of the earliest to propose a scheme that collects statistics, re-runs the query optimizer concurrently with the query, and migrates from the current query plan to the improved one, if doing so is predicted to improve performance. ey mainly address the challenges of trading o re-optimization vs. doing actual work and of re-using the partial executions from the old plan to avoid wasting work that is already done. ese challenges are easier in the context of map-reduce while collecting statistics is harder due to the distributed setting. Further, RoPE explores new opportunities arising due to the parallel nature of plans.</p><p>Eddies [] adapts query executions at a much ner, pertuple, granularity. To do so, Eddies (a) identiies points of symmetry in the plan at which re-ordering can happen without impacting the output, (b) creates tuple routing schemes that adapt to the varying selectivity and costs of operators. RoPE looks at a disjoint space of optimizations (choosing appropriate degrees of parallelism and operator implementations), which are not easily cast into Eddies' tuple routing algorithm.</p><p>Starsh [] examines Hadoop jobs, one map followed by one reduce, and tunes low-level connguration variables in Hadoop such as io.sort.mb. To do so, it constructs a what-if engine based on a classiier trained on experiments over a wide range of parameter choices. Results show that the prescriptions from Starsh improve on developer's rules-of-thumb on non-traditional servers (e.g., fewer memory or cores). RoPE is complementary because (a) it applies to jobs that are more complex than a map followed by a reduce, (b) explores a larger space of plans and (c) uses a cost-based optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head><p>Recurring jobs are a rst-order use case in our production system. We nd that RoPE achieves meaningful improvements to such jobs. However, it is important to note that statistics from a run only cover sub-graphs of operations used in that execution plan. is information may not suuce to nd the optimal plan. Aer a few runs, we usually nd all the necessary statistics since each run can explore new parts of the search space. However, plans chosen during this transition are not guaranteed to monotonically improve. In fact, there are no general ways to bound the worst case impact from plans chosen based on incomplete information. As a result, RoPE largely errs on the side of very conservative changes.</p><p>We defer to future work some advanced techniques that choose plans given uncertainity regimes over statistics or choose a set of plans, each of which has an associated validity range speciied over statistics, and switch between these plans at runtime depending on the observed statistics []. Clearly these techniques are more complex than RoPE, the risks from picking worse plans are larger here, and to the best of our knowledge using these ideas in the context of parallel plans is an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">FINAL REMARKS</head><p>Results from a deployment in Bing show that leveraging properties of the data, the code and their interaction signiicantly improves the execution of data parallel programs. e improvements derive from using statistics to generate better execution plans. Note that these improvements are mostly orthogonal to those from solving runtime issues during the execution of the plans (e.g., outliers, placing tasks). ey are also in addition to those accrued by a context-blind query optimizer over literally executing the programs as speciied by the users.</p><p>While RoPE leverages database ideas, we believe that the realization in the context of data parallel programs is interesting due to challenges that are new (e.g., distributed collection), or are more important in this context (e.g., user deened operations) and novel opportunities for improvement (e.g., recurring jobs, little data and the optimizations speciic to parallel plans such as choosing degree of parallelism to achieve balance).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) or use compiler techniques (e.g., FlumeJava []). Other declar- ative frameworks cast the execution plan choice as the traditional query optimization problem (e.g., SCOPE [], Tenzing [], Pig []).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure : Data that remains in ight when a job has executed for (or ) of its running time.</head><label></label><figDesc>Figure : Data that remains in ight when a job has executed for (or ) of its running time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure : Motivating examples for re-optimizing data paral- lel computing</head><label></label><figDesc>Figure : Motivating examples for re-optimizing data parallel computing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure : Variation in Operation costs; in time to process and memory usage, per unit data processed</head><label></label><figDesc>Figure : Variation in Operation costs; in time to process and memory usage, per unit data processed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure</head><label></label><figDesc>Figure : Imbalance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure :</head><label></label><figDesc>Figure : RoPE's architecture for re-optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure : A task can have many operations and hence, col- lectors. A job manager composes individual statistics.</head><label></label><figDesc>Figure : A task can have many operations and hence, collectors. A job manager composes individual statistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig plots the relative cost values attributed to the expensive operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure : Normalized memory and user CPU costs for the operators identiied as expensive.</head><label></label><figDesc>Figure : Normalized memory and user CPU costs for the operators identiied as expensive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure : e RoPE prototype consists of key components: a distributed statistics collection module, a pre-processor to the existing SCOPE compiler that provides matching func- tionality and a basic on-the--y runtime optimizer.</head><label></label><figDesc>Figure : e RoPE prototype consists of key components: a distributed statistics collection module, a pre-processor to the existing SCOPE compiler that provides matching functionality and a basic on-the--y runtime optimizer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure : Case Study: Evolution of the execution plan as RoPE provides more statistics. See Table for how well these plans do upon execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure : Aggregated results from production dataset (see ..)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Recent work on data-parallel cluster computing frame- work has mainly focused on solving issues that arise dur- ing the execution of jobs, by sharing the cluster [, ], tackling outliers [], fairness vs. locality [] and net- work scheduling []. Others incorporate new functional- ity such as the support for iterative and recursive control ow [].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>!</head><label></label><figDesc>Figure : Aggregated results from production dataset (see ..)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table : Statistics that RoPE collects for reoptimization</head><label>:</label><figDesc></figDesc><table>∑ 

task i 
task 1 

task n 

. 
. 
. 
. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Suman Nath, Peter Bodik, Andrew Ferguson, Jonathan Perry, our shepherd Roxana Geambasu and members of the UC Berkeley AMP Lab for comments that improved this work. e Cosmos team has built and keeps running the fabulous system without which this work would not have been possible.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reining in the Outliers in MapReduce Clusters Using Mantri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Eddies: continuously adaptive query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Avnur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Proactive re-optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting statistics on query expressions for optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SCOPE: Easy and EEcient Parallel Processing of Massive Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Flumejava: easy, eecient data-parallel pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raniwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tenzing a sql implementation on the mapreduce framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Managing data transfers in computer clusters with orchestra</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Loglog counting of large cardinalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Flajolet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESA</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Galindo-Legaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Waas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-C</forename><surname>Wu</surname></persName>
		</author>
		<title level="m">Statistics on views</title>
		<imprint/>
	</monogr>
	<note>VLDB</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic management of data and computation in datacenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Gunda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ravindranath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Ekkath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Starsh: A self-tuning system for big data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mesos: a platform for ne-grained resource sharing in the data center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hindman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed Data-parallel Programs from Sequential Building Blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurosys</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fair scheduling for distributed computing clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Currey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EEcient mid-query re-optimization of sub-optimal query execution plans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Optimizing data partitioning for data-parallel computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotOS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cardinality estimation using sample views with quality assurance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Larson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Approximate frequency counts over data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">C: a universal execution engine for distributed data--ow computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pig Latin: A Language for Data Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tritonsort: a balanced large-scale sorting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sharing the data center network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hive-a warehousing solution over a map-reduce framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Usoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delay scheduling: a simple technique for achieving locality and fairness in cluster scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Sarma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
