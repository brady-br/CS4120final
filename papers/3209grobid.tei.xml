<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T03:04+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Transparent System Call Based Performance Debugging for Cloud Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Khadke</surname></persName>
							<email>nkhadke@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Kasick</surname></persName>
							<email>mkasick@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soila</forename><forename type="middle">P</forename><surname>Kavulya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Tan</surname></persName>
							<email>jiaqit@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Narasimhan</surname></persName>
							<email>priyan@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Transparent System Call Based Performance Debugging for Cloud Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Problem diagnosis and debugging in distributed environments such as the cloud and popular distributed systems frameworks has been a hard problem. We explore an evaluation of a novel way of debugging distributed systems , such as the MapReduce framework, by using system calls. Performance problems in such systems can be hard to diagnose and to localize to a specific node or a set of nodes. Additionally, most debugging systems often rely on forms of instrumentation and signatures that sometimes cannot truthfully represent the state of the system (logs or application traces for example). We focus on evaluating the performance debugging of these frameworks using a low level of abstraction-system calls. By focusing on a small set of system calls, we try to extrapolate meaningful information on the control flow and state of the framework, providing accurate and meaningful automated debugging.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Performance problems are both common and inevitable in large scale computing, with root causes varying widely, from hardware issues to logical errors in software. One of the most prevalent forms of large scale computing is afforded by Google's MapReduce framework. MapReduce is a programming framework and paradigm for parallel distributed computation on commodity computer clusters <ref type="bibr" target="#b9">[10]</ref>, which allows programmers to easily process large data, by abstracting away low-level details of distributed execution from user code. The leading open-source implementation, Hadoop is used daily at large companies such as Yahoo! and Facebook to process petabyte-scale data <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b3">[4]</ref>.</p><p>Debugging MapReduce frameworks is a conventionally hard problem due to its massive scale and distributed nature. Often various forms of instrumentation have been used to debug MapReduce frameworks that include programmer-chosen debugging logs, MapReduce system logs, application traces, etc. Although these methods have various benefits, they are often highly dependent on programmer responsibility and often have a limited use in a distributed setting, as they often only provide information from the context of the application, node or environment they are running and cannot be effectively used in the use of debugging the overall state of the MapReduce framework.</p><p>Our work with system calls focuses on diagnosing performance issues that cause significant degradation in the system's overall performance. Specifically, we focus on diagnosing disk and network related issues that can affect MapReduce performance. Our work seeks to explore and evaluate the extent to which syscall-based instrumentation is useful in diagnosing these problems in MapReduce frameworks.</p><p>The contributions of this paper are -(1) a new approach that exploits system call instrumentation to automatically and transparently diagnose performance problems in MapReduce frameworks, (2) a statistical diagnosis algorithm that correlates system calls occurrences and timings to localize node(s) that is/are responsible for a performance problem, and (3) a semantic diagnosis algorithm that parses and correlates system call output from different nodes to identify a node that is responsible for a performance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background &amp; Problem Statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MapReduce Framework</head><p>A MapReduce job consists of two main abstractions, a Map task and a Reduce task that are specified by the programmer <ref type="bibr" target="#b9">[10]</ref>. The Map task is first applied locally on each node on some segment of the input data, and its output is then acted upon by the Reduce task. The MapReduce framework splits the input dataset into smaller independent partitions, and creates multiple instances of Map and Reduce tasks to operate on each partition in parallel. Our work focuses on Hadoop, which is an opensource, Java implementation of MapReduce. Hadoop has a master-slave architecture, with a single master and multiple slave hosts. Hadoop consists of an execution layer which executes Map and Reduce tasks, and the Hadoop Distributed Filesystem (HDFS), which is a an implementation of the Google FileSystem <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation</head><p>We propose the use of system calls (syscalls) as a novel way of debugging MapReduce frameworks, as we believe that syscall event-streams present a rich source of statistical and semantic information for performance problem diagnosis in MapReduce frameworks. Syscalls are preferred in MapReduce frameworks for the following reasons:</p><p>i High Reliability. Syscalls in various architectures are essentially consistent and provide a uniform way of analyzing a given characteristic, as opposed to application traces or programmer-inspired debugging that can be variable and unreliable. ii No dependence on Hardware Architecture. Often debugging solutions assume a given architecture, which is not a safe assumption in MapReduce frameworks that themselves make no assumptions on the underlying system architecture. iii Insight into underlying system information.</p><p>Syscalls involve switches to kernel space and can provide more information on kernel decisions, the file system, networking, threading, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Goals</head><p>i Application-transparency. There should be no modifications to current MapReduce programs. ii Minimize false-positive rate. Our approach should be able to correctly distinguish between anomalous behavior with a low rate of false-positives. iii Problem Coverage. Our approach aims to diagnose performance problems that involve network and disk related issues that result in degraded performance of the MapReduce instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Non-Goals</head><p>i Code-level debugging. We do not aim to provide indicators of where software might be failing such as isolating responsible sections or lines of source code, but only seek to identify the culprit node and the performance problem it is facing in the MapReduce framework. ii Optimized instrumentation overheads. Our current implementation of collecting syscall instrumentation imposes significant monitoring overhead and we focus only on an evaluation of a proof-ofconcept implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Assumptions</head><p>i A majority of the MapReduce nodes exhibit fault-free behavior.</p><p>ii All of the MapReduce nodes have identical hardware configurations, memory, network access, etc. This is a reasonable assumption as most instances of MapReduce clusters are designed to have the same "technical specifications".</p><p>iii Time on each MapReduce node is synchronized. We rely on this assumption, since we use timebased syscall instrumentation to correlate behaviour across nodes in the framework.</p><p>3 System Call Instrumentation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tools</head><p>We are using the unix tool strace <ref type="bibr" target="#b11">[12]</ref> to attain system call instrumentation. strace provides various utilities that are useful in attaining time-based and countbased syscall instrumentation on a running MapReduce instance. Specifically we use two modes: strace -cf. This provides the following information about each monitored syscall: total number of invocations, number of failed invocations, average and total time spent in a syscall and the percentage of time spent in a syscall with respect to other monitored syscalls. strace -fTttt. This provides a time-based log that prints out syscall invocations with their arguments, which can be used to trace the control flow of the MapReduce instance in a distributed manner. It also prints the total time spent in each syscall, with the appropriate return codes. The -f flag in both modes of strace ensures that child processes spawned on a given node are also recursively monitored and profiled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Call Scope</head><p>We focus on a small set of syscalls that to our knowledge can be used in determining common performance issues in a MapReduce framework. We focus on 2 primary classes of syscalls -network (accept(), connect(), bind(), socket()) and filesystem related syscalls (access(), stat()). We also monitor execve().</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Network related syscalls</head><p>We only monitor the above system calls because they form the basis of TCP/IP network communication <ref type="bibr" target="#b1">[2]</ref>. We do not monitor any data sent across the network because there is a high overhead in tracing these calls and typically these calls had variable invocation times based on the data that each system call was responsible for sending/receiving. As a result, it was hard to tell if a node was undergoing genuine network-related problems or transferring variable data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Filesystem related syscalls</head><p>We believe that distributed filesystems check the information of a file before performing a read/write operation on the file. Like the network related calls, we do not monitor read and write syscalls, because their invocation times are dependent on the data they act on. Hence, we only monitor calls that check the information of a file, which are typically quick to return unless possible diskrelated issues may cause them not to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Other syscalls</head><p>We believe that execve() and its variants provide an idea of how a spawned task performs on the overall in the MapReduce framework, and as a result can be useful in providing information on any underperforming nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Statistical Diagnosis</head><p>For each run and syscall on a node, we construct a histogram of the number of syscall invocations for each invocation time bucket. Essentially, we bucket the system calls by their invocation times and present a histogram that has the number of such invocations for each bucket. We also store the total time spent in each system call on each node, which should equal the total area of the histogram referred to above. Across runs, we mantain and update the minimum and maximum counts for each time bucket and the total time spent in each system call. It is these minima and maxima for both histograms' buckets and overall time for a system call that constitute the lower and upper thresholds for admissible Hadoop behavior. Theoretically with sufficient runs, any behavior outside these minimum and maximum thresholds indicate the possibility of anomalous behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Semantic Diagnosis</head><p>For each control run, we store the time taken for each unique system call invocation for each node and over many runs update the minimum and maximum times. It is these minima and maxima that constitute the thresholds for non-anomalous Hadoop beahvior in the semantic algorithm. This way when we semantically parse verbose syscall logs, a system call invocation with matching parameters and return codes that has a timing that is outside these admissible thresholds, could indicate the possibilty of a performance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Statistical System Call Based Diagnosis</head><p>In this approach, we build a histogram of the number of invocations of a given syscall for different invocation time buckets for that syscall (like above) and also store the total time spent in each syscall for each node. We then compare this histogram and total time values to that constructed in the predefined analysis above and use the comparisons to automatically detect possible performance problems. We base our algorithm on the hypotheses highlighted above. Our algorithm in rough pseudocode is:</p><p>1. disk-hog detection: examine the following stat() information across all nodes:</p><p>(a) max ← node with the highest total time spent in stat(). stat max ← total time in stat() for max.</p><p>(b) if stat max is outside the total time stat() threshold then: compute pairwise percentage differences between the total timings of stat() across all nodes. if percentage differences between all other nodes and max is greater than 25% and the total timing in access() on max is also outside threshold then: we have disk-hog. goto Node Isolation.</p><p>2. network-hog detection: examine the following connect() information across all nodes.</p><p>(a) max ← node with the highest total time spent in connect(). connect max ← total time in connect() for max.</p><p>(b) if connect max is outside the total time connect() threshold then: compute pairwise percentage differences between the total timings of connect() across all nodes. if percentage differences between all other nodes and max is greater than 25% then: we have network-hog. goto Node Isolation.</p><p>3. Node Isolation: examine the following information across all nodes.</p><p>(a) foreach system call s in (bind,socket,execve) do:</p><p>For every syscall bucket value that is outside the threshold for that bucket, add the difference to scr s . Compute pairwise percentage comparison between scr s values across all nodes if any node has more than a 50% difference then: mark that node as "possibly faulty".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Examine total time spent in accept() across all</head><p>nodes. if there exist values that are outside thresholds of accept() timings and we did not arrive here from a possible network-hog then: mark that node as "possibly faulty". 5. return the set of all nodes that are marked max and "possibly faulty".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic Syscall Call Based Diagnosis</head><p>In this approach, we consider syscall invocations as events in time stream and parse strace output for each syscall and try to isolate spikes. The algorithm bases itself on scores scr for each pairwise node combination and semantic parsing of strace logs. We take each sycall invocation and check if it matches (same syscall, arguments and return code) any syscall stored in the predefined analysis. If a match exists and the invocation time is outside the admissible thresholds for that syscall invocation, we mark that node as "possibly faulty". If the error involves filesystem calls, we report a disk-hog.</p><p>If we see out-of-threshold connect values we report a network-hog. In the case of network syscalls we try to parse parameter information when available to see which node the current node is trying to communicate with. In the case the system call invocation is out-of-threshold, we increment the corresponding node pair's scr value.</p><p>At the end, we compare the scr values across all pairs and isolate a node as "faulty" if all the highest scr values involve that node with a different node. If we are unable to satisfy this condition or we cannot find a possible hog, we do not diagnose anything. This algorithm aims to isolate the single node that is undergoing a performance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We perform our experiments and instrumentation on a cluster of 5 identical machines running Hadoop 0.20.1. Each node consists of an AMD Opeteron 1220 dualcore CPU with 4GB memory, Gigabit Ethernet, dedicated 320GB disk for Hadoop and runs the amd64 version Debian/GNU Linux 4.0. The machines run in stock configuration and with no background tasks. The results we report are based on Hadoop MapReduce jobs being run on these 5 machines, with a single master node and 4 slave nodes. These experiments are designed to run in approximately at most 20 minutes. To ensure a consistent experimental setup, we reboot and reinitialize Hadoop settings on each machine before each experiment. From here we run a given workload and either run a control run (no fault injection) or inject a given fault. Once the workload begins, we monitor the syscall activity with strace and record the local output of strace on each node. After the workload is finished, we run into the diagnosis phase if a fault was injected. Here we analyze all the logs from each node and make an assertion on which node is a culprit node in the setup, and diagnose it as a problematic node with a given cause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Workloads</head><p>We run one of two possible MapReduce workloads: i wc : a naive wordcount program that outputs the total number of occurrences of all the words in a given text corpus of 100,000 words. ii sort : a naive program that sorts a randomly generated set of 100,000 integers. Additionally, we run each workload an equal number of times in both speculative and non-speculative execution. Hadoop attempts to realize performance problems in its nodes under speculative execution <ref type="bibr" target="#b18">[19]</ref>. In this mode of execution, Hadoop makes an effort to reduce the impact of underperforming nodes by scheduling redundant copies of a given Map task on various nodes, and picking the fastest node's result for that computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Problems Injection</head><p>When we run the workloads above they run in two possible modes, either as a control experiment or as an instance that has a fault injected into it. If we choose to execute a fault-injected run of a workload, we inject a fault into a predetermined node at a specific time and for a fixed duration of 360s during the workload. Specifically these faults are:</p><p>i disk-hog. Write 2GB chunks continually to the disk and "hog" access to the disk. ii network-hog. Drop 5%, 20% and 50% of the packets in a network stream. To handle the fast-growing and verbose output of strace, we have designed a framework that is able to run strace on all the nodes in MapReduce cluster and synchronize this information across the nodes. After the experiment terminates, we collect the log information in an automated manner and diagnose a possible bug or problem on a specific node in the MapReduce instance, by analyzing the syscall instrumentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results &amp; Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Terminology</head><p>We define the output node set as all the nodes that are outputted by our agorithms (and believed to be faulty). We define the diagnosis success or true-positive rate as the ratio of runs where a fault-injected node was correctly identified in the problem node set of size at most 3 with the right cause over that of all runs for that workload with that fault. If the output node set is non-empty and does not contain the fault-injected node or we provide a wrong cause, we contribute this run to a false positive run and define false positive to be the ratio of false positive runs over all fault-injected runs for a particular fault.</p><p>Diagnosis Success False Positive Speculative 0.88 0.13 Non-Speculative 0.88 0.13 <ref type="table">Table 1</ref>: Statistical Diagnosis for disk-hog</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Statistical Syscall Based Diagnosis</head><p>We realize that for disk-hog behaviour we have the following characteristics for nodes in the output node set:</p><p>i Majority of process' time is spent in a stat() syscall. Since the node is experiencing a disk-hog, it takes it a much larger time to access file information for files. ii The average time spent in a stat() and access call is significantly higher than that of other nodes. Since disk-hog is affecting a common distributed filesystem, the HDFS, we see that it should take a longer time for file information to be accessed across nodes affected by the disk hog. iii With high occurrence, a much smaller chunk of the process' time is spent in the execve() call as compared to that of other non-faulty nodes.</p><p>A large file-related time reduces the percentage of time spent in other non-file related calls such as execve() We realize that for network-hog behaviour we have the following characteristics for a node in the output node set:</p><p>i A significant increase in connect() latency.</p><p>Since the faulty node is experiencing a networkhog, it follows that there should be an increased latency for making an end-to-end connection with another listening/broadcasting network port. ii A significant drop in accept() times as compared to that of other nodes. Non-faulty nodes can accept connections normally, however because the faulty node is experiencing a network-hog, the faulty node cannot validate and accept a connection as fast as non-faulty nodes and often returns quickly as an error. iii More marked exhibition of the above behaviour at higher network-hog levels. This intuitively follows since the greater we hog the network, the more apparent the effects it would have on the syscalls responsible on handling network-related information and flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Semantic Syscall Based Diagnosis</head><p>We summarize our results for disk-hog in <ref type="table">Table 2</ref>.</p><p>For network-hog <ref type="figure" target="#fig_1">(Figure 2)</ref>, we realize that at smaller network-hog levels, it becomes semantically harder to differentiate genuine performance problems due to network-hog as opposed to inherent network latency that  <ref type="table">Table 2</ref>: Semantic Diagnosis for disk-hog is possible in normal execution. We realized that at lower levels, the "spikes" we were seeking were admissible in the context of normal execution, and as a result this reduced the effectiveness of our semantic diagnosis. On higher network-hog levels, the "spikes" due to network hog were more apparent and lasted consistently enough for our diagnosis algorithm to identify culprit nodes. Under speculative execution, the Hadoop framework reduces the effective underperformance on a culprit node, by reducing the overall execution on that node. As a result, it became harder to realize faults on a node under speculative execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Execution Models</head><p>We realize the following: i Statistical diagnosis is more effective on nonspeculative execution as compared to speculative execution workloads. Since Hadoop makes a speculative effort to reduce the load on what it thinks is an underperforming node, by scheduling redundant copies of a task on various nodes, and choosing the first successful completion, it reduces the effectiveness of our semantic diagnosis algorithm. Since an underperforming node will traditionally run for a much longer period and get a signal to abandon its computation after another node successfully computes the same result, its period of poor performance becomes less significant over the running time of the workload. As a result, the performance problem on a given culprit node become increasingly semantically insignificant, and it becomes harder to realize and diagnose. ii Semantic diagnosis is less effective than statistical diagnosis under speculative execution. Since speculative execution prevents further execution of a given task on an underperforming node, our semantic algorithm is less exposed to faulty behavior and as a result this makes it harder to realize a performance problem. From a statistical point of view, the underperforming node consistently fails across various Map tasks and as a result can be statistically isolated as a probable underperforming node in a run of many Map tasks in a given workload. More importantly, our semantic algorithm focuses strictly on isolating only 1 node that is responsible for the performance problem. In the statistic case, a set of nodes are sometimes returned, which is much easier to ascertain in any runtime environment. Because of this qualitative difference, it is easier for the statistic algorithm to identify at least the node which is undergoing a performance problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Workload Independence</head><p>We realized that in all workload runs under various settings, the performance issues largely affected the framework, and the diagnosis success was independent of the workload run. Since the Hadoop framework is a complex, distributed framework, we hypothesize that a large amount of the system calls being traced were attributed to calls to initialize, run and maintain the Hadoop framework.</p><p>Since the performance issues we tested were generic performance problems that are common to most distributed systems, we can possibly extend this approach to other complex distributed frameworks. Firstly, this finding implies that we can diagnose a greater variety of problems on various workloads with a relatively similar diagnosis success since a large amount of the system calls are attributed to the Hadoop framework, and we can analyze the changes in the system call behavior in the framework itself to diagnose generic performance problems that can apply to a wide variety of workloads. Secondly, this implies that further work must be directed into exploring Hadoop-specific bugs such as faulty/slow Map and Reduce keys and evaluating its associated system call instrumentation.</p><p>7 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Diagnosis for MapReduce frameworks</head><p>X-Trace <ref type="bibr" target="#b6">[7]</ref> was used to instrument Hadoop and provided fine-grained trace events and a summarized views as a form of instrumentation. Additionally, <ref type="bibr" target="#b17">[18]</ref> [13] <ref type="bibr" target="#b0">[1]</ref> focused on mining various log/error tracing methods in addition to supervised learning to aid in problem diangosis. State machine views of logs and visualization tools on top of this analysis was explored by <ref type="bibr" target="#b16">[17]</ref> [11] <ref type="bibr" target="#b15">[16]</ref>. These approaches use many other forms of instrumentation, but our approach to our knowledge is the first syscall based performance debugging method that identifies the rootcause of a performance problem in MapReduce frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">System Call Based Diagnosis</head><p>[5] looked at developing a reliable system view based on system call instrumentation to prevent and detect any security issues. <ref type="bibr" target="#b5">[6]</ref> [9] use sequences of system calls to isolate atypical program execution. Forensix <ref type="bibr" target="#b7">[8]</ref> uses system calls to capture system call timing and parameters to infer possible security flaws or incidents. <ref type="bibr" target="#b13">[14]</ref> focused on a similar use of syscalls to debug the PVFS framework, but our approach differs from that of both Forensix's and <ref type="bibr" target="#b13">[14]</ref>, as we deal with a qualitatively different framework from PVFS or do not concern ourselves with security. Additionally, we do not use the error-based semantic correlation approach highlighted in <ref type="bibr" target="#b13">[14]</ref> or log security concerns as in Forensix. To the best of our knowledge, we do not know any other approach that uses system calls to diagnose performance problems in MapReduce frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion &amp; Future Work</head><p>We have presented a novel way of debugging performance problems in MapReduce frameworks using system call instrumentation. We realize that this a relatively effective way to diagnose performance problems, with greatest effect in non-speculative environments. Our approach resulted in the greatest success when isolating disk related performance problems. In the future, we aim to reduce strace's runtime overhead with a custom syscall tracer. We are also presently evaluating our diagnosis apporach on a 20-node cluster with real-world workloads, with a focus on diagnosing Hadoop-specific bugs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Plot of various metrics against network-hog levels for statistical diagnosis. Diagnosis Success False Positive Speculative 0.63 0.00 Non-Speculative 0.69 0.06 Table 2: Semantic Diagnosis for disk-hog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Plot of various metrics against network-hog levels for semantic diagnosis.</figDesc></figure>

			<note place="foot" n="4"> Approach 4.1 Predefined Analysis and Thresholds Both the statistical and semantic algorithms, rely on predefined analysis of a workload, which we refer to as the process of profiling the steady, average state of the system. To do this, we run a series of control experiments for each workload and record the following information: All of strace -cf information and a record of every unique system call invocation with corresponding arguments, the return code and the time taken for the method to return. We call this time the syscall invocation time. Based on this information we do the following:</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgements</head><p>This research was sponsored in part by the project CMUPT/RNQ/0015/2009 (TRONE-Trustworthy and Resilient Operations in a Network Environment), and by Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Blind Men and the Elephant: Piecing Together Hadoop for Diagnosis</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Software Reliability Engineering (ISSRE)</title>
		<meeting><address><addrLine>Mysuru, India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Know your TCP system call sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupama</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Software Foundation</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://hadoop.apache.org/core" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apache</forename><surname>Software Foundation</surname></persName>
		</author>
		<ptr target="http://wiki.apache.org/hadoop/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A building block approach to intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Crosbie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>And Kuperman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Intrusion Detection (RAID)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling system calls for intrusion detection with dynamic window sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eskin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
		<idno>2001. DISCEX&apos;01. Proceedings</idno>
	</analytic>
	<monogr>
		<title level="m">DARPA Information Survivability Conference &amp; Exposition II</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="165" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">X-trace: A pervasive network tracing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fonseca</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX conference on Networked systems design &amp; implementation</title>
		<meeting>the 4th USENIX conference on Networked systems design &amp; implementation</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="20" to="20" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Forensix: A robust, high-performance reconstruction system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>And Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walpole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Computing Systems Workshops, 2005. 25th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="155" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Intrusion detection using sequences of system calls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hofmeyr</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Somayaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer security</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="151" to="180" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Ghemawat</forename><surname>Mapreduce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Operating Systems Design and Implementation</title>
		<meeting><address><addrLine>San Francisco, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-12" />
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Log-Based Causal Tracing for Performance Debugging of MapReduce Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kavulya</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gandhi</forename><forename type="middle">R P</forename><surname>And Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 30th International Conference. Mysuru, India</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note>Distributed Computing Systems (ICDCS)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linux</forename><surname>Manual Pages</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strace</surname></persName>
		</author>
		<ptr target="http://linux.die.net/man/1/strace" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining dependency in distributed systems through unstructured logs analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lou</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>And Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="91" to="96" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">System-Call Based Problem Diagnosis for PVFS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Kasick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><forename type="middle">A</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">E</forename><surname>Marinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev Gandhi And Priya</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Hot Topics in System Dependability</title>
		<meeting>the 5th Workshop on Hot Topics in System Dependability<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Google file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles. Lake George</title>
		<meeting><address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-10" />
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mochi: visual log-analysis based tools for debugging hadoop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And Kavulya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 conference on Hot topics in cloud computing</title>
		<meeting>the 2009 conference on Hot topics in cloud computing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SALSA: analyzing logs as state machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>And Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>And Kavulya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soila</forename><surname>And Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First USENIX conference on Analysis of system logs</title>
		<meeting>the First USENIX conference on Analysis of system logs<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6" to="6" />
		</imprint>
	</monogr>
	<note>WASL&apos;08, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting large-scale system problems by mining console logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles</title>
		<meeting>the ACM SIGOPS 22nd symposium on Operating systems principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="117" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yahoo Developer</forename><surname>Network</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Speculative Execution</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
