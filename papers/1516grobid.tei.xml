<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:01+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Preliminary Analysis of TCP Performance in an Enterprise Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Nechaev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Helsinki Institute for Information Technology, ‡ International Computer Science Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Allman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Helsinki Institute for Information Technology, ‡ International Computer Science Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vern</forename><surname>Paxson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Helsinki Institute for Information Technology, ‡ International Computer Science Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Gurtov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Helsinki Institute for Information Technology, ‡ International Computer Science Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Preliminary Analysis of TCP Performance in an Enterprise Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Although TCP behavior is one of the most studied aspects of Internet traffic, little is known about TCP performance within modern enterprise networks. In this paper we analyze aspects of TCP performance observed in packet traces taken over four months from a medium-sized enterprise. We assess the prevalence of broken TCP transactions, applications used, throughput of TCP connections, and phenomena that influence performance , such as retransmissions, out-of-order delivery, and packet corruption. While much remains to explore, this work represents a first step towards understanding TCP performance in the under-studied environment.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Researchers have studied TCP performance in many ways and in many different environments over the years <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>. However, one area that has remained understudied is within complex enterprise networks. We believe two key reasons account for the lack of attention paid to enterprise networks. First, passively monitoring enterprise traffic is logistically difficult. Whereas monitoring wide-area traffic between a particular institution and the rest of the Internet can be readily accomplished by instrumenting at most a handful of vantage points, enterprise traffic often does not traverse central monitoring locations. Therefore, assessing enterprise networks involves taking measurements from a large number of vantage points-either from switches, as we do, or from end hosts themselves <ref type="bibr" target="#b1">[2]</ref>-and then synthesizing a more complete picture from these. The second issue with enterprise networks is that they are often viewed as working "well enough". While complex, these networks do not have the same sort of cross-organizational issues that wide-area networks must contend with. In addition, low latencies and abundant network capacity can often mask problems with protocols and algorithms.</p><p>While enterprise networks may work "well enough," they are largely a black box at present: we have little idea whether the performance they deliver in fact matches our mental models of low-latency, loss-free, high-bandwidth pipes. Therefore, while logistically daunting, we argue that developing an understanding of performance across such networks holds the potential to drive innovation in terms of better-functioning network technologies.</p><p>This paper represents an initial step towards grappling with assessing performance issues within enterprise networks. We base our analysis on a dataset consisting of switch-level packet traces taken at the Lawrence Berkeley National Laboratory (LBNL) over the course of four months. Our study builds on our extensive previous efforts to calibrate the traces <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The dataset used in this study comes from monitoring individual switch ports inside the Lawrence Berkeley National Laboratory (LBNL) from <ref type="bibr">October 24 2005</ref><ref type="bibr">through March 7 2006</ref>. Each of our 50 traces contains all traffic seen on each of ten switch ports mirrored simultaneously. Our goal was to monitor each set of switch ports for 24 hours and then move our monitoring apparatus to a different set of ports (a manual process). However, for logistical reasons we did not always achieve this ideal, and while our dataset includes 44 traces that cover more than 20 hours, we also have shorter traces. In total, the traces directly monitored 351 distinct hosts out of an estimated 8,000-10,000 wired hosts at the institute during that time. See <ref type="bibr" target="#b4">[5]</ref> for a more detailed description of our data collection methodology, and for a discussion of the calibration techniques we applied to the data in this paper to remove measurement artifacts.</p><p>The overall dataset consists of 509 million TCP packets. As our interest in this work concerns intra-enterprise TCP traffic, we winnow the traces to the 292 million TCP packets that did not leave LBNL's network. We analyze these packet traces using Bro <ref type="bibr" target="#b7">[8]</ref> 1.5.1 to generate connection logs for each trace (including using Bro's analy.bro analysis script and the corresponding TCPStats module). The dataset contains 532K TCP connections.</p><p>Note, we disabled Bro's standard TCP inactivity timeout. We found that it led to multiple connections with the same addresses and ports being counted as different connections because of packets coming widely apart. These appear to reflect switch flooding, whereby the switch does not know to which port to direct a packet, and therefore transmits it on each port before (re-)learning which port it should be directed towards. (Note, these packets likely represent legitimate connections that are not within the general purview of our monitoring system.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Connection Status</head><p>We first analyze each connection in our dataset for its "final state" as computed by Bro, which represents how each connection was instantiated and terminated. For instance, the "SF" state indicates a connection that was observed to have been established using TCP's standard three-way SYN handshake and terminated with a FIN handshake, whereas "REJ" indicates a SYN was used to start a connection, but the connection was immediately RST (thus rejected) by the responder. We observe 13 final states in our dataset in various frequencies across trace files. <ref type="figure" target="#fig_0">Figure 1</ref> shows the relative frequencies of the states we observed for each trace in the dataset (sorted by percentage of SF connections). We include each state that comprises at least 1% of the overall connections in the dataset. In total, SF connections represent 58% of the connections in the dataset-however, this proportion varies greatly across different traces. Coupled with the RSTO and RSTR connections-which are properly established, but later closed with RSTs instead of FINs, sent by the originator or responder, respectively-these connections represent the 363K "good" connections we use in the analysis presented in the subsequent sections. In total these connections transferred about 50 GB of data. <ref type="figure" target="#fig_0">Figure 1</ref> shows that a large fraction of connections in each trace was rejected. Across the entire dataset we identify 158K (30%) rejected connections. However, we find that 129K of these originate from a single host, which scans nearly every port on two particular machines. This trace is on the far right of <ref type="figure" target="#fig_0">Figure 1</ref>, for which 98% of the connections are in the REJ state. The originator of these scans is a machine used by the network operators, and these scans reflect normal operations.</p><p>Additionally, <ref type="figure" target="#fig_0">Figure 1</ref> shows that "OTH" connections also happen frequently. This state indicates that Bro did not observe the connection establishment nor teardown. As discussed above, these connections likely represent legitimate traffic that is mostly out of view of our monitoring point (i.e., we see a small bit of flooded traffic and nothing else). Across the dataset we observe 5.5K OTH connections. While 66% of these involve a single host (sending to port 80 in 99.5% of the cases), the remainder come from 275 distinct hosts, destined to 182 hosts. Over 90% of these connections contain only a single ACK or data packet.</p><p>We note that <ref type="bibr" target="#b2">[3]</ref> uses the fraction of successfully established connections as its measure of "network health" and further finds the overall health of the network monitored to be roughly 66%. While our overall dataset yields a similar percentage of good connections (68%) when removing the port scan traffic-which we stress is a legitimate operational tool in this case-we find that just over 90% of the connections we observe are good. In addition, as shown in the figure individual traces (i.e., collections of hosts) contain a wide range of variability in the percentage of good connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Connection Characteristics</head><p>In this section we examine some basic characteristics of the connections in our dataset. We first note that we observe 202K connections (56%) involve a host outside the monitored subnet, while 161K (44%) involve a peer inside the subnet. (Again, this figure has high variability across traces, which is examined further in <ref type="bibr" target="#b4">[5]</ref>.) This shows that monitoring methodologies that focus on core routers in enterprise networks-such as used in a previous study of LBNL traffic <ref type="bibr" target="#b5">[6]</ref>-miss a large fraction of the traffic. Therefore, switch measurements or end host measurements <ref type="bibr" target="#b1">[2]</ref> are preferred to better understand the overall traffic characteristics.  <ref type="table">Table 1</ref> shows the prevalent applications in our dataset, each representing at least 1% of either the bytes or the connections across our dataset. The applications are broken into three groups. The top group contains applications seen in at least 80% of the traces in our dataset, while the applications in the bottom group are observed in at most 20% of the traces. The middle group of applications are represented in between 20% and 80% of the traces. The final column of the table indicates the scope of the traffic with "I" indicating the application meets the 1% byte or connection threshold when only considering intranet traffic and "O" indicating the threshold was met by traffic traversing subnet boundaries.</p><p>While the table shows a number of well-known and expected applications it also shows less well-known (and even unknown!) applications are not rare. In addition, we see that most applications are "unbalanced" in that they contribute a significant fraction of connections or bytes but not both. For instance, the Dantz backup system and NFS contribute a relatively large fraction of bytes (26.7% and 16.7%), but only a modest fraction of the connections (0.3% and 2.3%). On the other hand, NetBIOS-SSN and Warewulf Cluster monitoring show only small fractions of the bytes observed (1.5% and 2.0%), but larger fractions of connections (10.0% and 18.8%).</p><p>As a final piece of background we turn our attention to connection sizes. First, we note that we remove nearly 13K connections from this analysis due to a lack of a byte count in at least one of the directions. 1 <ref type="figure" target="#fig_1">Figure 2</ref> shows the distribution of the total number of bytes (in both directions) transferred across each connection in our dataset. The median transfer size was just under 2 KB, while we observed a total of 4 connections that transferred a bit more than 5 GB. We find that in 78K (22.3%) connections, the connection originator sends no data bytes, while the responder does not send data in 10K (2.9%) connections. No data is exchanged in either direction in 8K (2.3%) connections. These are all "SF" connections (established and torn down correctly) and largely (92%) consist of "printer" connections. We find these printer connections to be between two pairs of IP addresses. The connections happen every 90 seconds in the first pair (36% of the cases). In the second pair we observe a similar 90 second periodicity, however in this case two connections are established-neither of which transfers any data-every 90 seconds. <ref type="figure" target="#fig_1">Figure 2</ref> also shows the distribution of the ratio of the data sent by the originator to the data sent by the responder to provide a sense of directionality. <ref type="bibr" target="#b1">2</ref> The figure shows that nearly 60% of the connections consist of the responder sending more data than the originator (up to 1.2 million times as much data). Similarly in over 30% of the connections we find more bytes from the originator than the responder (up to 1.2 billion times as much data).</p><p>We also note that the top 15 flows (by volume) in our dataset (or 0.004% of the flows) account for 57% of the aggregate number of bytes transferred. This is on the low end of the findings in <ref type="bibr" target="#b8">[9]</ref>, which found that 50-60% of the bytes came from 0.006-0.09% of the connections (across several datasets, all but one of which had an order of magnitude larger percentages than what we ob-serve). We further note that 90% of the traffic in our dataset comes from a mere 160 connections.</p><p>We conclude this section by considering the sizes of transactions across different application protocols. Table 2 provides various statistics for connections involving some of the more prevalent applications in our dataset. The data shows a range of application behavior. In general we see heavy tails with medians of only a small fraction of the 99 th percentiles and maximums. However, while the heavy tails are present, the magnitude of the transfers varies across protocols. In addition, there are exceptions such as the port mapper and the Warewulf Cluster monitoring which do not show heavy tails.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>App</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Performance</head><p>We next examine different facets of performance observed within the enterprise. First, regarding basic network errors, we found that 583 TCP packets contained checksum errors. These were confined to eight traces, with three of the traces accounting for nearly 90% of the failures (likely indicating isolated hardware flakiness). Given their low rate, we do not believe these problems affect the insights obtained from the dataset. We assessed out-of-order packet delivery by inspecting IP ID changes in TCP data packets. Bro tracks whether a given sender increments IP ID in a generally monotone fashion (either little-endian or big-endian), and flags a reordered packet if increments are generally monotone but a packet occurs in a trace with an IP ID slightly smaller than its predecessor in the trace. Using this definition, we observe reordering in 484 connections (0.1%), out of which 16 connections have reordered packets in both directions. Overall, only 0.0025% of data packets arrived out of order, a negligible level compared to the analogous figures reported in <ref type="bibr" target="#b6">[7]</ref> and other widearea studies. However, within a uni-directional flow that experienced reordering, the share of packets arriving out of order ranges from 0.003% to 100% (i.e., every packet that had an opportunity to arrive out of order indeed arrived out of order), with a median of 4.9% and 95th percentile of 40%. However, we note that a thorough assessment of packet reordering necessarily entails taking into account the interval between when the packets were originally sent; we will generally see lower reordering rates or larger intervals. Such an analysis is beyond the scope of our present initial study.</p><p>We also looked for replicated packets (sent once but arriving multiple times), as were (very rarely) noted in <ref type="bibr" target="#b6">[7]</ref>. We did not observe any. However, we note that any replicated packets that arrived within 5 msec of each other would have been considered phantoms and removed from the traces during our calibration phase, as described in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Another important aspect of TCP behavior is retransmissions. Assessing these robustly requires a measurement vantage point close to data sender, because the further away we monitor from the sender, the higher the risk we will miss retransmitted packets that get lost en route between the sender and the monitor's vantage point. For this reason, we consider only inter-subnet connections, for which given our knowledge of netmasks in each trace we can accurately identify which of the two hosts is in the monitored subnet (and thus for which we have a senderside vantage point), and calculate the number of retransmitted packets sent by the local senders.</p><p>We find that 1,089 inter-subnet connections (0.5%) experienced retransmissions. A majority of these connections sent only one (724 conns.), two (159 conns.), or three (75 conns.) retransmissions. We also note that the maximum number of retransmissions we observed for a connection was 139, and the total number of retransmitted packets across all inter-subnet connections was 2,736.</p><p>Next we study the maximum flight sizes attained by uni-directional flows in the enterprise. We define this value as the maximum observed difference between a sender's highest outstanding sequence number and the cumulative acknowledgment point. (For this analysis we omit flows with sequence number "gaps" due to measurement loss.)</p><p>We find median and 99-th percentile maximum flight size for all flows equal to 214 and 5,296 bytes, respectively. 99.8% of flows had maximum flight sizes below 12.5KB. The remaining 0.2% flows have median and 99-th percentile maximum flight size of 19,280 and 65,334 bytes. (The highest value we observed is 878,860 bytes for a flow that lasted 6.5 hours and transmitted 2 GB in 16.7 M packets.)</p><p>Other than these final values (for just 0.2% of flows), these levels potentially indicate bandwidth underutilization; for 100Mb/s links such as those common within LBNL, a 1 msec propagation delay equates to a bandwidth-delay product of 12.5 KB. However, if actual latencies are significantly below 1 msec, then they might indeed completely utilize the available capacity. Thus, the next steps in this analysis will be to undertake a robust analysis of connection round-trip latencies in order to confirm whether indeed most connections fail to utilize the available bandwidth, and, if so, why.  <ref type="figure" target="#fig_2">Figure 3</ref> shows the distribution of transfer rates for all connections (solid line), as well as by location of the peers. The rates are computed as the total number of bytes in each direction divided by the duration of the connection. We calculated the rates for each direction separately and as predicted by the transfer sizes discussed in the last section the server (responder) generally shows greater throughput than the originator. The median rates are roughly 2 KBps and 10 KBps for the originator and responder, respectively. In our traces 50% of the connections are faster than 100 Kbps which is about three times more than in <ref type="bibr" target="#b8">[9]</ref>, and 5-12 times more than in <ref type="bibr" target="#b9">[10]</ref>. We believe the higher rates are likely explained by the low delays and high raw capacities provided in the enterprise environment. This underscores the point made in § 1 that enterprise networks are often found "good enough" in comparison to wide area networks. The figure also shows that transfers within a subnet are generally an order of magnitude faster than those cross subnet boundaries, even though the latter remain within the enterprise. We note that more than 20% of the transfers within a subnet obtain rates of more than 10 Mbps, while less than 1% of connections cross subnet boundaries achieve such rates.  We next separate flows into four classes by duration and size. We use a total transfer size threshold of 10 KB to classify each connection as "small" or "large" and a threshold of 1 second to classify each connection as "short" or "long". While the thresholds are arbitrarily chosen we note that using 50 KB and 5 seconds yields nearly the same classification as our chosen thresholds. <ref type="table" target="#tab_4">Table 3</ref> shows shares of connections and bytes in each category. As expected we observe that most connections (89%) are small, but most of the bytes are carried in the large connections (98.6% total).  <ref type="figure">Figure 4</ref>: Connection rates. <ref type="figure">Figure 4</ref> shows the distribution of transfer rates for each of the four classes of connections. As expected, long-small show the worst performance. On the other hand, the short-small transfers show that it is possible to well utilize the capacity to send short transactions with nearly 20% of the connections sending at roughly 10 Mbps or more. The long-large flows somewhat surprisingly send at slow rates compared to the network capacity. While these connections carry 98% of the bytes, they do not transmit those bytes efficiently; these observations point to an obvious area for future study and performance improvement. The short-large flows have highest rates. Note, this class represents little of the network traffic in either connections or bytes (per <ref type="table" target="#tab_4">Table 3</ref>), mostly consisting of HTTP traffic between two particular hosts and manifesting two modes in the rate distribution, at 1.7 Mbps and 37 Mbps.</p><p>Correlations between duration, size and rate have been studied in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9]</ref> and in more detail in <ref type="bibr" target="#b0">[1]</ref>. <ref type="table">Table 4</ref> suggests that correlations between (log D,log S) are similar to those in all the three other studies which also reported weak positive correlation. We observe strong negative correlation between (log D,log R), which is stronger than in <ref type="bibr" target="#b8">[9]</ref> and much stronger than in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>. Our traces show negative (log S,log R) correlation for  <ref type="table">Table 4</ref>: Correlation coefficients of log(data). the short-large subset. However, the three other classes have medium (as in <ref type="bibr" target="#b8">[9]</ref>) to strong (as in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>) correlations. In <ref type="bibr" target="#b0">[1]</ref> authors found that strong correlation between rate and size can be explained by TCP and application specifics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented a preliminary analysis of TCP transport behavior seen inside a medium-sized enterprise. To do so we drew upon a collection of 50 traces that each record activity from 10 switch ports at the Lawrence Berkeley National Laboratory. In general, we confirm the common presumption that enterprise connections enjoy loss-less, high speed interconnection. But we also find that weighted by connections, the traces contain a wide range in the proportion of connections that are cleanly established and terminated vs. problematic; the proportion of TCP enterprise traffic that stays confined inside a single subnet likewise can vary greatly by vantage point and measurement time; and that applications differ sharply in whether their use remains within a subnet, includes both intra-and extra-subnet connections, or always leaves the subnet. We confirm that the distribution of the byte volume of most applications exhibits a heavy tail, but some applications quite prevalent within the enterprise lack this skew. We find out-of-order packet delivery much more rare than observed for wide-area traffic, and likewise for packet corruption and replication. Additionally, we find that 0.5% of TCP senders send at least one retransmission. Our analysis of maximum flight sizes indicates that effective TCP windows are generally under 8 KB, though we have not yet determined whether these windows prevent connections from making full use of the available path capacity.</p><p>We find a wide range of transfer rates, with connections achieving throughputs of 3-12 times those seen for wide-area TCP. Far and away most bytes occur in longlived, large transfers. When examining (log) correlations between connection sizes, rates, and durations, we often find agreement with previous work examining these relationships for wide-area traffic, other than the strong negative correlation we find between duration and rate, which may reflect the enterprise's "cleaner" paths, which have both little congestive loss due to cross-traffic, and high end-to-end capacity.</p><p>This analysis is only a beginning. Our immediate next steps are (1) an analysis of packet latency dynamics, (2) an assessment of retransmission timeout behavior, and, much more generally, (3) incorporation of a large set (1,000 switch ports) of traces that we have recently completed capturing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of final connection states.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of connection sizes and ratio of originator data bytes to responder data bytes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Observed transfer rates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Sizes of various prevalent applications.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 : Breakdown of traffic classes.</head><label>3</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> By &quot;lack of byte count&quot; we do not mean zero bytes, but rather that we were unable to determine the number of bytes being transferred in a given direction-usually due to some puzzle whereby we do not observe a valid connection. 2 Note, we added 1 byte to every byte count before computing the ratio to alleviate divide-by-zero issues.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported in part by TEKES as part of the Future Internet program of the ICT cluster of the Finnish Strategic Centre for Science, Technology and Innovation; and by the US National Science Foundation under grants CNS-0722035 and CNS-0831535.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A measurement study of correlation of Internet flow characteristics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidemann</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006-01" />
		</imprint>
		<respStmt>
			<orgName>Computer Networks</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Cubicle vs. The Coffee Shop: Behavioral Modes in Enterprise End-Users</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giroire</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papa-Giannaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schooler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taft</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PAM</title>
		<meeting>PAM</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">How healthy are today&apos;s enterprise networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandrashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Papagian-Naki</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM/USENIX Internet Measurement Conference</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inferring TCP connection characteristics through passive measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaiswal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iannaccone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kurose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Towsley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On Calibrating Enterprise Switch Measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nechaev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurtov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIG-COMM/USENIX Internet Measurement Conference</title>
		<imprint>
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A first look at modern enterprise traffic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Allman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tierney</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM/USENIX Internet Measurement Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-End Internet Packet Dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paxson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="1997-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bro: a system for detecting network intruders in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paxson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Netw</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2435" to="2463" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TCP revisited: a fresh look at TCP in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Spatscheck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willinger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM/USENIX Internet Measurement Conference</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the characteristics and origins of Internet flow rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Breslau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paxson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenker</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
