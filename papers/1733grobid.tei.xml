<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-16T20:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-06">June 6, 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Whitaker</surname></persName>
							<email>whitaker.213@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Newman-Griffis</surname></persName>
							<email>newman-griffis.1@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
							<email>aparajita.haldar@warwick.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Ferhatosmanoglu</surname></persName>
							<email>h.ferhatosmanoglu@warwick.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<settlement>Coventry</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
							<email>fosler-lussier.1@osu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Characterizing the Impact of Geometric Properties of Word Embeddings on Task Performance</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</title>
						<meeting>the 3rd Workshop on Evaluating Vector Space Representations for NLP <address><addrLine>Minneapolis, USA</addrLine></address>
						</meeting>
						<imprint>
							<biblScope unit="page" from="8" to="17"/>
							<date type="published" when="2019-06-06">June 6, 2019</date>
						</imprint>
					</monogr>
					<note>8</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing nearest neighbors. However, geometric properties of the continuous feature space contribute directly to the use of embedding features in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely: position relative to the origin, distribution of features in the vector space, global pairwise distances , and local pairwise distances. We define a sequence of transformations to generate new embeddings that expose subsets of these properties to downstream models and evaluate change in task performance to understand the contribution of each property to NLP models. We transform publicly available pre-trained embeddings from three popular toolk-its (word2vec, GloVe, and FastText) and evaluate on a variety of intrinsic tasks, which model linguistic information in the vector space, and extrinsic tasks, which use vectors as input to machine learning models. We find that intrinsic evaluations are highly sensitive to absolute position, while extrinsic tasks rely primarily on local similarity. Our findings suggest that future embedding models and post-processing techniques should focus primarily on similarity to nearby points in vector space.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learned vector representations of words, known as word embeddings, have become ubiquitous throughout natural language processing (NLP) applications. As a result, analysis of embedding spaces to understand their utility as input features has emerged as an important avenue of inquiry, in order to facilitate proper use of embeddings in downstream NLP tasks. Many analyses have focused on nearest neighborhoods, as a viable proxy for semantic information (Rogers et al., * These authors contributed equally to this <ref type="bibr">work.</ref> 2018; <ref type="bibr" target="#b11">Pierrejean and Tanguy, 2018)</ref>. However, neighborhood-based analysis is limited by the unreliability of nearest neighborhoods <ref type="bibr" target="#b17">(Wendlandt et al., 2018)</ref>. Further, it is intended to evaluate the semantic content of embedding spaces, as opposed to characteristics of the feature space itself.</p><p>Geometric analysis offers another recent angle from which to understand the properties of word embeddings, both in terms of their distribution ( <ref type="bibr" target="#b3">Mimno and Thompson, 2017)</ref> and correlation with downstream performance ( <ref type="bibr">Chandrahas et al., 2018)</ref>. Through such geometric investigations, neighborhood-based semantic characterizations are augmented with information about the continuous feature space of an embedding. Geometric features offer a more direct connection to the assumptions made by neural models about continuity in input spaces ( <ref type="bibr" target="#b16">Szegedy et al., 2014)</ref>, as well as the use of recent contextualized representation methods using continuous language models ( <ref type="bibr" target="#b10">Peters et al., 2018;</ref><ref type="bibr">Devlin et al., 2018)</ref>.</p><p>In this work, we aim to bridge the gap between neighborhood-based semantic analysis and geometric performance analysis. We consider four components of the geometry of word embeddings, and transform pretrained embeddings to expose only subsets of these components to downstream models. We transform three popular sets of embeddings, trained using word2vec ( <ref type="bibr" target="#b2">Mikolov et al., 2013</ref>), 1 GloVe ( <ref type="bibr" target="#b9">Pennington et al., 2014</ref>), 2 and FastText ( <ref type="bibr">Bojanowski et al., 2017</ref>), <ref type="bibr">3</ref> and use the resulting embeddings in a battery of standard evaluations to measure changes in task performance.</p><p>We find that intrinsic evaluations, which model linguistic information directly in the vector space,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Affine CDE NNE T r a n s la t io n R e f le c t io n R o t a t io n D il a t io n T h r e s h o ld e d W e ig h t e d U n w e ig h t e d</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>H o m o t h e t y <ref type="figure">Figure 1</ref>: Sequence of transformations applied to word embeddings, including transformation variants. Note that each transformation is applied independently to source word embeddings. Transformations are presented in order of decreasing geometric information retained about the original vectors.</p><p>are highly sensitive to absolute position in pretrained embeddings; while extrinsic tasks, in which word embeddings are passed as input features to a trained model, are more robust and rely primarily on information about local similarity between word vectors. Our findings, including evidence that global organization of word vectors is often a major source of noise, suggest that further development of embedding learning and tuning methods should focus explicitly on local similarity, and help to explain the success of several recent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Word embedding models and outputs have been analyzed from several angles. In terms of performance, evaluating the "quality" of word embedding models has long been a thorny problem. While intrinsic evaluations such as word similarity and analogy completion are intuitive and easy to compute, they are limited by both confounding geometric factors (Linzen, 2016) and task-specific factors <ref type="bibr">(Faruqui et al., 2016;</ref><ref type="bibr" target="#b13">Rogers et al., 2017)</ref>. <ref type="bibr">Chiu et al. (2016)</ref> show that these tasks, while correlated with some semantic content, do not always predict downstream performance. Thus, it is necessary to use a more comprehensive set of intrinsic and extrinsic evaluations for embeddings. Nearest neighbors in sets of embeddings are commonly used as a proxy for qualitative semantic information. However, their instability across embedding samples <ref type="bibr" target="#b17">(Wendlandt et al., 2018</ref>) is a limiting factor, and they do not necessarily correlate with linguistic analyses <ref type="bibr">(Hellrich and Hahn, 2016)</ref>. Modeling neighborhoods as a graph structure offers an alternative analysis method <ref type="bibr">(Cuba Gyllensten and Sahlgren, 2015)</ref>, as does 2-D or 3-D visualization <ref type="bibr">(Heimerl and Gleicher, 2018)</ref>. However, both of these methods provide qualitative insights only. By systematically analyzing geometric information with a wide variety of evaluations, we provide a quantitative counterpart to these understandings of embedding spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In order to investigate how different geometric properties of word embeddings contribute to model performance on intrinsic and extrinsic evaluations, we consider the following attributes of word embedding geometry:</p><p>• position relative to the origin;</p><p>• distribution of feature values in R d ;</p><p>• global pairwise distances, i.e. distances between any pair of vectors; • local pairwise distances, i.e. distances between nearby pairs of vectors.</p><p>Using each of our sets of pretrained word embeddings, we apply a variety of transformations to induce new embeddings that only expose subsets of these attributes to downstream models. These are: affine transformation, which obfuscates the original position of the origin; cosine distance encoding, which obfuscates the original distribution of feature values in R d ; nearest neighbor encoding, which obfuscates global pairwise distances; and random encoding. This sequence is illustrated in <ref type="figure">Figure 1</ref>, and the individual transformations are discussed in the following subsections.</p><p>General notation for defining our transformations is as follows. Let W be our vocabulary of words taken from some source corpus. We associate with each word w ∈ W a vector v ∈ R d resulting from training via one of our embedding generation algorithms, where d is an arbitrary dimensionality for the embedding space. We define V to be the set of all pretrained word vectors v for a given corpus, embedding algorithm, and parameters. The matrix of embeddings M V associated with this set then has shape |V | × d. For simplicity, we restrict our analysis to transformed embeddings of the same dimensionality d as the original vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Affine transformations</head><p>Affine transformations have been previously utilized for post-processing of word embeddings. For example, <ref type="bibr">Artetxe et al. (2016)</ref> learn a matrix transform to align multilingual embedding spaces, and <ref type="bibr">Faruqui et al. (2015)</ref> use a linear sparsification to better capture lexical semantics. In addition, the simplicity of affine functions in machine learning contexts ( <ref type="bibr">Hofmann et al., 2008</ref>) makes them a good starting point for our analysis.</p><p>Given a set of embeddings in R d , referred to as an embedding space, affine transformations</p><formula xml:id="formula_0">f affine : R d → R d</formula><p>change positions of points relative to the origin.</p><p>While prior work has typically focused on linear transformations, which fix the origin, we consider the broader class of affine transformations, which do not. Thus, affine transformations such as translation cannot in general be represented as a square matrix for finite-dimensional spaces.</p><p>We use the following affine transformations:</p><p>• translations;</p><p>• reflections over a hyperplane;</p><p>• rotations about a subspace;</p><p>• homotheties.</p><p>We give brief definitions of each transformation.</p><formula xml:id="formula_1">Definition 1. A translation is a function T x : R d → R d given by T x (v) = v + x (3.1) where x ∈ R d . Definition 2. For every a ∈ R d , we call the map Refl a : R d → R d given by Refl a (v) = v − 2 v · a a · a a (3.2)</formula><p>the reflection over the hyperplane through the origin orthogonal to a.</p><p>Definition 3. A rotation through the span of vectors u, x by angle θ is a map Rot u,x :</p><formula xml:id="formula_2">R d → R d given by Rot u,x (v) = Av (3.3)</formula><p>where</p><formula xml:id="formula_3">A = I + sin θ(xu T − ux T ) + (cos θ − 1)(uu T + xx T ) (3.4) and I ∈ Mat d,d (R) is the identity matrix. Definition 4. For every a ∈ R d and λ ∈ R \ { 0 }, we call the map H a,λ : R d → R d given by H a,λ (v) = a + λ(v − a) (3.5)</formula><p>a homothety of center a and ratio λ. A homothety centered at the origin is called a dilation.</p><p>Parameters used in our analysis for each of these transformations are provided in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cosine distance encoding (CDE)</head><p>Our cosine distance encoding transformation</p><formula xml:id="formula_4">f CDE : R d → R |V |</formula><p>obfuscates the distribution of features in R d by representing a set of word vectors as a pairwise distance matrix. Such a transformation might be used to avoid the non-interpretability of embedding features <ref type="bibr">(Fyshe et al., 2015</ref>) and compare embeddings based on relative organization alone.</p><formula xml:id="formula_5">Definition 5. Let a, b ∈ R d . Then their cosine distance d cos : R d × R d → [0, 2] is given by d cos (a, b) = 1 − a · b ||a||||b|| (3.6)</formula><p>where the second term is the cosine similarity.</p><p>As all three sets of embeddings evaluated in this study have vocabulary size on the order of 10 6 , use of the full distance matrix is impractical. We use a subset consisting of the distance from each point to the embeddings of the 10K most frequent words from each embedding set, yielding</p><formula xml:id="formula_6">f CDE : R d → R 10 4</formula><p>This is not dissimilar to the global frequencybased negative sampling approach of word2vec ( <ref type="bibr" target="#b2">Mikolov et al., 2013</ref>). We then use an autoencoder to map this back to R d for comparability.</p><formula xml:id="formula_7">Definition 6. Let v ∈ R |V | , W 1 , W 2 ∈ R |V |×d . Then an autoencoder over R |V | is defined as h = ϕ(vW 1 ) (3.7) ˆ v = ϕ(W 2 T h) (3.8)</formula><p>Vector h ∈ R d is then used as the compressed representation of v.</p><p>In our experiments, we use ReLU as our activation function ϕ, and train the autoencoder for 50 epochs to minimize L 2 distance between v andˆvandˆ andˆv.</p><p>We recognize that low-rank compression using an autoencoder is likely to be noisy, thus potentially inducing additional loss in evaluations. However, precedent for capturing geometric structure with autoencoders ( <ref type="bibr">Li et al., 2017b)</ref> suggests that this is a viable model for our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Nearest neighbor encoding (NNE)</head><p>Our nearest neighbor encoding transformation</p><formula xml:id="formula_8">f NNE : R d → R |V |</formula><p>discards the majority of the global pairwise distance information modeled in CDE, and retains only information about nearest neighborhoods. The output of f NNE (v) is a sparse vector.</p><p>This transformation relates to the common use of nearest neighborhoods as a proxy for semantic information <ref type="bibr" target="#b17">(Wendlandt et al., 2018;</ref><ref type="bibr">Pierre- jean and Tanguy, 2018)</ref>. We take the previously proposed approach of combining the output of f NNE (v) for each v ∈ V to form a sparse adjacency matrix, which describes a directed nearest neighbor graph <ref type="bibr">(Cuba Gyllensten and Sahlgren, 2015;</ref><ref type="bibr" target="#b4">Newman-Griffis and Fosler-Lussier, 2017)</ref>, using three versions of f NNE defined below.</p><p>Thresholded The set of non-zero indices in f NNE (v) correspond to word vectors˜vvectors˜ vectors˜v such that the cosine similarity of v and˜vand˜ and˜v is greater than or equal to an arbitrary threshold t. In order to ensure that every word has non-zero out degree in the graph, we also include the k nearest neighbors by cosine similarity for every word vector. Non-zero values in f NNE (v) are set to the cosine similarity of v and the relevant neighbor vector.</p><p>Weighted The set of non-zero indices in f NNE (v) corresponds to only the set of k nearest neighbors to v by cosine similarity. Cosine similarity values are used for edge weights.</p><p>Unweighted As in the previous case, only k nearest neighbors are included in the adjacency matrix. All edges are weighted equally, regardless of cosine similarity.</p><p>We report results using k = 5 and t = 0.05; other settings are discussed in Appendix B.</p><p>Finally, much like the CDE method, we use a second mapping function</p><formula xml:id="formula_9">ψ : R |V | → R d</formula><p>to transform the nearest neighbor graph back to d-dimensional vectors for evaluation. Following Newman-Griffis and Fosler-Lussier (2017), we use node2vec <ref type="bibr">(Grover and Leskovec, 2016)</ref> with default parameters to learn this mapping. Like the autoencoder, this is a noisy map, but the intent of node2vec to capture patterns in local graph structure makes it a good fit for our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Random encoding</head><p>Finally, as a baseline, we use a random encoding</p><formula xml:id="formula_10">f Rand : R d → R d</formula><p>that discards original vectors entirely.</p><p>While intrinsic evaluations rely only on input embeddings, and thus lose all source information in this case, extrinsic tasks learn a model to transform input features, making even randomlyinitialized vectors a common baseline ( <ref type="bibr">Lample et al., 2016;</ref><ref type="bibr">Kim, 2014)</ref>. For fair comparison, we generate one set of random baselines for each embedding set and re-use these across all tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Other transformations</head><p>Many other transformations of a word embedding space could be included in our analysis, such as arbitrary vector-valued polynomial functions, rational vector-valued functions, or common decomposition methods such as principal components analysis (PCA) or singular value decomposition (SVD). Additionally, though they cannot be effectively applied to the unordered set of word vectors in a raw embedding space, transformations for sequential data such as discrete Fourier transforms or discrete wavelet transforms could be used for word sequences in specific text corpora.</p><p>For this study, we limit our scope to the transformations listed above. These transformations align with prior work on analyzing and post-processing embeddings for specific tasks, and are highly interpretable with respect to the original embedding space. However, other complex transformations represent an intriguing area of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In order to measure the contributions of each geometric aspect described in Section 3 to the utility of word embeddings as input features, we evaluate embeddings transformed using our sequence of operations on a battery of standard intrinsic evaluations, which model linguistic information directly in the vector space; and extrinsic evaluations, which use the embeddings as input to learned models for downstream applications Our intrinsic evaluations include: • Word similarity and relatedness, using co- Given the well-documented issues with using vector arithmetic-based analogy completion as an intrinsic evaluation <ref type="bibr">(Linzen, 2016;</ref><ref type="bibr" target="#b13">Rogers et al., 2017;</ref>), we do not include it in our analysis.</p><p>We follow <ref type="bibr" target="#b14">Rogers et al. (2018)</ref> in evaluating on a set of five extrinsic tasks: <ref type="bibr">5</ref> • Relation classification: SemEval-2010 Task 8 ( <ref type="bibr">Hendrickx et al., 2010)</ref>, using a CNN with word and distance embeddings ( <ref type="bibr" target="#b18">Zeng et al., 2014</ref>).</p><p>• Sentence-level sentiment polarity classification: MR movie reviews (Pang and <ref type="bibr" target="#b8">Lee, 2005</ref>), with a simplified CNN model from <ref type="bibr">(Kim, 2014</ref>).</p><p>• Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM.</p><p>• Subjectivity/objectivity classification: Rotten Tomato snippets ( <ref type="bibr" target="#b7">Pang and Lee, 2004</ref>), using a logistic regression over summed word embeddings ( <ref type="bibr">Li et al., 2017a</ref>).</p><p>• Natural language inference: SNLI (Bowman et al., 2015), using separate LSTMs for premise and hypothesis, combined with a feed-forward classifier. <ref type="figure" target="#fig_0">Figure 2</ref> presents the results of each intrinsic and extrinsic evaluation on the transformed versions of our three sets of word embeddings. <ref type="bibr">6</ref> The largest drops in performance across all three sets for intrinsic tasks occur when explicit embedding features are removed with the CDE transformation. While some cases of NNE-transformed embeddings recover a measure of this performance, they remain far under affine-transformed embeddings. Extrinsic tasks are similarly affected by the CDE transformation; however, NNE-transformed embeddings recover the majority of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis and Discussion</head><p>Comparing within the set of affine transformations, the innocuous effect of rotations, dilations, and reflections on both intrinsic and extrinsic tasks suggests that the models used are robust to simple linear transformations. Extrinsic evaluations are also relatively insensitive to translations, which can be modeled with bias terms, though the lack of learned models and reliance on cosine similarity for the intrinsic tasks makes them more sensitive to shifts relative to the origin. Interestingly, homothety, which effectively combines a translation and a dilation, leads to a noticeable drop in performance across all tasks. Intuitively, this result makes sense: by both shifting points relative to the origin and changing their distribution in the space, angular similarity values used for intrinsic tasks can be changed significantly, and the zero mean feature distribution preferred by neural models <ref type="bibr">(Clevert et al., 2016</ref>) becomes harder to achieve. This suggests that methods for tuning embeddings should attempt to preserve the origin whenever possible.</p><p>The large drops in performance observed when using the CDE transformation is likely to relate <ref type="bibr">6</ref> Due to their large vocabulary size, we were unable to run Thresholded-NNE experiments with word2vec embeddings.</p><p>to the instability of nearest neighborhoods and the importance of locality in embedding learning ( <ref type="bibr" target="#b17">Wendlandt et al., 2018)</ref>, although the effects of the autoencoder component also bear further investigation. By effectively increasing the size of the neighborhood considered, CDE adds additional sources of semantic noise. The similar drops from thresholded-NNE transformations, by the same token, is likely related to observations of the relationship between the frequency ranks of a word and its nearest neighbors <ref type="bibr">(Faruqui et al., 2016)</ref>. With thresholded-NNE, we find that the words with highest out degree in the nearest neighbor graph are rare words (e.g., "Chanterelle" and "Courtier" in FastText, "Tiegel" and "demangler" in GloVe), which link to other rare words. Thus, node2vec's random walk method is more likely to traverse these dense subgraphs of rare words, adding noise to the output embeddings.</p><p>Finally, we note that Melamud et al. (2016) showed significant variability in downstream task performance when using different embedding dimensionalities. While we fixed vector dimensionality for the purposes of this study, varying d in future work represents a valuable follow-up.</p><p>Our findings suggest that methods for training and tuning embeddings, especially for downstream tasks, should explicitly focus on local geometric structure in the vector space. One concrete example of this comes from <ref type="bibr">Chen et al. (2018)</ref>, who demonstrate empirical gains when changing the negative sampling approach of word2vec to choose negative samples that are currently near to the target word in vector space, instead of the original frequency-based sampling (which ignores geometric structure). Similarly, successful methods for tuning word embeddings for specific tasks have often focused on enforcing a specific neighborhood structure <ref type="bibr">(Faruqui et al., 2015)</ref>. We demonstrate that by doing so, they align qualitative semantic judgments with the primary geometric information that downstream models learn from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Analysis of word embeddings has largely focused on qualitative characteristics such as nearest neighborhoods or relative distribution. In this work, we take a quantitative approach analyzing geometric attributes of embeddings in R d , in order to understand the impact of geometric properties on downstream task performance. We character-ized word embedding geometry in terms of absolute position, vector features, global pairwise distances, and local pairwise distances, and generated new embedding matrices by removing these attributes from pretrained embeddings. By evaluating the performance of these transformed embeddings on a variety of intrinsic and extrinsic tasks, we find that while intrinsic evaluations are sensitive to absolute position, downstream models rely primarily on information about local similarity.</p><p>As embeddings are used for increasingly specialized applications, and as recent contextualized embedding methods such as ELMo ( <ref type="bibr" target="#b10">Peters et al., 2018</ref>) and BERT <ref type="bibr">(Devlin et al., 2018)</ref> allow for dynamic generation of embeddings from specific contexts, our findings suggest that work on tuning and improving these embeddings should focus explicitly on local geometric structure in sampling and evaluation methods. The source code for our transformations and complete tables of our results are available online at https://github.com/OSU-slatelab/ geometric-embedding-properties. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Parameters</head><p>We give the following library of vectors in R d used as parameter values:  </p><formula xml:id="formula_11">v diag =    1 √ d . . . 1 √ d    ; v diagNeg =       − 1 √ d 1 √ d . . . 1 √ d       .<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B NNE settings</head><p>We experimented with k ∈ {5, 10, 15} for our weighted and unweighted NNE transformations. For thresholded NNE, in order to best evaluate the impact of thresholding over uniform k, we used the minimum k = 5 and experimented with t ∈ {0.01, 0.05, 0.075}; higher values of t increased graph size sufficiently to be impractical. We report using k = 5 for weighted and unweighted settings in our main results for fairer comparison with the thresholded setting. The effect of thresholding on nearest neighbor graphs was a strongly right-tailed increase in out degree for a small portion of nodes. Our reported value of t = 0.05 increased the out degree of 20,229 nodes for FastText (out of 1M total nodes), with the maximum increase being 819 ("Chanterelle"), and 1,354 nodes increasing out degree by only 1. For GloVe, 7,533 nodes increased in out degree (out of 2M total), with maximum increase 240 ("Tiegel"), and 372 nodes increasing out degree by only 1. <ref type="table" target="#tab_3">Table 2 compares averaged performance values  across all intrinsic tasks for these settings, and  Table 3</ref> compares average extrinsic task performance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NNE params FastText word2vec GloVe</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Performance metrics on intrinsic and extrinsic tasks, comparing across different transformations applied to each set of word embeddings. Dotted lines are for visual aid in tracking performance on individual tasks, and do not indicate continuous transformations. Transformations are presented in order of decreasing geometric information about the original vectors, and are applied independent of one another to the original source embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>sine similarity: WordSim-353 (Finkelstein et al., 2001), SimLex-999 (Hill et al., 2015), RareWords (Luong et al., 2013), RG65 (Rubenstein and Goodenough, 1965), MEN (Bruni et al., 2014), and MTURK (Radinsky et al., 2011). 4 • Word categorization, using an oracle combi- nation of agglomerative and k-means clus- tering: AP (Almuhareb and Poesio, 2005), BLESS (Baroni and Lenci, 2011), Battig (Battig and Montague, 1969), and the ESS- LLI 2008 shared task (Baroni et al. (2008), performance averaged across nouns, verbs, 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as the full dataset. and concrete nouns). 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 : Transform parameters.</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Mean performance on intrinsic tasks under 
different NNE settings. 

NNE params FastText word2vec GloVe 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Mean performance on extrinsic tasks under 
different NNE settings. </table></figure>

			<note place="foot" n="1"> 3M 300-d GoogleNews vectors from https://code. google.com/archive/p/word2vec/ 2 2M 300-d 840B Common Crawl vectors from https: //nlp.stanford.edu/projects/glove/ 3 1M 300-d WikiNews vectors with subword information from https://fasttext.cc/docs/en/ english-vectors</note>

			<note place="foot" n="5"> https://github.com/drgriffis/ Extrinsic-Evaluation-tasks</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We gratefully acknowledge the use of Ohio Supercomputer Center <ref type="bibr">(Ohio Supercomputer Cen- ter, 1987)</ref> resources for this work, and thank our anonymous reviewers for their insightful comments. Denis is supported via a Pre-Doctoral Fellowship from the National Institutes of Health, Clinical Center. Aparajita is supported via a Feuer International Scholarship in Artificial Intelligence.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge-based wsd on specific domains: Performing better than generic supervised WSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CAI International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1501" to="1506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<title level="m">the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT &apos;16</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1030" to="1040" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The strange geometry of skip-gram with negative sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2873" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Griffis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08488</idno>
		<title level="m">Second-order word embeddings from nearest neighbor topological features</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Insights into Analogy Completion from the Biomedical Domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Newman-Griffis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Foslerlussier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="19" to="28" />
			<pubPlace>Vancouver, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ohio Supercomputer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Center</surname></persName>
		</author>
		<ptr target="http://osc.edu/ark:/19495/f5s1ph73" />
		<imprint>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Seeing Stars: Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Contextualized Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards Qualitative Word Embeddings Evaluation: Measuring Neighbors Variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedicte</forename><surname>Pierrejean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Tanguy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Word at a Time: Computing Word Relatedness Using Temporal Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on World Wide Web, WWW &apos;11</title>
		<meeting>the 20th International Conference on World Wide Web, WWW &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="337" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The (too Many) Problems of Analogical Reasoning with Word Vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Drozd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bofang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</title>
		<meeting>the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">What&apos;s in Your Embedding, And How It Predicts Task Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Shashwath Hosur Ananthakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, NM, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2690" to="2703" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contextual correlates of synonymy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Goodenough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="627" to="633" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factors Influencing the Surprising Instability of Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Wendlandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">K</forename><surname>Kummerfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2092" to="2102" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relation Classification via Convolutional Deep Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
		<respStmt>
			<orgName>Dublin City University and Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
