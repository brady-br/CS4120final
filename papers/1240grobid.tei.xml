<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Turtles Project: Design and Implementation of Nested Virtualization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Ben-Yehuda</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Day</surname></persName>
							<email>mdday@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zvi</forename><surname>Dubitzky</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Factor</surname></persName>
							<email>factor@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Har&amp;apos;el</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gordon</surname></persName>
							<email>abelg@il.ibm.comaliguori@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Liguori</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orit</forename><surname>Wasserman</surname></persName>
							<email>oritw@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Ami</forename><surname>Yassour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research -Haifa ‡ IBM Linux Technology Center</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Turtles Project: Design and Implementation of Nested Virtualization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In classical machine virtualization, a hypervisor runs multiple operating systems simultaneously, each on its own virtual machine. In nested virtualization, a hypervi-sor can run multiple other hypervisors with their associated virtual machines. As operating systems gain hyper-visor functionality-Microsoft Windows 7 already runs Windows XP in a virtual machine-nested virtualization will become necessary in hypervisors that wish to host them. We present the design, implementation, analysis, and evaluation of high-performance nested virtualization on Intel x86-based systems. The Turtles project, which is part of the Linux/KVM hypervisor, runs multiple unmodified hypervisors (e.g., KVM and VMware) and operating systems (e.g., Linux and Windows). Despite the lack of architectural support for nested virtualization in the x86 architecture, it can achieve performance that is within 6-8% of single-level (non-nested) virtualization for common workloads, through multi-dimensional paging for MMU virtualization and multi-level device assignment for I/O virtualization.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Commodity operating systems increasingly make use of virtualization capabilities in the hardware on which they run. Microsoft's newest operating system, Windows 7, supports a backward compatible Windows XP mode by running the XP operating system as a virtual machine. Linux has built-in hypervisor functionality via the KVM <ref type="bibr" target="#b28">[29]</ref> hypervisor. As commodity operating systems gain virtualization functionality, nested virtualization will be required to run those operating systems/hypervisors themselves as virtual machines.</p><p>Nested virtualization has many other potential uses. Platforms with hypervisors embedded in firmware <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref> need to support any workload and specifically other hypervisors as guest virtual machines. An Infrastructureas-a-Service (IaaS) provider could give a user the ability to run a user-controlled hypervisor as a virtual machine. This way the cloud user could manage his own virtual machines directly with his favorite hypervisor of choice, and the cloud provider could attract users who would like to run their own hypervisors. Nested virtualization could also enable the live migration <ref type="bibr" target="#b13">[14]</ref> of hypervisors and their guest virtual machines as a single entity for any reason, such as load balancing or disaster recovery. It also enables new approaches to computer security, such as honeypots capable of running hypervisor-level rootkits <ref type="bibr" target="#b42">[43]</ref>, hypervisor-level rootkit protection <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>, and hypervisor-level intrusion detection <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>-for both hypervisors and operating systems. Finally, it could also be used for testing, demonstrating, benchmarking and debugging hypervisors and virtualization setups.</p><p>The anticipated inclusion of nested virtualization in x86 operating systems and hypervisors raises many interesting questions, but chief amongst them is its runtime performance cost. Can it be made efficient enough that the overhead doesn't matter? We show that despite the lack of architectural support for nested virtualization in the x86 architecture, efficient nested x86 virtualizationwith as little as 6-8% overhead-is feasible even when running unmodified binary-only hypervisors executing non-trivial workloads.</p><p>Because of the lack of architectural support for nested virtualization, an x86 guest hypervisor cannot use the hardware virtualization support directly to run its own guests. Fundamentally, our approach for nested virtualization multiplexes multiple levels of virtualization (multiple hypervisors) on the single level of architectural support available. We address each of the following areas: CPU (e.g., instruction-set) virtualization, memory (MMU) virtualization, and I/O virtualization. x86 virtualization follows the "trap and emulate" model <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. Since every trap by a guest hypervisor or operating system results in a trap to the lowest (most privileged) hypervisor, our approach for CPU virtualization works by having the lowest hypervisor inspect the trap and forward it to the hypervisors above it for emulation. We implement a number of optimizations to make world switches between different levels of the virtualization stack more efficient. For efficient memory virtualization, we developed multi-dimensional paging, which collapses the different memory translation tables into the one or two tables provided by the MMU <ref type="bibr" target="#b12">[13]</ref>. For efficient I/O virtualization, we bypass multiple levels of hypervisor I/O stacks to provide nested guests with direct assignment of I/O devices <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref> via multilevel device assignment.</p><p>Our main contributions in this work are:</p><p>• The design and implementation of nested virtualization for Intel x86-based systems. This implementation can run unmodified hypervisors such as KVM and VMware as guest hypervisors, and can run multiple operating systems such as Linux and Windows as nested virtual machines. Using multidimensional paging and multi-level device assignment, it can run common workloads with overhead as low as 6-8% of single-level virtualization.</p><p>• The first evaluation and analysis of nested x86 virtualization performance, identifying the main causes of the virtualization overhead, and classifying them into guest hypervisor issues and limitations in the architectural virtualization support. We also suggest architectural and software-only changes which could reduce the overhead of nested x86 virtualization even further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Nested virtualization was first mentioned and theoretically analyzed by Popek and Goldberg <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. Belpaire and Hsu extended this analysis and created a formal model <ref type="bibr" target="#b9">[10]</ref>. Lauer and Wyeth <ref type="bibr" target="#b29">[30]</ref> removed the need for a central supervisor and based nested virtualization on the ability to create nested virtual memories. Their implementation required hardware mechanisms and corresponding software support, which bear little resemblance to today's x86 architecture and operating systems. Belpaire and Hsu also presented an alternative approach for nested virtualization <ref type="bibr" target="#b8">[9]</ref>. In contrast to today's x86 architecture which has a single level of architectural support for virtualization, they proposed a hardware architecture with multiple virtualization levels.</p><p>The IBM z/VM hypervisor <ref type="bibr" target="#b34">[35]</ref> included the first practical implementation of nested virtualization, by making use of multiple levels of architectural support. Nested virtualization was also implemented by Ford et al. in a microkernel setting <ref type="bibr" target="#b15">[16]</ref> by modifying the software stack at all levels. Their goal was to enhance OS modularity, flexibility, and extensibility, rather than run unmodified hypervisors and their guests.</p><p>During the last decade software virtualization technologies for x86 systems rapidly emerged and were widely adopted by the market, causing both AMD and Intel to add virtualization extensions to their x86 platforms (AMD SVM <ref type="bibr" target="#b3">[4]</ref> and Intel VMX <ref type="bibr" target="#b47">[48]</ref>). KVM <ref type="bibr" target="#b28">[29]</ref> was the first x86 hypervisor to support nested virtualization. Concurrent with this work, Alexander Graf and Joerg Roedel implemented nested support for AMD processors in KVM <ref type="bibr" target="#b22">[23]</ref>. Despite the differences between VMX and SVM-VMX takes approximately twice as many lines of code to implement-nested SVM shares many of the same underlying principles as the Turtles project. Multi-dimensional paging was also added to nested SVM based on our work, but multi-level device assignment is not implemented.</p><p>There was also a recent effort to incorporate nested virtualization into the Xen hypervisor <ref type="bibr" target="#b23">[24]</ref>, which again appears to share many of the same underlying principles as our work. It is, however, at an early stage: it can only run a single nested guest on a single CPU, does not have multi-dimensional paging or multi-level device assignment, and no performance results have been published.</p><p>Blue Pill <ref type="bibr" target="#b42">[43]</ref> is a root-kit based on hardware virtualization extensions. It is loaded during boot time by infecting the disk master boot record. It emulates VMX in order to remain functional and avoid detection when a hypervisor is installed in the system. Blue Pill's nested virtualization support is minimal since it only needs to remain undetectable <ref type="bibr" target="#b16">[17]</ref>. In contrast, a hypervisor with nested virtualization support must efficiently multiplex the hardware across multiple levels of virtualization dealing with all of CPU, MMU, and I/O issues. Unfortunately, according to its creators, Blue Pill's nested VMX implementation can not be published.</p><p>ScaleMP vSMP is a commercial product which aggregates multiple x86 systems into a single SMP virtual machine. ScaleMP recently announced a new "VM on VM" feature which allows running a hypervisor on top of their underlying hypervisor. No details have been published on the implementation.</p><p>Berghmans demonstrates another approach to nested x86 virtualization, where a software-only hypervisor is run on a hardware-assisted hypervisor <ref type="bibr" target="#b11">[12]</ref>. In contrast, our approach allows both hypervisors to take advantage of the virtualization hardware, leading to a more efficient implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Turtles: Design and Implementation</head><p>The IBM Turtles nested virtualization project implements nested virtualization for Intel's virtualization technology based on the KVM <ref type="bibr" target="#b28">[29]</ref> hypervisor. It can host multiple guest hypervisors simultaneously, each with its own multiple nested guest operating systems. We have tested it with unmodified KVM and VMware Server as guest hypervisors, and unmodified Linux and Windows as nested guest virtual machines. Since we treat nested hypervisors and virtual machines as unmodified black boxes, the Turtles project should also run any other x86 hypervisor and operating system.</p><p>The Turtles project is fairly mature: it has been tested running multiple hypervisors simultaneously, supports SMP, and takes advantage of two-dimensional page table hardware where available in order to implement nested MMU virtualization via multi-dimensional paging. It also makes use of multi-level device assignment for efficient nested I/O virtualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theory of Operation</head><p>There are two possible models for nested virtualization, which differ in the amount of support provided by the underlying architecture. In the first model, multi-level architectural support for nested virtualization, each hypervisor handles all traps caused by sensitive instructions of any guest hypervisor running directly on top of it. This model is implemented for example in the IBM System z architecture <ref type="bibr" target="#b34">[35]</ref>.</p><p>The second model, single-level architectural support for nested virtualization, has only a single hypervisor mode, and a trap at any nesting level is handled by this hypervisor. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, regardless of the level in which a trap occurred, execution returns to the level 0 trap handler. Therefore, any trap occurring at any level from 1 . . . n causes execution to drop to level 0. This limited model is implemented by both Intel and AMD in their respective x86 virtualization extensions, VMX <ref type="bibr" target="#b47">[48]</ref> and SVM <ref type="bibr" target="#b3">[4]</ref>.</p><p>Since the Intel x86 architecture is a single-level virtualization architecture, only a single hypervisor can use the processor's VMX instructions to run its guests. For unmodified guest hypervisors to use VMX instructions, this single bare-metal hypervisor, which we call L 0 , needs to emulate VMX. This emulation of VMX can work recursively. Given that L 0 provides a faithful emulation of the VMX hardware any time there is a trap on VMX instructions, the guest running on L 1 will not know it is not running directly on the hardware. Building on this infrastructure, the guest at L 1 is itself able use the same techniques to emulate the VMX hardware to an L 2 hypervisor which can then run its L 3 guests. More generally, given that the guest at L n−1 provides a faithful emulation of VMX to guests at L n , a guest at L n can use the exact same techniques to emulate VMX for a guest at L n+1 . We thus limit our discussion below to L 0 , L 1 , and L 2 .</p><p>Fundamentally, our approach for nested virtualization works by multiplexing multiple levels of virtualization (multiple hypervisors) on the single level of architectural support for virtualization, as can be seen in <ref type="figure" target="#fig_2">Figure 2</ref>  When L 1 wishes to run a virtual machine, it launches it via the standard architectural mechanism. This causes a trap, since L 1 is not running in the highest privilege level (as is L 0 ). To run the virtual machine, L 1 supplies a specification of the virtual machine to be launched, which includes properties such as its initial instruction pointer and its page table root. This specification must be translated by L 0 into a specification that can be used to run L 2 directly on the bare metal, e.g., by converting memory addresses from L 1 's physical address space to L 0 's physical address space. Thus L 0 multiplexes the hardware between L 1 and L 2 , both of which end up running as L 0 virtual machines.</p><p>When any hypervisor or virtual machine causes a trap, the L 0 trap handler is called. The trap handler then inspects the trapping instruction and its context, and de-cides whether that trap should be handled by L 0 (e.g., because the trapping context was L 1 ) or whether to forward it to the responsible hypervisor (e.g., because the trap occurred in L 2 and should be handled by L 1 ). In the latter case, L 0 forwards the trap to L 1 for handling.</p><p>When there are n levels of nesting guests, but the hardware supports less than n levels of MMU or DMA translation tables, the n levels need to be compressed onto the levels available in hardware, as described in Sections 3.3 and 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CPU: Nested VMX Virtualization</head><p>Virtualizing the x86 platform used to be complex and slow <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>. The hypervisor was forced to resort to on-the-fly binary translation of privileged instructions <ref type="bibr" target="#b2">[3]</ref>, slow machine emulation <ref type="bibr" target="#b7">[8]</ref>, or changes to guest operating systems at the source code level <ref type="bibr" target="#b5">[6]</ref> or during compilation <ref type="bibr" target="#b31">[32]</ref>.</p><p>In due time Intel and AMD incorporated hardware virtualization extensions in their CPUs. These extensions introduced two new modes of operation: root mode and guest mode, enabling the CPU to differentiate between running a virtual machine (guest mode) and running the hypervisor (root mode). Both Intel and AMD also added special in-memory virtual machine control structures (VMCS and VMCB, respectively) which contain environment specifications for virtual machines and the hypervisor.</p><p>The VMX instruction set and the VMCS layout are explained in detail in <ref type="bibr" target="#b26">[27]</ref>. Data stored in the VMCS can be divided into three groups. Guest state holds virtualized CPU registers (e.g., control registers or segment registers) which are automatically loaded by the CPU when switching from root mode to guest mode on VMEntry. Host state is used by the CPU to restore register values when switching back from guest mode to root mode on VMExit. Control data is used by the hypervisor to inject events such as exceptions or interrupts into virtual machines and to specify which events should cause a VMExit; it is also used by the CPU to specify the VMExit reason to the hypervisor.</p><p>In nested virtualization, the hypervisor running in root mode (L 0 ) runs other hypervisors (L 1 ) in guest mode. L 1 hypervisors have the illusion they are running in root mode. Their virtual machines (L 2 ) also run in guest mode.</p><p>As can be seen in <ref type="figure" target="#fig_3">Figure 3</ref>, L 0 is responsible for multiplexing the hardware between L 1 and L 2 . The CPU runs L 1 using VMCS 0→1 environment specification. Respectively, VMCS 0→2 is used to run L 2 . Both of these environment specifications are maintained by L 0 . In addition, L 1 creates VMCS 1→2 within its own virtualized environment. Although VMCS 1→2 is never loaded into the processor, L 0 uses it to emulate a VMX enabled CPU for L 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">VMX Trap and Emulate</head><p>VMX instructions can only execute successfully in root mode. In the nested case, L 1 uses VMX instructions in guest mode to load and launch L 2 guests, which causes VMExits. This enables L 0 , running in root mode, to trap and emulate the VMX instructions executed by L 1 .</p><p>In general, when L 0 emulates VMX instructions, it updates VMCS structures according to the update process described in the next section. Then, L 0 resumes L 1 , as though the instructions were executed directly by the CPU. Most of the VMX instructions executed by L 1 cause, first, a VMExit from L 1 to L 0 , and then a VMEntry from L 0 to L 1 .</p><p>For the instructions used to run a new VM, vmresume and vmlaunch, the process is different, since L 0 needs to emulate a VMEntry from L 1 to L 2 . Therefore, any execution of these instructions by L 1 cause, first, a VMExit from L 1 to L 0 , and then, a VMEntry from L 0 to L 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">VMCS Shadowing</head><p>L 0 prepares a VMCS (VMCS 0→1 ) to run L 1 , exactly in the same way a hypervisor executes a guest with a single level of virtualization. From the hardware's perspective, the processor is running a single hypervisor (L 0 ) in root mode and a guest (L 1 ) in guest mode. L 1 is not aware that it is running in guest mode and uses VMX instructions to create the specifications for its own guest, L 2 . L 1 defines L 2 's environment by creating a VMCS (VMCS 1→2 ) which contains L 2 's environment from L 1 's perspective. For example, the VMCS 1→2 GUEST-CR3 field points to the page tables that</p><formula xml:id="formula_0">L 1 prepared for L 2 . L 0 cannot use VMCS 1→2 to execute L 2 directly, since VMCS 1→2 is not valid in L 0 's environment and L 0 can- not use L 1 's page tables to run L 2 . Instead, L 0 uses VMCS 1→2 to construct a new VMCS (VMCS 0→2 ) that holds L 2 's environment from L 0 's perspective.</formula><p>L 0 must consider all the specifications defined in VMCS 1→2 and also the specifications defined in VMCS 0→1 to create VMCS 0→2 . The host state defined in VMCS 0→2 must contain the values required by the CPU to correctly switch back from L 2 to L 0 . In addition, VMCS 1→2 host state must be copied to VMCS 0→1 guest state. Thus, when L 0 emulates a switch between L 2 to L 1 , the processor loads the correct L 1 specifications.</p><p>The guest state stored in VMCS 1→2 does not require any special handling in general, and most fields can be copied directly to the guest state of VMCS 0→2 .</p><p>The control data of VMCS 1→2 and VMCS 0→1 must be merged to correctly emulate the processor behavior. For example, consider the case where L 1 specifies to trap an event E A in VMCS 1→2 but L 0 does not trap such event for L 1 (i.e., a trap is not specified in VMCS 0→1 ). To forward the event E A to L 1 , L 0 needs to specify the corresponding trap in VMCS 0→2 . In addition, the field used by L 1 to inject events to L 2 needs to be merged, as well as the fields used by the processor to specify the exit cause.</p><p>For the sake of brevity, we omit some details on how specific VMCS fields are merged. For the complete details, the interested reader is encouraged to refer to the KVM source code <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">VMEntry and VMExit Emulation</head><p>In nested environments, switches from L 1 to L 2 and back must be emulated. When L 2 is running and a VMExit occurs there are two possible handling paths, depending on whether the VMExit must be handled only by L 0 or must be forwarded to L 1 .</p><p>When the event causing the VMExit is related to L 0 only, L 0 handles the event and resumes L 2 . This kind of event can be an external interrupt, a non-maskable interrupt (NMI) or any trappable event specified in VMCS 0→2 that was not specified in VMCS 1→2 . From L 1 's perspective this event does not exist because it was generated outside the scope of L 1 's virtualized environment. By analogy to the non-nested scenario, an event occurred at the hardware level, the CPU transparently handled it, and the hypervisor continued running as before.</p><p>The second handling path is caused by events related to L 1 (e.g., trappable events specified in VMCS 1→2 ). In this case L 0 forwards the event to L 1 by copying VMCS 0→2 fields updated by the processor to VMCS 1→2 and resuming L 1 . The hypervisor running in L 1 believes there was a VMExit directly from L 2 to L 1 . The L 1 hypervisor handles the event and later on resumes L 2 by executing vmresume or vmlaunch, both of which will be emulated by L 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MMU: Multi-dimensional Paging</head><p>In addition to virtualizing the CPU, a hypervisor also needs to virtualize the MMU: A guest OS builds a guest page table which translates guest virtual addresses to guest physical addresses. These must be translated again into host physical addresses. With nested virtualization, a third layer of address translation is needed.</p><p>These translations can be done entirely in software, or assisted by hardware. However, as we explain below, current hardware supports only one or two dimensions (levels) of translation, not the three needed for nested virtualization. In this section we present a new technique, multi-dimensional paging, for multiplexing the three needed translation tables onto the two available in hardware. In Section 4.1.2 we demonstrate the importance of this technique, showing that more na¨ıvena¨ıve approaches (surveyed below) cause at least a three-fold slowdown of some useful workloads.</p><p>When no hardware support for memory management virtualization was available, a technique known as shadow page tables <ref type="bibr" target="#b14">[15]</ref>     Shadow-on-EPT is the most straightforward approach to use when the processor supports EPT. L 0 uses the EPT hardware, but L 1 cannot use it, so it resorts to shadow page tables. L 1 uses SPT 1→2 to run L 2 . L 0 configures the MMU to use SPT 1→2 as the first translation table and EPT 0→1 as the second translation table. In this way, the processor first translates from L 2 guest virtual address to L 1 host physical address using SPT 1→2 , and then translates from the L 1 host physical address to the L 0 host physical address using the EPT 0→1 .</p><p>Though the Shadow-on-EPT approach uses the EPT hardware, it still has a noticeable overhead due to page faults and page table modifications in L 2 . These must be handled in L 1 , to maintain the shadow page table. Each of these faults and writes cause VMExits and must be forwarded from L 0 to L 1 for handling. In other words, Shadow-on-EPT is slow for the exactly the same reasons that Shadow itself was slow for single-level virtualization-but it is even slower because nested exits are slower than non-nested exits.</p><p>In multi-dimensional page tables, as in twodimensional page tables, each level creates its own separate translation By using huge pages <ref type="bibr" target="#b33">[34]</ref> to back guest memory, L 0 can create smaller and faster EPT tables. Finally, to further improve performance, L 0 also allows L 1 to use VPIDs. With this feature, the CPU tags each translation in the TLB with a numeric virtual-processor id, eliminating the need for TLB flushes on every VMEntry and VMExit. Since each hypervisor is free to choose these VPIDs arbitrarily, they might collide and therefore L 0 needs to map the VPIDs that L 1 uses into valid L 0 VPIDs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">I/O: Multi-level Device Assignment</head><p>I/O is the third major challenge in server virtualization. There are three approaches commonly used to provide I/O services to a guest virtual machine. Either the hypervisor emulates a known device and the guest uses an unmodified driver to interact with it <ref type="bibr" target="#b46">[47]</ref>, or a para-virtual driver is installed in the guest <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b41">42]</ref>, or the host assigns a real device to the guest which then controls the device directly <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. Device assignment generally provides the best performance <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b52">53]</ref>, since it minimizes the number of I/O-related world switches between the virtual machine and its hypervisor, and although it complicates live migration, device assignment and live migration can peacefully coexist <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54]</ref>.</p><p>These three basic I/O approaches for a single-level guest imply nine possible combinations in the two-level nested guest case. Of the nine potential combinations we evaluated the more interesting cases, presented in Table 1. Implementing the first four alternatives is straightforward. We describe the last option, which we call multi-level device assignment, below. Multi-level device assignment lets the L 2 guest access a device directly, bypassing both hypervisors. This direct device access requires dealing with DMA, interrupts, MMIO, and PIOs <ref type="bibr" target="#b52">[53]</ref>.</p><formula xml:id="formula_1">I/O virtualization method I/O virtualization method between L 0 &amp; L 1 between L 1 &amp; L 2 Emulation Emulation Para-virtual Emulation Para-virtual</formula><p>Para-virtual Device assignment</p><p>Para-virtual Device assignment Device assignment Device DMA in virtualized environments is complicated, because guest drivers use guest physical addresses, while memory access in the device is done with host physical addresses. The common solution to the DMA problem is an IOMMU <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>, a hardware component which resides between the device and main memory. It uses a translation table prepared by the hypervisor to translate the guest physical addresses to host physical addresses. IOMMUs currently available, however, only support a single level of address translation. Again, we need to compress two levels of translation tables onto the one level available in hardware.</p><p>For modified guests this can be done using a paravirtual IOMMU: the code in L 1 which sets a mapping on the IOMMU from L 2 to L 1 addresses is replaced by a hypercall to L 0 . L 0 changes the L 1 address in that mapping to the respective L 0 address, and puts the resulting mapping (from L 2 to L 0 addresses) in the IOMMU.</p><p>A better approach, one which can run unmodified guests, is for L 0 to emulate an IOMMU for L 1 <ref type="bibr" target="#b4">[5]</ref>. L 1 believes that it is running on a machine with an IOMMU, and sets up mappings from L 2 to L 1 addresses on it. L 0 intercepts these mappings, remaps the L 1 addresses to L 0 addresses, and builds the L 2 -to-L 0 map on the real IOMMU.</p><p>In current x86 architecture, interrupts always cause a guest exit to L 0 , which proceeds to forward the interrupt to L 1 . L 1 will then inject it into L 2 . The EOI (end of interrupt) will also cause a guest exit. In Section 4.1.1 we discuss the slowdown caused by these interrupt-related exits, and propose ways to avoid it.</p><p>Memory-mapped I/O (MMIO) and Port I/O (PIO) for a nested guest work the same way they work for a singlelevel guest, without incurring exits on the critical I/O path <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Micro Optimizations</head><p>There are two main places where a guest of a nested hypervisor is slower than the same guest running on a baremetal hypervisor. First, the transitions between L 1 and L 2 are slower than the transitions between L 0 and L 1 . Second, the exit handling code running in the L 1 hypervisor is slower than the same code running in L 0 . In this section we discuss these two issues, and propose optimizations that improve performance. Since we assume that both L 1 and L 2 are unmodified, these optimizations require modifying L 0 only. We evaluate these optimizations in the evaluation section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Optimizing transitions between L 1 and L 2</head><p>As explained in Section 3.2.3, transitions between L 1 and L 2 involve an exit to L 0 and then an entry. In L 0 , most of the time is spent merging the VMCS's. We optimize this merging code to only copy data between VMCS's if the relevant values were modified. Keeping track of which values were modified has an intrinsic cost, so one must carefully balance full copying versus partial copying and tracking. We observed empirically that for common workloads and hypervisors, partial copying has a lower overhead. VMCS merging could be further optimized by copying multiple VMCS fields at once. However, according to Intel's specifications, reads or writes to the VMCS area must be performed using vmread and vmwrite instructions, which operate on a single field. We empirically noted that under certain conditions one could access VMCS data directly without ill side-effects, bypassing vmread and vmwrite and copying multiple fields at once with large memory copies. However, this optimization does not strictly adhere to the VMX specifications, and thus might not work on processors other than the ones we have tested.</p><p>In the evaluation section, we show that this optimization gives a significant performance boost in microbenchmarks. However, it did not noticeably improve the other, more typical, workloads that we have evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Optimizing exit handling in L 1</head><p>The exit-handling code in the hypervisor is slower when run in L 1 than the same code running in L 0 . The main cause of this slowdown are additional exits caused by privileged instructions in the exit-handling code.</p><p>In Intel VMX, the privileged instructions vmread and vmwrite are used by the hypervisor to read and modify the guest and host specification. As can be seen in Section 4.3, these cause L 1 to exit multiple times while it handles a single L 2 exit.</p><p>In contrast, in AMD SVM, guest and host specifications can be read or written to directly using ordinary memory loads and stores. The clear advantage of that model is that L 0 does not intervene while L 1 modifies L 2 specifications. Removing the need to trap and emulate special instructions reduces the number of exits and improves nested virtualization performance.</p><p>One thing L 0 can do to avoid trapping on every vmread and vmwrite is binary translation <ref type="bibr" target="#b2">[3]</ref> of problematic vmread and vmwrite instructions in the L 1 instruction stream, by trapping the first time such an instruction is called and then rewriting it to branch to a non-trapping memory load or store. To evaluate the potential performance benefit of this approach, we tested a modified L 1 that directly reads and writes VMCS 1→2 in memory, instead of using vmread and vmwrite. The performance of this setup, which we call DRW (direct read and write) is described in the evaluation section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We start the evaluation and analysis of nested virtualization with macro benchmarks that represent real-life workloads. Next, we evaluate the contribution of multilevel device assignment and multi-dimensional paging to nested virtualization performance. Most of our experiments are executed with KVM as the L 1 guest hypervisor. In Section 4.2 we present results with VMware Server as the L 1 guest hypervisor.</p><p>We then continue the evaluation with a synthetic, worst-case micro benchmark running on L 2 which causes guest exits in a loop. We use this synthetic, worstcase benchmark to understand and analyze the overhead and the handling flow of a single L 2 exit.</p><p>Our setup consisted of an IBM x3650 machine booted with a single Intel Xeon 2.9GHz core and with 3GB of memory. The host OS was Ubuntu 9.04 with a kernel that is based on the KVM git tree version kvm-87, with our nested virtualization support added. For both L 1 and L 2 guests we used an Ubuntu Jaunty guest with a kernel that is based on the KVM git tree, version kvm-87. L 1 was configured with 2GB of memory and L 2 was configured with 1GB of memory. For the I/O experiments we used a Broadcom NetXtreme 1Gb/s NIC connected via crossover-cable to an e1000e NIC on another machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Macro Workloads</head><p>kernbench is a general purpose compilation-type benchmark that compiles the Linux kernel multiple times. The compilation process is, by nature, CPU-and memory-intensive, and it also generates disk I/O to load the compiled files into the guest's page cache.</p><p>SPECjbb is an industry-standard benchmark designed to measure the server-side performance of Java run-time environments. It emulates a three-tier system and is primarily CPU-intensive.</p><p>We executed kernbench and SPECjbb in four setups: host, single-level guest, nested guest, and nested guest optimized with direct read and write (DRW) as described in Section 3.5.2. The optimizations described in Section 3.5.1 did not make a significant difference to these benchmarks, and are thus omitted from the results. We used KVM as both L 0 and L 1 hypervisor with multidimensional paging. The results are depicted in  We compared the impact of running the workloads in a nested guest with running the same workload in a singlelevel guest, i.e., the overhead added by the additional level of virtualization. For kernbench, the overhead of nested virtualization is 14.5%, while for SPECjbb the score is degraded by 7.82%. When we discount the Intel-specific vmread and vmwrite overhead in L 1 , the overhead is 10.3% and 6.3% respectively.</p><p>To analyze the sources of overhead, we examine the time distribution between the different levels. <ref type="figure">Figure 5</ref> shows the time spent in each level. It is interesting to compare the time spent in the hypervisor in the singlelevel case with the time spent in L 1 in the nested guest case, since both hypervisors are expected to do the same work. The times are indeed similar, although the L 1 hypervisor takes more cycles due to cache pollution and TLB flushes, as we show in Section 4.3. The significant part of the virtualization overhead in the nested case comes from the time spent in L 0 and the increased number of exits.</p><p>For SPECjbb, the total number of cycles across all levels is the same for all setups. This is because SPECjbb executed for the same pre-set amount of time in both cases and the difference was in the benchmark score.</p><p>Efficiently virtualizing a hypervisor is hard. Nested virtualization creates a new kind of workload for the L 0 hypervisor which did not exist before: running another hypervisor (L 1 ) as a guest. As can be seen in <ref type="figure">Figure 5</ref>, for kernbench L 0 takes only 2.28% of the overall cycles in the single-level guest case, but takes 5.17% of the overall cycles for the nested-guest case. In other words, L 0 has to work more than twice as hard when running a nested guest.</p><p>Not all exits of L 2 incur the same overhead, as each type of exit requires different handling in L 0 and L 1 . In <ref type="figure" target="#fig_6">Figure 6</ref>, we show the total number of cycles required to handle each exit type. For the single level guest we measured the number of cycles between VMExit and the consequent VMEntry. For the nested guest we measured the number of cycles spent between L 2 VMExit and the consequent L 2 VMEntry.</p><p>There is a large variance between the handling times of different types of exits. The cost of each exit comes primarily from the number of privileged instructions performed by L 1 , each of which causes an exit to L 0 . For example, when L 1 handles a PIO exit of L 2 , it generates on average 31 additional exits, whereas in the cpuid case discussed later in Section 4.3 only 13 exits are required. Discounting traps due to vmread and vmwrite, the average number of exits was reduced to 14 for PIO and to 2 for cpuid.</p><p>Another source of overhead is heavy-weight exits. The external interrupt exit handler takes approximately 64K cycles when executed by L 0 . The PIO exit handler takes approximately 12K cycles when executed by L 0 . However, when those handlers are executed by L 1 , they take much longer: approximately 192K cycles and 183K cycles, respectively. Discounting traps due to vmread and vmwrite, they take approximately 148K cycles and 130K cycles, respectively. This difference in execution times between L 0 and L 1 is due to two reasons: first, the handlers execute privileged instructions causing exits to L 0 . Second, the handlers run for a long time compared with other handlers and therefore more external events such as external interrupts occur during their run-time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.1 I/O Intensive Workloads</head><p>To examine the performance of a nested guest in the case of I/O intensive workloads we used netperf, a TCP streaming application that attempts to maximize the amount of data sent over a single TCP connection. We measured the performance on the sender side, with the default settings of netperf (16,384 byte messages). <ref type="figure" target="#fig_7">Figure 7</ref> shows the results for running the netperf TCP stream test on the host, in a single-level guest, and in a nested guest, using the five I/O virtualization combinations described in Section 3.4. We used KVM's default emulated NIC (RTL-8139), virtio <ref type="bibr" target="#b41">[42]</ref> for a paravirtual NIC, and a 1 Gb/s Broadcom NetXtreme II with device assignment. All tests used a single CPU core.</p><p>On bare-metal, netperf easily achieved line rate (940 Mb/s) with 20% CPU utilization.</p><p>Emulation gives a much lower throughput, with full CPU utilization: On a single-level guest we get 25% of the line rate. On the nested guest the throughput is even lower and the overhead is dominated by the cost of device emulation between L 1 and L 2 . Each L 2 exit is trapped by L 0 and forwarded to L 1 . For each L 2 exit, L 1 then executes multiple privileged instructions, incurring multiple exits back to L 0 . In this way the overhead for The para-virtual virtio NIC performs better than emulation since it reduces the number of exits. Using virtio all the way up to L 2 gives 75% of line rate with a saturated CPU, better but still considerably below bare-metal performance.</p><p>Multi-level device assignment achieved the best performance, with line rate at 60% CPU utilization <ref type="figure" target="#fig_7">(Fig- ure 7</ref>, direct/direct). Using device assignment between L 0 and L 1 and virtio between L 1 and L 2 enables the L 2 guest to saturate the 1Gb link with 92% CPU utilization <ref type="figure" target="#fig_7">(Figure 7</ref>, direct/virtio).</p><p>While multi-level device assignment outperformed the other methods, its measured performance is still suboptimal because 60% of the CPU is used for running a workload that only takes 20% on bare-metal. Unfortunately on current x86 architecture, interrupts cannot be assigned to guests, so both the interrupt itself and its EOI cause exits. The more interrupts the device generates, the more exits, and therefore the higher the virtualization overhead-which is more pronounced in the nested case. We hypothesize that these interrupt-related exits are the biggest source of the remaining overhead, so had the architecture given us a way to avoid these exits-by assigning interrupts directly to guests rather than having each interrupt go through both hypervisors-netperf performance on L 2 would be close to that of bare-metal.</p><p>To test this hypothesis we reduced the number of interrupts, by modifying standard bnx2 network driver to work without any interrupts, i.e., continuously poll the device for pending events <ref type="figure" target="#fig_8">Figure 8</ref> compares some of the I/O virtualization combinations with this polling driver. Again, multi-level device assignment is the best option and, as we hypothesized, this time L 2 performance is close to bare-metal. With netperf's default 16,384 byte messages, the throughput is often capped by the 1 Gb/s line rate, so we ran netperf with smaller messages. As we can see in the figure, for 64-byte messages, for example, on L 0 (bare metal) a throughput of 900 Mb/s is achieved, while on L 2 with multi-level device assignment, we get 837 Mb/s, a mere 7% slowdown. The runner-up method, virtio on direct, was not nearly as successful, and achieved just 469 Mb/s, 50% below bare-metal performance. CPU utilization was 100% in all cases since a polling driver consumes all available CPU cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Impact of Multi-dimensional Paging</head><p>To evaluate multi-dimensional paging, we compared each of the macro benchmarks described in the previous sections with and without multi-dimensional paging. For each benchmark we configured L 0 to run L 1 with EPT support. We then compared the case where L 1 uses shadow page tables to run L 2 ("Shadow-on-EPT") with the case of L 1 using EPT to run L 2 ("multi-dimensional paging"). Figure 9: Impact of multi-dimensional paging <ref type="figure">Figure 9</ref> shows the results. The overhead between the two cases is mostly due to the number of page-fault exits. When shadow paging is used, each page fault of the L 2 guest results in a VMExit. When multi-dimensional pag-ing is used, only an access to a guest physical page that is not mapped in the EPT table will cause an EPT violation exit. Therefore the impact of multi-dimensional paging depends on the number of guest page faults, which is a property of the workload. The improvement is startling in benchmarks such as kernbench with a high number of page faults, and is less pronounced in workloads that do not incur many page faults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VMware Server as a Guest Hypervisor</head><p>We also evaluated VMware as the L 1 hypervisor to analyze how a different guest hypervisor affects nested virtualization performance. We used the hosted version, VMWare Server v2.0.1, build 156745 x86-64, on top of Ubuntu based on kernel 2.6.28-11. We intentionally did not install VMware tools for the L 2 guest, thereby increasing nested virtualization overhead. Due to similar results obtained for VMware and KVM as the nested hypervisor, we show only kernbench and SPECjbb results below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benchmark</head><p>% overhead vs. single-level guest kernbench 14.98 SPECjbb 8.85 <ref type="table">Table 3</ref>: VMware Server as a guest hypervisor</p><p>Examining L 1 exits, we noticed VMware Server uses VMX initialization instructions <ref type="bibr">(vmon, vmoff, vmptrld, vmclear)</ref> several times during L 2 execution. Conversely, KVM uses them only once. This dissimilitude derives mainly from the approach used by VMware to interact with the host Linux kernel. Each time the monitor module takes control of the CPU, it enables VMX. Then, before it releases control to the Linux kernel, VMX is disabled. Furthermore, during this transition many non-VMX privileged instructions are executed by L 1 , increasing L 0 intervention.</p><p>Although all these initialization instructions are emulated by L 0 , transitions from the VMware monitor module to the Linux kernel are less frequent for Kernbench and SPECjbb. The VMware monitor module typically handles multiple L 2 exits before switching to the Linux kernel. As a result, this behavior only slightly affected the nested virtualization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Micro Benchmark Analysis</head><p>To analyze the cycle-costs of handling a single L 2 exit, we ran a micro benchmark in L 2 that does nothing except generate exits by calling cpuid in a loop. The virtualization overhead for running an L 2 guest is the ratio between the effective work done by the L 2 guest and the overhead of handling guest exits in L 0 and L 1 . Based on this definition, this cpuid micro benchmark is a worst case workload, since L 2 does virtually nothing except generate exits. We note that cpuid cannot in the general case be handled by L 0 directly, as L 1 may wish to modify the values returned to L 2 . <ref type="figure" target="#fig_0">Figure 10</ref> shows the number of CPU cycles required to execute a single cpuid instruction. We ran the cpuid instruction 4 * 10 6 times and calculated the average number of cycles per iteration. We repeated the test for the following setups: 1. native, 2. running cpuid in a single level guest, and 3. running cpuid in a nested guest with and without the optimizations described in Section 3.5. For each execution, we present the distribution of the cycles between the levels: L 0 , L 1 , L 2 . CPU mode switch stands for the number of cycles spent by the CPU when performing a VMEntry or a VMExit. On bare metal cpuid takes about 100 cycles, while in a virtual machine it takes about 2,600 cycles ( <ref type="figure" target="#fig_0">Figure 10</ref>, column 1), about 1,000 of which is due to the CPU mode switching. When run in a nested virtual machine it takes about 58,000 cycles ( <ref type="figure" target="#fig_0">Figure 10</ref>, column 2). To understand the cost of handling a nested guest exit compared to the cost of handling the same exit for a single-level guest, we analyzed the flow of handling cpuid: In general, step 5 can be repeated multiple times. Each iteration consists of a single VMExit from L 1 to L 0 . The total number of exits depends on the specific implementation of the L 1 hypervisor. A nesting-friendly hypervisor will keep privileged instructions to a minimum. In any case, the L 1 hypervisor must interact with VMCS 1→2 , as described in Section 3.2.2. In the case of cpuid, in step 5, L 1 reads 7 fields of VMCS 1→2 , and writes 4 fields to VMCS 1→2 , which ends up as 11 VMExits from L 1 to L 0 . Overall, for a single L 2 cpuid exit there are 13 CPU mode switches from guest mode to root mode and 13 CPU mode switches from root mode to guest mode, specifically in steps: 2, 4, 5b, 5d, 8, 10.</p><p>The number of cycles the CPU spends in a single switch to guest mode plus the number of cycles to switch back to root mode, is approximately 1,000. The total CPU switching cost is therefore around 13,000 cycles.</p><p>The other two expensive steps are 3 and 9. As described in Section 3.5, these switches can be optimized. Indeed as we show in <ref type="figure" target="#fig_0">Figure 10</ref>, column 3, using various optimizations we can reduce the virtualization overhead by 25%, and by 80% when using non-trapping vmread and vmwrite instructions.</p><p>By avoiding traps on vmread and vmwrite <ref type="figure" target="#fig_0">(Fig- ure 10</ref>, columns 4 and 5), we removed the exits caused by VMCS 1→2 accesses and the corresponding VMCS access emulation, step 5. This optimization reduced the switching cost by 84.6%, from 13,000 to 2,000. While it might still be possible to optimize steps 3 and 9 further, it is clear that the exits of L 1 while handling a single exit of L 2 , and specifically VMCS accesses, are a major source of overhead. Architectural support for both faster world switches and VMCS updates without exits will reduce the overhead.</p><p>Examining <ref type="figure" target="#fig_0">Figure 10</ref>, it seems that handling cpuid in L 1 is more expensive than handling cpuid in L 0 . Specifically, in column 3, the nested hypervisor L 1 spends around 5,000 cycles to handle cpuid, while in column 1 the same hypervisor running on bare metal only spends 1500 cycles to handle the same exit (note that these numbers do not include the mode switches). The code running in L 1 and in L 0 is identical; the difference in cycle count is due to cache pollution. Running the cpuid handling code incurs on average 5 L2 cache misses and 2 TLB misses when run in L 0 , whereas running the exact same code in L 1 incurs on average 400 L2 cache misses and 19 TLB misses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In nested environments we introduce a new type of workload not found in single-level virtualization: the hypervisor as a guest. Traditionally, x86 hypervisors were designed and implemented assuming they will be running directly on bare metal. When they are executed on top of another hypervisor this assumption no longer holds and the guest hypervisor behavior becomes a key factor.</p><p>With a nested L 1 hypervisor, the cost of a single L 2 exit depends on the number of exits caused by L 1 during the L 2 exit handling. A nesting-friendly L 1 hypervisor should minimize this critical chain to achieve better performance, for example by limiting the use of trapcausing instructions in the critical path.</p><p>Another alternative for reducing this critical chain is to para-virtualize the guest hypervisor, similar to OS paravirtualization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. While this approach could reduce L 0 intervention when L 1 virtualizes the L 2 environment, the work being done by L 0 to virtualize the L 1 environment will still persist. How much this technique can help depends on the workload and on the specific approach used. Taking as a concrete example the conversion of vmreads and vmwrites to non-trapping load/stores, para-virtualization could reduce the overhead for kernbench from 14.5% to 10.3%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Architectural Overhead</head><p>Part of the overhead introduced with nested virtualization is due to the architectural design choices of x86 hardware virtualization extensions.</p><p>Virtualization API: Two performance sensitive areas in x86 virtualization are memory management and I/O virtualization. With multi-dimensional paging we compressed three MMU translation tables onto the two available in hardware; multi-level device assignment does the same for IOMMU translation tables. Architectural support for multiple levels of MMU and DMA translation tables-as many tables as there are levels of nested hypervisors-will immediately improve MMU and I/O virtualization.</p><p>Architectural support for delivering interrupts directly from the hardware to the L 2 guest will remove L 0 intervention on interrupt delivery and completion, intervention which, as we explained in Section 4.1.1, hurts nested performance. Such architectural support will also help single-level I/O virtualization performance <ref type="bibr" target="#b32">[33]</ref>.</p><p>VMX features such as MSR bitmaps, I/O bitmaps, and CR masks/shadows <ref type="bibr" target="#b47">[48]</ref> proved to be effective in reducing exit overhead. Any architectural feature that reduces single-level exit overhead also shortens the nested critical path. Such features, however, also add implementation complexity, since to exploit them in nested environments they must be properly emulated by L 0 hypervisors.</p><p>Removing the (Intel-specific) need to trap on every vmread and vmwrite instruction will give an immediate performance boost, as we showed in Section 3.5.2.</p><p>Same Core Constraint: The x86 trap-and-emulate implementation dictates that the guest and hypervisor share each core, since traps are always handled on the core where they occurred. Due to this constraint, when the hypervisor handles an exit the guest is temporarily stopped on that core. In a nested environment, the L 1 guest hypervisor will also be interrupted, increasing the total interruption time of the L 2 guest. Gavrilovska, et al., presented techniques for exploiting additional cores to handle guest exits <ref type="bibr" target="#b18">[19]</ref>. According to the authors, for a single level of virtualization, they measured 41% average improvements in call latency for null calls, cpuid and page table updates. These techniques could be adapted for nested environments in order to remove L 0 interventions and also reduce privileged instructions call latencies, decreasing the total interruption time of a nested guest.</p><p>Cache Pollution: Each time the processor switches between the guest and the host context on a single core, the effectiveness of its caches is reduced. This phenomenon is magnified in nested environments, due to the increased number of switches. As was seen in Section 4.3, even after discounting L 0 intervention, the L 1 hypervisor still took more cycles to handle an L 2 exit than it took to handle the same exit for the single-level scenario, due to cache pollution. Dedicating cores to guests could reduce cache pollution <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and increase performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Efficient nested x86 virtualization is feasible, despite the challenges stemming from the lack of architectural support for nested virtualization. Enabling efficient nested virtualization on the x86 platform through multidimensional paging and multi-level device assignment opens exciting avenues for exploration in such diverse areas as security, clouds, and architectural research.</p><p>We are continuing to investigate architectural and software-based methods to improve the performance of nested virtualization, while simultaneously exploring ways of building computer systems that have nested virtualization built-in.</p><p>Last, but not least, while the Turtles project is fairly mature, we expect that the additional public exposure stemming from its open source release will help enhance its stability and functionality. We look forward to seeing in what interesting directions the research and open source communities will take it.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Nested traps with single-level architectural support for virtualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>.</head><label></label><figDesc>Traps are forwarded by L 0 between the different levels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multiplexing multiple levels of virtualization on a single hardware-provided level of support</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Extending VMX for nested virtualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 shows</head><label>4</label><figDesc>three differ-ent nested MMU virtualization models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: MMU alternatives for nested virtualization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: CPU cycle distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of netperf in various setups</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance of netperf with interrupt-less network driver</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: CPU cycle distribution for cpuid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>was used. A guest creates a guest page table, which translates guest virtual addresses to guest physical addresses. Based on this table, the hy- pervisor creates a new page table, the shadow page ta- ble, which translates guest virtual addresses directly to the corresponding host physical address [3, 6]. The hy- pervisor then runs the guest using this shadow page table instead of the guest's page table. The hypervisor has to trap all guest paging changes, including page fault excep- tions, the INVLPG instruction, context switches (which cause the use of a different page table) and all the guest updates to the page table. To improve virtualization performance, x86 architec- tures recently added two-dimensional page tables [13]- a second translation table in the hardware MMU. When translating a guest virtual address, the processor first uses the regular guest page table to translate it to a guest phys- ical address. It then uses the second table, called EPT by Intel (and NPT by AMD), to translate the guest physi- cal address to a host physical address. When an entry is missing in the EPT table, the processor generates an EPT violation exception. The hypervisor is responsible for maintaining the EPT table and its cache (which can be flushed with INVEPT), and for handling EPT viola- tions, while guest page faults can be handled entirely by the guest. The hypervisor, depending on the processors capabil- ities, decides whether to use shadow page tables or two- dimensional page tables to virtualize the MMU.</figDesc><table>In nested 
environments, both hypervisors, L 0 and L 1 , determine 
independently the preferred mechanism. Thus, L 0 and 
L 1 hypervisors can use the same or a different MMU 
virtualization mechanism. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>table . Fortables should be compressed into one: Let us assume that L 0 runs L 1 using EPT 0→1 , and that L 1 cre- ates an additional table, EPT 1→2 , to run L 2 , because L 0 exposed a virtualized EPT capability to L 1 . The L 0 hy- pervisor could then compress EPT 0→1 and EPT 1→2 into a single EPT 0→2 table as shown in Figure 4. Then L 0 could run L 2 using EPT 0→2 , which translates</head><label>.</label><figDesc></figDesc><table>L 1 to create an EPT table, L 0 
exposes EPT capabilities to L 1 , even though the hard-
ware only provides a single EPT table. 
Since only one EPT table is available in hardware, the 
two EPT directly 
from the L 2 guest physical address to the L 0 host physi-
cal address, reducing the number of page fault exits and 
improving nested virtualization performance. In Sec-
tion 4.1.2 we demonstrate more than a three-fold speedup 
of some useful workloads with multi-dimensional page 
tables, compared to shadow-on-EPT. 
The L 0 hypervisor launches L 2 with an empty EPT 0→2 
table, building the table on-the-fly, on L 2 EPT-violation 
exits. These happen when a translation for a guest phys-
ical address is missing in the EPT table. If there is no 
translation in EPT 1→2 for the faulting address, L 0 first 
lets L 1 handle the exit and update EPT 1→2 . L 0 can now 
create an entry in EPT 0→2 that translates the L 2 guest 
physical address directly to the L 0 host physical address: 
EPT 1→2 is used to translate the L 2 physical address to a 
L 1 physical address, and EPT 0→1 translates that into the 
desired L 0 physical address. 
To maintain correctness of EPT 0→2 , the L 0 hypervisor 
needs to know of any changes that L 1 makes to EPT 1→2 . 
L 0 sets the memory area of EPT 1→2 as read-only, thereby 
causing a trap when L 1 tries to update it. L 0 will then up-
date EPT 0→2 according to the changed entries in EPT 1→2 . 
L 0 also needs to trap all L 1 INVEPT instructions, and in-
validate the EPT cache accordingly. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 : I/O combinations for a nested guest</head><label>1</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table>Kernbench 
Host 
Guest Nested Nested DRW 
Run time 
324.3 355 
406.3 
391.5 
STD dev. 
1.5 
10 
6.7 
3.1 
% overhead 
vs. host 
-
9.5 
25.3 
20.7 
% overhead 
vs. guest 
-
-
14.5 
10.3 
%CPU 
93 
97 
99 
99 
SPECjbb 
Host 
Guest Nested Nested DRW 
Score 
90493 83599 77065 78347 
STD dev. 
1104 
1230 
1716 
566 
% degradati-
on vs. host 
-
7.6 
14.8 
13.4 
% degradati-
on vs. guest 
-
-
7.8 
6.3 
%CPU 
100 
100 
100 
100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 : kernbench and SPECjbb results</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="1"> http://en.wikipedia.org/wiki/Turtles all the way down</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Alexander Graf and Joerg Roedel, whose KVM patches for nested SVM inspired parts of this work. The authors would also like to thank Ryan Harper, Nadav Amit, and our shepherd Robert English for insightful comments and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoenix</forename><surname>Hyperspace</surname></persName>
		</author>
		<ptr target="http://www.hyperspace.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Intel virtualization technology for directed I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abramson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muthrasanallur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Neiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Regnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schoinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vembu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wiegert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="179" to="192" />
			<date type="published" when="2006-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of software and hardware techniques for x86 virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agesen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2006-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Secure virtual machine architecture reference manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amd</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IOMMU: Strategies for mitigating the IOTLB bottleneck</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassour</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIOSCA &apos;10: Sixth Annual Workshop on the Interaction between Operating Systems and Computer Architecture</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barham</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warfield</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;03: Symposium on Operating Systems Principles</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The multikernel: a new os architecture for scalable multicore systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Dagand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sch¨upbach</forename><surname>Sch¨upbachsch¨</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Singhania</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;09: 22nd ACM SIGOPS Symposium on Operating systems principles</title>
		<imprint>
			<biblScope unit="page" from="29" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">QEMU, a fast and portable dynamic translator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bellard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hardware architecture for recursive virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belpaire</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM &apos;75: 1975 annual ACM conference</title>
		<imprint>
			<biblScope unit="page" from="14" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Formal properties of recursive virtual machine architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belpaire</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="89" to="96" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Utilizing IOMMUs for virtualization in Linux and Xen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Yehuda</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xenidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mallick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wahlig</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OLS &apos;06: The 2006 Ottawa Linux Symposium</title>
		<imprint>
			<biblScope unit="page" from="71" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Nesting virtual machines in virtualization test frameworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berghmans</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2010-05" />
		</imprint>
		<respStmt>
			<orgName>University of Antwerp</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Accelerating two-dimensional page walks for virtualized systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhargava</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serebrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Spadini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS &apos;08: 13th intl. conference on architectural support for programming languages and operating systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Live migration of virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Jul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Limpach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warfield</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI &apos;05: Second Symposium on Networked Systems Design &amp; Implementation</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="273" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Virtualization system including a virtual machine monitor for a computer with a segmented architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devine</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Bugnion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosenblum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US</title>
		<imprint>
			<biblScope unit="volume">6397242</biblScope>
			<date type="published" when="2002-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microkernels meet recursive virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ford</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hibler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lepreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tullmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clawson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;96: Second USENIX symposium on Operating systems design and implementation</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="137" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compatibility is not transparency: VMM detection myths and realities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garfinkel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Warfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franklin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HOTOS&apos;07: 11th USENIX workshop on Hot topics in operating systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A virtual machine introspection based architecture for intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garfinkel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosenblum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network &amp; Distributed Systems Security Symposium</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="191" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">High-performance hypervisor architectures: Virtualization in hpc systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavrilovska</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nathuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranadive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saraiya</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCVIRT &apos;07: 1st Workshop on System-level Virtualization for High Performance Computing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lala: a late launch application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gebhardt</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalton</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STC &apos;09: 2009 ACM workshop on Scalable trusted computing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Architecture of virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on virtual computer systems</title>
		<meeting>the workshop on virtual computer systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1973" />
			<biblScope unit="page" from="74" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Survey of virtual machine research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Magazine</title>
		<imprint>
			<biblScope unit="page" from="34" to="45" />
			<date type="published" when="1974-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nesting the virtualized world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graf</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roedel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linux Plumbers Conference</title>
		<imprint>
			<date type="published" when="2009-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Nested virtualization on xen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Xen Summit Asia</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ally: Ostransparent packet inspection using sequestered cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Monchiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WIOV &apos;10: The Second Workshop on I/O Virtualization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nomad: migrating OS-bypass networks in virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panda</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEE &apos;07: 3rd international conference on Virtual execution environments</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="158" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Intel 64 and IA-32 Architectures Software Developers Manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Intel</forename><surname>Corporation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Live migration of direct-access devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kadav</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Workshop on I/O Virtualization (WIOV &apos;08)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">KVM: the linux virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kivity</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Laor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lublin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liguori</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ottawa Linux Symposium</title>
		<imprint>
			<date type="published" when="2007-07" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A recursive virtual machine architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lauer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wyeth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on virtual computer systems</title>
		<imprint>
			<date type="published" when="1973" />
			<biblScope unit="page" from="113" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unmodified device driver reuse and improved system dependability via virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levasseur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And G ¨ Otz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;04: 6th conference on Symposium on Opearting Systems Design &amp; Implementation</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pre-virtualization: Soft layering for virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levasseur</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhlig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chubb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACSAC &apos;08: 13th AsiaPacific Computer Systems Architecture Conference</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating standard-based self-virtualizing devices: A performance study on 10 GbE NICs with SR-IOV support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IPDPS &apos;10: IEEE International Parallel and Distributed Processing Symposium</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Practical, transparent operating system support for superpages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navarro</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cox</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;02: 5th symposium on Operating systems design and implementation</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="89" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Esa/390 interpretive-execution architecture, foundation for vm/esa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osisek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Formal requirements for virtualizable third generation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Popek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goldberg</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="412" to="421" />
			<date type="published" when="1974-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High performance and scalable I/O virtualization via self-virtualized devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schwan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPDC &apos;07: Proceedings of the 16th international symposium on High performance distributed computing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Achieving 10Gbps using safe and transparent network interface virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rixner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VEE &apos;09: The 2009 ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments</title>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Guest-transparent prevention of kernel rootkits with vmm-based memory shadowing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riley</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Intrusion Detection</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">5230</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Analysis of the intel pentium&apos;s ability to support a secure virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irvine</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th conference on USENIX Security Symposium</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vmware&apos;s virtual platform: A virtual machine monitor for commodity pcs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosenblum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Chips</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">virtio: towards a de-facto standard for virtual I/O devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Subverting vista kernel for fun and profit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rutkowska</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2006-08" />
		</imprint>
	</monogr>
<note type="report_type">Blackhat</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Secvisor: a tiny hypervisor to provide lifetime kernel code integrity for commodity oses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seshadri</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perrig</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP &apos;07: 21st ACM SIGOPS symposium on Operating systems principles</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="335" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Isostack-highly efficient network processing on dedicated cores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Borovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Satran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Yehuda</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC &apos;10: The 2010 USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Loosely coupled tcp acceleration architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shalev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Makhervaks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Machulsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Satran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimony</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HOTI &apos;06: Proceedings of the 14th IEEE Symposium on High-Performance Interconnects</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Virtualizing I/O devices on VMware workstation&apos;s hosted virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sugerman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Venkitachalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uhlig</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Santoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Smith</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel virtualization technology. Computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="48" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Memory resource management in VMware ESX server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waldspurger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;02: 5th Symposium on Operating System Design and Implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Denali: a scalable isolation kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitaker</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gribble</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EW &apos;10: 10th ACM SIGOPS European workshop</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scale and performance in the denali isolation kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Whitaker</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gribble</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="195" to="209" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>SI</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Concurrent direct network access for virtual machine monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willmann</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zwaenepoel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="306" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Direct device assignment for untrusted fully-virtualized virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassour</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-A</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasserman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IBM Research Report H-0263</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Live migration with pass-through device for Linux VM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhai</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OLS &apos;08: The</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ottawa Linux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Symposium</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008-07" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
