<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Guard: A Track-Based Translation Layer for Shingled Disks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansour</forename><surname>Shafaei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Northeastern University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Desnoyers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Northeastern University</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Virtual Guard: A Track-Based Translation Layer for Shingled Disks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Virtual Guard (Vguard) is a track-based static mapping translation layer for shingled magnetic recording (SMR) drives. Data is written in-place by caching data from the next track in the shingling direction, allowing direct overwrite of sectors in the target track. This enables Vguard to take advantage of track-level locality, nearly eliminating cleaning for many workloads. We compare performance of Vguard to an available drive-managed SMR drive analyzed and modeled in previous research. Vguard reduces the 99.9% latency by 15× for real-world traces, and maximum latency by 32% for synthetic random write workloads.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and Related Work</head><p>Shingled Magnetic Recording (SMR) offers the opportunity for significant increases in disk drive density without radical changes in drive design and manufacturing. This comes at a cost-when writing track T , data on another track (i.e. track T +1) may be lost. This limitation may be addressed on the host, using new SCSI and SATA extensions; however many SMR drives sold today are drive-managed SMR (DM-SMR) devices, using an internal translation layer for plug-compatibility with non-SMR disks.</p><p>A number of shingling translation layer (STL) mechanisms have been proposed to date <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8]</ref>; however based on measurements of shipping drives, the dominant ones may be termed E-Region STLs, where writes are made to an exception region (or persistent cache), and later migrated to a permanent location elsewhere on disk. In a common variant <ref type="bibr" target="#b0">[1]</ref> the drive is divided into bands separated by unused tracks or guard tracks, allowing a band to be completely overwritten by the cleaning process without damage to "downstream" tracks, and LBAs are statically mapped to "home locations" in these bands.</p><p>With one exception, STLs proposed to date have used LBA mapping, leaving details of the disk geometry to lower layers of drive firmware. In contrast, He and Du <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> have proposed two mechanisms for track mapping STLs, where translation is performed after mapping an LBA to a track/head/sector location. Their first work describes a number of track mappings which allow efficient writes to low-numbered tracks in cases where the high-numbered tracks have yet to be written; more recently SMaRT <ref type="bibr" target="#b3">[4]</ref> is more general, using dynamic mapping of tracks. SMaRT is highly sensitive to utilization, with a large (64 MB) map which requires 100s of milliseconds to persist to disk and must be held in DRAM, and with poor performance at 90% utilization or above <ref type="bibr" target="#b0">1</ref> . Finally, SMaRT does not handle differences in track size due to varying track length, adaptive formatting <ref type="bibr" target="#b6">[7]</ref>, or slip sparing <ref type="bibr" target="#b10">[11]</ref>.</p><p>We propose Virtual Guard (Vguard), a novel track-mapped STL. Before modifying track T , Vguard migrates track T +1 to cache, allowing multiple in-place modifications to track T . It maps each track to a static home location, eliminating most track size issues. The persistent cache is comprised of large, outer-diameter tracks, allowing most cached tracks to occupy a single cache track along with a small metadata header. On-disk overhead is modest, consisting of a small (&lt;0.5%) persistent cache and a single guard track per band, and RAM overhead is almost negligible.</p><p>We present the design of Vguard, as well as simulation results showing significant performance improvements over traditional E-region STLs while imposing the same or lower disk space and DRAM overheads. Vguard is seen to take advantage of strong spatial locality in these workloads, avoiding cleaning in all cases except synthetic traces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Virtual Guard</head><p>Virtual Guard is a track-mapped STL, where LBAs are mapped to a track number, which is then mapped to a physical track location. A small region at the outer diameter is reserved as a persistent cache; the remainder of the drive is divided into bands separated by empty (guard) tracks, and the LBA space of the drive is mapped onto these non-cache tracks. Unlike most STLs (e.g. E-Region) which direct writes to the persistent cache, Vguard copies the next track (in the shingling direction) into the persistent cache, forming a "virtual guard" which allows the target track to be modified in place without data corruption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">In-Place Writes</head><p>Given a host write to LBA X, Vguard calculates the track and band number (T X ,B X ), and then checks if (a) T X is the last track in band B X or (b) track T X+1 is already cached.</p><p>If neither is the case, then Vguard reads the content of T X+1 and writes it to the next free track in persistent cache (T w f ). Track T X may now be modified in place 2 without risk to data in track T X+1 . Assuming the persistent cache is also shingled, Vguard uses the same approach when writing to tracks in cache-if the following track is valid then it is copied to the next free track in cache, and then writes are performed in-place. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Map Persistence</head><p>Vguard maintains an exception map, indicating which tracks have been relocated into cache and where. Due to the relatively small size of the persistent cache and the large granularity (one track) of the mapping, this map is extremely small in comparison to the ∼60 MB reported in SMaRT <ref type="bibr" target="#b3">[4]</ref> or the ∼1 MB map seen in commercial E-region-based devices <ref type="bibr" target="#b11">[12]</ref>. In particular, if N MT E is the number of entries in the mapping table and N tr and N trc are size of the drive and the persistent cache, respectively, in units of tracks, the mapping table size would be roughly:</p><formula xml:id="formula_0">MapSize=N MT E × (log(N tr ) + log(Ntrc))<label>(1)</label></formula><p>For a 5 TB drive with 24 GB of persistent cache and ∼ 4×10 6 tracks (see Skylight <ref type="bibr" target="#b0">[1]</ref>), the mapping table size would be roughly 30K. The map is small enough that a full copy can be appended to each track in persistent cache with minor overhead. (Note that on occasion this, or other track <ref type="bibr" target="#b1">2</ref> The number of times it may be safely overwritten may be limited due to upstream track interference, but is 1.</p><formula xml:id="formula_1">Tχ-1 Tχ Tχ+1 Guard Tν Map (a) (b) (c) (d)</formula><p>Figure 1: (a) Write in-place on T X+1 (the last track in its band with the guard track in front); (b) Write in-place on T X−1 after caching T X ; (c) Write in-place on cached track T X where it is located right before the log head in persistent cache; (d) Write on track T X after relocating it to the write frontier in cache (as there was a valid track (T υ ) in front of it in its previous location).</p><p>size differences, will make it necessary to allocate two cache tracks; however this will be rare.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cleaning</head><p>The cleaning process moves tracks from the persistent cache back to their home locations. Vguard does not perform background cleaning, but rather triggers cleaning when either:</p><p>1. the number of free tracks in the mapping table drops below a threshold α, or 2. the distance between the oldest track in persistent cache and the write frontier (including any invalidated tracks) exceeds a second threshold β.</p><p>In a single cleaning episode Vguard will clean two bands, for each band repeating the following process: pick the first two tracks in the tail of the log, and then clean all tracks from cache with home locations in the same band as either of the two tracks. The specific cleaning steps are:</p><p>1. Pick the track at the tail of the log, with home location in band B. 2. Read all other tracks in cache with home locations in band B. 3. Read band B. 4. Merge data for band B in memory and write to the scratch pad, a reserved section of persistent cache; this prevents data loss if power fails while overwriting the data band. 5. Overwrite band B with the merged data.</p><p>Vguard also adds the following two optimizations to the cleaning process: 1) If the first N tracks are not in cache, they can be skipped by the cleaning process (both the read-data-bandcopy-to-scratch and the overwrite-data-band parts). This reduces the worst-case cleaning overhead (when only one track is in cache) by 50% on average. The same optimization is used in DM-SMR drives on the market <ref type="bibr" target="#b0">[1]</ref>.</p><p>2) If multiple tracks from the same band are cached, Vguard may choose to perform a partial cleaning of the band. Rather than re-writing to the end of the band, rewriting stops at track T −1, where track T is in cache when cleaning starts.</p><p>To reduce the buffer memory required and duration for which host I/Os are stalled, a band may be cleaned in multiple stages, with each stage reading, persisting, and then re-writing a single buffer of data; read (and sometimes write) operations are interleaved between stages. Vguard uses a cleaning buffer size of 15 MB, roughly the same that seen in commercial DM-STL drives analyzed to date <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>Although E-region-based STLs typically perform background cleaning, this would be counter-productive for Vguard. By moving tracks into cache and introducing virtual guard tracks, Vguard allows repeated writes to write-hot locations without the need to copy data; cleaning would eliminate those spaces and require additional data movement before the hot locations could be written to again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Vguard is implemented as extensions to an existing and accurate Python simulator for DM-SMR devices <ref type="bibr" target="#b11">[12]</ref>. We compare Vguard with the simulated performance of the Seagate 5 TB drive ("DM-SMR"), using parameters and drive settings shown in <ref type="table">Table 1</ref>. No modification or scaling of trace LBAs was performed; traces were replayed "flat out" without delay between operations and with a queue depth of 1, and the (simulated) write cache was disabled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MSR Traces</head><p>Sixteen traces from the Microsoft Research trace set <ref type="bibr" target="#b9">[10]</ref> were simulated, representing a wide range of read/write ratios and total trace size. In <ref type="figure" target="#fig_2">Figure 2</ref> latency CDFs are shown for both Vguard and DM-SMR, with Vguard latency being seen to be dramatically lower. In some cases, e.g. src2 0 and mds 0, the 99 th percentile latency for Vguard is significantly lower than the 10 th percentile latency for DM-SMR. Details of tail latency may be seen in  <ref type="table">Table 1</ref>: Simulated drive parameters for Vguard and DM-SMR rotation (∼10ms) for all traces tested, and maximum latency is reduced from multiple seconds to ∼50ms in many cases 3 .</p><p>Due to spatial locality in the traces examined, the guard track set fits completely within cache and Vguard is able to perform an arbitrary number of writes in place (subject to upstream interference limitations) without cleaning. In contrast, DM-STL consumes space in both persistent cache and the map with every write, entering cleaning for all of the longer write-intensive traces. <ref type="figure" target="#fig_3">Figure 3</ref> shows the Vguard cache utilization for various MSR traces, showing both the maximum number of active tracks and the total number of tracks utilized, both as a percentage of total persistent cache size. We see that none of these traces require more than 13% of the 24 GB cache at any instance, and the total number of tracks copied to cache (i.e. the log length) is never more than 38% of the cleaning threshold trigger (β).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CloudPhysics Traces</head><p>Additional experiments were performed on a more modern set of traces courtesy of CloudPhysics <ref type="bibr" target="#b12">[13]</ref>, representing real workloads on large volumes. In <ref type="table" target="#tab_3">Table 3</ref> we see the total number of writes and the volume capacity for each tested trace. In <ref type="figure">Figure 4</ref> we see the results of these runs; again Vguard is able to handle the entire trace without entering cleaning.</p><p>In fact, analysis of the number of unique tracks modified in each of the 100+ CloudPhysics traces, seen in <ref type="figure" target="#fig_4">Figure 5</ref>, shows that none of these traces have a write footprint large enough to force Vguard cleaning, even with a small 24 GB persistent cache. <ref type="bibr" target="#b2">3</ref> In fact, the 100ms+ maximum latencies seen for rsrch 0 and stg 0 in <ref type="table" target="#tab_0">Table 2</ref> are mostly due to very large (e.g. 6 MB) read operations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Random Writes</head><p>To evaluate Vguard's cleaning performance, we generate 4 synthetic traces with 100K random writes spanned over 500GB, 1TB, 2TB, and 4TB address spaces and run them on Vguard and DM-SMR. Latency CDFs for these experiments are shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Vguard shows moderate improvements of about 30% in 99 th percentile and maximum latency, while 90 th percentile latency improves drastically for the smaller-footprint trace (500 GB) but is slightly worse than DM-SMR for the other cases. The similarity of results between Vguard and DM-STL for the larger traces is expected, as in each case each write forces one band to be cleaned, with only modest differences in overhead between the two STLs. (Note also that random write performance is thus limited by band size-larger band sizes with lower space overhead will result in poorer random write performance.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Vguard represents a novel approach to shingling translation layers, using persistent cache space for non-written tracks while performing writes in-place. As a result, consumption of cache space is not a function of the volume of data written, but rather of the pattern of LBAs which are written to,    <ref type="bibr">w27</ref> 3,182,636 1.95TB <ref type="bibr">w29</ref> 2,707,559 1.95TB w53 4,162,497 1.5TB <ref type="bibr">w54</ref> 8,648,118 3.6TB Figure 4: Latency CDFs for CloudPhysics traces on Vguard without regard to the number of times they are overwritten. In many real-world cases the guard track set is seen to fit comfortably within a rather small persistent cache, potentially offering near-conventional-drive levels of performance as all writes may be performed in-place. Further work is needed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Randomness Space</head><p>Vguard-90% DM-SMR-90%</p><p>Vguard-99% DM-SMR-99%</p><p>Vguard-Max DM-SMR-Max </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 demonstrates</head><label>1</label><figDesc>Figure 1 demonstrates examples of different write scenarios in Vguard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Latency CDFs for MSR traces on Vguard vs DM-SMR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Vguard cache utilization by traces (all MSR traces) Trace Number of writes Drive Size w27 3,182,636 1.95TB w29 2,707,559 1.95TB w53 4,162,497 1.5TB w54 8,648,118 3.6TB</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Approximate number of tracks modified in CloudPhysics traces, assuming fixed 1MB track size</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Latency CDFs for random traces on Vguard vs DM-SMR to compare Vguard with conventional drive performance, as well as to validate performance on real hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 . 95% latency for Vguard is seen to be roughly a single</head><label>2</label><figDesc></figDesc><table>Parameter 
Vguard 
DM-SMR 
Size 
5TB 
5TB 
Form factor 
3.5" 
3.5" 
RPM 
5980 
5980 
Track lengths 
1.8-0.9MB 
1.8-0.9MB 
Mapping type 
Static 
Static 
Band size 
20 tracks 
20 tracks 
Cleaning granularity 
2 Bands 
2 Bands 
Cache location 
Outer diameter 
Outer diameter 
Cache size 
13.8K Tracks (24GB) 13.8K Tracks (24GB) 
Mapping table size 
∼30KB 
∼1.3MB 
α 
9194 
22986 
β 
9194 
22986 
Write cache 
Disabled 
Disabled 
Read ahead 
Disabled 
Disabled 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>DM-SMR Vguard DM-SMR Vguard DM-SMR Vguard DM-SMR Vguard DM</head><label></label><figDesc></figDesc><table>95% 

99% 
99.5% 
99.9% 
Max 
Trace Vguard -SMR 
hm 0 
10.08 
24.78 
10.4 
95.01 
10.97 
97.58 
21.57 
324.78 
53.03 
7,234.81 
hm 1 
10.1 
24.78 
10.4 
56.55 
10.4 
64.06 
20.13 
74.62 
50.60 
592.04 
mds 0 
10.08 
24.78 
10.4 
95.01 
10.4 
95.81 
30.12 
324.78 
50.52 
1,789.66 
proj 0 
10.4 
24.78 
18.7 
95.01 
20.43 
99.78 
21.56 
324.78 
52.29 
4,341.17 
proj 3 
10.38 
24.78 
10.4 
24.78 
20.11 
30.5 
30.38 
102.35 
53.88 
1,573.94 
rsrch 0 
10.08 
28.79 
10.31 
96.24 
10.68 
105.24 
30.92 
324.78 
117.81 
1,657.22 
rsrch 1 
10.14 
24.78 
31.78 
24.78 
37 
41.18 
39.46 
324.78 
62.52 
324.78 
rsrch 2 
10.08 
24.78 
10.08 
31.59 
10.1 
81.64 
30.79 
324.78 
47.83 
4,082.31 
src1 2 
10.38 
24.78 
10.4 
85.64 
20.09 
95.63 
20.43 
324.78 
50.62 
2,400.70 
src2 0 
10.08 
24.78 
10.18 
95.09 
10.4 
98.77 
30.21 
324.78 
60.59 
3,428.12 
src2 1 
10.4 
20.18 
20.43 
24.78 
20.43 
24.78 
20.44 
29.72 
58.85 
332.20 
stg 0 
10.11 
24.78 
10.40 
95.08 
14.89 
97.47 
30.12 
324.78 
123.92 
1754.54 
usr 0 
10.16 
26.96 
10.4 
85.3 
20.18 
95.36 
24.85 
324.78 
158.74 
2,396.69 
wdev 0 
10.08 
24.78 
10.23 
95.01 
10.49 
97.19 
30.14 
324.78 
56.56 
1,710.40 
wdev 1 
10.08 
24.78 
10.08 
24.78 
10.38 
24.78 
38.04 
324.78 
39.25 
324.78 
wdev 2 
10.1 
24.78 
10.19 
24.78 
11.51 
95.01 
33.21 
324.78 
49.32 
1,628.12 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Latency percentiles for MSR traces on Vguard and DM-SMR in milliseconds 

0 

5 

10 

15 

20 

25 

30 

35 

40 

hm_0 
hm_1 
mds_0 
proj_0 
proj_3 
rsrch_0 
rsrch_1 
rsrch_2 
src1_2 
src2_0 
src2_1 
stg_0 
usr_0 
wdev_0 
wdev_1 
wdev_2 

Ocuupancy (%) 

Trace 

Tracks 
Log Length 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Characteristics of tested CloudPhysics traces 

0 

20 

40 

60 

80 

100 

0.01 
0.1 
1 
10 
100 
IO Percentile (%) 

Latency (ms) 

w27 
w29 
w53 
w54 

</table></figure>

			<note place="foot" n="1"> The cost to increase RAM from 128MB to 256 MB to accommodate this table ($0.70) is large compared to an estimated 10% profit margin on drives which may sell wholesale at $50; a loss of 10% capacity is also likely to be economically unfeasible.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Skylight-a window on shingled disk operation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aghayev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shafaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desnoyers</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Indirection systems for shingled-recording disk drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassuto</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sanvido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bandic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mass Storage Systems and Technologies (MSST), 2010 IEEE 26th Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Novel address mappings for shingled write disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX conference on Hot Topics in Storage and File Systems</title>
		<meeting>the 6th USENIX conference on Hot Topics in Storage and File Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5" to="5" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An approach to shingled magnetic recording translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Smart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Conference on File and Storage Technologies</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="121" to="134" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classifying data to reduce long-term data movement in shingled write disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D E</forename><surname>Pitchumani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Strong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Caveat-Scriptor: Write Anywhere Shingled Disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kadekodi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pimpale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gibson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 15)</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disks are like snowflakes: no two are alike</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krevat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tucek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX conference on Hot topics in operating systems</title>
		<meeting>the 13th USENIX conference on Hot topics in operating systems</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="14" to="14" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating hot data identification into shingled write disks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hswd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Modeling, Analysis &amp; Simulation of Computer and Telecommunication Systems (MAS-COTS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="321" to="330" />
		</imprint>
	</monogr>
	<note>IEEE 20th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Zea, a data management approach for smr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzanares</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lemoal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maltzahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bandic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th USENIX Workshop on Hot Topics in Storage and File Systems</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Write off-loading: practical power management for enterprise storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narayanan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowstron</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 6th USENIX Conference on File and Storage Technologies<address><addrLine>San Jose, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An introduction to disk drive modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruemmler</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilkes</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="17" to="28" />
			<date type="published" when="1994-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling smr drive performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafaei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hajkazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Desnoyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aghayev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science</title>
		<meeting>the 2016 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Science<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="389" to="390" />
		</imprint>
	</monogr>
	<note>SIGMETRICS &apos;16, ACM</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient MRC Construction with SHARDS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waldspurger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garthwaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 13th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="95" to="110" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
