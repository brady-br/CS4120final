<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Common Substrate for Cluster Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Hindman</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Konwinski</surname></persName>
							<email>andyk@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
							<email>istoica@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Common Substrate for Cluster Computing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The success of MapReduce has sparked many efforts to design cluster computing frameworks. We argue that no single framework will be optimal for all applications , and that we should instead enable organizations to run multiple frameworks efficiently in the same cloud. Furthermore, to ease development of new frameworks, it is critical to identify common abstractions and modularize their architectures. To achieve these goals, we propose Nexus, a low-level substrate that provides isolation and efficient resource sharing across frameworks running on the same cluster, while giving each framework freedom to implement its own programming model and fully control the execution of its jobs. Nexus fosters innovation in the cloud by letting organizations run new frameworks alongside existing ones and by letting framework developers focus on specific applications rather than building one-size-fits-all frameworks.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It has become increasingly clear that different cloud applications benefit from different programming models. MapReduce <ref type="bibr" target="#b6">[8]</ref> is attractive for its simplicity, but this simplicity makes it difficult to express some computations. This led to the development of higher-level, domain-specific abstractions that use MapReduce as an execution substrate, like Sawzall <ref type="bibr" target="#b13">[15]</ref> and Pig <ref type="bibr" target="#b12">[14]</ref>, and of more general programming models that provide their own execution substrate, like Dryad <ref type="bibr" target="#b10">[12]</ref>. However, making a programming model too general increases complexity (Dryad is arguably more complex than MapReduce) and decreases the opportunity to optimize programs using application-specific knowledge. We have identified several applications that are hard to express efficiently even in Dryad. We believe that no single general programming model for the cloud will exist. Instead, multiple cluster computing frameworks will emerge, providing various programming models with different tradeoffs. In this context, it is critical to identify abstractions and system constructs that facilitate the development of such new frameworks.</p><p>As new programming models and new frameworks emerge, they will need to share computing resources and data sets. For example, a company using Hadoop <ref type="bibr" target="#b1">[3]</ref> should not have to build a second cluster and copy data into it to run a Dryad job. Sharing resources between frameworks is difficult today because frameworks perform both job execution management and resource management. For example, Hadoop acts like a "cluster OS" that allocates resources among users in addition to running jobs. To enable diverse frameworks to coexist, Nexus decouples job execution management from resource management by providing a simple resource management layer over which frameworks like Hadoop and Dryad can run. Nexus provides a "slot" abstraction, in which frameworks may run "tasks" that do arbitrary work. Along with a mechanism called "slot offers", the fine granularity of slots lets Nexus achieve more efficient resource sharing across frameworks than would be possible with traditional coarse-grained cluster scheduling systems like Torque <ref type="bibr" target="#b15">[17]</ref>. Nexus is analogous to a "cluster hypervisor": it provides isolation and multiplexing while giving each framework a high level of control over its own execution.</p><p>Nexus is beneficial even to organizations that only wish to run Hadoop. Two concerns in Hadoop today are scalability and robustness of the master process. Because the master manages all jobs running in the cluster (so it can perform resource allocation), it must be highly robust, and must scale to hundreds of jobs across thousands of nodes. Any scheduling logic that goes into the master must be tested extensively, because a bug in the master will crash the whole cluster. Consequently, companies like Yahoo! and Facebook run 6-10 month old versions of Hadoop even though new versions provide significant performance and functionality improvements. If these organizations ran Hadoop on Nexus, they would be able to run separate Hadoop masters for each job, mitigating the scaling and robustness problems. Furthermore, they could even run a stable version of Hadoop for production jobs and a faster but less stable version for experimental jobs simultaneously. The Nexus master can be made robust and scalable more easily than the Hadoop master because it is a smaller codebase, performs simpler logic, and rarely needs to change. In this way, Nexus resembles a microkernel architecture: the component that must be the most reliable (the resource manager) is made as small as possible.</p><p>This paper is organized as follows. In Section 2 we show examples of applications to which MapReduce and Dryad are not well suited. Section 3 describes the Nexus architecture. Section 4 describes our current prototype. We survey related work in Section 5 and present a discussion and future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Beyond MapReduce and Dryad</head><p>Although MapReduce and Dryad support many applications, these frameworks have limitations. We list four examples of applications for which these frameworks are not ideal. In all four cases, one could envision extending MapReduce and Dryad to better support the listed application. However, this would require extensive engineering and testing to ensure that the changes do not break existing applications, and would increase the complexity of these systems. Nexus provides an alternative: a developer can build a framework more suitable than MapReduce or Dryad for her application, and this framework can share a cluster efficiently with MapReduce and Dryad.</p><p>1. Iterative Jobs: Many machine learning algorithms start with a random value for a parameter and repeatedly evaluate a function of this parameter over a dataset to compute a direction in which to move the parameter. Each evaluation can be expressed as a MapReduce job. However, systems like Hadoop launch new worker processes for each job, which must re-read the input data, because they do not know that the jobs are related. In many applications, the amount of data per node is small enough that it would be more efficient to keep it in memory between iterations.</p><p>2. Nested Parallelism: As parallel programming libraries for clusters are developed, it becomes attractive to compose them. For example, a machine learning job might want to call a parallel matrix multiplication library from each task. Neither MapReduce nor Dryad support clean nesting of data flows: for example, if a Dryad vertex launches a computation as a second Dryad job, it has to stay running while the second job executes, consuming a node in the cluster. Nested parallelism is expressed more naturally in other programming models, like Cilk <ref type="bibr" target="#b4">[6]</ref>.  might wish to add objects to explore onto a "work queue" and prioritize them based on how attractive they look. Neither MapReduce nor Dryad allow this level of control over the execution order of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Existing Parallel Applications: It would be useful to run parallel applications like distcc [2] on</head><p>the same infrastructure as Hadoop. However, expressing these applications as maps and reduces is awkward. Nexus allows more natural wrapping of these applications into tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Nexus Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The goal of Nexus is to provide isolation and efficient resource sharing across cluster computing frameworks running on the same cluster. To accomplish this, Nexus provides abstractions of computing resources and an API to let frameworks access these resources. Nexus exports two abstractions: tasks and slots. A task represents a unit of work, such as a map task in MapReduce. A slot represents a computing resource in which a framework may run a task, such as a core and some associated memory on a multicore machine. We rely on standard OS facilities (e.g. setrlimit in Linux) to isolate slots. We are also considering using virtual machines. Nexus employs two-level scheduling. At the first level, Nexus allocates slots between frameworks using fair sharing. At the second level, each framework is responsible for dividing its work into tasks, selecting which tasks to run in each slot, and as we shall explain, deciding which slots to use. This lets frameworks perform application-specific optimizations. <ref type="figure" target="#fig_1">Figure 1</ref> shows the architecture of Nexus. Like Hadoop, Nexus has a master process that controls a slave daemon running on each node in the cluster. Each framework that uses Nexus has a scheduler process that registers with the master. Schedulers launch tasks in their allocated slots by providing task descriptors. These descriptors are passed to a framework-specific executor process that Nexus launches on slave nodes. A framework assigned multiple slots on the same node has one executor per slot. Executors are also reused for subsequent tasks run in their slot. This amortizes initialization costs and lets executors cache data shared across tasks in memory which is highly beneficial for jobs like the iterative one described in Section 2. Finally, Nexus passes status updates about tasks to schedulers, including notification if a task fails or a node is lost. The elements of Nexus -schedulers, executors and tasks -map closely to components of frameworks like Hadoop and Dryad. Nexus factors these elements out into a common layer.</p><p>Nexus purposely does not provide abstractions for storage and communication. We concentrate only on allocating computing resources (slots), and allow tasks to use whichever storage and communication libraries they wish. For example, we expect many sites to install Hadoop's HDFS distributed file system for storage. We do, however, plan to build communication and storage abstractions for framework developers to use on top of Nexus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Slot Assignment</head><p>The main challenge with Nexus's two-level scheduling design is ensuring that frameworks are allocated slots they wish to run in. For example, a MapReduce framework needs to run maps on the nodes that contain its input file to avoid reading data over the network. Nexus addresses this issue through a mechanism called slot offers. When a slot becomes free, Nexus offers it to each framework in turn, in order of how far each framework is below its fair share. Each framework may accept the slot and launch a task in it, or refuse the slot if, for example, it has no data on that machine. Refusing a slot keeps the framework below its fair share, ensuring that it is offered future slots before other frameworks. If the framework has still not found a data-local slot after waiting for some time, it can always accept a non-local slot. While it may seem counterintuitive that refusing slots can be beneficial to frameworks, experience with job scheduling in Hadoop <ref type="bibr" target="#b2">[4]</ref> has shown that this simple policy is enough to achieve 99% data locality 1 and high fairness in workloads with dozens of jobs.</p><p>In general, slot offers let frameworks use arbitrary criteria for selecting slots without requiring Nexus to be aware of these criteria. For example, while a MapReduce job wishes to run on nodes that have its input data, the iterative job in Section 2 might prefer to reuse its previous slots, to take advantage of data loaded into memory by its executors.</p><p>A second concern is how to reassign slots when frameworks' demands change (e.g., a new framework registers). In normal operation, Nexus simply reassigns slots to new frameworks as tasks from overallocated frameworks finish. As long as tasks are short, this is sufficient to redistribute slots quickly. For example, if the average task length is one minute in a 100-node cluster with 4 slots per node is, then there will be about 6.7 tasks finishing per second. If we need to reassign, say, 40 slots (10% of the cluster), we only need to wait 6 seconds. To ensure that frameworks are not starved even when some tasks are long, Nexus can also reclaim a slot by killing the task executing in it after a timeout. Nexus reclaims most-recently launched tasks from each framework first, minimizing wasted work. Because frameworks running on large clusters need to tolerate losing tasks due to hardware failures, reclamation does not impact them significantly -it is as if they never received the slot. We are also considering mechanisms for letting frameworks choose which slots they prefer to lose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation Status</head><p>We have implemented a prototype of Nexus in approximately 2,000 lines of C and C++. By limiting the scope of Nexus's concerns we were able to focus on optimizing for performance and scalability; for example, we used asynchronous I/O techniques for network communication. A framework registers and unregisters with Nexus using the API presented in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="table" target="#tab_1">Table 2</ref> lists the callbacks that Nexus uses to communicate with a framework for task management. Once registered, a framework receives slot offers and task status updates from the Nexus master via the schedule and status callbacks. The Nexus slave passes a task descriptor to an executor by invoking its execute callback. Preliminary evaluation shows that Nexus can schedule thousands of tasks per second, an order of magnitude more than Hadoop. We have also implemented an iterative machine learning job (logistic regression) as described in Section 2 against Nexus, and measured factor of 10 performance gains from executor reuse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Infrastructure as a Service</head><p>Cloud infrastructures such as Amazon EC2 <ref type="bibr" target="#b0">[1]</ref> and Eucalyptus <ref type="bibr" target="#b11">[13]</ref> allow for sharing between users by allowing virtual machines in a shared cloud to be rented by the hour. In such an environment, it would be pos-  Invoked when Nexus offers a slot to a framework. status() Invoked when Nexus conveys information about a scheduled task to a framework. execute(task id, args...) Invoked in the slot the framework scheduled the task. sible for separate frameworks to run concurrently on the same physical cluster by creating separate virtual clusters (i.e., EC2 allocations). However, VMs can take minutes to start and sharing data between separate virtual clusters is difficult to accomplish and can result in poor data locality. Nexus provides abstractions (i.e., tasks and slots) that eradicate the need for many applications to use heavyweight VMs. In addition, Nexus allows frameworks to select where to run tasks via the slot offer mechanism, allowing multiple frameworks to share data while achieving good data locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cluster Computing Frameworks</head><p>MapReduce was originally described in <ref type="bibr" target="#b6">[8]</ref>. <ref type="bibr">Sawzall [15]</ref> and Pig <ref type="bibr" target="#b12">[14]</ref> built richer programming models, like relational operators, on top of MapReduce. However, running all jobs over MapReduce can be inefficient: for example, a Pig query that is broken into a series of MapReduce jobs cannot pipeline data directly between them.</p><p>Dryad <ref type="bibr" target="#b10">[12]</ref> addresses this problem by providing a more general execution model: data flow DAGs. Clustera <ref type="bibr" target="#b7">[9]</ref> provides a similar execution model. These frameworks are valuable because they supply common execution abstractions, like worker vertices and channels, to applications. However, the set of abstractions provided is still limited. For example, vertices in Dryad form a DAG, which makes it difficult to write the iterative job in Section 2 where one would like to send a parameter to a worker, have it compute a function, and use this result to compute a new parameter for the same worker. Rather than trying to build an even more general set of abstractions than Dryad, Nexus lets Dryad layer over it, but also enables other frameworks to be written against its lower-level interface of tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cluster Scheduling Systems</head><p>Nexus differs from previous cluster scheduling systems in two aspects: its slot offer mechanism and the fine-grained nature of slots.</p><p>Of the frameworks listed in the previous section, only Clustera <ref type="bibr" target="#b7">[9]</ref> describes multi-user support. Clustera uses a heuristic incorporating user priorities, data locality and starvation to match work to idle nodes. However, Clustera requires each job to explicitly list its locality needs (e.g. by listing its input files), meaning that needs that cannot be listed in this manner (e.g. reusing a node to reuse a worker process) cannot be taken into account. In contrast, Nexus's slot offer mechanism lets jobs use whichever heuristics they desire to select a slot.</p><p>Batch cluster schedulers like Torque <ref type="bibr" target="#b15">[17]</ref> give each job a fixed block of machines at once, rather than letting a job's allocation scale up and down in a finegrained manner, and do not account for data locality. Grid schedulers like Condor <ref type="bibr" target="#b16">[18]</ref> support locality constraints, though usually at the level of geographic sites. Condor uses a "ClassAd" mechanism match node properties to job needs. As in Clustera, this means that job needs that cannot be expressed as ClassAds cannot be accounted for. Nexus gives jobs finer control over where they run through slot offers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion and Future Work</head><p>By letting diverse cluster computing frameworks coexist efficiently, Nexus accelerates innovation in the cloud and encourages development of specialized frameworks rather than one-size-fits-all solutions. Nexus is strongly inspired by work on microkernels <ref type="bibr" target="#b3">[5]</ref>, exokernels <ref type="bibr" target="#b8">[10]</ref> and hypervisors <ref type="bibr" target="#b9">[11]</ref> in the OS community and by the success of the narrow-waist IP model <ref type="bibr" target="#b5">[7]</ref> in computer networks. Like a microkernel or hypervisor, Nexus is a stable, minimal core that provides performance and fault isolation to frameworks sharing a cluster. Like an exokernel, Nexus aims to give frameworks as much control over their execution as possible. Finally, like IP, Nexus encourages diversity and innovation in cluster computing by providing a "narrow waist" API which lets different frameworks run over shared hardware.</p><p>We have already written some applications directly against Nexus, and are currently porting Hadoop to run over it. However, to ease the development of new frameworks, we plan to build a stack of higher-level abstractions over Nexus for framework developers to use. For example, if Nexus represents the IP of cluster computing, then a fault-tolerance library providing a concept of "reliable tasks" might represent the TCP. Other reusable abstractions that we intend to build are a data flow DAG library similar to Dryad and a task-queue library similar to Intel Threading Building Blocks <ref type="bibr" target="#b14">[16]</ref> for frameworks that wish to employ nested or irregular parallelism.</p><p>Using Nexus, we intend to explore both how to provide better programming models for existing cloud applications and how to enable other applications to benefit from cloud computing. Some of the new applications we wish to explore include: 1. Scientific Computing: Many scientific applications are not communication-intensive and could therefore run on commodity clusters, but scientists typically write such applications using libraries like MPI rather than MapReduce. 2. Parallel Build/Test Tools: It would be beneficial to run build tools like distcc and distributed unit testing tools on the same infrastructure as Hadoop clusters to maximize utilization. 3. Web Servers: It may even be possible to use Nexus to share resources between batch computing frameworks and interactive web applications. Web servers are stateless, so they can be started and stopped dynamically as "tasks" as long as load balancers are aware of this. This would let an organization use a portion of its cluster for web serving during the day and batch applications at night, improving data center utilization. Finally, from an operational standpoint, Nexus allows an organization to use multiple cluster computing frameworks without having to allocate separate hardware to each one. For example, if a new implementation of MapReduce appears that is faster than Hadoop on some workloads, an organization can switch a portion of its jobs to it. If an organization wishes to run two isolated instances of Hadoop, or two different versions of Hadoop, it can do this as well. Lastly, if a developer builds a specialized framework optimized for only one type of application, an organization can run this framework alongside Hadoop.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>Irregular Parallelism: In applications like graph search, the data flow graph is not known in ad- vance. For example, a branch-and-bound algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing the architectures of Nexus and Hadoop. The shaded regions make up Nexus. Hadoop can run on top of Nexus, alongside Dryad.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Register framework's scheduler and executor with the Nexus master. nexus unregister() Terminate the current framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 : Nexus API.</head><label>1</label><figDesc></figDesc><table>Function 
Description 
schedule(slot id) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Nexus framework callbacks. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We would like to thank Randy Katz, Anthony Joseph, Scott Shenker, Krste Asanovi´cAsanovi´c, George Necula, and the anonymous reviewers. This research was supported by California MICRO, California Discovery, the Natural Sciences and Engineering Research Council of Canada, a National Science Foundation Graduate Research Fellowship 2 , and the Berkeley RAD Lab sponsors: Sun, Google, Microsoft, HP, Cisco, NetApp, VMWare, Amazon and Facebook.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ec2</forename><surname>Amazon</surname></persName>
		</author>
		<ptr target="http://aws.amazon.com/ec2/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadoop</surname></persName>
		</author>
		<ptr target="http://lucene.apache.org/hadoop" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<ptr target="http://issues.apache.org/jira/browse/HADOOP-4667" />
		<title level="m">Hadoop scheduling discussion</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mach: A new kernel foundation for unix development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Accetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Bolosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tevanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Summer</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="93" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cilk: An efficient multithreaded runtime system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Blumofe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Joerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Kuszmaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Randall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A protocol for packet network intercommunication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Icahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="637" to="648" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified data processing on large clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="137" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Clustera: an integrated computation and data management system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Dewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paulson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Royalty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krioukov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>PVLDB</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="28" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exokernel: An operating system architecture for application-level resource management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;toole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Evolution of a virtual machine subsystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Hartmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Systems Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="142" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dryad: distributed data-parallel programs from sequential building blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Budiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fetterly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys 07</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="59" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The eucalyptus open-source cloud-computing system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Grzegorczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obertelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Youseff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zagorodnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Cloud Computing and Its Applications</title>
		<meeting>Cloud Computing and Its Applications<address><addrLine>Chicago, Illinois</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pig latin: a not-so-foreign language for data processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1099" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interpreting the data: Parallel analysis with sawzall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dorward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Griesemer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="277" to="298" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Intel Threading Building Blocks: Outfitting C++ for Mul ti-core PRocessor Parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reinders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<pubPlace>O&apos;Reilly</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Torque -torque resource manager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Staples</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Distributed computing in practice: the Condor experience. Concurrency and Computation -Practice and Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="323" to="356" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
