<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:55+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Importance of Evaluating Storage Systems&apos; $Costs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Mukker</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Zadok</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Importance of Evaluating Storage Systems&apos; $Costs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modern storage systems are becoming more complex, combining different storage technologies with different behaviors. Performance alone is not enough to characterize storage systems: energy efficiency, durability, and more are becoming equally important. We posit that one must evaluate storage systems from a monetary cost perspective as well as performance. We believe that cost should consider the workloads used over the storage sys-tems&apos; expected lifetime. We designed and developed a versatile hybrid storage system under Linux that combines HDD and SSD. The SSD can be used as cache or as primary storage for hot data. Our system includes tun-able parameters to enable trading off performance, energy use, and durability. We built a cost model and evaluated our system under a variety of workloads and parameters , to illustrate the importance of cost evaluations of storage systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Storage systems are getting more complex with solidstate technologies rapidly taking hold, shingled devices available, and hybrids thereof being proposed and commercialized <ref type="bibr" target="#b14">[15]</ref>. As the amount of digital data grows rapidly, virtualization and cloud technologies highlight the need to consolidate storage and save on the longerterm data storage costs. Complex workloads play a key role in how storage systems behave. The interplay of hardware, software, and workloads has a significant impact on throughput, energy consumption <ref type="bibr" target="#b7">[8]</ref>, and device durability. We propose to evaluate modern storage systems from a monetary cost perspective that includes many dimensions including performance <ref type="bibr" target="#b2">[3]</ref>. We assume that server-class storage systems should be utilized at high yields, due to consolidation and virtualization. We propose that monetary costs should be evaluated over the expected lifetime of the storage system, typically years, and consider device wear-out and replacement <ref type="bibr" target="#b12">[13]</ref>.</p><p>Several studies integrate SSDs into storage systems, and some consider the original purchase cost or shortterm energy use, but neglect to consider the long term impact on device wear-out <ref type="bibr">[4-6, 9, 12, 14]</ref>. Some simulated the results instead of conducting empirical studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>. Few explore the pros and cons of tiering vs. caching approaches to hybrid storage systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>To facilitate this study, we developed a device-mapper target for the Linux kernel that combines HDD and SSD. Our target can use the SSD as either (1) a cache with asynchronous write-back of dirty data to the HDD, or (2) a primary store for hot data. Our target include versatile policies for management of hot/cold data between the SSD and HDD. We parameterized many aspects of our system and added counters and instrumentation to measure its behavior under various configurations. We conducted extensive experiments using many workloads and configurations-including single-drives and hybrids. We present a subset of these results here.</p><p>Next, we built a cost model that also includes lifetime cost of ownership: energy and power costs, replacement cost, and more. We populated the model with realistic figures from industry and our own empirical experiments. We observed that for some workloads, an SSD-only solution incurs the highest overall costs in the short term but much lower costs in the long run. We also observed that for some workloads, using the SSD as a cache had lower costs than when the SSD was used as primary hotdata storage; but other workloads completely reversed this trend. That is why we believe that future storage systems must be evaluated across dimensions of lifetime cost, performance, as well as workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Cost Model</head><p>A cost metric is important to justify storage systems' expenditures <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref>. Generally, monetary costs include upfront purchase and the Total Cost of Ownership (TCO) <ref type="bibr" target="#b2">[3]</ref>. We use a time factor to estimate longer-term costs. We summarize our model below.</p><p>1 ≤ i ≤ n (n : the number of devices) (1)</p><formula xml:id="formula_0">1 ≤ α (time f actor, def ault = 1) (2) Cost = P urchase + T CO (3) P urchase = n i=1 C devi (4) C devi = N ormalized P rice devi × Capacity devi (5) T CO = α × (C energy + C power + C endu ) + C ser (6) C energy = Lookup LIP A (Amount energy ) (7) C power = Lookup LIP A (Amount power ) (8) C endu = n i=1 C endui (9) C endui = C devi × dev i wearout Limit i<label>(10)</label></formula><formula xml:id="formula_1">dev i wearout = writes if dev i = SSD #startstop if dev i = HDD<label>(11)</label></formula><formula xml:id="formula_2">Limit i = Limit writes if dev i = SSD Limit cycles if dev i = HDD<label>(12)</label></formula><p>C ser = f ixed estimation (13) Equation 1 names a variable i for each device. Equation 2 specifies α as the time factor to project future cost estimates (i.e., run the same amount of workload multiple times). Equation 3 shows that the total cost (Cost) depends on the upfront purchase cost (P urchase) and the T CO. Equations 4 and 5 show that the upfront purchase cost depends on a normalized price of each device (N ormalized P rice devi ) and the capacity of each device (Capacity devi ). Note that the normalized price of each device can change over time. In our paper we present results based on prices the the Intel SSD and Seagate HDD we purchased in 2012.  Equation 6 shows that the TCO depends on the energy cost (C energy ), the power cost (C power ), the endurance cost (C endu ), and the service cost (C ser ). We also use α as the time factor to predict future costs associated with the energy, power, and endurance (or replacement) in the longer run (i.e., assuming we run the same workload multiple times). Equations 7 and 8 show that we can get the energy and power cost by looking up the price table (Lookup LIP A ) provided by the local electricity authority (Long Island Power Authority), as shown in <ref type="table" target="#tab_0">Table 1</ref>. We assume that: (1) the energy we collected is distributed by 3/8, 1/4, and 3/8 in accordance with off-peak, peak, and intermediate; (2) the power we collected in off-peak, peak, and intermediate is the average power. The energy and power measurement is based on the whole system. We used a simplified method to estimate the energy and power cost. Equation 9 shows that we can get the total endurance cost by summarizing each device's endurance cost (C endui ). Equation 10 shows that we can get each device's endurance cost by multiplying the wear out degree ( devi wearout Limiti ) of each device type by the device's cost (C devi ). Note that the wear-out degree and the endurance limit of each device may be different.</p><p>Equations 11 and 12 show that the Flash-based SSD endurance depends more on the writes (writes). Note that reads also affect SSD's endurance: we convert the effect of reads to writes based on a parameterized ratio (e.g., writes caused by reads is calculated as reads/10). We also show that for HDD, the number of start-stop cycles (#startstop) is a major factor. Other factors include vibration, sector errors, and more <ref type="bibr" target="#b10">[11]</ref>. We use the number of HDD start-stop cycles for simplicity. Based on manufacturers' specifications, our SSD can sustain a total of 36.5TB writes, and our HDD can handle at most 300,000 spin up/down cycles. Equation 13 shows that we use fixed estimation as the service cost (C service ) for the hardware setup. Service costs may include manpower and air-conditioning costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Systems</head><p>We implemented both tiering and caching hybrid systems in the Linux Device Mapper framework. We wrote around 4,000 LoC of kernel code in twelve months. Both systems are scalable: they can be easily configured to use multiple drives with minor code change. However, to better analyze the behavior of our system, we used a twodrive setup in this paper: one SSD and one HDD. We present the data management of the two system in <ref type="figure" target="#fig_1">Fig- ure 1</ref>. The two systems are fairly similar in terms of design and implementation: frequently accessed data goes to the faster device and less frequently accessed data goes to slower device. The two systems are versatile to enable adaptation to different workloads. We support several configurable system parameters: (1) Extent Size (ES); (2) Promotion/Pre-fetching Threshold (PT)-access counts before being promoted/fetched; and (3) Maximum Concurrent Migration Limit (MCML). We summarize the key differences between the two systems below.</p><p>Capacity. In the caching system, since the SSD is not counted toward the total capacity, the HDD capacity needs to be expanded to yield the same amount of total capacity as the tiering system has. When the SSD capacity is not largely different from the total capacity, a tiering system can have better purchase cost per GB than the caching system does.</p><p>Management Unit. The caching system uses a cache entry table and the tiering system uses a mapping table. The cache entry table maintains mapping information only from the cache device to the lower-level device, and contains not only the four fields in the mapping table of the tiering system (i.e., extent ID, state, usage counter, and time-stamp of the latest access), but also a dirty flag to indicate whether a cached extent is updated or not.</p><p>Data Movement. The two systems use the same method to move data around. We name the hot data moving process promotion and pre-fetch in the tiering system and caching system, respectively. We name the cold data moving process demotion and eviction, respectively. The caching system does not need to reserve extra extents in the HDD for eviction to succeed, as it is guaranteed to map an extent from the SSD to the HDD.</p><p>Read/Write Policy. In a tiering system, since the SSD is used as primary storage, reads and writes access the data from the current location either on the SSD or HDD according to the mapping    kernel threads. In a caching system, reads and writes access data from the SSD if the data is still there, else from the HDD. If it is an SSD write hit, the system stores information of the pending write-back I/O in a queue, and an asynchronous write-back kernel thread wakes up to flush dirty writes from the SSD to the HDD. I/O access can be slow during write-back activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Experimental setup. We experimented on two identical Lenovo R ThinkCenter computers. Each has 4GB RAM and one Intel R Core-2 TM Quad 2.66GHz CPU. For storage, we used parts of an Intel SSDSA2CW300G3 300GB SSD and Seagate ST32000641AS 2TB HDD. A Linux 3.5.0 kernel ran on a separate SATA drive. We connected each computer to a WattsUP Pro ES in-line power meter to measure energy and power use.</p><p>Benchmarks. We evaluated with two trace workloads: Web-search trace from the UMass Trace Repository and the FIU's online trace. Trace details are shown in <ref type="table" target="#tab_3">Table 2</ref>. We set up 32GB and 8GB storage capacities for the Websearch and online traces, respectively. The "green" is our tiering hybrid drive and the "cache" is our caching hybrid drive. "Mylinear" is another tiering hybrid drive based on the Linux DM "linear" target that linearly maps from the virtual block address to the logical block address without any additional data management. For the two tiering hybrids, we chose 1/4 as an example ratio for the SSD over total capacity. To show comparable results, we used the same SSD and total capacities for the caching system. We ran all tests three times. We computed the standard deviations and presented as error bars in figures.</p><p>Results. We show the results in <ref type="figure">Figure 2</ref>. We also have results for Filebench's file-server workload but omit them for brevity because we observed similar trends. For Web-search, the caching system achieves slightly higher throughput (4-9%) than the tiering system does when the Pre-fetching Threshold (PT) is 4 and 16. It achieves similar throughput as the tiering system does when PT is 64 <ref type="figure">(Figure 2(a)</ref>). The SSD hit ratio ranges from 81-98% for the caching system, and ranges from 85-98% for the tiering system. Both the SSD hit ratio and the data movement affect the throughput for hybrids. Mylinear achieves an SSD hit ratio of only 8%. This workload has many more reads than writes (see <ref type="table" target="#tab_3">Table 2</ref>). Thus the overhead of the write-back is not significant as there are only a few writes. Moreover, as the SSD in the tiering system contains either cold or hot data beforehand, it can add some overhead to the overall throughput. However, the cache device in the caching system only contains hot data. It suggests that overall throughput of the caching system could be higher than the tiering system if the tiering primary storage (SSD) initially contains cold data.</p><p>The caching system has lower total cost (8-20%) in the long run than the tiering system does <ref type="figure">(Figure 2(e)</ref>). For Web-search, when the time factor is 100,000, it translates to an average of 2.1 years (ranging from 0.2-7.7 years) for all types of benchmarks, a reasonable time-frame for the expected lifetime of storage systems. The reasons are: (1) there are no additional primary I/Os to the SSD in the caching system, but the tiering system does since its SSD is used as primary storage; and (2) the SSD endurance reduction counts more toward the total cost of ownership in the long run. When the time factor is 1 <ref type="figure">(Figure 2(c)</ref>), the caching system incurs little additional dollar cost compared to the tiering system because the caching system only has to pay for the expanded HDD capacity.</p><p>For online, the caching system achieves lower throughput (58-82%) than the tiering system as the ES varies <ref type="figure">(Figure 2(b)</ref>). The SSD hit ratio ranges from 92-99% for the caching system, and from 98-99% for the tiering system. Both the SSD hit ratio and data movement affect system throughput. Mylinear achieves an SSD hit ratio of 84%. When the ES is 4K, the throughput of the caching system is 82% lower because it has even more write-back I/Os. This workload has lots of writes (Table 2). It suggests that the overhead of the write-back can be a throughput bottleneck. The caching system has a higher total cost (5-23%) in the long run than the tiering system <ref type="figure">(Figures 2(f)</ref>). For FIU online, when the time factor is 100,000, it translates to an average of 3.3 years (ranging from 0.7-9.8 years) for all types of benchmarks. There are no additional primary I/Os to the SSD for the caching system, but the caching system has many more write-back I/Os. Therefore, the caching system reduces the SSD endurance faster, and incurs more long-term cost than the tiering system. When the time factor is 1 <ref type="figure">(Figure 2(d)</ref>), the caching system incurs just a little bit more cost than the tiering system due to the same reason.</p><p>Overall, we observed six trends. (1) For read-intensive workloads, a larger PT value reduces long-term costs; for write-intensive workloads, a smaller ES value reduces long-term costs. <ref type="formula">(2)</ref> The HDD-only system has the least initial capital investment and lowest long-term dollar cost, but it has the lowest performance. <ref type="formula">(3)</ref> The SSDonly system has the highest initial capital investment, and can incur low and high long-term costs for read-intensive and write-intensive workloads, respectively; but it has the highest performance. (4) Tiering and caching systems have the benefits of incurring only medium initial capital investments, and can incur some long-term costs to a different degree depending on the workloads; both systems have medium performance. (5) The tiering and caching systems incur more long-term cost than Mylinear does due to data movement; but both systems achieve better performance than Mylinear does. (6) Different tiering and caching system configurations lead to variations in cost, which increases as the time factor increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Few have investigated the long-term costs of storage systems with SSDs. Some use simulation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, instead of empirical experiments. Some do not consider the SSD replacement cost in their total cost calculation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. Industry also discusses this, but detailed cost models that include TCOs are not publicly available <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Several have compared caching and tiering systems. MAID <ref type="bibr" target="#b0">[1]</ref> briefly discusses the pros and cons of caching and migration based policies for massive storage sys-tems. With the advent of Phase Change Memories (PCMs), Kim et al. <ref type="bibr" target="#b4">[5]</ref> evaluate PCMs for enterprise storage systems using case studies of caching and tiering approaches. However, there is no direct comparison study performed for the caching and tiering approaches from the perspective of total cost of ownership.</p><p>Our work is different in five aspects. (1) We collect real energy and power numbers from experiments. <ref type="formula">(2)</ref> We consider the SSD's endurance cost. (3) We scale the experiments to observe long-term effects. (4) We developed and discussed a cost model containing the total cost of ownership. (5) We built two realistic systems (i.e., tiering and caching) with similar strategies and environment to evaluate fairly the pros and cons of the caching and tiering based hybrid storage systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations and Future Work</head><p>Modeling storage systems' monetary costs is challenging. Our model has several limitations. We do not fully consider the following three aspects yet: computer hardware cost, air-conditioning cost, and labor cost. We also do not yet consider equipment financing cost with different interest rates. We simplify several conditions to facilitate easier understanding: (1) the hardware setup in a real data center may be more complex than ours; (2) the service cost may vary accordingly; and (3) the workloads in a real data center may be more complex than ours. It is our hope that this work helps others build more elaborate cost models in the future.</p><p>Caching and tiering systems share several design traits. Our caching system is fairly similar to the tiering one. Although both systems estimate the endurance metric by counting SSD reads and writes and the HDD startstop cycles, the endurance metric can be improved. Detailed access to the SSD internals (e.g., erasure cycle counts, FTL behavior) could improve the SSD's endurance model. The size ratios of the SSD vs. HDD in both systems affects throughput, energy and power, device endurance, and dollar cost. We are currently investigating that <ref type="bibr" target="#b6">[7]</ref>, especially where in large scale storage servers, a cache is much smaller than total capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We developed a device-mapper target for the Linux kernel that combines HDD and SSD together. Our system can use the SSD as either a cache or a primary storage for hot data. We built a cost model that also considers the lifetime cost of ownership: energy and power costs, replacement cost, and more. Our extensive evaluation results show that for some workloads, an SSD-only solution incurs the highest overall costs in the short term but much lower costs in the long run. We also observed that for some workloads, using the SSD as a cache had lower costs than when the SSD was used as primary hotdata storage; but other workloads completely reversed this trend. It is therefore important that future storage systems be evaluated across dimensions of lifetime cost, performance, as well as workloads. It is our hope that this work would encourage new research into more realistic long term cost models of storage systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data Management in Two Modes of Our System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>SSDCost with Time Factor 100, 000 Figure 2 :</head><label>0002</label><figDesc>Figure 2: Two Traces Replay Results. For Web-Search, we set ES to 1MB. For Online, we set MCML to 16 and PT to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>LIPA energy and power prices for commercial use as 
of May 2013, based on per KWh and per KW. "Egy" is Energy; 
"Pow" is Power. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>table . Cold</head><label>.</label><figDesc></figDesc><table>data migrates to the 
HDD and hot data eventually migrates to the SSD using ... ... 

N 
0 
1 
2 
3 

VBA 

Virtual Extents (VEs) 

Mapping Table 

PE Flags 
5 
... 

M 
... 

3 
... 

... ... 

VE 
0 

N 

Extra Extents 

LBA 

Logical Extents (LEs) 

M M+1 M+2 

... ... 
N N+1 ... 

HDD 
SSD 

... ... 

1 
0 

Promotion 
Demotion 

1 

Map BIO 

(a) Tiering System 

5 

... 
... 

... 

... ... 

0 

M 

K K+1 K+2 

... ... 

1 
0 

1 

Flags 

Cache Entry 

M 
... 
1 
0 

Extents 

... ... 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 : Trace Workloads Summary</head><label>2</label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massive Arrays of Idle Disks for Storage Archives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Colarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2002 ACM/IEEE conference on Supercomputing</title>
		<meeting>the 2002 ACM/IEEE conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enterprise Flash Drive Cost and Technology Projections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Floyer</surname></persName>
		</author>
		<ptr target="http://wikibon.org/wiki/v/EnterpriseFlashDriveCostandTechnologyProjections" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename><surname>Gartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Server Storage</surname></persName>
		</author>
		<imprint/>
	</monogr>
<note type="report_type">Technical report</note>
	<note>Gartner Group/Dataquest, 1999. www.gartner.com</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cost Effective Storage Using Extent Based Dynamic Tiering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pucha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Belluomini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX FAST</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating Phase Change Memory for Enterprise Storage Systems: A Study of Caching and Tiering Approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Dickey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on File and Storage Technologies</title>
		<meeting>the 12th USENIX Conference on File and Storage Technologies<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">HybridStore: A Cost-Efficient, High-Performance Storage System Combining SSDs and HDDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE MAS-COTS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">GreenDM: A Versatile Tiering Hybrid Drive for the TradeOff Evaluation of Performance, Energy, and Endurance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-05" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Stony Brook University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Power Consumption in Enterprise-Scale Backup Storage Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Greenan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth USENIX Conference on File and Storage Technologies (FAST &apos;12)</title>
		<meeting>the Tenth USENIX Conference on File and Storage Technologies (FAST &apos;12)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Migrating server storage to ssds: analysis of tradeoffs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rowstron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;09: Proceedings of the 4th ACM European conference on Computer systems</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="145" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The logstructured merge-tree (LSM-tree)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gawlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Inf</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="351" to="385" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Failure trends in a large disk drive population</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth USENIX Conference on File and Storage Technologies (FAST &apos;07)</title>
		<meeting>the Fifth USENIX Conference on File and Storage Technologies (FAST &apos;07)<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2007-02" />
			<biblScope unit="page" from="17" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SieveStore: A Highly-Selective, Ensemble-level Disk Cache for Cost-Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pritchett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thottethodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture, ISCA &apos;10</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture, ISCA &apos;10</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The economics of long-term digital storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">C</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">F</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erez</forename><surname>Storer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zadok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Memory of the World in the Digital age: Digitization and Preservation. United Nations Educational, Scientific and Cultural Organization (UNESCO)</title>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid Aggregates: Combining SSDs and HDDs in a Single Storage Pool</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="page" from="50" to="56" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tintri</forename><surname>Vmstore</surname></persName>
		</author>
		<ptr target="www.tintri.com/resources/videos/introduction-to-tintri/" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
