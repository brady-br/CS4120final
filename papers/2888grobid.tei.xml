<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:54+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Memory Performance at Reduced CPU Clock Speeds: An Analysis of Current x86 64 Processors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schöne</surname></persName>
							<email>robert.schoene@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hackenberg</surname></persName>
							<email>daniel.hackenberg@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Molka</surname></persName>
							<email>daniel.molka@tu-dresden.de</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Information Services and High Performance Computing (ZIH)</orgName>
								<orgName type="institution">Technische Universität Dresden</orgName>
								<address>
									<postCode>01062</postCode>
									<settlement>Dresden</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Memory Performance at Reduced CPU Clock Speeds: An Analysis of Current x86 64 Processors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Reducing CPU frequency and voltage is a well-known approach to reduce the energy consumption of memory-bound applications. This is based on the conception that main memory performance sees little or no degradation at reduced processor clock speeds, while power consumption decreases significantly. We study this effect in detail on the latest generation of x86 64 compute nodes. Our results show that memory and last level cache bandwidths at reduced clock speeds strongly depend on the processor microarchitecture. For example, while an Intel Westmere-EP processor achieves 95 % of the peak main memory bandwidth at the lowest processor frequency, the bandwidth decreases to only 60 % on the latest Sandy Bridge-EP platform. Increased efficiency of memory-bound applications may also be achieved with concurrency throttling, i.e. reducing the number of active cores per socket. We therefore complete our study with a detailed analysis of memory bandwidth scaling at different concurrency levels on our test systems. Our results-both qualitative developments and absolute bandwidth numbers-are valuable for scientists in the areas of computer architecture, performance and power analysis and modeling as well as application developers seeking to optimize their codes on current x86 64 systems.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Many research efforts in the area of energy efficient computing make use of dynamic voltage and frequency scaling (DVFS) to improve energy efficiency <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b8">[8]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The incentive is often that a system's main memory bandwidth is unaffected by reduced clock speeds, while the power consumption decreases significantly. Therefore, DVFS is a common optimization method when dealing with memory-bound applications.</p><p>This paper presents a study of this method on the latest generation of x86 64 systems from Intel (Sandy Bridge) and AMD (Interlagos) as well as the previous generation processors Intel Westmere/Nehalem and AMD Magny-Cours/Istanbul. We use synthetic benchmarks that are capable of maxing out the memory bandwidth for each level of the memory hierarchy. We focus on read accesses to main memory and also take the last level cache performance into account. Our analysis reveals that main memory as well as last level cache performance at reduced clock speeds strongly depends on the processor microarchitecture.</p><p>Besides DVFS, another common approach is to reduce the number of concurrent tasks in a parallel program to reduce power consumption (i.e., concurrency throttling -CT). This can be very effective, as a single thread may be able to fully utilize the memory controller bandwidth. We therefore extend our analysis to also cover CT on the latest x86 64 systems. Again, our results prove the importance of taking the individual properties of the target microarchitecture into account for any concurrency optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>DVFS related runtime and energy performance data for three generations of AMD Opteron processors has been published by <ref type="bibr">LeSueur et al. [5]</ref>. They provide a small section regarding the correlation of memory frequency and processor frequency. Full memory frequency at the lowest processor frequency is only supported by their newest system, an AMD Opteron 2384 (Shanghai). While they sketch several DVFS related performance issues, we focus on a single important attribute and describe it in detail for newer architectures and also take concurrency scaling into account.</p><p>Choi et al. <ref type="bibr" target="#b0">[1]</ref> divide the data access into external transfer and on die transfer, with the latter depending on the CPU frequency. <ref type="bibr">Liang et al.</ref> [6] describe a cache miss based prediction model for energy and performance degradation, while Spiliopoulos et al. <ref type="bibr" target="#b8">[8]</ref> describe a slack time based model. Neither of them takes the architecture dependent correlation of CPU frequency and main memory bandwidth sufficiently into account.</p><p>Curtis-Maury et al. <ref type="bibr" target="#b1">[2]</ref> analyze how decreased concurrency can improve energy efficiency of non-scaling regions. Their main reason from an architectural perspective to do concurrency throttling is that main memory bandwidth can be saturated at reduced concurrency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. TEST SYSTEMS</head><p>We perform our studies on current and last generation processors from Intel and AMD. The test systems are detailed in <ref type="table" target="#tab_0">Table I</ref>. All processors have a last level cache and an integrated memory controller on each die, being competitively shared by multiple cores. AMD processors as well as previous generation Intel processors (Nehalem-EP/Westmere-EP) use a crossbar to connect the cores with the last level cache and the memory controller. In contrast, current Intel processors based on the Sandy Bridge microarchitecture have a local last level cache slice per core and use a ring bus to connect all cores and cache slices. All processors support dynamic voltage and frequency scaling to adjust performance and energy consumption to the workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BENCHMARKS</head><p>We use an Open Source microbenchmark <ref type="bibr" target="#b7">[7]</ref>, <ref type="bibr" target="#b2">[3]</ref> to perform a low-level analysis of each system's cache and memory performance. Highly optimized assembler routines and time stamp counter based timers enable precise performance measurements of data accesses in different levels of the memory hierarchy of 64 Bit x86 processors <ref type="bibr" target="#b7">[7]</ref>. The benchmark is parallelized using pthreads, the individual threads are pinned to single cores, and all threads use only local memory. We use this benchmark to determine the read bandwidth of last level cache and main memory accesses for different processor frequencies and various numbers of threads. 1 4 sockets, 2 dies per socket, 8 cores per die 2 C6 disabled, therefore no MAX TURBO available <ref type="bibr" target="#b2">3</ref> The Magny-Cours main memory bandwidth is currently not fully understood, confirmatory measurement are underway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BANDWIDTH AT MAXIMUM THREAD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCURRENCY</head><p>We use our microbenchmarks to determine the bandwidth depending on the core frequency while using all cores of each processor (maximum thread concurrency). <ref type="figure">Figure 1a</ref> shows that main memory bandwidth is frequency independent only on specific Intel architectures. While this group includes Intel Nehalem, Westmere and Sandy Bridge-HE, the second group (frequency dependent) consists of the AMD systems and Intel Sandy Bridge-EP. The consistent behavior of the AMD processors can be attributed to their similar northbridge design. <ref type="bibr" target="#b2">3</ref> Intel processors have undergone more significant redesigns. While the new ring-bus based uncore still provides high bandwidths at lower frequencies for the desktop processor Sandy Bridge-HE, the results for the server equivalent Sandy Bridge-EP are unexpected.</p><p>Regarding the L3 cache bandwidth depicted in <ref type="figure">Fig- ure 1b</ref>, three categories can be distinguished: frequency independent, converging, and linearly scaling. Only on the Intel Westmere-EP system, the L3 cache bandwidth is independent of the processor clock speed. Although the uncore architecture is very similar to Nehalem-EP, the six-core design allows to utilize the L3 cache fully even at lower frequencies. With only four cores, Nehalem-EP cannot sustain the L3 performance at lower clock speeds. AMD processors show a similar con- verging L3 pattern, which is again due to the largely unchanged northbridge design of all generations. For Sandy Bridge processors, the L3 bandwidth scales perfectly linear with the processor frequency. This is not surprising, as there is no dedicated uncore frequency. Instead, the cores, the L3 cache slices, and the interconnection ring are in the same frequency domain. <ref type="figure">Figure 2</ref> highlights the recent development of Intel processors. It showcases that memory bandwidth at reduced processor clock speeds may not be deduced from experiences with previous systems, but instead needs to be evaluated for each new processor generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. BANDWIDTH AT VARIABLE CONCURRENCY AND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FREQUENCY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Intel Sandy Bridge-HE</head><p>In addition to the very good main memory bandwidth at low clock frequencies, Sandy Bridge-HE also performs exceptionally well in low concurrency scenarios. <ref type="figure" target="#fig_1">Figure 3a</ref> shows that using only one of the four cores is sufficient to achieve almost 90 percent of the maximum main memory bandwidth (HyperThreading enabled, i.e. two threads on one core). The remaining three cores may be power gated to tap a significant power saving potential. With two or more active cores, the maximum bandwidth can be achieved even at reduced clock speeds. In terms of absolute memory bandwidth, 19.9 GB/s is well above 90% of the theoretical peak bandwidth of two DDR3-1333 memory channels.</p><p>The L3 cache of Sandy Bridge-HE scales linearly with the processor frequency (see <ref type="figure" target="#fig_1">Figure 3b)</ref>. We measure a 2.1 GB/s bandwidth bonus for each frequency increase of 200 MHz. Unlike main memory bandwidth, the cache bandwidth also profits from Turbo Boost. L3 cache bandwidth also scales well with the number of cores. At base frequency, a per-core bandwidth of about 35.3 GB/s can be achieved. The Turbo Boost bonus depends on the number of cores, as does the actual Turbo Boost processor frequency. Using two threads per core (HyperThreading) results in a small but measurable bandwidth increase of about 5 %.</p><p>We have repeated all measurements on a newer Ivy Bridge test system (Core i5-3470) with very similar results. Therefore, the performance numbers we report for Sandy Bridge-HE are representative for Ivy Bridge as well.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intel Sandy Bridge-EP</head><p>The pattern for main memory bandwidth depicted in <ref type="figure" target="#fig_2">Figure 4</ref> differs noticeably from the Sandy Bridge-HE platform. Five (instead of two) active cores are required for full main memory bandwidth. Fewer cores can not fully utilize the bandwidth because of their limited number of open requests. While for a smaller number of active cores HyperThreading provides a performance benefit, it degrades the bandwidth for higher core counts. Furthermore, main memory bandwidth strongly depends on the processor frequency, thus Turbo is needed for maximum performance. If the data is not available in the local L3 cache, a request to the remote socket via QPI is performed to check for updated data in its caches. This coherence mechanism likely scales with the core/ring frequency and therefore limits the memory request rate at reduced frequency.</p><p>The resemblance of <ref type="figure" target="#fig_1">Figure 3b</ref> and <ref type="figure" target="#fig_3">Figure 5</ref> shows that the L3 cache bandwidth on Sandy Bridge-EP scales similarly to the Sandy Bridge-HE processor. The additional performance due to HyperThreading is about 10 %, compared to 5 % for Sandy Bridge-HE. However, the bandwidth per core is approximately 10 % lower on Sandy Bridge-EP. This could be due to the longer ring bus in the uncore and the increased number of L3 slices, that results in a higher percentage of non-local accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. AMD Interlagos</head><p>The results obtained from the Interlagos platform have to be considered with AMDs northbridge architecture in mind. All cores are connected to the L3 cache via the System Request Interface. This interface also connects to the crossbar, which itself connects further to the memory controller. These and other components form the northbridge. It is clocked with an individual frequency that corresponds to the northbridge p-State. On our test system, this frequency is fixed at 2 GHz. <ref type="figure">Figure 6a</ref> depicts the main memory performance of a single AMD Interlagos processor die. The bandwidth scales to three active modules. For lower module counts using two cores per modules increases the achieved bandwidth per module. This can be explained by the per core Load-Store units which enable more open requests to cover the access latency when using two cores. For three and four active modules, two active cores can reduce the maximal bandwidth due to ressource contention caused by the concurrent accesses. <ref type="figure">Figure 6b</ref> shows the L3 cache bandwidth on Interlagos. Although a core-frequency independent monolithic block of L3 cache should be able to provide a high bandwidth to a single core even at low frequencies, this is clearly not the case. We see two bottlenecks that limit the L3 cache bandwidth. The first one depends on the core frequency and is therefore located neither in the L3 cache nor the SRI. This bottleneck can not be explained with per core Load-Store buffers, as using two cores per module does not increase the bandwidth. Apparently the number of requests per module to the SRI is limited. The second bottleneck is located in the SRI/L3 construct, which limits the number of requests a single module can send to the L3 cache. Therefore, the bandwidth also scales with the number of accessing modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION AND FUTURE WORK</head><p>In this paper we study the correlation of processor frequency and main memory as well as last level cache performance in detail. Our results show that this correlation strongly depends on the processor microarchitecture. Relevant qualitative differences have been discovered between AMD and Intel x86 64 processors, between consecutive generations of Intel processors, and even between the server and desktop parts of one processor generation. Some systems provide full bandwidth at the lowest CPU frequency. However, decreasing CPU frequencies also degrades memory performance in many cases. Similarly, there is also no general answer regarding the effect of concurrency throttling. Based on our findings we make the case that all DVFS/CT based performance or energy efficiency efforts need to take the fundamental properties of the targeted microarchitecture into account. For example, results that have been obtained on a Westmere platform are likely not applicable to a Sandy Bridge system.</p><p>The results presented in this paper can be used to save energy on under-provisioned systems. Scheduling multiple task per NUMA node before populating the next one allows other nodes to use deeper sleep states. This could be implemented similar to sched_mc_power_savings in Linux. However, our study focuses on the performance effect of frequency scaling and concurrency throttling, leaving further measurements of power and energy for future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Figure 1. Scaling of shared resources with maximum thread concurrency on recent AMD and Intel processors. Main memory and L3 cache bandwidth are normalized to the peak at maximum frequency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Sandy Bridge-HE main memory and last level cache bandwidth at different processor clock speeds and core counts. (0-1) refers to two threads on one core (HyperThreading) while (0,2) refers to two cores with one thread each (no HyperThreading).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Main memory bandwidth on Sandy Bridge-EP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. L3 cache bandwidth on Sandy Bridge-EP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table I HARDWARE CONFIGURATION OF TEST SYSTEMS</head><label>I</label><figDesc></figDesc><table>Vendor 
Intel 
AMD 
Processor 
Xeon X5560 
Xeon X5670 
Core i7-2600K 
Xeon E5-2670 
Opteron 2435 
Opteron 6168 
Opteron 6274 
Codename 
Nehalem-EP 
Westmere-EP 
Sandy Bridge-HE 
Sandy Bridge-EP 
Istanbul 
Magny-Cours 
Interlagos 
Cores 
2x 4 
2x 6 
4 
2x 8 
2x 6 
2x 2x 6 
4x 2x 8 1 
Base clock 
2.8 GHz 
2.933 GHz 
3.4 GHz 
2.6 GHz 
2.6 GHz 
1.9 GHz 
2.2 GHz 
max. turbo clock 
3.2 GHz 
3.333 GHz 
3.8 GHz 
3.3 GHz 
-
-
2.5 GHz 2 
L3 cache per die 
8 MiB 
12 MiB 
8 MiB 
20 MiB 
6 MiB 
6 MiB 
8 MiB 
IMC chan. per die 
3x RDDR3 
3x RDDR3 
2x DDR3 
4x RDDR3 
2x RDDR2 
2x RDDR3 
2x RDDR3 
DIMMs per die 
3x 2 GiB 
3x 2 GiB 
4x 2 GiB 
4x 4 GiB 
4x 2 GiB 
4x 4 GiB 
2x 4 GiB 
Memory type 
PC3-10600R 
PC3L-10600R 
PC3-10600 
PC3-12800R 
PC2-5300R 
PC3-10600R 
PC3-12800R 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic voltage and frequency scaling based on workload decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 International Symposium on</title>
		<meeting>the 2004 International Symposium on</meeting>
		<imprint>
			<date type="published" when="2004-08" />
			<biblScope unit="page" from="174" to="179" />
		</imprint>
	</monogr>
	<note>Low Power Electronics and Design</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prediction-based power-performance adaptation of multithreaded scientific codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Curtis-Maury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Blagojevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Antonopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Nikolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1396" to="1410" />
			<date type="published" when="2008-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing cache architectures and coherency protocols on x86-64 multicore SMP systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO 42: Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A power-aware run-time system for high-performance computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005</title>
		<meeting>the 2005</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">ACM/IEEE conference on Supercomputing, SC &apos;05</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dynamic voltage and frequency scaling: the laws of diminishing returns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Sueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 international conference on Power aware computing and systems, HotPower&apos;10</title>
		<meeting>the 2010 international conference on Power aware computing and systems, HotPower&apos;10<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory-aware dynamic voltage and frequency prediction for portable devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Embedded and Real-Time Computing Systems and Applications, 2008. RTCSA &apos;08. 14th IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2008-08" />
			<biblScope unit="page" from="229" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory performance and cache coherency effects on an Intel Nehalem multiprocessor system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schöne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT &apos;09: Proceedings of the 2009 18th International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Green governors: A framework for continuously adaptive dvfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Spiliopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Keramidas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Green Computing Conference and Workshops (IGCC), 2011 International</title>
		<imprint>
			<date type="published" when="2011-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
