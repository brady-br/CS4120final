<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-10-01T14:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DRN: A Deep Reinforcement Learning Framework for News Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanjie</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Beijing</settlement>
									<country>China §</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xie</surname></persName>
							<email>xingx@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhui</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Beijing</settlement>
									<country>China §</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Pennsylvania State University †</orgName>
								<orgName type="institution" key="instit2">Microsoft Research Asia § University Park</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DRN: A Deep Reinforcement Learning Framework for News Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3178876.3185994</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reinforcement learning</term>
					<term>Deep Q-Learning</term>
					<term>News recommendation</term>
				</keywords>
			</textClass>
			<abstract>
				<p>In this paper, we propose a novel Deep Reinforcement Learning framework for news recommendation. Online personalized news recommendation is a highly challenging problem due to the dynamic nature of news features and user preferences. Although some online recommendation models have been proposed to address the dynamic nature of news recommendation, these methods have three major issues. First, they only try to model current reward (e.g., Click Through Rate). Second, very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation. Third, these methods tend to keep recommending similar news to users, which may cause users to get bored. Therefore, to address the aforementioned challenges, we propose a Deep Q-Learning based recommendation framework, which can model future reward explicitly. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, an effective exploration strategy is incorporated to find new attractive news for users. Extensive experiments are conducted on the offline dataset and online production environment of a commercial news recommendation application and have shown the superior performance of our methods.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>: Deep Reinforcement Recommendation System</head><p>Our deep reinforcement recommender system can be shown as <ref type="figure" target="#fig_2">Figure 2</ref>. We follow the common terminologies in reinforcement learning <ref type="bibr" target="#b37">[37]</ref> to describe the system. In our system, user pool and news pool make up the environment, and our recommendation algorithms play the role of agent. The state is defined as feature representation for users and action is defined as feature representation for news. Each time when a user requests for news, a state representation (i.e., features of users) and a set of action representations (i.e., features of news candidates) are passed to the agent. The agent will select the best action (i.e., recommending a list of news to user) and fetch user feedback as reward. Specifically, the reward is composed of click labels and estimation of user activeness. All these recommendation and feedback log will be stored in the memory of the agent. Every one hour, the agent will use the log in the memory to update its recommendation algorithm.</p><p>Our contribution can be summarized as below:</p><p>• We propose a reinforcement learning framework to do online personalized news recommendation. Unlike previous studies, this framework applies a DQN structure and can take care of both immediate and future reward. Although we focus on news recommendation, our framework can be generalized to many other recommendation problems.</p><p>• We consider user activeness to help improve recommendation accuracy, which can provide extra information than simply using user click labels.</p><p>• A more effective exploration method Dueling Bandit Gradient Descent is applied, which avoids the recommendation accuracy drop induced by classical exploration methods, e.g., ϵ-greedy and Upper Confidence Bound.</p><p>• Our system has been deployed online in a commercial news recommendation application. Extensive offline and online experiments have shown the superior performance of our methods. The rest of the paper is organized as follows. Related work is discussed in Section 2. Then, in Section 3 we present the problem definitions. Our method is introduced in Section 4. After that, the experimental results are shown in Section 5. Finally, brief conclusions are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 News recommendation algorithms</head><p>Recommender systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> have been investigated extensively because of its direct connection to profits of products. Recently, due to the explosive grow of online content, more and more attention has been drawn to a special application of recommendation -online personalized news recommendation. Conventional news recommendation methods can be divided into three categories. Content-based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33]</ref> will maintain news term frequency features (e.g., TF-IDF) and user profiles (based on historical news). Then, recommender will select news that is more similar to user profile. In contrast, collaborative filtering methods <ref type="bibr" target="#b11">[11]</ref> usually make rating prediction utilizing the past ratings of current user or similar users <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b34">34]</ref>, or the combination of these two <ref type="bibr" target="#b11">[11]</ref>. To combine the advantages of the former two groups of methods, hybrid methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref> are further proposed to improve the user profile modeling. Recently, as an extension and integration of previous methods, deep learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b52">52]</ref> have shown much superior performance than previous three categories of models due to its capability of modeling complex user-item relationship. Different from the effort for modeling the complex interaction between user and item, our algorithm focuses on dealing with the dynamic nature of online news recommendation, and modeling of future reward. However, these feature construction and user-item modeling techniques can be easily integrated into our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reinforcement learning in recommendation</head><p>2.2.1 Contextual Multi-Armed Bandit models. A group of work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b50">50]</ref> begin to formulate the problem as a Contextual Multi-Armed Bandit (MAB) problem, where the context contains user and item features. <ref type="bibr" target="#b23">[23]</ref> assumes the expected reward is a linear function of the context. <ref type="bibr" target="#b39">[39]</ref> uses an ensemble of bandits to improve the performance, <ref type="bibr" target="#b40">[40]</ref> proposes a parameter-free model, and <ref type="bibr" target="#b50">[50]</ref> addresses the time-varying interest of users. Recently, some people try to combine bandit with clustering based collaborative filtering <ref type="bibr" target="#b14">[14]</ref>, and matrix factorization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b51">51]</ref>, in order to model more complex user and item relationship, and utilize the social network relationship in determining the reward function. However, our model is significantly different from these works, because by applying Markov Decision Process, our model is able to explicitly model future rewards. This will benefit the recommendation accuracy significantly in the long run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Markov Decision Process models.</head><p>There are also some literature trying to use Markov Decision Process to model the recommendation process. In contrast to MAB-based methods, MDP-based methods can not only capture the reward of current iteration, but also the potential reward in the future iterations. <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38]</ref> try to model the item or n-gram of items as state (or observation in Partially Observed MDP), and the transition between items (recommendation for the next item) as the action. However, this can not scale to large dataset, because when the item candidate set becomes larger, the size of state space will grow exponentially. In addition, the state transitions data is usually very sparse, and can only be used to learn the model parameters corresponding to certain state transitions. Therefore, the model is really hard to learn. Different from the literature, we propose a MDP framework with continuous state and action representation, which enables the system to scale up and the effective learning of model parameters by using all the state, action, reward tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM DEFINITION</head><p>We define our problem as follows:</p><p>When a user u sends a news request to the recommendation agent G at time t, given a candidate set I of news, our algorithm is going to select a list L of top-k appropriate news for this user. The notations used in this paper are summarized in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHOD</head><p>Personalized news recommendation has attracted a lot of attention in recent years <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b45">45]</ref>. The current methods can be generally categorized as content based methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b33">33]</ref>, collaborative filtering based methods <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b34">34]</ref>, and hybrid methods <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b25">25]</ref>. Recently, many deep learning models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b52">52]</ref> are further proposed in order to model more complex user item interactions. News recommendation problem becomes even more challenging when it happens in an online scenario due to three reasons. First, online learning are needed due to the highly dynamic nature of news characteristics and user preference. Second, only using click / no click labels will not capture users' full feedback towards news. Third, traditional recommendation methods tend to recommend similar items and will narrow down user's reading choices. This will make users bored and lead to decrease of user satisfaction in the long run.</p><p>To address these three challenges, we propose a DQN-based Deep Reinforcement Learning framework to do online personalized news recommendation. Specifically, we use a continuous state feature representation of users and continuous action feature representation of items as the input to a multi-layer Deep Q-Network to predict the potential reward (e.g., whether user will click on this piece of news). First, this framework can deal with the highly dynamic nature of news recommendation due to the online update of DQN. Meanwhile, DQN is different from common online methods, because of its capability to speculate future interaction between user and news. Second, we propose to combine user activeness (i.e., how frequent a user returns to the App after one recommendation) and click labels as the feedback from users. Third, we propose to apply Dueling Bandit Gradient Descent exploration strategy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b49">49]</ref> to our algorithm which can both improve recommendation diversity and avoid the harm to recommendation accuracy induced by classical exploration strategies like ϵ-greedy <ref type="bibr" target="#b31">[31]</ref> and Upper Confidence Bound <ref type="bibr" target="#b23">[23]</ref>.</p><p>Our method is significantly different from the MAB group of methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b50">50]</ref> due to its explicit modeling of future rewards, and different from previous MDP methods <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b38">38]</ref> using user log due to its continuous representation of state and action, and the capability to scale to large systems.</p><p>In this section, we will first introduce the model framework in Section 4.1. Then, we will illustrate the feature construction in Section 4.2 and the deep reinforcement learning model in Section 4.3. After that, the design of user activeness consideration is discussed in Section 4.4. Finally, the exploration module is introduced in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model framework</head><p>As shown in <ref type="figure">Figure 3</ref>, our model is composed of offline part and online part. In offline stage, four kinds of features (will be discussed in Section 4.2) are extracted from news and users. A multi-layer Deep Q-Network is used to predict the reward (i.e., a combination of user-news click label and user activeness) from these four kinds of features. This network is trained using the offline user-news click logs. Then, during the online learning part, our recommendation agent G will interact with users and update the network in the following way:</p><p>(1) PUSH: In each timestamp (t 1 , t 2 , t 3 , t 4 , t 5 , ...), when a user sends a news request to the system, the recommendation agent G will take the feature representation of the current user and news candidates as input, and generate a top-k list of news to recommend L. L is generated by combining the exploitation of current model (will be discussed in Section 4.3) and exploration of novel items (will be discussed in Section 4.5). (2) FEEDBACK: User u who has received recommended news L will give their feedback B by his clicks on this set of news. (3) MINOR UPDATE: After each timestamp (e.g., after timestamp t 1 ), with the feature representation of the previous user u and news list L, and the feedback B, agent G will update the model by comparing the recommendation performance of exploitation network Q and exploration network˜Qnetwork˜ network˜Q (will be discussed in Section 4.5). If˜QIf˜ If˜Q gives better recommendation result, the current network will be updated towards˜Q towards˜ towards˜Q. Otherwise, Q will be kept unchanged. Minor update can happen after every recommendation impression happens. (4) MAJOR UPDATE: After certain period of time T R (e.g., after timestamp t 3 ), agent G will use the user feedback B and user activeness stored in the memory to update the network Q.</p><p>Here, we use the experience replay technique <ref type="bibr" target="#b31">[31]</ref> to update the network. Specifically, agent G maintains a memory with recent historical click and user activeness records. When each update happens, agent G will sample a batch of records to update the model. Major update usually happens after a certain time interval, like one hour, during which thousands of recommendation impressions are conducted and their feedbacks are collected. (5) Repeat step (1)-(4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Feature construction</head><p>In order to predict whether user will click one specific piece of news or not, we construct four categories of features:</p><p>• News features includes 417 dimension one hot features that describe whether certain property appears in this piece of news, including headline, provider, ranking, entity name, category, topic category, and click counts in last 1 hour, 6 hours, 24 hours, 1 week, and 1 year respectively.</p><p>• User features mainly describes the features (i.e., headline, provider, ranking, entity name, category, and topic category) of the news that the user clicked in 1 hour, 6 hours, 24 hours, 1 week, and 1 year respectively. There is also a total click count for each time granularity. Therefore, there will be totally 413 × 5 = 2065 dimensions.</p><p>• User news features. These 25-dimensional features describe the interaction between user and one certain piece of news, i.e., the frequency for the entity (also category, topic category and provider) to appear in the history of the user's readings.</p><p>• Context features. These 32-dimensional features describe the context when a news request happens, including time, weekday, and the freshness of the news (the gap between request time and news publish time). In order to focus on the analysis of the reinforcement learning recommendation framework, we did not try to add more features, e.g., textual features <ref type="bibr" target="#b45">[45]</ref>. But they can be easily integrated into our framework for better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Deep Reinforcement Recommendation</head><p>Considering the previous mentioned dynamic feature of news recommendation and the need to estimate future reward, we apply a Deep Q-Network (DQN) <ref type="bibr" target="#b31">[31]</ref> to model the probability that one user may click on one specific piece of news. Under the setting of reinforcement learning, the probability for a user to click on a piece of news (and future recommended news) is essentially the reward that our agent can get. Therefore, we can model the total reward as Equation 1.</p><formula xml:id="formula_0">y s,a = Q(s, a) = r immediat e + γ r f utur e</formula><p>(1) where state s is represented by context features and user features, action a is represented by news features and user-news interaction features, r immediat e represents the rewards (e.g., whether user click on this piece of news) for current situation, and r f utur e represents the agent's projection of future rewards. γ is a discount factor to balance the relative importance of immediate rewards and future rewards. Specifically, given s as the current state, we use the DDQN <ref type="bibr" target="#b41">[41]</ref> target to predict the total reward by taking action a at timestamp t as in Equation 2.</p><formula xml:id="formula_1">y s,a,t = r a,t +1 + γ Q(s a,t +1 , arg max a ′ Q(s a,t +1 , a ′ ; W t ); W ′ t ) (2)</formula><p>where r a,t +1 represents the immediate reward by taking action a (the subscript t + 1 is because the reward is always delayed 1  <ref type="figure">Figure 3</ref>: Model framework timeslot than the action). Here, W t and W ′ t are two different sets of parameters of the DQN. In this formulation, our agent G will speculate the next state s a,t +1 , given action a is selected. Based on this, given a candidate set of actions {a ′ }, the action a ′ that gives the maximum future reward is selected according to parameter W t . After this, the estimated future reward given state s a,t +1 is calculated based on W ′ t . Every a few iterations, W t and W ′ t will be switched. This strategy has been proven to eliminate the overoptimistic value estimates of Q <ref type="bibr" target="#b41">[41]</ref>. Through this process, DQN will be able to make decision considering both immediate and future situations.</p><p>As shown in <ref type="figure">Figure 4</ref>, we feed the four categories of features into the network. User features and Context features are used as state features, while User news features and Context features are used as action features. On one hand, the reward for taking action a at certain state s is closely related to all the features. On the other hand, the reward that determined by the characteristics of the user himself (e.g., whether this user is active, whether this user has read enough news today) is more impacted by the status of the user and the context only. Based on this observation, like <ref type="bibr" target="#b47">[47]</ref>, we divide the Q-function into value function V (s) and advantage function A(s, a), where V (s) is only determined by the state features, and A(s, a) is determined by both the state features and the action features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">User Activeness</head><p>Traditional recommender systems only focus on optimizing CTRlike metrics (i.e., only utilizing click / no click labels), which only depicts part of the feedback information from users. The performance of recommendation might also influence whether users want to use the application again, i.e., better recommendation will increase the frequency for users to interact with the application. Therefore, the change of user activeness should also be considered properly. Users request for news in a non-uniform pattern. Users usually read news for a short period (e.g., 30 minutes), during which they will request or click news with high frequency. Then they might leave the application and return to the application when they want to read more news after several hours. A user return happens when a user requests for news (users will always request for news before they click on news, therefore, user click is also implicitly considered).</p><p>We use survival models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">30]</ref> to model user return and user activeness. Survival analysis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">30]</ref> has been applied in the field of estimating user return time <ref type="bibr" target="#b20">[20]</ref>. Suppose T is the time until next event (i.e., user return) happens, then the hazard function (i.e., instantaneous rate for the event to happen) can be defined as Equation 3 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30]</ref> </p><formula xml:id="formula_2">λ(t) = lim dt →0 Pr {t ≤ T &lt; t + dt |T ≥ t } dt<label>(3)</label></formula><p>Then the probability for the event to happen after t can be defined as Equation 4 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30]</ref> </p><formula xml:id="formula_3">S(t) = e − ∫ t 0 λ(x )dx<label>(4)</label></formula><p>and the expected life span T 0 can be calculated as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30]</ref> </p><formula xml:id="formula_4">T 0 = ∫ ∞ 0 S(t)dt<label>(5)</label></formula><p>In our problem, we simply set λ(t) = λ 0 , which means each user has a constant probability to return. Every time we detect a return of user, we will set S(t) = S(t) + S a for this particular user. The user activeness score will not exceed 1. For instance, as shown in <ref type="figure">Figure 5</ref>, user activeness for this specific user starts to decay from S 0 at time 0. At timestamp t 1 , the user returns and this results in a S a increase in the user activeness. Then, the user activeness continues to decay after t 1 . Similar things happen at t 2 , t 3 , t 4 and t 5 . Note that, although this user has a relatively high request frequency during t 4 to t 9 , the maximum user activeness is truncated to 1.</p><p>The parameters S 0 , S a , λ 0 , T 0 are determined according to the real user pattern in our dataset. S 0 is set to 0.5 to represent the random initial state of a user (i.e., he or she can be either active or inactive). We can observe the histogram of the time interval between every two consecutive requests of users as shown in <ref type="figure" target="#fig_4">Fig- ure 6</ref>. We observe that besides reading news multiple times in a day, people usually return to the application on a daily regular basis. So we set T 0 to 24 hours. The decaying parameter λ 0 is set to 1.2 × 10 −5 second −1 according to Equation 4 and Equation 5. In addition, the user activeness increase S a for each click is set to 0.32 to make sure user will return to the initial state after one daily basis request, i.e.,</p><formula xml:id="formula_5">S 0 e −λ 0 T 0 + S a = S 0 .</formula><p>The click / no click label r cl ick and the user activeness r act ive are combined as in Equation 6. r t ot al = r cl ick + βr act ive</p><p>Although we use survival models here to estimate the user activeness, other alternatives like Poisson point process <ref type="bibr" target="#b13">[13]</ref> can also be applied and should serve similar function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Explore</head><p>The most straightforward strategies to do exploration in reinforcement learning are ϵ-greedy <ref type="bibr" target="#b31">[31]</ref> and UCB <ref type="bibr" target="#b23">[23]</ref>. ϵ-greedy will randomly recommend new items with a probability of ϵ, while UCB will pick items that have not been explored for many times (because these items may have larger variance). It is evident that these trivial exploration techniques will harm the recommendation performance in a short period. Therefore, rather than doing random exploration, we apply a Dueling Bandit Gradient Descent algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b49">49]</ref> to do the exploration. Intuitively, as shown in <ref type="figure" target="#fig_5">Figure 7</ref>, the agent G is going to generate a recommendation list L using the current  </p><formula xml:id="formula_7">Q. ∆W = α · rand(−1, 1) · W (7)</formula><p>where α is the explore coefficient, and rand(−1, 1) is a random number between -1 and 1. Then, the agent G will do a probabilistic interleave <ref type="bibr" target="#b15">[16]</ref> to generate the merged recommendation listˆLlistˆ listˆL using L and˜Land˜ and˜L. To determine the item for each position in the recommendation listˆLlistˆ listˆL, the probabilistic interleave approach basically will first randomly select between list L and˜Land˜ and˜L. Suppose L is selected, then an item i from L will be put intô L with a probability determined by its ranking in L (items with top rankings will be selected with higher probability). Then, listˆLlistˆ listˆL will be recommended to user u and agent G will obtain the feedback B. If the items recommended by the explore network˜Qnetwork˜ network˜Q receive a better feedback, the agent G will update the network Q towards˜Qtowards˜ towards˜Q, with the parameters of the network being updated as Equation 8</p><formula xml:id="formula_8">W ′ = W + η ˜ W.<label>(8)</label></formula><p>Otherwise, the agent G will keep network Q unchanged. Through this kind of exploration, the agent can do more effective exploration without losing too much recommendation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT 5.1 Dataset</head><p>We conduct experiment on a sampled offline dataset collected from a commercial news recommendation application and deploy our system online to the App for one month. Each recommendation algorithm will give out its recommendation when a news request arrives and user feedback will be recorded (click or not). The basic statistics for the sampled data is as in <ref type="table" target="#tab_2">Table 2</ref>. In the first offline stage, the training data and testing data are separated by time order (the last two weeks are used as testing data), to enable the online models to learn the sequential information between different sessions better. During the second online deploying stage, we use the offline data to pre-train the model, and run all the compared methods in the real production environment. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, the dataset is very skewed. The number of requests for each user follows a long tail distribution and most users only request news for less than 500 times. The number of times each news are pushed also follow a long tail distribution and most news are pushed to user less than 200 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation measures</head><p>• CTR. <ref type="bibr" target="#b10">[10]</ref> Click through rate is calculated as Equation 9. </p><formula xml:id="formula_9">DCG(f ) = n r =1 y f r D(r )<label>(11)</label></formula><p>with</p><formula xml:id="formula_10">D(r ) = 1 loд(1 + r )<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experiment setting</head><p>In our experiment, the parameters are determined by grid search of parameter space to find the ones with best CTR. The detailed settings are shown in <ref type="table" target="#tab_3">Table 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Compared methods</head><p>Variations of our model. Our basic model is named as "DN ", which uses a dueling-structure <ref type="bibr" target="#b47">[47]</ref> Double Deep Q-network <ref type="bibr" target="#b41">[41]</ref> without considering future reward. Then, by adding future reward into consideration, this becomes "DDQN ". After that, we add more components to "DDQN ". "U " stands for user activeness, "EG" stands for ϵ-greedy, and "DBGD" stands for Dueling Bandit Gradient Descent. Baseline algorithms. We compared our algorithms with following five baseline methods. All these five methods will conduct online update during the testing stage. Some state-of-art methods can not be applied due to their inapplicability to our problem, like <ref type="bibr" target="#b43">[43]</ref> (user graph and item graph is oversized and can not be updated incrementally), <ref type="bibr" target="#b45">[45]</ref> (similar with W&amp;D when textual features are removed), and <ref type="bibr" target="#b48">[48]</ref> (user return is not applicable to experience replay update).</p><p>• LR. Logistic Regression is widely used in industry as baseline methods due to its easy implementation and high efficiency.</p><p>It takes all the four categories of features as input. It is implemented using Keras <ref type="bibr" target="#b9">[9]</ref>.</p><p>• FM <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b34">34]</ref>. For all compared algorithms, the recommendation list is generated by selecting the items with top-k estimated potential reward (for LinUCB, HLinUCB and our methods) or probability of click (for LR, FM and W&amp;D) of each item.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Offline evaluation</head><p>We first compare our methods with other baselines on the offline dataset. The offline dataset is static and only certain pairs of usernews interaction have been recorded. As a result, we can not observe the change of user activeness due to different recommendation decisions. Similarly, the exploration strategy can not explore well due to the limited candidate news set (i.e., only the click labels of a few candidate news are recorded). Hence, the benefit of considering user activeness and exploration is not very evident in the offline setting. Therefore, we only show the comparison of recommendation accuracy under this situation. For the offline experiment, we down-sample the click / no-click to approximately 1:11 for better model fitting purpose.</p><p>We design the algorithm to recommend the top-5 news, and show the results in terms of CTR and nDCG (we omit top-5 precision because it will be the same with CTR). <ref type="table" target="#tab_4">Table 4</ref>. As expected, our algorithms outperform all the baseline algorithms. Our base model DN already achieves very good results compared with the baselines. This is because the dueling network structure can better model the interaction between user and news. Adding future reward consideration (DDQN ), we achieve another significant improvement. Then, incorporating user activeness and exploration do not necessarily improve the performance under the offline setting, which might because under offline setting, the algorithm can not make the best interaction with user due to the limited static set of candidate news. (It is possible that our agent G want to recommend user u a news i for user activeness or exploration consideration, but actually the information about whether user u will click on news i or not does not exist in the offline log.) In addition, naive random exploration like ϵ-greedy will harm the recommendation accuracy.  <ref type="figure" target="#fig_8">Figure 9</ref> to illustrate the convergence process. The offline data are ordered by time and simulate the process that users send news request as time goes by. All the compared methods will update their models every 100 request sessions. As expected, our algorithm (DDQN + U + DBGD) converges to a better CTR faster than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Accuracy. The accuracy result is shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Online evaluation</head><p>In the online evaluation stage, we deployed our models and compared algorithms on a commercial news recommendation application. Users are divided evenly to different algorithms. In online setting, we can not only measure the accuracy of recommendation, but also observe the recommendation diversity for different algorithms. All the algorithms are designed to recommend the top-20 news to a user when a news request is received.</p><p>5.6.1 Accuracy. We compare different algorithms in terms of CTR, Precision@5, and nDCG. As shown in <ref type="table" target="#tab_5">Table 5</ref> However, further adding user activeness consideration U seems not very helpful in terms of recommendation accuracy. (But this component is helpful for improving user activeness and recommendation diversity. This will be demonstrated later.) In addition, using DBGD as exploration methods will help avoid the performance loss induced by classic ϵ-greedy methods. </p><formula xml:id="formula_11">ILS(L) = b i ∈L b j ∈L,b j b i S(b i , b j ) b i ∈L b j ∈L,b j b i 1<label>(13)</label></formula><p>where S(b i , b j ) represents the cosine similarity between item b i and item b j . We show the diversity for the news clicked by users as in <ref type="table" target="#tab_6">Table 6</ref>. In general, users in our algorithm DDQN + U + DBGD achieves the best click diversity. Interestingly, adding EG seems not improving the recommendation diversity. This is probably because, when random exploration (i.e., EG) is conducted, the recommender might recommend some totally unrelated items to users. Although these items have high diversity, users might be not interested in reading them and turn back to read more about the items that fit their interest better. This way, this exploration will not help improve the recommendation diversity. To our surprise, some baseline methods, like HLinUCB, also achieve comparable recommendation diversity, which indicates that UCB can also achieve reasonable exploration result (but this kind of unguided exploration will harm the recommendation accuracy). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a DQN-based reinforcement learning framework to do online personalized news recommendation. Different from previous methods, our method can effectively model the dynamic news features and user preferences, and plan for future explicitly, in order to achieve higher reward (e.g., CTR) in the long run. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, we apply an effective exploration strategy into our framework to improve the recommendation diversity and look for potential more rewarding recommendations. Experiments have shown that our method can improve the recommendation accuracy and recommendation diversity significantly. Our method can be generalized to many other recommendation problems.</p><p>For the future work, it will be more meaningful to design models for different users correspondingly (e.g., heavy users and one-time users), especially the user-activeness measure. It can bring more insights if different patterns are observed for different groups of users.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distribution of clicked categories of an active user in ten weeks. User interest is evolving over time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>Figure 2: Deep Reinforcement Recommendation System</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Q network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 5: User activeness estimation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Exploration by Dueling Bandit Gradient Descent</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: User and news basic illustration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Factorization Machines is a state-of-art context- aware recommendation methods. It takes all the four cate- gories of features as input, use the combination of features and their interactions to do the click prediction. • W&amp;D [8]. Wide &amp; Deep is a widely used state-of-art deep learning model combining the memorization (through a lo- gistic regression on wide combinations of categorical fea- tures) and generalization (through a deep neural network embedding of the raw features) to predict the click label. • LinUCB [23]. Linear Upper Confidence Bound [23] can select an arm (i.e., recommend a piece of news) according to the estimated upper confidence bound of the potential reward. Due to the long tail distribution of news request and click counts, we apply the same set of parameters for different news, which actually performs better than the original set- ting in [23] on our dataset.(An improved version of the orig- inal LinUCB-HLinUCB will also be compared.) • HLinUCB [42] is another state-of-art bandit-based approach in recommendation problem. Hidden Linear Upper Confi- dence Bound [42] further allows learned hidden feature to model the reward. We follow the original setting of keeping different sets of parameters for different users and different news. However, under this case, only News features intro- duced in Section 4.2 can be directly applied, while the other features describing the interaction between user and news are expected to be learned in the hidden features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Offline cumulative CTR of different methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 : Notations</head><label>1</label><figDesc></figDesc><table>Notation Meaning 

G 
Agent 
u, U 
User, User set 
a 
Action 
s 
State 
r 
Reward 
i, I 
News, Candidate news pool 
L 
List of news to recommend 
B 
List of feedback from users 
Q 
Deep Q-Network 
W 
Parameters of Deep Q-Network 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 : Statistics of the sampled dataset</head><label>2</label><figDesc></figDesc><table>Stage 
Duration # of users # of news 

Offline stage 6 months 541,337 
1,355,344 
Online stage 1 month 
64,610 
157,088 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 : Parameter setting</head><label>3</label><figDesc></figDesc><table>Parameter 
Setting 

Future reward discount γ (Equation 1) 
0.4 
User activeness coefficient β (Equation 6) 
0.05 
Explore coefficient α (Equation 7) 
0.1 
Exploit coefficient η (Equation 8) 
0.05 
Major update period T R (for DQN experience replay) 60 minutes 
Minor update period T D (for DBGD) 
30 minutes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 : Offline recommendation accuracy</head><label>4</label><figDesc></figDesc><table>Method 
CTR 
nDCG 

LR 
0.1262 0.3659 
FM 
0.1489 0.4338 
W&amp;D 
0.1554 0.4534 

LinUCB 
0.1447 0.4173 
HLinUCB 
0.1194 0.3491 

DN 
0.1587 0.4671 
DDQN 
0.1662 0.4877 
DDQN + U 
0.1662 0.4878 
DDQN + U + EG 
0.1609 0.4723 
DDQN + U + DBGD 0.1663 0.4854 

5.5.2 Model converge process. We further show the cumulative 
CTR of different methods in </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 5 : Online recommendation accuracy</head><label>5</label><figDesc></figDesc><table>Method 
CTR 
Precision@5 nDCG 

LR 
0.0059 
0.0082 
0.0326 
FM 
0.0072 
0.0078 
0.0353 
W&amp;D 
0.0052 
0.0067 
0.0258 

LinUCB 
0.0075 
0.0091 
0.0383 
HLinUCB 
0.0085 
0.0128 
0.0449 

DN 
0.0100 
0.0135 
0.0474 
DDQN 
0.0111 
0.0139 
0.0477 
DDQN + U 
0.0089 
0.0110 
0.0425 
DDQN + U + EG 
0.0083 
0.0100 
0.03391 
DDQN + U + DBGD 0.0113 
0.0149 
0.0492 

5.6.2 Recommendation diversity. Finally, in order to evaluate 
the effectiveness of exploration, we calculate the recommendation 
diversity of different algorithms using ILS. [2, 53]. It is calculated 
by Equation 13 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table>Diversity of user clicked news in the online exper-
iment. Smaller ILS indicates better diversity. Similarity be-
tween news is measured by the cosine similarity between 
the bag-of-words vectors of news. 

Method 
ILS 

LR 
0.1833 
FM 
0.2014 
W&amp;D 
0.1647 

LinUCB 
0.2636 
HLinUCB 
0.1323 

DN 
0.1546 
DDQN 
0.1935 
DDQN + U 
0.1713 
DDQN + U + EG 
0.1907 
DDQN + U + DBGD 0.1216 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work was supported in part by NSF awards #1639150, #1544455, #1652525, and #1618448. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://data.princeton.edu/wws509/notes/." />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes on Generalized Linear Models</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving aggregate recommendation diversity using ranking-based techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngok</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="896" to="911" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gediminas</forename><surname>Adomavicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Tuzhilin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="734" to="749" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Recommender systems survey. Knowledge-based systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesús</forename><surname>Bobadilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Gutiérrez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="109" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A contextualbandit algorithm for mobile context-aware recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amel</forename><surname>Bouzeghoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alda</forename><surname>Gançarski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="324" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A gang of bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Zappella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="737" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical evaluation of thompson sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2249" to="2257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ispir</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras." />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D Manning</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavan</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schütze</forename><surname>Hinrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An Introduction To Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page">177</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Google news personalization: scalable online collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayur</forename><surname>Abhinandan S Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From chatter to headlines: harnessing the real-time web for personalized news recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianmarco De Francisci</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Lucchese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth ACM international conference on Web search and data mining</title>
		<meeting>the fifth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time-sensitive recommendation from recurrent user activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3492" to="3500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online Clustering of Bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gentile</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Zappella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online learning to rank for information retrieval: SIGIR 2016 tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Grotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maarten De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1215" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reusing historical interaction data for faster online learning to rank for IR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Schuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Whiteson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth ACM international conference on Web search and data mining</title>
		<meeting>the sixth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bayesian survival analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hui</forename><surname>Joseph G Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debajyoti</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sinha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flavius Frasincar, and Frederik Hogenboom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wouter</forename><surname>Ijntema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Goossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010</title>
		<meeting>the 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Ontology-based news recommendation</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edbt/Icdt</forename><surname>Workshops</surname></persName>
		</author>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Neural survival recommender</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">How</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient Thompson Sampling for Online Matrix-Factorization Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Branislav</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Tran-Thanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1297" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Content-Based News Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kompan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mária</forename><surname>Bieliková</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EC-Web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A contextualbandit approach to personalized news article recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SCENE: a scalable two-stage personalized news recommendation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingding</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Knox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Padmanabhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Personalized news recommendation based on click behavior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elin</forename><forename type="middle">Rønby</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th international conference on Intelligent user interfaces</title>
		<meeting>the 15th international conference on Intelligent user interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07793</idno>
		<title level="m">Partially Observable Markov Decision Process for Recommender Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning and adaptivity in interactive recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tariq</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth international conference on Electronic commerce</title>
		<meeting>the ninth international conference on Electronic commerce</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The multiple multiplicative factor model for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">73</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">tffm: TensorFlow implementation of an arbitrary order Factorization Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander Novikov Mikhail</forename><surname>Trofimov</surname></persName>
		</author>
		<ptr target="https://github.com/geffy/tffm" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rupert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Survival analysis</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A ucb-like strategy of collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuyoshi</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="315" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Terms of a feather: Content-based news recommendation and discovery using twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="448" to="459" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">New recommendation system using reinforcement learning. Special Issue of the Intl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pornthep</forename><surname>Rojanavasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phaitoon</forename><surname>Srinil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ouen</forename><surname>Pinngern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Computer, the Internet and Management</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>SP</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An MDP-based recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><forename type="middle">I</forename><surname>Brafman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1265" to="1295" />
			<date type="published" when="2005-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew G</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Usage-based web recommendations: a reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Taghipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Kardan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed Shiry</forename><surname>Ghidary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2007 ACM conference on Recommender systems</title>
		<meeting>the 2007 ACM conference on Recommender systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ensemble contextual bandits for personalized recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th ACM Conference on Recommender Systems</title>
		<meeting>the 8th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Personalized recommendation via parameter-free contextual bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yexi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqiu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning with Double Q-Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Hidden Features for Contextual Bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1633" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Factorization Bandits for Interactive Recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2695" to="2702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploration in interactive personalized music recommendation: a reinforcement learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic Attention Deep Model for Article Recommendation by Learning Human Editors&apos; Demonstration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuejian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A theoretical analysis of ndcg type ranking measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hado</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando De</forename><surname>Freitas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06581</idno>
		<title level="m">Dueling network architectures for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Returning is Believing: Optimizing Long-term User Engagement in Recommender Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interactively optimizing information retrieval systems as a dueling bandits problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1201" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunqiu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2025" to="2034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interactive collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management</title>
		<meeting>the 22nd ACM international conference on Conference on information &amp; knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1411" to="1420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Joint deep modeling of users and items using reviews for recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="425" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Improving recommendation lists through topic diversification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cai-Nicolas</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">M</forename><surname>Mcnee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Joseph A Konstan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web</title>
		<meeting>the 14th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
