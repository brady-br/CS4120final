<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T01:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VAMOS: Virtualization Aware Middleware</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abel</forename><surname>Gordon</surname></persName>
							<email>abelg@il.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa † Technion-Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⋆</forename><surname>Muli</surname></persName>
							<email>muli@cs.technion.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa † Technion-Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben-Yehuda</forename><surname>†⋆</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa † Technion-Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Filimonov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa † Technion-Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maor</forename><surname>Dahan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Research-Haifa † Technion-Israel Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VAMOS: Virtualization Aware Middleware</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine virtualization is undoubtedly useful, but does not come cheap. The performance cost of virtualiza-tion, for I/O intensive workloads in particular, can be heavy. Common approaches to solving the I/O virtual-ization overhead focus on the I/O stack, thereby missing optimization opportunities in the overall stack. We propose VAMOS, a novel software architecture for middle-ware, which runs middleware modules at the hypervisor level. VAMOS reduces I/O virtualization overhead by cutting down on the overall number of guest/hypervisor switches for I/O intensive workloads. Middleware code can be adapted to VAMOS at only a modest cost, by exploiting existing modular design and abstraction layers. Applying VAMOS to a database workload improved its performance by up to 32%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine virtualization provides many benefits: it enables server consolidation, makes it possible to run legacy environments on new platforms, and helps simplify system management. However, it also degrades the performance of common workloads, particularly if they are I/O intensive workloads.</p><p>Extensive research has been carried out in order to reduce the virtualization (including I/O virtualization) performance penalty, both at the hardware and software layers. However, these works were mainly focused on improving the interaction between the hardware <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b23">22]</ref>, the hypervisor <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b19">18</ref>] and the operating system <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b22">21]</ref>, avoiding changes at the application layer.</p><p>While virtualization is quickly becoming ubiquitous, applications remain oblivious to the changes in the underlying platform. Software architects and designers still use the same principles and models that used to apply to non-virtualized systems for building their applications.</p><p>We argue that changes at the application layer are inevitable: the shift to machine virtualization everywhere requires that we adapt our applications to cooperate with the virtualization layer.</p><p>Adapting a nearly infinite number of applications is not feasible. However, in practice most server applications depend on one or more middleware layers, such as a database server, a web server, or an application server. By adapting the middleware to virtualized platforms, we can indirectly adapt many applications and regain lost performance. We need to re-think the way we build middleware. Most middleware have a modular design that already provides abstraction layers for operating system services, such as memory management, disk or network I/O, and CPU scheduling. We can exploit this to allow middleware software modules to cooperate with the hypervisor or run at host level, without necessitating a rewrite of the entire middleware. This novel architecture model raises new opportunities for performance optimizations at the middleware layer, which do not conflict with today's optimization at lower layers. For example, the middleware can interact with the hypervisor I/O sub-systems using interfaces or protocols defined at the application level. Instead of using a para-virtual block <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b18">17]</ref>, network <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b18">17]</ref>, file system <ref type="bibr" target="#b10">[9]</ref> or socket <ref type="bibr" target="#b20">[19]</ref> interfaces, a database server could use a para-virtual SQL interface while a web server could use a para-virtual HTTP interface. By using high level interfaces we could remove the cycles spent in the hypervisor emulating the virtual hardware and we could also decrease the number of transitions between the hypervisor and the guest, thus improving both I/O throughput and latency <ref type="bibr" target="#b20">[19]</ref>.</p><p>Virtual appliances and Platform as a Service (PaaS) clouds are natural places to apply this new architecture model, primarily because the middleware layer there is under the control of the provider, and the adaptations will be transparent to the customer's applications. We can extend the model for Infrastructure as a Service (IaaS) clouds, implementing the adaptations at the hypervisor level and exposing them as optional accelerators to the virtual machines. Assuming the customers decide to install required extensions into the virtual machines image, as they might install para-virtual drivers such as VMware-tools or virtio drivers <ref type="bibr" target="#b18">[17]</ref>, they will benefit from the improvements.</p><p>The main contribution in this work is VAMOS, a novel architecture for middleware running in virtual environments. Using VAMOS middleware cooperates with the hypervisor to improve the overall application performance. A proof of concept of VAMOS shows up to 32% I/O performance improvements for database workloads when compared with standard approach to I/O virtualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VAMOS Architecture</head><p>In this section we present VAMOS, an architecture model that allows cooperation between the hypervisor and the middleware, improving I/O performance. First, we explain the causes for the virtualization overhead and then we describe the VAMOS architecture and its advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causes of Virtualization Overhead</head><p>It is well known that virtualization has a significant overhead <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">5]</ref>. When we run a workload in a virtual machine, it does not get exclusive access to all the hardware resources, even when we use hardware virtualization support such as Intel VMX or AMD SVM. Usually, the VM runs directly on top of the physical CPU, however, when a sensitive instruction is executed, the CPU transfers the control to the hypervisor for handling. During this handling period, the guest is temporary interrupted and the hypervisor consumes precious cycles handling the sensitive instruction, which might require emulating virtual hardware. In addition, the transitions have a fixed cost caused by the CPU switching between the guest and the hypervisor contexts, and variable cost caused by the hypervisor polluting the guest cache <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b9">8]</ref>.</p><p>I/O intensive applications suffer the most <ref type="bibr" target="#b19">[18]</ref> because they cause relatively many guest/host transitions. To get rid of the virtualization overhead, we need to reduce the number of transitions and/or reduce the handling cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Architecture</head><p>The number of abstraction layers required to access the hardware resources and the absence of cooperation between the hypervisor and the application cause many transitions to the hypervisor context. We can solve these problems running part of the middleware in the hypervisor context. Using this approach, the middleware can cooperate with the hypervisor and obtain direct access to the physical resources, bypassing the virtualization layer and mitigating part of the overhead. <ref type="figure" target="#fig_0">Figure 1</ref> compares the traditional software architecture model with the VAMOS architecture. However, for VAMOS to be feasible, we need:</p><p>1. An isolated runtime environment at the hypervisor level which is capable of executing middleware code and providing the necessary services to access physical resources, such as network devices or disk drives.</p><p>2. A communication channel between the middleware running in the guest, the middleware running in the host and the hypervisor.</p><p>3. A methodology for adapting existing middleware code at a reasonable cost.</p><p>Today's hypervisors, such as KVM or Xen, or hosted hypervisors, already provide a fully operational operating system to manage the physical hardware. We can take advantage of this fact and run middleware modules as additional processes or runtime libraries in the host. The cooperation between the middleware and hypervisor can be implemented using regular mechanisms to communicate between user-space processes or kernel modules. In addition, the middleware can access physical resources via the host OS services, such as file systems, network interfaces, and virtual memory. For the guest to host communication, we can exploit the channels implemented to run para-virtualized guests <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b18">17]</ref>. Alternatively, we can use more efficient channels, based on polling and side core invocation techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b14">13]</ref>.</p><p>Machine virtualization typically requires isolating the physical resources from untrusted virtual machines. If we treat the host side middleware modules as trusted entities, we can allow these modules to access physical resources. However, such middleware modules might contain security vulnerabilities. Thus, to minimize the effects of potential client attacks on the host, the hypervisor should restrict the privileges of those modules. This can be achieved using plugin-isolation techniques such as Vx32 <ref type="bibr" target="#b8">[7]</ref> or Native Client <ref type="bibr" target="#b24">[23]</ref>.</p><p>While we have the appropriate environment to run part of middleware at the hypervisor level, we still need a methodology to adapt existing code. Modular design <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b21">20]</ref> and abstraction layers <ref type="bibr" target="#b11">[10]</ref> are well known techniques implemented in most if not all middleware products to decouple different components and to abstract OS services. For example, as shown in <ref type="figure">Figure 2</ref>, MySQL abstracts and implements the I/O logic into pluggable storage engines. Another example is Apache Tomcat, which encapsulates the logic responsible for handling the communication with the clients into independent modules, called connectors. JVMs usually abstract OS services to have a single cross platform core implementation and easy to maintain platform dependent modules <ref type="bibr" target="#b3">[3]</ref>. We can take advantage of the modular design and run part of the middleware at the hypervisor context. The following criteria characterize the modules which will improve the application performance when moved to the hypervisor:</p><p>1. Modules which interact directly with system resources, generating many transitions to the hypervisor context and requiring emulation of virtual hardware.</p><p>2. Modules which can be easily re-factored into a client side running in the guest and a server side running in the host.</p><p>3. Modules which do not share state with other components, avoiding data sharing and synchronization between the guest and the hypervisor.</p><p>4. All the internal state used by the server side can be discarded on live migration, checkpoint or restore operations. In other words, internal state on the server side is only used as a cache.</p><p>Other characteristics might also be added to the previous list, such as clients capable of caching or batching requests, to reduce the number of transitions. By running the middleware modules responsible for I/O operations at the hypervisor level, we can reduce the number of transitions between the guest and the hypervisor as well as part of the virtual hardware emulation. Instead of switching on every low-level event such as packet sent or disk block written, we can switch when higher-level events (e.g., an SQL query) occur. Since higher-level events typically cause many lowerlevel events, we end up switching less often, with all of the resulting benefits. Another advantage of the proposed approach is that we end up running less operating system code in guest context (e.g., less file system code or networking code), which also consumes CPU cycles. For example, as we show in Section 3, a database server can run the module responsible for storing and retrieving the persisted data at the hypervisor level and use the host OS file system services to directly access the physical disk. A Web Server or Application Server can run the module responsible for communicating with the clients at the hypervisor level and use the host OS network services to access directly the physical network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Applicability</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proof of Concept</head><p>We implemented a proof of concept of VAMOS for the MySQL database server, running on top of the KVM hypervisor. As can be seen in <ref type="figure">Figure 2</ref>, within MySQL storage engines are responsible for disk I/O. We adapted the default storage engine, MyISAM, to access persistent data using the host OS file system. MyISAM was selected based on the criteria defined in Section 2.2.</p><p>The KVM hypervisor is implemented as a Linux kernel module that extends the kernel with hypervisor ca- The MyISAM client side module running in the guest delegates requests to the server side module running in the host. Communication between the client and the server is done using virtio <ref type="bibr" target="#b18">[17]</ref>. To reduce the number of transitions from the guest to hypervisor context, we optimized the MyISAM client module to batch multiple requests into a single batch. This new architecture and the data-flow path are shown in <ref type="figure" target="#fig_2">Figure 3</ref>.</p><p>Using this model the MyISAM storage engine performs the I/O operations directly on the host, skipping the virtual hardware emulation, the guest OS VFS, file system, and block device, shortening overall code path lengths and reducing the number of transitions.</p><p>We analyzed MySQL performance by measuring the time needed to insert rows of different sizes to a database table. We used the following configurations: MySQL with emulated drivers (baseline), MySQL with paravirtualized drivers, VAMOS-MySQL without request batching, and VAMOS-MySQL with request batching. <ref type="figure">Figure 4</ref> shows the results for different workloads. The Y-axis shows the normalized runtime improvement compared to the baseline. The X-axis shows the size of the database rows inserted for each workload. We can see that VAMOS improves performance for all the workloads. However, for workloads with row sizes up to 16KB, using only para-virtualized drivers we can obtain a better improvement. For sizes starting from 32KB VAMOS does better than para-virtualized drivers.  Each of these workloads represent the trade-off between the amount of data the server side storage engine reads and writes to the file system and the number of transitions generated by the client side. In other words, when the developers adapt the middleware code they need to choose an interface that minimizes the number of guest/host transitions. Our implementation required too many transitions for small rows. We believe VAMOS can outperform para-virtual I/O even for small rows, by optimizing the code and selecting an alternative interface for the storage engine that requires less transitions from the guest to the hypervisor context. <ref type="figure" target="#fig_4">Figure 5</ref> compares VAMOS-MySQL with the traditional MySQL version running in an unmodified guest. We show, for each row size, the number of cycles spent in the guest and the number of cycles spent in the host. It is interesting to note that the number of cycles in the guest does not change, proving that the execution of the guest was not affected, leading us to believe that the storage engine consumes relatively few cycles. By moving the storage engine to the hypervisor, we reduce overall hypervisor overhead. This is because VAMOS reduced the number of transitions and part of the virtual hardware emulation, which overall consumed more cycles than the storage engine itself.</p><formula xml:id="formula_0">d i f i e d V A − M y S Q L V A − M y S Q L − b a t c h U n m o d i f i e d V A − M y S Q L V A − M y S Q L − b a t c h U n m o d i f i e d V A − M y S Q L V A − M y S Q L − b a t c h U n m o d i f i e d V A − M y S Q L V A − M y S Q L − b a t c h U n m o d i f i e d V A − M y S Q L V A − M y S Q L − b a t c</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>Running additional code at the host level raises new concerns with regards to the complexity of the hypervisor and potential security risks. We believe that with an appropriate design the hypervisor code can remain clean and simple, and the middleware modules could be loaded as isolated plugins. In our proof of concept, the KVM kernel module itself required no changes. The QEMU user-space process required only two minor changes: to load the middleware shared library and to delegate requests coming from the guest over the virtio channel.</p><p>As described in Section 2.2, to neutralize security vulnerabilities the hypervisor should restrict the privilege of the middleware modules. In the case of KVM, each virtual machine has a dedicated QEMU instance and middleware modules run in user-space as QEMU plugins. The first level of defense against compromised middleware components is for QEMU to use a plugin-isolation mechanism <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b24">23]</ref>. The second level of defense is to use the standard Linux access control mechanisms (e.g., SELinux, AppArmor, seccomp, chroot) to limit QEMU instances from accessing different resources depending on the virtual machine being run. For example, each QEMU instance could be restricted to only access the parts of the file system that are relevant to the virtual machine it runs.</p><p>Virtualization systems often struggle to find the correct balance between guest transparency and performance (e.g., para-virtualized I/O reduces guest transparency but increases performance). While VAMOS improves I/O performance, it also ties the guest virtual machine closer with the hypervisor and makes the resulting system more complex. We believe that there is no single right trade-off, and VAMOS presents an interesting and useful design point in the spectrum of possible trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Para-virtualization is commonly used to reduce the virtualization overhead. Previous works <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b18">17]</ref> show how to para-virtualize devices drivers to improve the I/O performance, however, they still use low level interfaces to interact with the hypervisor which cause transitions and require emulation of virtual hardware.</p><p>SR-IOV devices <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b13">12]</ref> improve the I/O performance by directly assigning hardware resources to the guest, bypassing part of the virtualization layer. This approach requires special hardware and does not completely mitigate the overhead caused by the transitions to the hypervisor context. In addition, using dedicated hardware, the hypervisor software lose the ability to intercept and control the data flow path.</p><p>The use of higher abstraction levels to interact with the hypervisor was also proposed in Virt-FS <ref type="bibr" target="#b10">[9]</ref>, Libra <ref type="bibr" target="#b3">[3]</ref> and Scalable I/O <ref type="bibr" target="#b20">[19]</ref>. Virt-FS presents a para-virtualized file system. While it shares some concepts with VAMOS, it still limits the abstraction level of guest/host interaction to inside the guest kernel, whereas VAMOS takes it up into userspace. Libra is a small operating system capable of running only an adapted JVM, taking advantage of the host OS services to improve the workloads performance. VAMOS does not require changes in the guest OS and take advantage of the host OS by running selected middleware modules directly at the hypervisor level, minimizing the adaption cost and re-using existing code. Scalable I/O presents a totally new architecture for the I/O stack, requiring major software modifications and targeting only I/O performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>Virtualization has a performance penalty caused by the overhead of transitions between the guest and the host context. In general, I/O intensive workloads are the most affected, because they create a significant number of transitions and require emulation of virtual hardware.</p><p>VAMOS is a new approach for mitigating I/O virtualization overhead by breaking free from traditional boundaries and running part of the middleware directly at the hypervisor level. By exploiting existing modular designs and abstraction layers, middleware can be adapted to run at the hypervisor level with modest cost. Since there are many different middleware, we are currently analyzing whether it is possible to create a layer of common services at the hypervisor level, shared across different middleware.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparing architecture models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: MySQL Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Virtualization Aware MySQL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: VAMOS MySQL runtime improvement for different workloads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Cycles distribution between guest and host</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A comparison of software and hardware techniques for x86 virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Agesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Architectural Support for Programming Languages &amp; Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Schuster. vIOMMU: efficient IOMMU emulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsafrir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ammons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Appavoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Butrico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Da</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawachiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Hensbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Wisniewski</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Libra: a library operating system for a jvm in a virtualized execution environment</title>
	</analytic>
	<monogr>
		<title level="m">ACM/USENIX Int&apos;l Conference on Virtual Execution Environments (VEE)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="44" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Warfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Operating Systems Principles (SOSP)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Turtles project: Design and implementation of nested virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Day</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dubitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Factor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Har&amp;apos;el</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liguori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wasserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-A</forename><surname>Yassour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Opearting Systems Design &amp; Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="423" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SR-IOV networking in Xen: architecture, design and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on I/O Virtualization (WIOV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vx32: lightweight user-level sandboxing on the x86</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="293" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-Performance Hypervisor Architectures: Virtualization in HPC Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gavrilovska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nathuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranadive</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saraiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on System-level Virtualization for HPC (HPCVirt)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">VirtFS-A virtualization aware File System passthrough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jujjuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Hensbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liguori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ottawa Linux Symposium (OLS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Software reuse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Krueger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="131" to="183" />
			<date type="published" when="1992-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rearchitecting VMMs for Multicore Systems: The Sidecore Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ganev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Interaction between Opearting Systems &amp; Computer Architecture (WIOSCA)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating standard-based self-virtualizing devices: A performance study on 10 GbE NICs with SR-IOV support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Parallel &amp; Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Virtualization Polling Engine (VPE): Using Dedicated CPU Cores to Accelerate I/O Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Abali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int&apos;l Conference on Supercomputing (ICS)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the criteria to be used in decomposing systems into modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Parnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM (CACM)</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1053" to="1058" />
			<date type="published" when="1972-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">High performance and scalable I/O virtualization via self-virtualized devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l Symposium on High Performance Distributed Computer (HPDC)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Achieving 10Gbps using Safe and Transparent Network Interface Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/USENIX Int&apos;l Conference on Virtual Execution Environments</title>
		<imprint>
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">virtio: towards a de-facto standard for virtual I/O devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review (OSR)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="95" to="103" />
			<date type="published" when="2008-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bridging the Gap between Software and Hardware Techniques for I/O Virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">G</forename><surname>Janakiraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Pratt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (ATC)</title>
		<imprint>
			<date type="published" when="2008-06" />
			<biblScope unit="page" from="29" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scalable I/O -a well-architected way to do scalable, secure and virtualized I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Satran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Machulsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Workshop on I/O Virtualization (WIOV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The structure and value of modularity in software design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Griswold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hallen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European software engineering conference</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="99" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scale and performance in the denali isolation kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Gribble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Symposium on Opearting Systems Design &amp; Implementation (OSDI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="195" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Concurrent Direct Network Access for Virtual Machine Monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Willmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int&apos;l Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="306" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Native Client: A Sandbox for Portable, Untrusted x86 Native Code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dardyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ormandy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Okasaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Narula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fullagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security &amp; Privacy</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="79" to="93" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
