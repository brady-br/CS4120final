<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Load Balancing of Heterogeneous Workloads in Memcached Clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">IBM T. J. Watson Research Center</orgName>
								<orgName type="institution" key="instit1">Beihang University</orgName>
								<orgName type="institution" key="instit2">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wood</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Howie</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Hwang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The George Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Load Balancing of Heterogeneous Workloads in Memcached Clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Web services, large and small, use in-memory caches like memcached to lower database loads and quickly respond to user requests. These cache clusters are typically provisioned to support peak load, both in terms of request processing capabilities and cache storage size. This kind of worst-case provisioning can be very expensive (e.g., Facebook reportedly uses more than 10,000 servers for its cache cluster) and does not take advantage of the dynamic resource allocation and virtual machine provisioning capabilities found in modern public and private clouds. Further, there can be great diversity in both the workloads running on a cache cluster and the types of nodes that compose the cluster, making manual management difficult. This paper identifies the challenges in designing large-scale self-managing caches. Rather than requiring all cache clients to know the key to server mapping, we propose an automated load balancer that can perform line-rate request redirection in a far more dynamic manner. We describe how stream analytic techniques can be used to efficiently detect key hotspots. A controller then guides the load balancer&apos;s key mapping and replication level to prevent overload, and automatically starts additional servers when needed.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In-memory caching has become a popular technique for enabling highly scalable web applications. These caches typically store frequently accessed or expensive to compute query results to lower the load on database servers that are typically difficult to scale up. Memcached has become the standard caching server for a wide range of applications.</p><p>Large web companies like Facebook and Twitter provision their caching infrastructure to support the peak load and to hold a majority of data in cache <ref type="bibr" target="#b0">[1]</ref>. Given the dynamic nature of Internet workloads, this kind of static provisioning is generally very expensive. For example, the workload handled by the Facebook cache was shown to have a peak workload about two times higher than the minimum seen over a 24 hour period <ref type="bibr" target="#b1">[2]</ref>, and this may be even more dramatic for websites with a less global reach. Thus provisioning a cache infrastructure for peak load can be highly wasteful in terms of hardware and energy costs.</p><p>At the same time, public-facing applications need to be wary of flash crowds that direct a large workload to a small portion of the application's total content. Inmemory caches are crucial for handling this type of load, but even they may become overloaded if popular data is not efficiently replicated or the system is unable to scale up the number of cache nodes in time. Further, one caching cluster is often multiplexed for several different applications, each of which may have distinct workload characteristics. The cache must be able to balance the competing needs of these applications despite differences in get/set rates, data churn, and the cost of a cache miss.</p><p>Heterogeneity can occur not only within keys and workloads, but also among the servers that make up the caching cluster. A wide range of key-value store architectures (many supporting the same memcached protocol) have been proposed, ranging from energy efficient FPGA designs <ref type="bibr" target="#b2">[3]</ref> to high-powered data stores capable of saturating multiple 10 gigabit NICs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. These approaches provide different trade-offs in the energy efficiency, throughput, latency, and data volatility of the cache, suggesting that a heterogeneous deployment of different server and cache types may offer the best overall performance.</p><p>While many resource management systems have been proposed for web applications, the caching tier is often ignored, leaving it statically partitioned and sized for worst case workloads. In part, this is because caches are typically accessed by a distributed set of clients (usually web servers), so dynamically adjusting the cache setup requires coordination across a large number of nodes. To get around this problem, we eschew the traditional approach where clients know precise key-server mappings, and instead propose a middlebox-based load balancer capable of making dynamic adaptations within the caching infrastructure. Having a centralized load balancer is made possible by recent advances in high performance network cards and multi-core processors that allow network functions to be run on commodity servers <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Our system will be built upon the following components:</p><p>• A high speed memcached load balancer that can forward millions of requests per second.</p><p>• A hot spot detection algorithm that uses stream data mining techniques to efficiently determine the hottest keys.</p><p>• A two-level key mapping system that combines consistent hashing with a lookup table to flexibly control the placement and replication level of hot keys.</p><p>• An automated server management system that takes inputs from the load balancers and overall application performance levels to determine the number and types of servers in the caching cluster.</p><p>In this paper we describe our preliminary work on designing this dynamically scalable caching infrastructure. Our architecture provides greater flexibility than existing approaches that place complexity and intelligence in either the clients or memcached servers. By removing the reliance on manual, administrator specified policies, our self-managing cache cluster can automatically tune key placement and server settings to provide high performance at low cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this paper we focus on the memcached in-memory key value store. Memcached provides a simple put/get/delete interface and is primarily used to store small (e.g., &lt; 1KB) data values <ref type="bibr" target="#b1">[2]</ref>. Clients, such as a PHP web application, generally follow the pattern of first requesting data from a cache node, but querying a database and loading the relevant entry into the cache if it was not found.</p><p>A memcached client can directly connect to a memcached server, or a proxy can be used to help manage the mappings of keys to servers. Individual memcached servers are designed so that they are unaware of each other. Many distributed key-value stores employ consistent hashing <ref type="bibr" target="#b8">[9]</ref> to determine how keys are mapped to different servers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. This provides both an even key distribution and simplifies the addition and removal of servers into the key space.</p><p>To prevent a centralized proxy from becoming a bottleneck, each client machine typically runs its own local proxy instance that maintains the mapping of all key ranges to servers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>. While this reduces latency, coordinating their consistency can be a problem if there are a very large number of clients. As a result, the key to server mapping in these clusters is typically kept relatively static, limiting the flexibility with which the cluster can be managed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Workload Heterogeneity</head><p>Workload characteristics can have a large impact on the performance of a memcached cluster since requests often follow a heavy tailed, Zipfian distribution. <ref type="figure" target="#fig_0">Figure 1</ref> shows how the amount of skew in a Zipfian request distribution affects the number of key requests that occur on the most loaded server in a simulated cluster of 100 machines (normalized relative to the number of requests under a uniform workload). As the workload becomes more focused on a smaller number of keys, the imbalance across servers can rise significantly, but if the hot keys can be replicated to even a small number of servers, the balance is significantly improved.</p><p>In addition to varied key popularity, analysis of the Facebook memcached workload <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> shows that different applications can have different read/write rates, churn rates, costs for cache misses, quality-of-service demands, etc. To handle these heterogeneous workloads, Facebook breaks their memcached cluster into groups, each of which services a different application or set of applications <ref type="bibr" target="#b0">[1]</ref>. However, from their descriptions it appears that this partitioning is done in a manual fashion. This leaves it susceptible to inefficient allocations under dynamic conditions and may not be feasible for companies with less expertise in memcached cluster management. Figure 2: On our test system, a Gigabit NIC achieves higher performance for small value sizes, but the network quickly becomes saturated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Server Heterogeneity</head><p>The hardware and software that make up a cache cluster can also be diverse. Not only are there several research <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref> and commercial <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> key-value store software solutions, many of them support the standard memcached protocol (even recent versions of MySQL). These alternatives are optimized for different use cases: e.g., couchbase <ref type="bibr" target="#b13">[13]</ref> supports key replication for high availability and MICA <ref type="bibr" target="#b3">[4]</ref> provides extremely high throughput, but only supports small value sizes. While many alternatives exist, memcached continues to be the most popular in-memory cache. <ref type="figure">Figure 2</ref> illustrates the performance of memcached using either an Intel 82599EB ten gigabit NIC or a Broadcom 5720 gigabit NIC on the same server with dual Intel Xeon X5650 CPUs. The ten gigabit NIC appears to suffer from inefficient processing of small packets, reducing its maximum throughput for very small sized objects. However, the gigabit network card quickly reaches its bandwidth limit when using larger value sizes. Since many web applications store primarily small objects <ref type="bibr" target="#b1">[2]</ref>, the added energy and hardware cost of ten gigabit adapters is not always necessary, suggesting that an intelligently scheduled mixed deployment could provide the best performance per dollar spent. Other hardware options such as low-power Atom CPUs have also been shown to provide valuable trade-offs when designing a memcached cluster <ref type="bibr" target="#b15">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Memcached Load Balancer Design</head><p>This section describes our preliminary design for a scalable, in-network dynamic load balancer for memcached clusters. Our overall architecture is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The load balancer is composed of a Lossy Counter used to detect key hot spots, a two level Key Director that stores where a key should be routed, and Key and Server managers that run the control algorithms to decide how the system should respond to workload changes. We describe these components in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Middlebox Platform</head><p>Recent advances in network interface cards (NICs) and user-level packet processing libraries <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">16]</ref> have enabled high speed packet processing on commodity servers. We are using these techniques to build an efficient load-balancing platform that can direct traffic to a large number of back-end memcached servers. Further, the load balancer's placement within the network path allows it to observe important statistics about the servers and their workloads.</p><p>Our prior work has demonstrated that even when a load balancer such as this is run inside a virtual machine, it is possible to achieve full 10 Gbps line rates <ref type="bibr" target="#b7">[8]</ref>. A typical memcached request packet is approximately 96 bytes 1 , so the maximum rate that can be handled by a single 10 Gbps NIC port is 13 million requests per second. Our current prototype can handle approximately 10 million 64-byte requests per second when using a single core to run a simplified version of the key redirection system described below.</p><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, client requests are sent as UDP packets to the IP of the load balancer, but replies are returned directly from the memcached servers back to the client. This significantly lowers the processing requirements of the load balancer since memcached responses are often much larger (and thus more expensive to process) than requests. The load balancer acts as a "bump in the wire", so it does not need to maintain any connection state, unlike existing proxies such as Twemproxy which establishes separate socket connections with each client and each server <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hot Spot Detection</head><p>The load balancer must determine how to efficiently forward requests to memcached servers, while preventing some of them from becoming overloaded. Currently, our focus is on handling skewed workloads that cause a small number of servers to become overloaded. To prevent this, the load balancer must be able to detect which keys are causing the greatest load imbalance.</p><p>Since memcached does not store much data per key (e.g., true frequency over time) to limit overheads, we cannot simply rely on the servers sending this info. We propose a frequency counting mechanism to build a table of hot items in the request stream. Once hot keys are detected, requests for them can be either directed to more powerful servers or to replicas spread across several machines. The Lossy Counting algorithm <ref type="bibr" target="#b17">[17]</ref> is a one-pass   deterministic algorithm that efficiently calculates frequency counts over data streams by guaranteeing to identify hot items based on user-defined parameters−support threshold s and error rate ε, where ε &lt;&lt; s. When the stream length is N, the lossy counting algorithm returns all keys with frequency at least sN, and there are no false negatives, which means no item with true frequency less than (s − ε)N is returned.</p><p>Depending on the workload's skew, the actual number of hot items can be very different as shown in <ref type="figure" target="#fig_4">Fig- ure 4</ref>. Thus the number of hot items found by the counter for a given s parameter depends both on the total number of keys being accessed in the cache and the workload distribution. Unfortunately, it is non-trivial to predict in advance what a workload will look like, and it may change with time. As a result, we have modified the Lossy Counting algorithm so that it will adjust itself to store a specified number of hot keys; we then adapt the target number of hot keys based on our observations of the workload during the previous observation window.</p><p>Hot Key Analysis: <ref type="figure" target="#fig_5">Figure 5</ref> shows the estimated frequency (i.e., request rate) seen by the top keys measured by the Lossy Counter for a sample workload; the frequencies are an estimate guaranteed to be at most ε * N smaller than the true counts <ref type="bibr" target="#b17">[17]</ref>. Clearly, not all items reported by the counter should be treated identically since they have very different loads. The goal of our hot key analysis phase is to determine which of these potential keys need to be replicated or moved to faster servers.</p><p>The Lossy Counter can be used to separate keys into groups with similar request rates. Due to the nature of long tail distributions common in web workloads, the number of keys in the group with the highest average frequency will be smaller than the number of keys in the group with the second highest, and so on. This is shown in <ref type="figure" target="#fig_5">Figure 5</ref> by the increasing width of each step. We consider the workload as a set of groups g 1 , g 2 , ..., g i , ..., g n ordered such that g 1 is the key group with the highest request frequency, f 1 . Our goal is to find the group g i that splits the groups into two sets: g 1 ...g i represents the hot keys while g i+1 ...g n are the "regular" keys.</p><p>The intuition behind our approach is to select g i such that |g i |, the number of keys in the group, is large enough to be evenly distributed. Any subsequent g j where j &gt; i will have |g j | &gt; |g i | since we expect a heavy tailed workload distribution, and thus can be load balanced with simple consistent hashing.</p><p>We can use the common Balls and Bins analysis to understand how many keys (balls) are expected to be placed on each server (bin), if keys are uniformly assigned to each server. For a given group, we consider the request rate of all keys to be equivalent, so bounding the number of keys from a group that can be assigned to the most loaded server in turn bounds the maximum request rate it can achieve. For the case where there are fewer keys in a group than there are servers (i.e., |g j | &lt; # servers/log(# servers), which is common for the hottest sets of keys) we can adapt the theorem from Mitzenmacher <ref type="bibr" target="#b18">[18]</ref> that bounds the number of balls assigned to the most loaded bin with high probability (p = 1 − 1/# servers):</p><formula xml:id="formula_0">MaxLoad ≤ f j × log(# servers) log(# servers/|g j |)<label>(1)</label></formula><p>where f j is the maximum request frequency to keys in g j . The Hot Spot Detector considers each group in order starting with j = 1. If the maximum load exceeds a threshold, then group g j+1 will be considered for the split point. Once the split point g i is found, all groups less than i are replicated as described in Section 3.3, and the rest are forwarded using consistent hashing. Adaptive Sized Lossy Counter: The above analysis assumes that group g i is included in the groups of keys returned by the Lossy Counter. However, if the counter is not configured with the appropriate support parameter, s, then the counter will track either too few groups (and not enough keys will be replicated) or too many groups (wasting memory and increasing the cost of lookups in the counter). To prevent this, we adapt the size of the Lossy Counter during each measurement interval to ensure it is tracking the correct number of keys.</p><p>The key request stream passes through the lossy counting algorithm for a configurable time window. At the end of the window, the algorithm compares the number of returned keys with T , which is a target level of hot items. We adapt T based on the request rates of the groups returned by the counter; if the last group returned by the counter will cause too much skew if it is not replicated, then T must be increased since the optimal g i is not included in the counter's results. <ref type="figure" target="#fig_6">Figure 6</ref> illustrates the generic lossy counting algorithm and our autonomic lossy counting algorithm to show how we can adjust the desired level. Under the same workload, the generic lossy counting algorithm shows a steady amount of hot items, where as our autonomic lossy counting algorithm tries to reach the target level−here, the target level is defined as 200.</p><p>Lossy Counter Overhead: Since the Lossy Counter runs within the packet processing path, minimizing its overhead is critical. Our tests give an average total processing time of 367 nanoseconds per key counted (269 ns for lookup and 98 ns for insertion). Memcached request latencies are typically in the hundreds of microseconds, so this processing will add negligible performance impact. However, to achieve full line rate with a single core on the load balancer, each packet must be examined and redirected within about 80 nanoseconds. We are investigating how the counter operations can be done outside the critical path by separate cores that only sample a portion of the requests passing through the load balancer. The memory size is also very small: only 48 bytes plus 8 bytes (key hash and frequency) + key size per key.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Request Redirection</head><p>Directing keys to servers should be as fast as possible. Consistent Hashing is well known to effectively balance load across servers while supporting fast lookups and the flexibility to add or remove servers. We use consistent hashing to direct most keys to their destination, but use a lookup table to provide a more flexible mapping for hot keys. Incoming requests are first queried against the lookup table, and if not found there they are handled by the consistent hash ring.</p><p>The Hot Spot Detector provides a set of key/frequency pairs (k 1 , f 1 )...(k i , f i ) where k i is the last key in g i . The Key Redirector can then either replicate each key proportionally to its request frequency, or select some keys to be forwarded to servers that are known to be more powerful or less loaded. Recently there have been several efficient, concurrent hash table data structures proposed which we are exploring to allow the load balancer to efficiently check whether an incoming request is for a hot or regular key <ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref><ref type="bibr" target="#b21">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Server Management</head><p>The final component of our load balancer is the server manager. This component aggregates information from both the hot spot detector and from the memcached servers themselves. This will allow the load balancer to also respond to broader workload dynamics that require servers to be added or removed from the memcached cluster.</p><p>There has been a significant amount of related work on how to dynamically manage virtual servers in response to workload changes. For example, VMware's Distributed Resource Scheduler can dynamically adjust CPU and memory allocations or migrate virtual machines. Yet without insight into statistics such as the cache's hit rate and the overall application's performance, these management actions will not be effective. An important distinction when managing caching servers is that workload skew can have a significant effect not only on request rates (as described in the previous sections), but also on the cache hit rate. Skewed workloads are actually easier to cache, meaning that an intelligent controller may be able to safely reduce the number of cache servers with minimal impact on the cache's hit rate <ref type="bibr" target="#b22">[22]</ref>. Therefore, we are investigating what new control mechanisms and algorithms are necessary to provide a QoS management system that controls a cache cluster based on its internal behavior and the overall application's needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There has been a large amount of work on improving the performance and scalability of individual memcached servers. Some work proposes using new hardware such as RDMA <ref type="bibr" target="#b23">[23]</ref>, high speed NICs <ref type="bibr" target="#b3">[4]</ref>, and FPGAs <ref type="bibr" target="#b24">[24]</ref>, while others have improved memcache's internal data structures <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21]</ref>. These approaches all focus on maximizing the performance or energy efficiency of a single cache node.</p><p>A large scale analysis of Facebook's workload was presented by Atikoglu et al., which illustrates the high skew and time variation seen by what is probably the largest memcached deployment <ref type="bibr" target="#b1">[2]</ref>. Facebook has also improved the efficiency of individual nodes and has deployed a key replication system to help balance load and increase the chance of finding multiple related keys in one server lookup <ref type="bibr" target="#b0">[1]</ref>. Their system relies on individual clients knowing the mapping of all keys to servers, which we argue reduces the agility of the system compared to an in-network load balancer like we propose. They also appear to rely on manual classification of application workloads into different pools.</p><p>Fan et al. propose using a fast, small cache in front of a memcached cluster to prevent workload skew across servers even under adversarial workloads <ref type="bibr" target="#b25">[25]</ref>. Our load balancer could potentially include a cache for fast local lookups, although the scalability of such an approach may be limited. This also increases the complexity of maintaining consistency. Our approach of forwarding some requests to high powered servers should provide a similar load balancing effect. Replication of Memcached keys was proposed by Hong et al <ref type="bibr" target="#b26">[26]</ref>. Their system requires modification to both the clients and servers to maintain state about replicated keys, which we try to avoid with our transparent middle-box approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>Large-scale web applications rely on in-memory caches such as memcached to reduce the cost of processing common user requests. However, memcached deployments are typically statically sized and provisioned for peak workloads. We are developing a load balancing network middlebox that can automatically detect hotspots and balance load across memcached servers through request redirection and replication. Contrary to most resource management systems that require software to be installed either on clients or on the servers being managed, our in-network approach can be transparently deployed without any changes to applications. We believe that recent advances that allow such middleboxes to run at high speed even in virtual machines will open up new possibilities for a wide range of resource management systems that can be flexibly reconfigured and deployed.</p><p>In our ongoing work, we are continuing to extend our load balancer to optimize the number of servers and replicas of hot keys. We are also exploring how this kind of middle-box platform can be used to transparently monitor and manage other types of data center applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: As workloads become more skewed (larger θ ), the imbalance across nodes rises significantly relative to the load under a uniform workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Requests are intercepted by the load balancer and directed based on either the forwarding table (hot items) or consistent hashing. Replies return directly back to the clients, minimizing the processing requirements of the load balancer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Number of (Steady-State) Hot Items with Different Workloads−Here, s is 1%, and ε is 0.1%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Frequency of each key measured by a Lossy Counter where s is 1%, ε is 0.1%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Autonomic Scaling on Number of Hot Items−Workload is a Zipfian distribution; initial s is 1%, ε is 0.1%, and α is 0.1%</figDesc></figure>

			<note place="foot" n="1"> This is calculated based on 52 bytes for the MAC, IP, and UDP headers, 8 bytes for memcached application level header, plus the median memcached key size for Facebook&apos;s ETC pool [2]</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their help improving this paper. This work was supported in part by NSF grants CNS-1253575, CNS-1350766, OCI-0937875, National Natural Science Foundation of China under Grant No. <ref type="bibr">61370059 and No. 61232009, and Beijing Natural Sci- ence Foundation under Grant No. 4122042.</ref> </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scaling memcache at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Fugal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herman</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcelroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Saab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX conference on Networked Systems Design and Implementation</title>
		<meeting>the 10th USENIX conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">385398</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Workload analysis of a large-scale key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Atikoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Frachtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Paleczny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems</title>
		<meeting>the 12th ACM SIGMETRICS/PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">5364</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An FPGA-based in-line accelerator for memcached</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maysam</forename><surname>Lavasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hari</forename><surname>Angepat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Chiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RapidPosts</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MICA: a holistic approach to fast in-memory key-value storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">429444</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smartswitch: Blurring the line between network infrastructure &amp; cloud applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th USENIX Workshop on Hot Topics in Cloud Computing</title>
		<meeting><address><addrLine>Philadelphia, PA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
	<note>HotCloud 14</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Intel data plane development kit: Getting started guide</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Intel Corporation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">netmap: A novel framework for fast packet I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Rizzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 2012 USENIX Annual Technical Conference</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">NetVM: high performance and flexible networking using virtualization on commodity platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Networked System Design and Implementation</title>
		<imprint>
			<date type="published" when="2014-04" />
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Web caching with consistent hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Berkheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Bogstad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rizwan</forename><surname>Dhanidina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Matkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Yerushalmi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Netw</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1203" to="1213" />
			<date type="published" when="1999-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FAWN: a fast array of wimpy nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGOPS 22Nd Symposium on Operating Systems Principles</title>
		<meeting>the ACM SIGOPS 22Nd Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">114</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Twemproxy: A fast, light-weight proxy for memcached</title>
		<ptr target="https://blog.twitter.com/2012/twemproxy" />
		<imprint>
			<date type="published" when="2012-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mazì Eres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasish</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Parulkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Stratmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Stutsman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>The case for ramcloud,&quot; Commun</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">vbuckets: The core enabling mechanism for couchbase server data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Couchbase</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Redis</surname></persName>
		</author>
		<ptr target="http://redis.io" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hadoop and memcached: Performance and power characterization and analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Figueira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cloud Computing: Advances</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
	<note>Systems and Applications</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">mTCP: a highly scalable user-level TCP stack for multicore systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunyoung</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinae</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Jamshed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haewon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghwan</forename><surname>Ihm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Symposium on Networked Systems Design and Implementation</title>
		<meeting>the 11th USENIX Symposium on Networked Systems Design and Implementation<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">489502</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Approximate frequency counts over data streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurmeet</forename><surname>Singh Manku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Very Large Data Bases. 2002, VLDB &apos;02</title>
		<meeting>the 28th International Conference on Very Large Data Bases. 2002, VLDB &apos;02</meeting>
		<imprint>
			<biblScope unit="page" from="346" to="357" />
		</imprint>
		<respStmt>
			<orgName>VLDB Endowment</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Power of Two Choices in Randomized Load Balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
		<respStmt>
			<orgName>UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable, high performance ethernet forwarding lookup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)</title>
		<meeting>9th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MemC3: compact and concurrent memcache with dumber caching and smarter hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th USENIX NSDI</title>
		<meeting>10th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cache craftiness for fast multicore keyvalue storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddie</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert Tappan</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM European Conference on Computer Systems</title>
		<meeting>the 7th ACM European Conference on Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">183196</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Saving cash by using less cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Harchol-Balter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Hot Topics in Cloud Computing</title>
		<meeting><address><addrLine>Berkeley, CA; USENIX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wimpy nodes with 10GbE: leveraging onesided operations in soft-RDMA to boost memcached</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Stuedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">347353</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Achieving 10Gbps line-rate key-value stores with FPGAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimon</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 5th USENIX Workshop on Hot Topics in Cloud Computing</title>
		<meeting><address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Kees Vissers, Jeremia Br, and Zsolt Istvn</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Small cache, big effect: Provable load balancing for randomly partitioned cluster services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeontaek</forename><surname>Bin Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding and mitigating the impact of load imbalance in the memory caching tier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ju</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mithuna</forename><surname>Thottethodi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="13" to="113" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
