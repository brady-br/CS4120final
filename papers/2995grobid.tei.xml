<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Driving Cache Replacement with ML-based LeCaR</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Vietri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liana</forename><forename type="middle">V</forename><surname>Rodriguez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">A</forename><surname>Martinez</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lyons</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Rangaswami</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giri</forename><surname>Narasimhan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Florida International University</orgName>
								<orgName type="institution" key="instit2">Arizona State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Driving Cache Replacement with ML-based LeCaR</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Can machine learning (ML) be used to improve on existing cache replacement strategies? We propose a general framework called LeCaR that uses the ML technique of regret minimization to answer the question in the affirmative. We show that the LeCaR framework outperforms ARC using only two fundamental eviction policies, LRU and LFU, by more than 18x when the cache size is small relative to the size of the working set.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">The Case for ML in Cache Management</head><p>Can machine learning (ML) be used to learn from the best cache replacement policies? This question was answered in the affirmative as far back as 2002 by the ACME system by Ari et al. <ref type="bibr" target="#b1">[2]</ref>, which assumes that there exist a pool of multiple "experts" (i.e., strategies). Each expert had an associated weight and the expert with the highest weight made the recommendation for an item to be evicted on a miss. The weight of an expert at any given time was a function of its recent performance. ACME improved on the performance of LRU and LFU. Since 2002, both cache replacement and machine learning have seen major advances (e.g., adaptive replacement <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>; regret minimization and online learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>), suggesting a revisit of this question.</p><p>Can ML improve on existing cache replacement strategies? In this paper we answer this question in the affirmative. This is non-trivial given the diversity of data sets for which ARC has been shown to perform at or close to the best among its competitors. We identify one area where ARC may not have been adequately put through its paces, i.e., when cache is much smaller than the working set. Although ARC may outperform other algorithms even for small cache sizes, when a "stable" working set does not fit in cache, it suffers a loss in performance (referred to as the proverbial "ARChilles' Heel" (sic) <ref type="bibr" target="#b28">[29]</ref>), leaving room for further improvements.</p><p>Can we design "scalable" ML solutions for cache replacement? Scalable solutions can maintain optimal performance regardless of workloads. It has been shown that working sets are telescoping in nature with larger working sets fully containing one or more smaller working sets, motivating the utility of caches at various granularities <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Although a one-time correct sizing of cache memory is not a trivial problem, it is possible to provision sufficient cache memory relative to the entire working set in order to maintain acceptable performance. However, this may not be achievable because (a) caches are expensive; (b) small caches are par for the course (e.g., in mobile and IoT devices); (c) workloads are highly consolidated and dynamically change over time, and it may be unaffordable to provision for peak workloads; (d) it may be impossible to anticipate all the possible workloads the system would have to face in the near future; and (e) reprovisioning hardware caches is either infeasible or not straightforward. Thus, we need caching algorithms that scale well when workloads get larger relative to cache sizes. <ref type="figure" target="#fig_0">Figure 3</ref> shows that our ML-based LeCaR (Learning Cache Replacement) is competitive with ARC for relatively large cache sizes, but is markedly superior to it when cache sizes become smaller. Given the "online" nature of the cache replacement problem, we use techniques from the subarea of online learning with regret minimization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. LeCaR outperforms ARC by as much as 18x in hit rate across the 8 production storage I/O workloads from FIU <ref type="bibr" target="#b25">[26]</ref> when caches are set to 0.1% of workload size and up to 30.5% when caches are at 1% of workload size. For larger caches LeCaR performs competitively relative to ARC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Motivation</head><p>"Single gear" strategies are limited. The best-known strategies for cache replacement are LRU and CLOCK <ref type="bibr" target="#b8">[9]</ref>, both of which tend to retain pages with high recency, and LFU, which retains pages based on how frequently they have been referenced. These static strategies cannot adapt to changes in workloads and fail to have good allround performance, especially when recent pages are not frequently accessed or when pages are accessed a number of times and then lapse into long periods of infrequent access. In practice, LRU can evict frequently accessed pages, while LFU can evict fairly recent pages while "hoarding" entries that were frequently accessed in the distant past.</p><p>"Adapting" to the road. A common theme in most improved algorithms is that they combine actions based on recency and frequency. A spectrum of adaptive algorithms combining the strengths of LRU, LFU, and/or CLOCK have been proposed. These include LRFU <ref type="bibr" target="#b21">[22]</ref>, DUELINGCLOCK <ref type="bibr" target="#b14">[15]</ref>, LRU-K <ref type="bibr" target="#b24">[25]</ref>, LIRS <ref type="bibr" target="#b16">[17]</ref>, CLOCK-PRO <ref type="bibr" target="#b15">[16]</ref>, 2Q <ref type="bibr" target="#b18">[19]</ref>, and more. The best among these were the two adaptive algorithms called Adaptive Replacement Cache (ARC) <ref type="bibr" target="#b23">[24]</ref> and CLOCK with Adaptive Replacement (CAR) <ref type="bibr" target="#b2">[3]</ref>. The core idea behind ARC and CAR was that they separated the recent pages accessed only once from the frequent pages into two partitions of the cache, and used clues from a limited set of history pages to decide the relative sizes of the partitions. The two partitions were managed as FIFO queues and eviction decisions were made using a complex set of deterministic conditions. An alternative view is that ARC and CAR reduced eviction to a choice between LRU and a version of LFU that does not differentiate between entries that are accessed more than twice.</p><p>Driving down Machine Learning avenue. Theoretical research is limited in its ability to distinguish between LRU and ARC (and CAR as well) <ref type="bibr" target="#b7">[8]</ref>. Researchers continue to actively pursue ways to improve the algorithms. A natural question is: can Machine Learning and related predictive techniques help to improve cache replacement algorithms. Predicting branchings <ref type="bibr" target="#b17">[18]</ref> and time before re-referencing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> have been tried; other soft computing techniques such as Neural Networks (NN), genetic algorithms (GA), Classification and Regression Trees (CART), Multivariate Adaptive Regression Splines (MARS), Random Forest (RF) and TreeNet (TN) have also been used for this problem <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30</ref>] with limited success. Adaptive caching with ML was attempted <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12]</ref> and they are known to outperform only the "static" strategies, LRU and LFU. Also, they are expensive to implement and were not pursued. The main goal here was to overcome the cost of implementation while outperforming more advanced algorithms developed since that work.</p><p>Turning into "online learning" lane. An appropriate model of machine learning for cache replacement is that of Reinforcement online learning (RL), which require that decisions be made online as requests arrive and that cumulative regret (or reward) be optimized. Reinforcement online learning constantly acquires new knowledge and adapts to changes in input characteristics <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31]</ref>. Cache replacement has been modeled as a Multi-Armed Bandit (MAB) problem <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> for highly specialized caches, but has not been adequately evaluated in standard settings <ref type="bibr" target="#b3">[4]</ref>. LeCaR explores online learning with variants of MAB.</p><p>This brings us to the proposed ML-based algorithm called LeCaR, which uses reinforcement online learning with regret minimization. It is important to note that the regret minimization approach allows room for theoretical guarantees of performance to go with its performance in practice. However, the theoretical framework is different from a regular online learning method because the feedback about the quality of the decision made at any given time is delayed and not instantaneous.</p><p>The high octane fuel. While the online learning provides the scaffold, the secret of success of the LeCaR framework lies in "combining" two fundamental, yet orthogonal, policies -recency and frequency. Even though LFU is much reviled and regularly puts up poor performances in its pure form, its contribution (weight) as managed by LeCaR is often very high, suggesting that LFU is a blunt but powerful tool that needs a regular infusion of cleanup best provided by the orthogonal recency policy. LeCaR allows items with high frequency to be efficiently evicted based on their recency. Surprisingly, using LFU with decay instead of pure LFU lowers the efficacy of LeCaR. Furthermore, replacing LRU with ARC also lowers the efficacy of LeCaR. One possible explanation is that both ARC and LFU with decay tamper with pure LFU, destroying its orthogonality from native LRU.</p><p>Our work also suggests that for every workload there is some combination (i.e., a probability distribution) of LRU and LFU that can handle it as well as any other deterministic cache replacement scheme. ARC may be combining recency and frequency in artful ways, but does not have access to the full probability distribution that our proposed LeCaR framework has.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Peeking under the hood of LeCaR</head><p>Unlike ACME, we do not assume that our system has access to a set of best strategies (i.e., experts). Note that simulating a collection of experts can be an expensive proposition. Instead, LeCaR assumes that at every instant, the workload is best handled by a judicious "mix" (i.e., a probability distribution) of only two fundamental policies: recency-based and frequency-based evictions. Thus the goal of LeCaR is to "learn" the "optimal" probability distribution for every state of the system. Unlike ACME, LeCaR maintains a probability distribution of two policies instead of a probability distribution of a panel of expert strategies. Surprisingly, this minimalist approach to learning achieves outstanding results.</p><p>Another distinguishing feature of our system is that the weight associated with the two policies is not a function of their current hit rate, but of the current associated regret. Thus, we model cache replacement as an online learning problem involving regret minimization <ref type="bibr" target="#b30">[31]</ref>. To handle any cache miss, one of the two policies is chosen at random (probabilities derived from their associated cumulative regret values due to the misses they "caused").</p><p>In order to manage regret, the cache manages a FIFO</p><formula xml:id="formula_0">Algorithm 1: LeCaR(LRU,LFU) Input: requested page q if q is in C then C.UPDATEDATASTRUCTURE(q) else if q is in H LRU then H LRU .DELETE(q) else if q is in H LFU then H LFU .DELETE(q) UPDATEWEIGHT(q, λ , d) if (Cache C is full) then action = (LRU, LFU) w/ prob (w LRU , w LFU ) if (action == LRU) then if H LRU is full then H LRU .DELETE(LRU(H LRU )) H LRU .ADD(LRU(C)) C.DELETE(LRU(C)) else if H LFU is full then H LFU .DELETE(LRU(H LFU )) H LFU .ADD(LFU(C)) C.DELETE(LFU(C)) C.ADD(q)</formula><p>history of metadata on the most recent evictions from the cache. When an entry is evicted from the cache it is moved to history. The number of entries in the history (as with ARC) is equal to the number of entries in the cache. Each history entry is labeled by the policy that evicted it from the cache. A decision is considered "poor" if a request causes a miss and if the requested page is found in history. The intent is that a miss found in history could have been rectified by a more judicious eviction, and hence the regret. Regret is graded and is larger if it entered the history more recently. When poor decisions are caused by a specific policy, that policy is penalized by increasing the "regret" associated with it. (See Algorithm 1.)</p><formula xml:id="formula_1">Algorithm 2: UPDATEWEIGHT(q, λ , d)</formula><p>Input: page q, learning rate λ , discount rate d t := time spent by page q in History r := d t if q is in H LRU then w LFU := w LFU * e λ * r // increase w LFU else if q is in H LFU then w LRU := w LRU * e λ * r // increase w LRU w LRU := w LRU /(w LRU + w LFU ) // normalize w LFU := 1 − w LRU Algorithm 2 calculates the weights. The weights start off being equal, although they may be initialized using <ref type="figure">Figure 1</ref>: Switching between favorable sequences some a priori information about the algorithms or the request sequence. When a "regrettable" miss is attributable to LRU (resp. LFU), because it is found in history H LRU (resp. H LFU ), the weight of the "other" policy, i.e., LFU (resp. LRU) is updated as recommended by regret minimization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>. Note that λ is the learning rate (initially 0.45), d is the discount rate (initially 0.005 1/N , where N is the cache size), and reward, r = d t . Performing a sensitivity analysis proves that the algorithm is robust to the learning rate and discount rate parameter and we and chose ones that worked reasonably well across the eight workloads we experimented with. Finally, the algorithm returns the weight of LRU and LFU, which are used to choose one of the two policies to apply probabilistically for the next miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Here we present a series of experiments designed to argue the viability of the ML framework presented above.</p><p>Does LeCaR learn? How quickly? We start with some simple experiments using synthetic data to establish LeCaR's ability to learn. We synthetically generated sequences that periodically switch back and forth from being favorable to LRU to being favorable to LFU. During the phase when it is favorable to LRU, it generates requests that deliberately cause a hit for LRU and a miss for LFU, and vice versa. The generator also includes a small amount (0.5%) of noise, i.e., requesting pages from the universe that are neither in LRU nor in LFU's cache. For these data sets, size of the cache was set at 500 entries, with a universe of pages of size 15 times the size of the cache. <ref type="figure">Figure 1</ref> shows a plot that is broken into four equal sections, each partitioned by blue vertical lines. Each <ref type="figure">Figure 2</ref>: Impact of phase change partition corresponds to a sequence that is favorable to one of LRU or LFU, as described above. The lower part of the figure shows the hit rates with each of the four algorithms under study. Hit rates for the four algorithms are represented by curves in four different colors as indicated. (Note that hit rates are computed over a sliding window of size 500.) The upper part of the plot shows the weights of LRU and LFU as LeCaR progresses through the request sequence. Note that the hit rates of ARC and LeCaR are very close to each other. The lower plot shows how quickly ARC and LeCaR adapt to the change. The learning for LeCaR is also reflected in fluctuations in the weights w LFU and w LRU . In additional experiments, we show that the learning occurs even if there are more partitions than 4, suggesting that the learning is relatively robust to the frequency of these switches. After experimenting with different learning rates λ in LeCaR for a variety of different workloads, we settled on a fixed learning rate of 0.45 for all the experiments.</p><p>Does LeCaR adapt to phase changes? In real data sets, phase changes are a frequent occurrence, where all or part of the working set gets replaced. Our next set of experiments study adaptiveness of the algorithms to different levels of phase change. As above, we show only one sample plot that explains the general behavior.</p><p>For these experiments, cache size was set at 50, with a universe of 2000 pages. Working set size was set at 30, and on each phase change 90% of the working set remained unchanged. Each phase had 5000 requests. <ref type="figure">Figure 2</ref> shows the results of the experiment. Again lower plot shows hit rates and upper plot reflects the inner workings of LeCaR with the weights of the two policies. Phase change causes a dip in the hit rate of all the algorithms. There is not much difference in the rate at which ARC and LeCaR recover, but LeCaR's recov- ery rate can be regulated with the learning rate (data not shown).</p><p>The "road tests" with LeCaR. We used multi-day, block-level, production storage I/O FIU Traces <ref type="bibr" target="#b25">[26]</ref> for our experiments. They include traces from an email server (mail workload), a virtual machine running two web servers (web-vm workload), and several user machines at the School of Computing and Information Sciences at FIU, collected for a duration of three weeks. The data sets labeled casa, ikki, madmax, topgun are workloads from home directories; data set online is from the server hosting the departments online course management system; webresearch is a document store for research projects; webusers is the home-pages of faculty/staff/students; and mail is the department's mail server.</p><p>We discuss our findings with Day 3 (a Monday) of each trace first. LeCaR outperforms ARC by as much as 18x in hit rate across the 8 production workloads from FIU <ref type="bibr" target="#b25">[26]</ref> when caches are 0.1% of the workload size and by -4% to 30.5% when caches are 1% of the workload size. For larger caches LeCaR performs competitively relative to ARC (within 0.33%). only 4 of the 8 traces; the remaining four were omitted due to lack of space. Two observations are obvious. When cache sizes are high (1%) relative to the size of the workload, then all the top performers including LeCaR perform more or less on par. When cache sizes are low (≤ 0.5%), LeCaR outperforms all the other competitors.</p><p>Similar characteristics were observed when we ran longer experiments with data sets representing days 1 through 7 from the collection mentioned above (data not shown). For most real data sets saturation of the hit rate seems to happen somewhere between cache sizes of 1% to 5% of the workload sizes.</p><p>Hoarding for better performance? In order to better understand the behavior of LeCaR, we introduce the concept of hoarding rate. It is defined as the percentage of entries in cache that have been accessed at least twice in the past since entering the cache, but is not among the last 2N unique pages accessed. By definition, LRU has zero hoarding rate because every page in the cache (including those with frequency ≥ 2) is among the last N pages accessed. LFU tends to have high hoarding rates because it cannot discard items with higher frequency as long as there is at least one lower frequency entry. LFU hoarding rate does not often decrease, except when hoarded pages are requested. Algorithms like ARC and LeCaR do selective hoarding, hedging their bets that some of these pages are likely to be accessed in the future. Our goal was to explain algorithm behavior by observing their hoarding behavior. See <ref type="figure">Fig. 4</ref>; here hoarding curves are smoothed by averaging over a sliding window to avoid distractions of frequent fluctuations.</p><p>We discuss regions labeled A, B, C, and D in <ref type="figure">Figure   4</ref>. In A, LeCaR is hoarding more than the other algorithms, but it is a relatively stable period where the hoarding is paying off in terms of higher hit rates. In B, LFU gets penalized because of prior poor choices (reflected by lowering of its weight), and LeCaR reacts by applying more recency criteria, thus getting rid of much of its hoarded pages. After an initial dip in hit rate, it recovers and tracks the performance of ARC, which uses its own mechanisms to react to the situation in B, possibly by evicting items from its high frequency queue (T 2 ). In C, some (frequent) pages are being requested after a long time, reflected by higher hit rate for LFU and a dip in its hoarding (as with LeCaR and ARC). However, the increase in weight for LFU pays off handsomely for LeCaR, which sees the highest increase in performance over its competition. D is similar to A in terms of the stability of the weights, except that the higher hoarding rates of all the algorithms is reflected in more similar hit rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Conclusions</head><p>Large caches do not benefit from strong replacement strategies since working sets are already in cache; all good strategies perform roughly equally with insignificant differences. When cache sizes are small, subtleties of the replacement algorithms are observable. LeCaR relies strongly on frequency, which is important to effective cache replacement. However it tempers its reliance by using randomization and recency to clean up stale but frequent items in the cache. LeCaR manages two data structures of metadata for each cache entry, i.e., recency and frequency. A naive implementation of LeCaR will incur a space overhead of 3x over ARC. A more careful implementation will reduce this overhead to 2x. Futhermore, providing 2x additional metadata space does not improve ARC performance -on the contrary, for small cache sizes, doing so unexpectedly hurts ARC's performance. In conclusion, the reinforcement online algorithm with regret minimization when applied in a novel way to pure LRU and LFU policies results in high performance cache replacement. LeCaR boasts up to 18x improvement over the top competitor ARC for production storage I/O traces when caches are (1/1000)th of the workload size. Interestingly, the gap between LeCaR and ARC widens when size of the cache (relative to workload) decreases, suggesting that LeCaR is scalable to much larger workloads. We proposed hoarding rate as a means to understand the relative behavioral properties of these caching algorithms and to generate new insights into cache replacement analysis. The design of LeCaR is minimalist and uses only two policies -the vanilla LRU and LFU (without decay). If further improvements beyond LeCaR are to be achieved, other orthogonal measures must be identified.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LeCaR outperforms ARC and LRU at lower cache sizes and is competitive at high cache sizes for workloads from FIU Traces. X-axis: Cache size as a fraction of the workload's size. Y-axis: Cache hit rate. Details in Sec. 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 representsFigure 4 :</head><label>34</label><figDesc>Figure 4: Using hoarding rates to explain performance for madmax workload (day 3).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>We thanks the anonymous reviewers and our shepherd, Ioan Stefanovici, for their insightful feedback. This work is supported in part by NSF awards, CCF-1718335, CNS-1563883 and CNS-1562837, and a NetApp Faculty Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Performance improvement of least-recently-used policy in web proxy cache replacement using supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sulaiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advances in Soft Computing &amp; Its Applications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive caching using multiple experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Acme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WDAS</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="143" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CAR: CLOCK with adaptive replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bansal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Modha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX Conference on File and Storage Technologies</title>
		<meeting>the 3rd USENIX Conference on File and Storage Technologies<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="187" to="200" />
		</imprint>
	</monogr>
	<note>FAST &apos;04, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning-based optimization of cache content in a small cell base station</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blasco</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And G ¨ Und¨uzund¨ Und¨uz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1897" to="1903" />
		</imprint>
	</monogr>
	<note>Communications (ICC)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From external to internal regret</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansour</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1307" to="1324" />
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Regret analysis of stochastic and nonstochastic multi-armed bandit problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bubeck</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Al</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A ga-based cache replacement policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Z</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 2004 International Conference on</title>
		<meeting>2004 International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="263" to="266" />
		</imprint>
	</monogr>
	<note>Machine Learning and Cybernetics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Consuegra</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vietri</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07624v2</idno>
		<title level="m">Analyzing adaptive cache replacement strategies</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A paging experiment with the MULTICS system. Tech. rep., DTIC Document</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corbato</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online convex optimization in the bandit setting: gradient descent without a gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flaxman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mcmahan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the sixteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foster</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridharan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3375" to="3383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive caching by refetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gramacy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1489" to="1496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High performance cache replacement using rereference interval prediction (rrip)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaleel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">High performance cache replacement using rereference interval prediction (rrip)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaleel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Steely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th Annual International Symposium on Computer Architecture</title>
		<meeting>the 37th Annual International Symposium on Computer Architecture<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dueling CLOCK: adaptive cache replacement policy based on the CLOCK algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janapsatya</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignjatovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Peddersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parameswaran</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="920" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CLOCK-Pro: An effective improvement of the CLOCK replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="323" to="336" />
		</imprint>
	</monogr>
	<note>General Track</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">LIRS: An efficient low interreference recency set replacement policy to improve buffer cache performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Sigmetrics Conf</title>
		<meeting>ACM Sigmetrics Conf</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural methods for dynamic branch prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiménezjim´jiménez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="369" to="397" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2Q: A low overhead high performance buffer management replacement algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shasha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of VLDB</title>
		<meeting>of VLDB</meeting>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized ERSS Tree Model: Revisiting Working Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koller</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangaswami</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IFIP Performance</title>
		<meeting>of IFIP Performance</meeting>
		<imprint>
			<date type="published" when="2010-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimating Application Cache Requirement for Provisioning Caches in Virtualized Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koller</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangaswami</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE MASCOTS</title>
		<meeting>of IEEE MASCOTS</meeting>
		<imprint>
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A spectrum of policies that subsumes the least recently used and least frequently used policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Lrfu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="1352" to="1361" />
			<date type="published" when="2001-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The weighted majority algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Littlestone</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warmuth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and computation</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="212" to="261" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ARC: A self-tuning, low overhead replacement cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megiddo</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Modha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd USENIX Conference on File and Storage Technologies</title>
		<meeting>the 2nd USENIX Conference on File and Storage Technologies<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="115" to="130" />
		</imprint>
	</monogr>
	<note>FAST &apos;03, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">The LRU-K page replacement algorithm for database disk buffering. SIG-MOD Rec</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;neil</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>O&amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weikum</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1993-06" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">I/o deduplication: Utilizing content similarity to improve i/o performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename></persName>
		</author>
		<idno>FAST 10</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th USENIX Conference on File and Storage Technologies</title>
		<meeting>8th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online learning: Beyond regret</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakhlin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tewari</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Learning Theory</title>
		<meeting>the 24th Annual Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural network proxy cache replacement strategy and its implementation in the squid proxy server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romano</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elaarag</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computing and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="59" to="78" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">To arc or not to arc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santana</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<editor>HotStorage</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intelligent web caching using machine learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sulaiman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shamsuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sulaiman</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Network World</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">429</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinkevich</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML-03</title>
		<meeting>the 20th International Conference on Machine Learning (ICML-03</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="928" to="936" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
