<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:49+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Soroban: Attributing Latency in Virtualized Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Snee</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucian</forename><surname>Carata</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><forename type="middle">R A</forename><surname>Chick</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ripduman</forename><surname>Sohan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramsey</forename><forename type="middle">M</forename><surname>Faragher</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rice</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Hopper</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Soroban: Attributing Latency in Virtualized Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Applications executing on a hypervisor or in a container experience a lack of performance isolation from other services executing on shared resources. Latency-sensitive applications executing in the cloud therefore have highly-variable response times, yet attributing the additional latency caused by virtualization overheads on individual requests is an unsolved problem. We present Soroban, a framework for attributing la-tency to either the cloud provider or their customer. Soroban allows developers to instrument applications, such as web servers to determine, for each request, how much of the latency is due to the cloud provider, and how much is due to the consumer&apos;s application or service. With this support Soroban enables cloud-providers to provision based on acceptable-latencies, adopt fine-grained charging levels that reflect latency demands of users and attribute performance anomalies to either the cloud provider or their consumer. We apply Soroban to a HTTP server and show that it identifies when the cause of latency is due to a provider-induced activity, such as underprovisioning a host, or due to the software run by the customer.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The rise of cloud computing can primarily be attributed to multi-tenant hosting. Typically, multiple services are co-located together with the hypervisor or containerization mechanism scheduling services as necessary. While co-hosting increases utilization, it introduces a number of disadvantages. In particular: (i) The hypervisor introduces an additional (un-coordinated) level of indirection with respect to process scheduling. This property makes the service time of requests unpredictable. (ii) The service performance is influenced by overall system state of which individual services have no information, e.g. number of other services hosted, time scheduled in and contention on shared resources. Given the lack of mutual performance isolation, it is common for co-located services to affect each other's performance. While there has been previous work in improving the scheduling problem <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, the lack of performance isolation remains. This has lead to a situation where pinpointing the reasons for performance irregularities in cloud environments can be challenging. In particular, it is difficult to ascertain whether performance anomalies are being caused by the behaviour of the running service (e.g. garbage collection) or if they are due to external artifacts (e.g. CPU starvation).</p><p>Current techniques for characterizing system-wide effects in cloud platforms rely on benchmarking virtual machines to measure their performance. However, the utility of benchmarking is reduced by several factors: (i) benchmarks are typically not representative of true workloads <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref> (ii) benchmarks do not typically model real-world events that may significantly impact performance (e.g. domain creation, boot storms, a domain being slashdotted) and (iii) while benchmarks reveal a measurement of the throughput or latency of the system, they do not provide a root-cause analysis for performance anomalies.</p><p>To this end, we present Soroban, a framework for attribution of latency to either the cloud provider or their customer. With Soroban programs executing on virtualized hardware can be instrumented to indicate the semantics of processing actions, for example serving a request. Soroban then monitors the servicing of each action in the system and records all latency-influencing actions performed on the virtual machine or container by the cloud provider (e.g. an HTTP request taking a VM exit whilst being serviced). Furthermore, Soroban reports how much of the latency in serving each individual action is attributable to the cloud provider and how much is due to the consumer's software by comparing online latency against a machine-learning model constructed from running the application in a non-virtualized environment. We presently have implementations of Soroban for the Xen hypervisor <ref type="bibr" target="#b4">[5]</ref> and Linux Containers.</p><p>In summary the contributions of this paper are:</p><p>1. We show that by exposing the quality of service given to a virtual machine or container we can use supervised machine learning to determine the effect of virtualization on the latency of individual actions, such as serving HTTP requests ( §3).</p><p>2. We evaluate Soroban, showing that it can attribute an increase in server latency to cloud-provider actions, such as performing an antivirus scan ( §4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Soroban</head><p>Soroban allows services running in a virtualized environment to determine how much of their server-side latency is attributable to the underlying cloud infrastructure and how much is due to their own software stack. For instance, if a cloud provider creates a boot storm it induces latency on its clients, whereas if the load being served by the virtual machine increases, the latency will be caused by the client. Users deploy Soroban by instrumenting their application using the Soroban API. With this API services are able to receive event information specific to the underlying virtualization platform (e.g. in Xen we provide VM entry and VM exit events). Associated with every event we provide enough resource consumption information (e.g. cycle-count at time of event) so that Soroban can reconstruct the impact on latency of the hypervisor or container. Thus, services are able to monitor system-wide effects that impact the performance of their actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Motivating Soroban</head><p>We highlight three motivating scenarios where Soroban can be applied in production cloud environments:</p><p>Dynamic allocation. With Soroban, consumers can specify the upper bound on performance overhead that is tolerable. This allows the provider to react to users in a more accurate manner by allocating resources according to price and demand.</p><p>QoS-based, fine-grained charging. Current charging models are coarse-grained. Physical resources are priced at the unit level and users pay on a per-unitconsumed basis. However, this model does not account for the quality-of-service that is finally provided. A user is charged the same amount per-unit regardless of the effects imposed by the hypervisor. For example, a user will pay the same amount to service two HTTP requests even if one takes twice as long to complete due to hypervisor delays. With Soroban it is possible for providers to explore flexible new pricing models that set price points as a function of user demand and overall system-imposed</p><formula xml:id="formula_0">1 int sd = srbn_start () ; 2 ... 3 srbn_yield ( sd ) ; 4 ... 5 srbn_resume ( sd ) ; 6 ... 7 srbn_end ( sd ) ; 8</formula><p>auto &amp; srbn_data { srbn_read ( sd ) };</p><p>Figure 1: The Soroban API allows applications to be instrumented to mark the start, and end of requests.</p><p>delay, leading to more accurate and representative charging models.</p><p>Attributing performance anomalies. Slow server responses are a principal component of end-to-end latency for client-server systems <ref type="bibr" target="#b8">[9]</ref>. Soroban attributes slow responses to either the cloud provider or the customer software stack. Consumers can use this information to purchase more computing resource or focus their efforts on modifying their software. Soroban enables accurate pinpointing of performance anomalies at a request level enabling both providers and users to obtain a detailed fingerprint on online service performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation</head><p>We currently maintain two implementations of Soroban: one for virtual machines running on Xen and one for Docker containers. Most of the code is shared between those implementations, keeping the virtualizationspecific code targeted at capturing specific Xen or container scheduling events. In this section we outline the implementation details necessary to support Soroban on these platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design</head><p>Applications are instrumented to mark the start and end of each action by adding a call to the Soroban library. The Soroban library tracks the scheduling events performed by Xen or the Linux kernel using regions of memory shared either with the hypervisor or with the Linux kernel. At the end of processing a request the application can read back all those events that affected its performance. Moreover, the Soroban library provides a measure of how much of that request's server-side latency is caused by executing in the cloud, rather than on bare metal. This measure is ideally independent of load in the virtual machine, or container, and dependent on the state of the host machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Service API</head><p>Applications interact with Soroban via an API, designed to be minimally invasive and requiring little integration effort. <ref type="figure">Figure 1</ref> outlines the API. Services use srbn start (1) and srbn end (7) to start and stop tracking resource consumption. It is expected that the service will use those calls to signal a logical event where resource allocation for a particular action (e.g. serving a HTTP request) will start and stop.</p><p>A call to srbn start (1) returns an unique token for identifying the current application action to which measurements should be attributed.</p><p>Services distinguish between simultaneous logical events through multiple calls to the start and end operations, thereby enabling differentiation based on service classification (e.g. per user). A single user-level thread can differentiate between logical actions via the srbn yield (3) and srbn resume (5) calls, which allow user-level applications to signal that subsequent system calls should be attributed to a different logical event <ref type="bibr" target="#b2">[3]</ref>. User-level applications access a single logical event's resource usage via the srbn read (8) call which returns a breakdown of the resources consumed by the hypervisor. In particular, applications access a single srbn data-&gt;cloud latency field which contains the amount of latency for the event that is attributable to the hypervisor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Xen Implementation</head><p>We modify Xen to export to each domain data about all scheduling events on each of its vCPUs. In particular at every VM entry, and VM exit, we expose a timestamp, cycle count, the credit of the vCPU, the number of pending events in the event channel, the number of yields, and the number of blocks. These data are exposed to the guest using shared hypervisor memory, and are read by the guest Soroban API. We therefore do not add the overhead of increased interrupts on each vCPU scheduling activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Docker Implementation</head><p>A Linux scheduler instrumentation layer (∼ 200 LoC) interposes the host OS and tracks scheduling events that interact with the Soroban-enabled processes. The aggregated events are accessible to the Soroban library via shared memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Latency Attribution</head><p>Soroban uses data from Xen and Docker to attribute proportions of the latency of each individual action to the cloud provider or the user's software stack. The solid line is x = y, the line of complete attribution (time scheduled out equals server-side latency). The two major clusters represent request latencies for which a constant proportion (66% and 30%) can be attributed to the time being scheduled-out.</p><p>In principle, a hypervisor should fairly divide resources amongst contending VMs, with the latency of a particular service increasing proportionally to the time its VM has been scheduled out. Simply measuring the VM scheduled-out time for each application activity could thus be seen as an easy solution to the attribution problem. However, <ref type="figure" target="#fig_0">Figure 2</ref> shows a more complicated story: because of variations in concurrent workloads running on the same infrastructure and the adaptability of the scheduler, the time scheduled out can vary substantially (for example, going from 30% to 66%) without any increases in server-side latency. As a simple example of how this is possible, being scheduled out while a server process is blocked on a file descriptor operation will not add to the final latency.</p><p>Nevertheless, this also implies that looking at a given (latency, sched-out) pair alone, one can not precisely determine how much latency is added by running alongside other services in a virtualized environment. Instead, Soroban uses supervised machine learning to build a slowdown attribution model from multiple metrics, using both PMU data (cycles, cache data, per-subsystem kernel measurements) and scheduler information (time scheduled out, number of blocks, yields, number of events on the event channel). We use a Gaussian process regression that, after training, can map the set of measurements gathered for a given request to a number representing the latency introduced by the cloud provider for that request (or any other application activity).</p><p>The ground truth data for the training is obtained by measuring the shift between the latency distributions of the service running uncontended on bare metal, and it running in a virtualized environment (either Xen or Docker). Concretely, the shift, δ lat is computed by taking the difference between the latency of requests in the virtualized experiment and the latency of requests from the bare-metal experiment that are in the same percentile.</p><p>The Gaussian process regression is then trained to learn the relationships between each multi-dimensional feature vector containing the measurements for a particular request in the virtualized experiment (m 1 , m 2 , m 3 , ...) and the corresponding δ lat .</p><p>To ensure accurate predictions from the model, virtualized measurements are taken whilst increasing the number of other guests on the server, introducing resource contention with a mixed IO and CPU load. The whole training process need only be performed once at a coarse-grained level, based on all requests served by the virtual machine. However, whenever there are classes of requests that execute differing loads on the hypervisor, a new training set could be taken for increased precision.</p><p>The final output of this process is a model that when given a new vector of measurements, returns the predicted latency increase attributed to the current virtualization load, and a confidence interval. This scheme is robust to differences between bare-metal environments and virtualized environments-such as a micro VM executing on a high-end server-in that Soroban reports a large overhead imposed by virtualizing the application, which can be reduced by purchasing a larger VM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section we focus on identifying how well our proposed slowdown attribution model works in a controlled setup that mimics some of the conditions present in cloud environments, like the contention on resources or the existence of periodic running tasks. We use a controlled setup as some important metrics (scheduling information, pending interrupts) are not currently accessible to the application in third party cloud solutions. However, the proposed model itself is sufficiently general to allow for extensibility and consider more complex scenarios (feature vectors with more dimensions, realistic workloads and contention). We explore how changes in the distribution of request latencies can be attributed to the cloud provider or to client application/VM activities.</p><p>We run all workloads on a machine with an Intel Xeon E3-1230 V2 @ 3.3 GHz and depending on the virtualization type being evaluated, either Xen-unstable (compiled from source) in PV mode or Docker containers. In all cases we use Ubuntu 14.10 with the 3.19 upstream kernel compiled from the Linus branch. For Xen, the unstable branch has been chosen to make use of the Credit2 Scheduler, which is optimized for low latency.</p><p>We instrument lighttpd with the Soroban API to report when individual requests start, stop being processed or are multiplexed by the main event loop. At runtime, <ref type="figure">Figure 3</ref>: As the load on hypervisor/containers system increases, there is an increase in the latency at which requests are served. Soroban only identifies the cause of the latency as being due to the cloud provider when the hypervisor or host OS is under load.</p><p>Soroban can aggregate per-request resource consumption metrics and events, from both the OS and the hypervisor. This data can be read back in real time by lighttpd after a response has been sent or alternatively saved for later analysis. This builds the per-request feature vectors required by our latency attribution model. We then perform the training stage outlined in §3.5.</p><p>Understanding request slowdowns. In order to verify that our model is reasonable and useful, we run experiments by varying the number of virtualized guests running on the same host (and executing CPU-intensive tasks) and the load within the guest running lighttpd and serving requests. In each case we obtain measurements for approximately 50000 requests, and use the regression from the training phase to predict how much of the observed tail latency is due to the virtualization.</p><p>The results are summarized in <ref type="figure">Figure 3</ref>. Each cell shows the estimated number of cycles that are attributable to the cloud provider for increasing the tail (99 th percentile) latency of requests. Similar heatmaps could be used to investigate the causes of shifts in median latency or in other measured features.</p><p>Using the full Soroban results, a complete image describing the slowdown caused by the hypervisor can be obtained for the entire lighttpd latency distribution, with plots similar to <ref type="figure" target="#fig_1">Figure 4</ref> being available for each cell in the heatmap.</p><p>The request latency attributed to both Xen and Docker, as reported by Soroban, increases proportionally to the hypervisor contention (more VMs and containers), for any given guest load (every column in <ref type="figure">Figure 3)</ref>. In addition, for a given number of concurrent VMs or containers, as the load on the virtual machine increases, we do not see a significant change in attribution metrics, suggesting that Soroban does not attribute increases in guest load to the cloud provider.</p><p>Periodic events. Using the same training dataset, we also explore whether common repetitive tasks that exe- cute on other VMs or in dom0 and slow down requests can be detected and properly classified using our model. For this purpose, we compare the slowdown caused by the hypervisor in two cases ( <ref type="figure" target="#fig_1">Figure 4</ref>): The first case-running a realistic mix of CPU/Network and I/O workloads on sixteen other VMs sharing the same host, but keeping overall contention low (30%); The second case-running a periodic virus scan (clamav) in dom0. As expected, we are able to see a shift in the hypervisorinduced latency, with the 90 th percentile shifting from 36 ms to 47 ms (with lighttpd serving 500 KB files).</p><p>In the general case, we believe that the ability to understand fine-grained behavior under virtualization, such as in the examples above would offer ample opportunities for application optimization in cloud environments. The overheads imposed by Soroban on lighttpd are not statistically significant, and would therefore not prohibit any such usecases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Existing cloud service providers bill users based on their use of coarse-grained system resources such as the number of vCPUs or amount of memory. But work has suggested that this often leads to an underprovisioning of resources, something that AutoPro attempts to overcome by provisioning machines based on performance requirements <ref type="bibr" target="#b5">[6]</ref>. Agmon et al. predict new markets for fine-grained performance and resource requirements <ref type="bibr" target="#b0">[1]</ref>, with pricing that encourages users to accurately predict the requirements of their application's actions <ref type="bibr" target="#b1">[2]</ref>. Soroban supports these systems by providing a transparent layer between the provider and customer, allowing the user to quantify the performance impact the provider has on individual system actions, improving billing procedures <ref type="bibr" target="#b13">[14]</ref>.</p><p>The performance impact of virtualization has been well investigated <ref type="bibr" target="#b6">[7]</ref>. Work to overcome this has looked at finding compositions of applications that interact well together <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref>, but this requires accurate indicators of provider induced overhead and often falls back to observing overall system throughput, suggesting that better predictors be found. Understanding the performance of individual actions of a system provides a much more informative description of its behavior, and work has been carried out into investigating variance in request based systems <ref type="bibr" target="#b18">[19]</ref>.</p><p>Whilst existing work can detect cloud providers causing consistent, system-wide overheads, they cannot determine per-action impact on latency over a short time period with unstable interference <ref type="bibr" target="#b17">[18]</ref>, so are only suitable for performance anomalies <ref type="bibr" target="#b9">[10]</ref>. Similarly, techniques for detecting changes in data collected over a long period of time exist <ref type="bibr" target="#b21">[22]</ref>. Soroban's API allows users to annotate actions in the system and propagate IDs similarly to X-Trace <ref type="bibr" target="#b12">[13]</ref>, without requiring behavior traces to be re-built afterwards such as in Magpie <ref type="bibr" target="#b3">[4]</ref>. This provides a much more accurate view of the overhead imposed by service providers on individual actions than external measures such as throughput or coarse-grained system call tracking.</p><p>Soroban exposes the behavior of the underlying hypervisor layer's scheduler to the guest, using a similar technique to existing work that investigates the virtualization of the CPU's PMU <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented Soroban, a framework that allows servers hosted in either a virtual machine, or a container, to determine the latency overhead imposed by the cloud provider on serving individual actions. Soroban exposes additional information from the hypervisor or container system that allows clients to determine the quality of service that they received from their provider. Using this information, Soroban reports the overhead imposed on each action due to being hosted on cloud infrastructure, which may be contended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work was principally supported by internal funds from the Computer Laboratory at the University of Cambridge; and also by the Engineering and Physical Sciences Research Council [grant number EP/K503009/1].</p><p>The raw data and code required for reproducing the figures in this paper is available. <ref type="bibr" target="#b0">1</ref> We aim to release the source code for Soroban along with a more complete technical description as part of a full-length paper in 2016.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion Topics</head><p>Claims: We have shown the technical feasibility of a low-overhead system for collecting and correctly attributing resource consumption in cloud services. This system operates by providing services real-time access to key metrics describing system state.</p><p>Point 1: Useful for datacenter wide resource consumption? Our proof-of-concept implementation only operates on a single host. Whilst existing techniques can account for network delays in serving requests <ref type="bibr" target="#b8">[9]</ref> we should like to attribute resource consumption across all the elements in a datacenter at a logical request level. We believe providing this information to service operators and users will result in increased efficiency for both parties.</p><p>In order to be useful across an entire datacenter we believe it is necessary to instrument all data and control paths. Specifically, we expect that end-to-end resource consumption support will only be possible with the support of intermediate processing elements in the data plane (switches, routers and SAN and NAS devices). We expect the major problems that are required to be solved are: (i) How to represent a logical action as a series of discrete related sub-actions or sub-events. (ii) Correlating resource consumption metrics across these sub-events. (iii) Providing this support with acceptable (ideally ≤ 10%) space and time overheads and (iv) Extending this mechanism across the entire data plane in a homogeneous manner.</p><p>Point 2: A basis for tiered charging? This work can be used as the basis for finer-grained charging models. In particular, we believe it supports the creation of efficient tiered charging models where users can be grouped not only by the amount of resource they require but also by how quickly they require it. While a complete design of such a system would require a supporting economic model to ensure fair and equitable resource allocation, it should be possible to retrofit a simplified implementation of the idea as a basic Dutch auction mechanism in existing platforms.</p><p>Point 3: Pitfalls and limitations in Soroban. Soroban requires that applications are modified to indicate the start and end of every action that should be accounted for. This requires additional engineering effort for both cloud providers and application developers.</p><p>Moreover, we also require a training exercise to be performed with data from running the application on baremetal, executing the relevant codepaths. An alternative to this approach is that the cloud provider temporarily migrates virtual machines onto a dedicated host, and allocates all resources to the virtual machine whilst training Soroban. Rather than reporting the total overheads of virtualization such a scheme would report the overhead <ref type="figure">Figure 5</ref>: Soroban shows the poor performance isolation properties of containers running multiple processes (workers). This limits the ability of Soroban to correctly attribute increases in latency to guest activities.</p><p>of not having purchased the cloud provider's premium service.</p><p>Some of the benefits of Soroban could be achieved even without performing machine learning, at the expense of doing manual statistical analysis on multidimensional measurement data, while also having a good knowledge about the low-level behavior of the application under test.</p><p>Further issues may arise if cloud providers are reluctant to expose hypervisor memory or metrics to guests.</p><p>The lower performance isolation of some virtualization methods is also a concern. For example ( <ref type="figure">Figure 5)</ref> shows the poor isolation of containers when running multiple worker processes to generate load, as opposed to the one worker case seen in <ref type="figure">Figure 3</ref>. This is caused by the kernel scheduler being shared between all containers running on the same machine, and translates into a poor ability to distinguish between service and cloud provider-induced latency (significant changes within every line of the heatmap).</p><p>Point 4: Integration into existing platforms? Integrating Soroban into existing systems is a necessary component for its widescale adoption but it is still an open question whether system-wide adoption of the platform would be feasible across current production cloud platforms. In particular, it is an open question whether there is a business demand for this platform and how it would be integrated into user business models.</p><p>Point 5: Practicality of instrumentation. Soroban requires that developers augment their application with instrumentation that specifies the semantics of processing a request. At the moment this instrumentation is manual and restricted to a single machine. Typically, cloud services have a distributed model with many (virtual) machines serving requests. We are investigating ways of extending Soroban to operate across such distributed systems whilst minimizing the need for instrumentation. For instance, can Soroban be combined with Protocol Buffers or Apache Thrift to infer the start and end of requests?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Baseline Xen scheduling measurements for lighttpd requests, under low resource contention. The solid line is x = y, the line of complete attribution (time scheduled out equals server-side latency). The two major clusters represent request latencies for which a constant proportion (66% and 30%) can be attributed to the time being scheduled-out.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: When the cloud provider executes an antivirus scan the tail latency of serving HTTP requests increases from 36ms to 47ms. Soroban correctly attributes this increase in latency to the cloud provider.</figDesc></figure>

			<note place="foot" n="1"> https://www.cl.cam.ac.uk/research/dtg/rscfl</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The resource-as-a-service (raas) cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agmon</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsafrir</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX Conference on Hot Topics in Cloud Ccomputing</title>
		<meeting>the 4th USENIX Conference on Hot Topics in Cloud Ccomputing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="12" to="12" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;12, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Market-driven memory allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agmon</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Posener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Yehuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And Mu&amp;apos;alem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ginseng</surname></persName>
		</author>
		<idno>VEE &apos;14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments</title>
		<meeting>the 10th ACM SIG-PLAN/SIGOPS International Conference on Virtual Execution Environments<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="41" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Resource containers: A new facility for resource management in server systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banga</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mogul</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Symposium on Operating Systems Design and Implementation</title>
		<meeting>the Third Symposium on Operating Systems Design and Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
	<note>OSDI &apos;99, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using magpie for request extraction and workload modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barham</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mortier</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Symposium on Opearting Systems Design &amp; Implementation -Volume</title>
		<meeting>the 6th Conference on Symposium on Opearting Systems Design &amp; Implementation -Volume<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="18" to="18" />
		</imprint>
	</monogr>
	<note>OSDI&apos;04, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Xen and the art of virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barham</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dragovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neugebauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warfield</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Nineteenth ACM Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="164" to="177" />
		</imprint>
	</monogr>
	<note>SOSP &apos;03, ACM</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated fine-grained cpu provisioning for virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartolini</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santam-Brogio</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Measuring cpu overhead for i/o processing in the xen virtual machine monitor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherkasova</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gardner</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on USENIX Annual Technical Conference</title>
		<meeting>the Annual Conference on USENIX Annual Technical Conference<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="24" to="24" />
		</imprint>
	</monogr>
	<note>ATEC &apos;05, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Comparison of the three cpu schedulers in xen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherkasova</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahdat</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="42" to="51" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The mystery machine: End-to-end performance analysis of large-scale internet services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chow</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Flinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenisch</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation</title>
		<meeting>the 11th USENIX Conference on Operating Systems Design and Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="217" to="231" />
		</imprint>
	</monogr>
	<note>OSDI&apos;14, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perfcompass: Toward runtime performance anomaly fault localization for infrastructure-as-a-service clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Conference on Hot Topics in Cloud Computing</title>
		<meeting>the 6th USENIX Conference on Hot Topics in Cloud Computing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="16" to="16" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;14, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performance profiling of virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sehrawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zwaenepoel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments</title>
		<meeting>the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nfs tricks and benchmarking traps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellard</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seltzer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on USENIX Annual Technical Conference</title>
		<meeting>the Annual Conference on USENIX Annual Technical Conference<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="16" to="16" />
		</imprint>
	</monogr>
	<note>ATEC &apos;03, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A pervasive network tracing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fonseca</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Porter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stoica</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX Conference on Networked Systems Design &amp;#38; Implementation</title>
		<meeting>the 4th USENIX Conference on Networked Systems Design &amp;#38; Implementation<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="20" to="20" />
		</imprint>
	</monogr>
	<note>NSDI&apos;07, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A day late and a dollar short: The case for research on cloud billing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jellinek</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Swift</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Conference on Hot Topics in Cloud Computing</title>
		<meeting>the 6th USENIX Conference on Hot Topics in Cloud Computing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="21" to="21" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;14, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Performance modeling to support multi-tier application deployment to infrastructure-as-a-service clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lloyd</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pallickara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Lyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rojas</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 IEEE/ACM Fifth International Conference on Utility and Cloud Computing</title>
		<meeting>the 2012 IEEE/ACM Fifth International Conference on Utility and Cloud Computing<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
	<note>UCC &apos;12</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Qclouds: Managing performance interference effects for qos-aware clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathuji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghaffarkhah</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th European Conference on Computer Systems</title>
		<meeting>the 5th European Conference on Computer Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="237" to="250" />
		</imprint>
	</monogr>
	<note>EuroSys &apos;10, ACM</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perfctr-xen: A framework for performance counter virtualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaev</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments</title>
		<meeting>the 7th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepdive: Transparently identifying and managing performance interference in virtualized environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novakovi´cnovakovi´</forename><surname>Novakovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vasi´cvasi´ Vasi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Novakovi´cnovakovi´ Novakovi´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosti´ckosti´ Kosti´c</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianchini</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 2013 USENIX Annual Technical Conference (USENIX ATC 13)</title>
		<meeting><address><addrLine>San Jose, CA; USENIX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="219" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated diagnosis without predictability is a recipe for failure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sambasivan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX Conference on Hot Topics in Cloud Ccomputing</title>
		<meeting>the 4th USENIX Conference on Hot Topics in Cloud Ccomputing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="21" to="21" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;12, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The seven deadly sins of cloud computing research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Schwarzkopf</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>And Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th USENIX Conference on Hot Topics in Cloud Ccomputing</title>
		<meeting>the 4th USENIX Conference on Hot Topics in Cloud Ccomputing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;12, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Seawall: Performance isolation for cloud datacenter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shieh</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing</title>
		<meeting>the 2Nd USENIX Conference on Hot Topics in Cloud Computing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;10, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A novel technique for long-term anomaly detection in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vallis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hochenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kejariwal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Conference on Hot Topics in Cloud Computing</title>
		<meeting>the 6th USENIX Conference on Hot Topics in Cloud Computing<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="15" to="15" />
		</imprint>
	</monogr>
	<note>HotCloud&apos;14, USENIX Association</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
