<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 16th USENIX Conference on File and Storage Technologies. Open access to the Proceedings of the 16th USENIX Conference on File and Storage Technologies is sponsored by USENIX. RAID+: Deterministic and Balanced Data Distribution for Large Disk Enclosures RAID+: Deterministic and Balanced Data Distribution for Large Disk Enclosures</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zican</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhufan</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Tsinghua University, † Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zican</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Tsinghua University, † Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlin</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Tsinghua University, † Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhufan</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Tsinghua University, † Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Tsinghua University, † Qatar Computing Research Institute</orgName>
								<orgName type="institution" key="instit2">HBKU</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computing Research Institute</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>February 12-15, ; Xiaosong Ma</addrLine>
									<postCode>2018 •</postCode>
									<settlement>Oakland</settlement>
									<region>CA</region>
									<country>USA, Qatar, HBKU</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 16th USENIX Conference on File and Storage Technologies. Open access to the Proceedings of the 16th USENIX Conference on File and Storage Technologies is sponsored by USENIX. RAID+: Deterministic and Balanced Data Distribution for Large Disk Enclosures RAID+: Deterministic and Balanced Data Distribution for Large Disk Enclosures</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Existing RAID solutions partition large disk enclosures so that each RAID group uses its own disks exclusively. This achieves good performance isolation across underlying disk groups, at the cost of disk under-utilization and slow RAID reconstruction from disk failures. We propose RAID+, a new RAID construction mechanism that spreads both normal I/O and reconstruction workloads to a larger disk pool in a balanced manner. Unlike systems conducting randomized placement, RAID+ employs deterministic addressing enabled by the mathematical properties of mutually orthogonal Latin squares, based on which it constructs 3-D data templates mapping a logical data volume to uniformly distributed disk blocks across all disks. While the total read/write volume remains unchanged, with or without disk failures , many more disk drives participate in data service and disk reconstruction. Our evaluation with a 60-drive disk enclosure using both synthetic and real-world work-loads shows that RAID+ significantly speeds up data recovery while delivering better normal I/O performance and higher multi-tenant system throughput.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>For the past 30 years, Redundant Array of Inexpensive Disks (RAID) <ref type="bibr" target="#b39">[40]</ref> has been used pervasively in servers and shared computing platforms. With paritybased RAID levels (e.g., RAID-5 and RAID-6), users obtain high performance via parallel accesses and reliability via data redundancy.</p><p>With continued advance in disk capacity and slow improvement in speed, however, RAID rebuild time keeps increasing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b52">54]</ref>. For example, a recent NetApp document specifies that a 2TB SATA 7200-RPM disk takes 12.8 hours to rebuild on an idle system <ref type="bibr" target="#b11">[12]</ref>. When performed online on a heavily loaded system, rebuild can take dramatically longer. Such slow rebuild brings two consequences. First, it raises the risk of a second failure and consequently data loss. Second, prolonged recovery subjects foreground applications to long periods of I/O performance degradation. Note that highperformance solid-state drives (SSDs) actually exacerbate this problem, as their growing deployment promotes storage system construction using more high-density, low-performance hard disks <ref type="bibr" target="#b42">[43]</ref>.</p><p>One inherent reason for such slow recovery is that, with conventional RAID, each disk drive involved in RAID reconstruction is read or written entirely. Despite the growing width of RAID arrays (with each array typically containing several to around a dozen disks), the recovery time is determined by reading/writing an entire disk. No matter how many disk arrays coexist in a shared/virtual storage system, resources are isolated between underlying RAID arrays. Idle or lightly-loaded disks cannot offer help to peers in other RAID arrays, who might be overwhelmed by high access traffic, RAID recovery, or, in the worst case, both.</p><p>Many approaches to enhancing the reconstruction performance have been proposed <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56]</ref>, which fall into three categories: 1) designing better data layout in a disk group <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b46">48,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b54">56]</ref>, 2) optimizing the reconstruction workflow <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b52">54]</ref>, and 3) improving the rate control of RAID reconstruction <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">47]</ref>. Most methods focused on a single RAID group, and to our best knowledge, no solution yet has eliminated load imbalance both in normal operations and during RAID reconstruction. While random data placement can utilize larger groups of disks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">51]</ref>, it requires extra book keeping and lookup, and does not deliver load balance within shorter ranges of blocks (crucial for sequential access and RAID rebuild performance, as shown in our experiments). This paper presents RAID+, a new RAID construction mechanism that spreads both normal and reconstruction I/O to effectively utilize emerging commodity enclosures (such as the NetAPP DE6600 and EMC VNX-series) with dozens of or even 100+ disks. Unlike systems conducting random placement, RAID+ employs a deterministic addressing algorithm that leverages the mathe-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association 16th USENIX Conference on File and Storage Technologies 279</head><p>matical properties of mutually orthogonal Latin squares. Such properties allow RAID+ to construct 3-D data templates, each employing a user-specified RAID level and stripe width, that map logical data extents to uniformly distributed disk blocks within a larger disk pool. While the total read/write volume remains unchanged, with or without disk failures, RAID+ enlists many more disk drives in data service and disk reconstruction. This allows it to provide more consistent performance, much faster recovery, and better protection from permanent data loss. In addition, in multi-tenant settings it automatically lends elastic resources to individual workloads' varying intensity, via a flexible and scalable integration of multiple disk groups. We find that this often leads to higher overall resource utilization, though like most schemes for workload consolidation, in the worst case it may incur I/O interference. Such elasticity, combined with the capability of constructing multiple logical volumes adopting different RAID levels and stripe widths within the same physical pool, makes RAID+ especially attractive to cloud and shared datacenter environments employing large disk enclosures/trays.</p><p>We implemented a RAID+ prototype by modifying the Linux MD (Multiple Devices) driver, and evaluated it using a 60-drive disk enclosure. Results show that RAID+ in most cases outperforms both RAID-50 and randomized RAID-5 placement schemes, while offering faster reconstruction (2.1-7.5× over RAID-50, 1.0-2.5× over hash-based random placement). Like randomized placement, it significantly improves overall throughput in multi-tenant environments (average 2.1× over RAID-5). But unlike randomized placement, RAID+'s deterministic addressing allows simple implementation and delivers better sequential performance (for application and rebuild I/O) by guaranteeing uniform data distribution within smaller extents and retaining spatial locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RAID+ Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Latin Square Based Data Organization</head><p>With conventional k-disk RAID arrays, each data stripe is exactly k-block wide (including both data and parity), squarely striking through all disks. The RAID type (level) and stripe width both remain fixed throughout a given disk array. RAID+, instead, uses Latin-squarebased templates to allocate space from a larger n-disk array. A template constructs n×(n−1) k-block stripes, each mapped to a k-subset of the n disks. Different k values and RAID types can be adopted by different templates sharing the same n disks. Like conventional RAID, RAID+ arrays can be hardware-or software-based, offered as RAID+ enclosures with special RAID adapters or formed by software on top of connected disks. <ref type="figure">Figure 1</ref>(a) portrays conventional RAIDs, where disks are physically partitioned into two RAID groups, with potentially different RAID settings. Each disk belongs to one fixed RAID array, except the shared hot spares. One can integrate multiple underlying RAID groups (likely homogeneous in this case) into a logical volume, by concatenating them, or striping data across them. The widely adopted RAID-50, for example, belongs to the latter case. Alternatively, one can build a logical volume on each underlying RAID group, separately serving different workloads sharing the disk pool. These two options are used in our experiments for single-and multiworkload evaluation, respectively.</p><p>With conventional RAID organization, when a failure happens, the recovery process only involves disks within the same RAID group, reading from the k−1 surviving drives and reconstructing lost data on a spare drive in its entirety. As a result, the rebuild speed is capped by the slower between read and write speeds of a single disk. <ref type="figure">Figure 1</ref>(b) shows an alternative approach, where RAID volumes are built by distributing blocks in each k-block RAID stripe to randomly selected k disks. This retains the fault tolerance of RAID yet spreads each volume to all n disks within the pool. <ref type="figure">Figure 1</ref>(c) shows our proposed RAID+, also a flat organization of the same n-disk pool, where two data templates are used to carve space uniformly from all disks. Each template is designed by "stacking" a sequence of k n×n mutually orthogonal Latin squares, whose definition is given in the next section. Each Latin square cell stores a disk ID within [0, n − 1]. Cells at the same location through the k layers then form a k-width data stripe (highlighted). Given such a set of k Latin-squares, one can easily compute the locations of any stripe' blocks, on a k-subset of the n drives. The mathematical properties of mutually orthogonal Latin squares guarantee the uniform data distribution on all working drives, either for normal or single-failure recovery accesses. Since data distribution by each template is always uniform, users can host different RAID organizations within the same ndrive disk pool, such as RAID-5 using the red and RAID-6 using the blue template.</p><p>When a disk failure occurs, both random placement and RAID+ allow all surviving disks to equally participate in reconstruction, cutting theoretical RAID rebuild time to k/(n − 1) of that of conventional RAID. Also, hot spares are optional with these organizations. However, as we shall see later in the paper, RAID+' deterministic and uniform data placement enables it to achieve perfect load balance within smaller address extents and retain spatial locality, both significant advantages (crucial to sequential access and RAID rebuild) over random schemes.</p><p>In addition, a RAID+ pool can perform in an interim mode with multiple disk failures, by continuously main- Stripe Stripe RAID-6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 Stripe RAID 5</p><p>Stri n×n MOLS Data template 1 (c) RAID+ <ref type="figure">Figure 1</ref>: Different ways of utilizing a disk pool much larger than typical RAID array sizes <ref type="table">Table 1</ref>: Comparison of RAID-5 organizations taining its uniform or near-uniform data distribution. In fact, RAID+ reserves space for data recovery and always performs a fast all-to-all reconstruction. When hot spares are available or failed disks are repaired, the recovered data will be replicated to replacement disk(s) in background, hiding the slow single-disk writing latency. <ref type="table">Table 1</ref> gives several major metrics, comparing RAID+ with common existing solutions utilizing larger disk pools. s denotes the number of hot spare disks, while C and B denote single-disk capacity and bandwidth, respectively. Without loss of generality, we use RAID-5 as the elementary RAID level. Here RAID-5C and RAID-50 refer to the aforementioned "concatenated" and "striped" volume construction modes. We omit random placement schemes as they are similar to RAID+ in these aspects (but suffer from inferior load balance and locality). For application I/O, RAID-5C has good interapplication isolation, as different workloads are more likely to involve separate underlying RAID-5 arrays, while both RAID-50 and RAID+ would be subject to performance interference from concurrent workloads. The tradeoff is aggregate performance per volume: files on RAID-5C can only utilize 1-2 physical k-disk RAID array at a time, while RAID-50 and RAID+ could enlist most or all disks. This applies to both sequential accesses (in bandwidth) and random ones (in IOPS).</p><formula xml:id="formula_0">Application I/O Rebuild I/O Isol. T hrp T rebuild Interf. MT T DL RAID-5C High T · k C/B Part-High t RAID-50 Low T · (n − s) C/B Part-High t RAID+ Low T ·n C·k/(B·(n−1)) Univ-Low &gt; t · (k − 1)/k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Comparison of RAID Usage Modes</head><p>For RAID rebuild I/O, both RAID-5C and RAID-50 limit reconstruction to the physical array with the disk failure. Their recovery time <ref type="bibr">(T rebuild</ref> ) is equivalent to a single-disk full scan, assuming perfect read-write overlap. In contrast, RAID+ enlists all n−1 surviving disks in the read-write of the k-disk capacity (C·k), making recovery itself much faster. Regarding application-perceived interference, with RAID-5C the RAID reconstruction process is only visible to accesses to the same physical array. RAID-50 gets similar partial exposure with random accesses, but could be universally affected with larger, sequential reads/writes, as shown in our evaluation <ref type="table">(Table 3</ref>). With RAID+'s all-to-all data recovery, reconstruction traffic can be perceived by most user requests, but the interference is lighter and lasts shorter.</p><p>Finally, the MT T DL column describes mean time to data loss, considering the probability of non-recoverable failures (such as second disk failure before reconstruction completes with RAID-5). We find RAID-5C and RAID-50 with the same MT T DL and give a conservative lower bound for RAID+ relative to it. The bound is fairly close to 1 and configurable by k. With RAID-6, however, we found that RAID+ actually enjoys a significant improvement in MT T DL over conventional systems <ref type="bibr" target="#b47">[49]</ref>, by prioritizing the reconstruction of significantly fewer yet more vulnerable stripes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Latin Squares for Data Distribution</head><p>We first introduce basic concepts and theorems of mutually orthogonal Latin squares (MOLS), followed by an example illustrating its use in constructing RAID+.  perimposed, each of the n 2 ordered pairs occur exactly once across the overlapping cells of the two squares.</p><p>Definition 3. Given a set of Latin squares, if all its member pairs are mutually orthogonal, we call it a set of mutually orthogonal Latin squares (MOLS).</p><p>For example, the left side in <ref type="figure" target="#fig_0">Figure 2</ref> gives three sample 5-order MOLS. Here the item set I = {0, 1, 2, 3, 4} has each member appearing strictly once in any row/column of the three 5 × 5 squares. When any two of these MOLS are stacked together and one reads through the 25 aligned cell-pairs, each unique pair i, j(i ∈ I, j ∈ I) also appears exactly once. Theorem 1. With any given order n, there can be at most (n − 1) MOLS, with this upper bound achieved when n is a power of a prime number.</p><p>Theorem 2. When n is a power of a prime number, a complete set of (n−1) MOLS can be constructed by filling the i th square L i (0</p><formula xml:id="formula_1">&lt; i &lt; n) using L i [x, y] = i·x + y. 1</formula><p>With such construction, the first row of all n−1 MOLS are identical, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. However, below the first row, the corresponding values at coordinates <ref type="bibr">[x, y]</ref> across all or any subset of the n − 1 MOLS are guaranteed to be distinct.</p><p>Next we reuse the Latin squares shown in <ref type="figure" target="#fig_0">Figure 2</ref> to illustrate how RAID+ works. With RAID+ templates, the order of Latin squares (n, 5 in this case) corresponds to the disk pool size. The number of Latin squares "stacked together" (k, 3 in this case) corresponds to the RAID stripe width. Suppose we now construct a logical RAID-5 volume, distributing data blocks with a stripe width of 3 across 5 disks.</p><p>We ignore the first row of all squares and construct n(n − 1) stripes by copying contents from the remaining n(n − 1) cells across all three squares. The middle column in <ref type="figure" target="#fig_0">Figure 2</ref> gives a full list of these 20 stripes. The derived n(n−1) stripe sequence guides block assignment onto the n disks: each item maps to the corresponding disk ID. E.g., the first stripe ("stripe a") is made by looking up the <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> cell of L 0 , L 1 , and L 2 , resulting in tuple 1, 2, 3. Its 3 blocks (2 data and 1 parity) will thus reside on disks (1, 2, 3), respectively, while those from "stripe b" will reside on disks (2, 3, 4), and so on.</p><p>The right column in <ref type="figure" target="#fig_0">Figure 2</ref> gives the resulted data layout from the disks' point of view. As these 20 3-block stripes guarantee a uniform distribution of disk ID numbers, the 60 blocks form a 5×12 RAID+ template, to be repeatedly used in distributing data to the 5 disks.</p><p>The intuition is that aside from the first row, k MOLS give us uniform and deterministic data distribution across n disks, with k-block stripes. Unlike traditional RAID 1 "·" and "+" here denote finite field multiplication and addition <ref type="bibr" target="#b6">[7]</ref>. systems, where the disk array size equals the stripe width, our MOLS-based design allows k (and the RAID type) to be decoupled from n, enabling the construction of different virtual RAID volumes with small or moderate stripe widths on top of much larger disk pools.</p><p>As to be discussed in more details later, another desirable feature of MOLS is that, when one of the disks fails, blocks needed to recover the lost data are also uniformly distributed among the n−1 surviving disks. This allows for quick read, reproduction, and write of the (temporarily) lost data in parallel by these n−1 disks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Normal Data Layout</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Valid Disk Pool Sizes of RAID+</head><p>The precondition of building a RAID+ system is that we can construct (k + m) n-order MOLS, k of which used to construct the normal data layout and m reserved as spare MOLS for data redistribution in the face of disk failures (details in Section 5). m should be large enough to support the highest fault tolerance level among all RAID volumes within this RAID+ pool. For example, if a volume adopts RAID-6, then we need m ≥ 2.</p><p>Although the number of n-order MOLS for general n remains an open problem, (n−1) is known to exist when n is a power of a prime number <ref type="bibr" target="#b6">[7]</ref>. Therefore, as long as n, the total number of disks, is one such valid pool size, one can perform the deterministic calculation of n− 1 MOLS using the algorithm given in Theorem 2. Also, with these valid n values, the corresponding MOLS set possesses several attractive properties for balanced data and recovery load distribution.</p><p>The requirement may sound demanding, but it turns out qualifying numbers are abundant and not far apart. For example, between 4 and 128, we have the following <ref type="bibr">42 valid n values: 4, 5, 7, 8, 9, 11, 13, 16, 17, 19, 23, 25 27, 29, 31, 32, 37, 41, 43, 47, 49, 53, 59, 61, 64, 67, 71, 73, 79, 81, 83, 89, 97, 101, 103, 107, 109, 113, 121, 125, 127</ref>, and 128. Since our envisioned RAID+ disk pools contain dozens of to 100+ disks, there are plenty of valid n values to choose from.</p><p>The density of valid n values further allows physical performance isolation should it be desired. Multiple RAID+ logical sub-pools can be constructed within a larger physical pool. E.g., a 60-disk pool can support sub-pools with configuration (11+49), (23+37), (8+11+41), etc., all with valid sub-pool n values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stripes-to-Disks Mapping</head><p>RAID+ supports two modes, a normal layout, with guaranteed uniform distribution across all disks, and an interim layout, with uniform or slightly skewed data distribution among surviving disks, under one or more disk failures. <ref type="bibr">Below</ref>   organization with RAID+ under normal operation with a valid initial disk pool size n, with failure recovery and interim layout discussed in the next section.</p><p>Given k MOLS of order n, {L 0 , L 1 , ..., L k−1 }, a RAID+ template is constructed by traversing these k Latin squares simultaneously in a row-major order from the second row on. For each position [x, y] (0 &lt; x &lt; n, 0 ≤ y &lt; n), the k-block stripe S x,y is obtained by listing the corresponding values of L i at this position:</p><formula xml:id="formula_2">S x,y = {L 0 [x, y], L 1 [x, y], ..., L k−1 [x, y]},</formula><p>giving the disk IDs to place the k blocks of S x,y . Since n − 1 rows with n columns in the Latin squares are traversed, there are n(n−1) stripes in a full cycle of this RAID+ template.</p><p>Below are the major properties of MOLS-based data layout. The proofs are omitted due to the space limit. Property 1. With normal data layout, any two blocks within a data stripe are placed on separate disk drives.</p><p>This property guarantees that (1) the I/O workload in accessing each k-block stripe is uniformly distributed to k disks, and (2) a single disk failure results in the loss of at most one data block within any stripe.</p><p>Property 2. With normal data layout, the n disks are assigned equal shares of both data and parity blocks.</p><p>This property guarantees the same read-write load balancing as with RAID-5, allowing equal distribution of both data and parity blocks. This is particularly important to storage devices with asymmetric read-write performance and/or write leveling requirement, such as NAND flash disks. Unlike RAID-5, though, RAID+ decouples the stripe size k from a potentially much larger pool size n, allowing load balancing to be performed at a much wider scope, without sacrificing the fault tolerance allowed by the adopted RAID level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Throughput-Friendly Addressing</head><p>So far, the RAID+ template gives a deterministic mapping from data blocks in any k-block stripe to n disks. However, since each stripe will be mapped to a k-subset of n disks, the ordering of the n × (n − 1) stripes within the logical address space has impact on disk contention, I/O parallelism utilization, and spatial locality.</p><p>To this end, RAID+ allows stripe ordering (block addressing) to be done in different ways considering workload-specific needs. In particular, different RAID+ volumes sharing the same physical n-disk pool can each adopt its own addressing strategy. Below we describe two sample addressing algorithms targeting large sequential reads and writes, respectively (considering that block addressing matters less with random accesses). For the ease of illustration, we adopt simple RAID-4, where the first two blocks in each stripe are data blocks and the last one parity. The key difference between the two patterns here is that with RAID redundancy, sequential reads will skip parity blocks while sequential writes need to update both data and parity. RAID+ performs stripe ordering by rows in the MOLS, with the process repeated at each row. To form the n-stripe sequence for the x th row (S x,0 , S x,1 , ..., S x,n−1 ), RAID+ starts by setting S x,0 as the stripe given at the [x, 0] position of the MOLS, walking through the remainder of the row as follows:</p><p>• Sequential-read friendly ordering The head of each subsequent stripe is the tail of its predecessor <ref type="figure" target="#fig_2">(Fig- ure 3(a)</ref>). I.e., we choose S x,i such that S x,i (0) = S x,i−1 (k − 1). The rationale here is that the last block within a RAID-4 stripe is a parity block, which will not be involved in user read operations.</p><p>• Sequential-write friendly ordering The head of each subsequent stripe is the sum of the tail of the previous one and x <ref type="figure" target="#fig_2">(Figure 3(b)</ref>). I.e., we choose S x,i such that S x,i (0) = S x,i−1 (k − 1) + x. This is considering that for full-stripe writes resulted from sequential write workloads, all the blocks within a stripe will be updated.</p><p>Finally, such logical ordering of stripes within a RAID+ volume also corresponds to the relative ordering of blocks on each disk. E.g., the middle column in <ref type="figure" target="#fig_0">Figure 2</ref> gives a "plain" row-major stripe ordering (neither read-nor write-optimized). This ordering uniquely defines the block ordering on each of the 5 disks (the column below each disk ID). In this case, D 0 carries blocks assigned to "0": the 3rd block in stripe c, the 2nd in d, the 1st in e, ..., etc. Given n, k, the block addressing scheme, and the RAID level adopted, the logical to physical block mapping within any RAID+ template can be completed by simple calculation. Our implementation uses a tiny lookup table (sized 93KB for n = 59 and k = 7) to accelerate such in-template data addressing. Within the large n-disk physical address space, capacity is allocated at the granularity of RAID+ templates. RAID+ uses a per-volume index table to store the physical locations of its template instantiation. Since the templates are rather large, containing n(n−1) × k i blocks for the i th volume, maintaining such mapping (one "base address" per template instance) brings little overhead. Logical blocks of a given volume can then be easily mapped to its physical location by coupling the proper template instance offset with in-template addressing discussed earlier. Compared to random data distribution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">51]</ref>, RAID+ offers uniform data distribution and direct addressing while only requiring singlestep, template-level offset maintenance and lookup. The MOLS-based RAID+ data distribution offers allto-all fast data recovery involving all surviving disks in a disk pool, directly into an interim layout. When the failed disk gets repaired or replaced, the normal data layout can be restored in background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-Template Storage/Addressing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Recovery and Interim Layout</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Interim Layout under 1-disk Failure</head><p>Upon a disk failure in an n-disk pool, RAID+ performs fast data recovery to recalculate the lost blocks and distribute them to the (n−1) surviving disks. Thanks again to MOLS properties, when n is a valid pool size (power of prime), the resulted interim data layout preserves uniform data distribution. Suppose disk D f ( f ∈ [0, n−1]) fails, below we describe the construction of the interim layout with k-block stripes, reusing the former example. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates this process, where D f is D 0 .</p><p>Let R be the set of stripes affected by D f 's failure, which contain the item f (0 in this example). For any template, there are a total of n(n−1)k/n = (n−1)k blocks on each disk. As each stripe cannot have two blocks assigned to the same disk, these (n−1)k blocks correspond to the same number of stripes that are involved in data recovery, as shown in the middle column of <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>Recall that a valid pool size n allows for (n−1) n-order MOLS, out of which k are used for the normal layout. Now we select any one from the remaining (n−k−1) MOLS, as L k (Latin square L 3 in <ref type="figure" target="#fig_3">Figure 4</ref>). This additional Latin square will be used to guide the placement of blocks assigned to D f , with the item f in the affected stripes replaced with a new surviving disk ID.</p><p>The intuition is that when we append the new Latin square to the back of the existing k MOLS "stack" and read through each position <ref type="bibr">[x, y]</ref>, we extend the k-block stripes to (k+1)-block ones, with again uniformly distributed item-set permutations. Now take each affected stripe, and replace the (now missing) f with the item r at the corresponding position in L k , we relocate the missing block used to be assigned to D f to the surviving disk D r . E.g., in <ref type="figure" target="#fig_3">Figure 4</ref>, each "0" in these 12 stripes would be replaced with another integer in {1, 2, 3, 4}, such as stripe c transforming from (3, 4, 0) to (3, 4, 1), as the corresponding position in the additional Latin square (L 3 <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>) has item "1". The first two blocks in this affected stripe, on D 3 and D 4 respectively, would not need to move.</p><p>Below are the major properties associated with MOLS-based data layout concerning data recovery and the interim layout, assuming a valid pool size n. Property 3. With the n-disk normal layout, all blocks correlated with those on any given drive (i.e., blocks sharing stripes with blocks on this disk) are distributed evenly among the other disks.</p><p>The implication here is that when the first disk fails, the read workload to recover unavailable blocks is evenly distributed among all surviving disks. Property 4. With the (n−1)-disk interim layout, any two blocks within a data stripe are still placed on separate disk drives.</p><p>Property 5. All the (n−1)k missing blocks on any single failed disk can be redistributed to all the surviving (n−1) disks evenly, each receiving k additional blocks.</p><p>These two properties imply that (1) the write workload involved in RAID+ recovery from a single-disk failure is also uniformly distributed among all surviving disks, (2) data stripes in the (n−1)-disk interim layout preserves the same RAID fault tolerance as in the normal layout, 284 16th USENIX Conference on File and Storage Technologies USENIX Association and (3) the (n−1)-disk interim layout also retains the uniform data distribution to allow perfectly balanced I/O servicing even after losing one disk. The particular significance of (n−1)-disk interim layout lies in the fact that the probability of single-disk failure is much higher than that of having two or more failed disks, especially when hot spares are available. Considering this, plus that disk capacity is relatively abundant in typical server environments, RAID+ performs an additional performance optimization by reserving recovery data space with normal data layout. More specifically, RAID+ actually allocates n×k physical blocks per disk for a data template. (n−1)k of them are used to store data/parity blocks in the normal layout, while the remaining k blocks (the green area in <ref type="figure" target="#fig_3">Figure 4</ref>) are reserved for storing reconstructed data whenever there is a single-disk failure. This way, under such a failure, the reconstructed data are physically adjacent to the normal layout blocks, preserving spatial locality in data accesses. In our implementation, the content of the aforementioned small lookup table is modified to support fast interim data addressing, without additional space overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parallel Data Recovery</head><p>Under a single disk failure, the MOLS-based design lets both read and write workloads involved in RAID reconstruction and temporary relocation be uniformly distributed to the entire pool. This breaks the performance limit of conventional RAID systems, where the recovery work is only distributed within the RAID array affected.</p><p>However, even with uniform data distribution, the parallel read/write operations in data recovery could still generate resource contention, transient load imbalance, or unnecessary disk seeks, if care is not taken. To this end, RAID+ orchestrates its all-to-all data reconstruction by letting the surviving disks work on a subset of the (n−1)k affected stripes at a time, alternating between reader and writer roles. Barriers are used in between such iterations, creating a natural break point for RAID+ to check upon user I/O requests, potentially slowing down or temporarily suspending the recovery depending on the current application request intensity, QoS specifications, and configurable system policies (such as starvation prevention to ensure the completion of data recovery).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multiple Disk Failures</head><p>MOLS-based design also handles multiple failures gracefully. If another disk failure occurs after data recovery from a disk failure, we repeat the process described in Section 5.1 with another spare Latin square. When m disks are lost but tolerated by the adopted RAID level, by appending m spare MOLS to the stack of k used in the normal layout, we can calculate the eventual (n−m)-disk interim layout. Recognizing that the affected data stripes have different degrees of data loss, RAID+ prioritizes the reconstruction of the more vulnerable stripes.</p><p>Due to space limit, we give a brief summary of related results: when a RAID+ pool keeps losing disks (without disk replacement), Monte Carlo simulation shows very slight imbalance in data distribution (CoV of up to 0.29%), while system experiments show application performance degradation of up to 6% (except with sequential read, where RAID+ loses the benefit of its unique read-friendly addressing when more disks fail).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We implemented RAID+ in the MD (Multiple Devices) driver in Linux Kernel 3.14.35, a software RAID system that forms a common framework for all RAID systems tested in our evaluation. Despite theoretical properties appearing sophisticated, MOLS-based addressing is simple to implement, taking a mere 12 lines of code. Test platform Our testbed uses a SuperMicro 4U storage server with two 12-core Intel XEON E5-2650 V4 processors and 128GB DDR4 memory, running Ubuntu 14.04 with Linux kernel v3.14.35. Two AOC-S3008L-L8I SAS JBOD adapters, each connected to a 30-bay SAS3 expander backplane via two channels, host 60 Seagate Constellation 7200RPM 2TB HDDs. The I/O channels afford a total I/O bandwidth of 24GB/s (400MB/s for each disk), significantly exceeding the aggregate sequential bandwidth from the disks. In all experiments, 50GB capacity of each disk is used. RAID configurations Unless otherwise noted, our tests use 59 out of the aforementioned 60-disk pool (n = 59). The stripe width k is set at 7 (6+1 RAID).</p><p>For comparison, we evaluate two commonly adopted conventional RAID organizations utilizing such large disk pools, both of which build eight 6+1 RAID-5 arrays with 64KB stripe unit size, consuming 56 disks with the last 3 reserved as hot spares. RAID-5C divides each array into multiple 1GB extents and concatenates them in a round-robin manner, while RAID-50 stripes across these 8 arrays at block size of 12MB (2MB×6).</p><p>We also evaluate two randomized data placement schemes, RAID R and RAID H . Both place blocks within each stripe to different disks while aiming for balanced data distribution to all n disks. To assign a block to a disk, RAID R utilizes the system random number generator (with system time as seed) and is therefore nondeterministic. RAID H , instead, uses the Jenkins hash function <ref type="bibr" target="#b25">[26]</ref> adopted by systems such as Ceph <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b48">50]</ref>. If a block is mapped to a disk already used in the current stripe, mapping will be recalculated until collision free (following CRUSH <ref type="bibr" target="#b49">[51]</ref> for RAID H ). Our experiments find the two schemes often perform similarly, in which case we show only results of the better one.  Both RAID+ and the random schemes build (6+1) RAID-4 arrays, with 64KB stripe unit size. Such striping continues on the same set of disks until a 2MB space allocation unit is filled per disk, before starting a new 7-block stripe. We have also implemented RAID-6 and observed similar performance trends, but omit results here due to space limit. I/O Workloads We use three types of I/O workloads:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX</head><p>• Synthetic workloads: we use fio <ref type="bibr">[15]</ref>, a widely used I/O workload generator to produce four representative elementary workloads: sequential read, sequential write, random read, and random write.</p><p>• I/O traces: We use 8 public block-level I/O traces, namely src1 1, usr 1, prn 0, prn 1, proj 0, and prxy 1 from MSR Cambridge <ref type="bibr" target="#b36">[37]</ref>, plus Fin2 and WebS 2 from SPC <ref type="bibr" target="#b0">[1]</ref>. Based on load level observed, we followed existing practice in prior research <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">48]</ref> and accelerated the SPC traces (Fin2 by 5× and WebS 2 by 3×) while replaying all others a tempo.</p><p>• I/O-intensive applications: we also use four I/Oheavy real applications: GridGraph <ref type="bibr" target="#b58">[60]</ref> (an out-ofcore graph engine), TPC-C <ref type="bibr">[46]</ref> (in-house implementation of the well-known RDBMS transaction benchmark standard), a Facebook-like photo access workload (synthesized using Facebook's published workload characteristics <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">24]</ref>), and a MongoDB <ref type="bibr" target="#b34">[35]</ref> NoSQL workload from the YCSB suite <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Reconstruction Performance</head><p>We start by evaluating reconstruction performance, one major advantage of RAID+ over alternative schemes, with disk failures created by unplugging random disk(s).</p><p>Offline rebuild <ref type="table" target="#tab_4">Table 2</ref> gives the offline rebuild time with a single-disk failure. RAID+ is tested with two valid n values, 59 and 31 (shared by the random schemes). The same pools would give RAID-50 3 hotspares each, with 8 and 4 RAID-5 arrays respectively. RAID-5C results would be identical to RAID-50 here.</p><p>In both cases, RAID+ consistently outperforms RAID-50, delivering a speedup of 7.5× and 3.7×. Such results approach the theoretical speedup of 8.29 and 4.29, respectively, given in <ref type="table">Table 1</ref>. The gap is mainly due to less sequential reconstruction read/write patterns compared with RAID-50, as RAID+'s recovery load per disk is much smaller yet non-contiguous. Unlike RAID-50, with rebuild time independent of the disk pool size, RAID+ spreads the rebuild workload to larger pools uniformly and lowers the rebuild time proportionally.</p><p>The two random schemes outperform RAID-50 here also by having more disks participate in recovery. However, their rebuild takes significantly longer than that of RAID+. Further examination reveals that though they achieve overall balanced data distribution, random schemes suffer much higher skewness within each window of dozens/hundreds of blocks. E.g., within a RAID+ template size, the RAID H has CoV of rebuild read/write load distribution of 31.9%/38.9%, while RAID R has 12.72%/33.87%. <ref type="bibr" target="#b1">2</ref> Such "local load balance" is crucial for RAID rebuild, with sequential and synchronized operations, where overloaded stragglers could easily drag down the entire array's recovery progress. RAID+, in contrast, retains its absolute load balance within such smaller windows and delivers much higher rebuild speed.</p><p>Finally, this advantage grows with the disk pool size n, as such perfect local load balance gives RAID+ higher profit margin by evenly utilizing n disks. In this sense, RAID-50 has recovery bandwidth independent of n by utilizing a small fixed-size sub-pool. The random schemes perform between these two extremes, achieving good global yet poor local load balance.   <ref type="table">Table 3</ref>: Online rebuild performance comparison, in terms of speedup against corresponding RAID-50 results during the 900 seconds following the disk failure's onset, RAID+ manages to complete 44.43%, 139.70%, and 5.11% more transactions than RAID-50, RAID-5C, and RAID H , respectively. TPC-C throughput stays consistent as recovery progresses, as its degradation is dominated by the rebuild I/O activities rather than transactions that happen to hit the failed disk. <ref type="table">Table 3</ref> summarizes online reconstruction performance, giving both the application performance and the rebuild speed, all in the form of speedup with respect to corresponding RAID-50 results (the higher the better). We use the same RAID rebuild rate setting (minimal at 80MB/s and maximum at 200 MB/s) within the MD driver for RAID-50 and RAID-5C, and configure RAID+ and RAID H to avoid application performance degradation from the RAID-50 baseline during rebuild (with only one exception where RAID H achieves 99% of the baseline performance). The results reveal that RAID+ and RAID H simultaneously improve both the application and rebuild performance from RAID-50. Between them, RAID+ is consistently better, with significantly faster rebuild and slightly better application performance for TPC-C and NoSQL. RAID-5C, at least with the default rate control setting, loses on both fronts (except for FaceBook, where it slightly outperforms RAID-50). Multi-workload online rebuild For multi-workload evaluation, we use a smaller pool size of 29, <ref type="bibr" target="#b2">3</ref> with stripe width remaining at 7, to construct 4 logical RAID volumes. RAID-5 builds 4 disjoint 6+1 arrays, plus one last disk reserved as hot spare. RAID+ constructs 4 volumes with the same deterministic template (n = 29, k = 7) across the entire pool. RAID H randomly distributes blocks from 4 virtual 6+1 array volumes to all 29 disks.</p><p>In each experiment, we sample 4 out of 8 MSR/SPC I/O traces as a workload mix to run simultaneously on the RAID volumes, for 28 minutes. The requests are replayed using the original timestamps, therefore identical sets of requests are issued across tests. We create a single-disk failure in the whole pool at time 0 and perform reconstruction without stopping user applications. <ref type="figure">Figure 6</ref> illustrates one such test case, showing the average I/O request latency in 60-second episodes along the execution timeline, for each workload. With RAID-5, the failure is contained within one volume (running Fin2 in <ref type="figure">Figure 6(a)</ref>), while with other schemes, it affects all volumes. The vertical lines indicate time points when each scheme finishes online rebuild. Similar to singleworkload results, RAID+ has slightly faster rebuild than RAID H , both beating RAID-5 by almost 4 times. Intuitively, RAID+ and RAID H excel by spreading rebuild work to all 4 volumes rather than only one, which also enables them to eliminate dramatic latency increases brought by RAID-5's online rebuild <ref type="figure">(Figure 6(a)</ref>). Thus compared with RAID-5, during the entire reconstruction, RAID+ and RAID H reduces the Fin2 workload average latency by over 90%, and the 99% tail latency by 89% (48ms vs. 418ms).</p><p>As expected, involving all disks expose the failure to all 4 volumes, roughly doubling the average latency of RAID+/RAID H before rebuild completes over RAID-5 for the prxy 1 and WebS 2 workloads. However, the much shorter rebuild time not only reduces system vulnerability, but also prevents any volume to be under dramatic performance degradation for prolonged periods. Note that while inter-volume isolation is broken here, disk failures are not user application artifacts but anomalies from the underlying platform. Therefore, RAID+ allows a larger disk pool to become more resilient, recover faster from failures, and provide more consistent performance during recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Normal I/O Performance</head><p>Single-workload evaluation <ref type="figure">Figure 7</ref> gives the normal synthetic workload performance running fio, with varying request sizes. The access footprint is large enough to span all RAID-5 arrays with RAID-5C. All RAID systems, including RAID+, perform very similarly in random read/write tests. Therefore, we only show sequential performance here.</p><p>RAID+ slightly outperforms RAID-50 in most cases, by using 59 rather than 56 disks. Compared to them, RAID H offers moderately lower sequential performance, again due to poor local load balance and inferior spatial locality within each disk. RAID-5C predictably lags behind others with sequential accesses, as in most cases only one RAID-5 array is utilized. <ref type="figure">Figure 8</ref> shows results with two sample MSR traces, plotting latency data points (averaged over 60-second episodes) along the execution timeline. Again RAID+ outperforms RAID-50, with an average improvement of 6.23% under prxy 1 and 87.04% under usr 1. This is because RAID+ uses all 59 disks (rather than 56) and usr 1 is read-dominant <ref type="bibr" target="#b36">[37]</ref>, with RAID+ adopts read-friendly addressing in this set of tests. reasons, RAID R loses slightly to RAID+ under usr 1, and wins slightly under prxy 1, as it also uses all disks and the read-friendly addressing may bring minor sideeffects for the more write-intensive prxy 1 workload.</p><p>Finally, <ref type="table" target="#tab_7">Table 4</ref> compares application performance. Note that unlike other cases, TPC-C uses transactions per minute committed, the higher the better. With Facebooklike photo and MongoDB, both having primarily random accesses, all four RAID organizations have data distributed to all disks and report very similar performance results. Since GridGraph is primarily sequential, RAID-5C can mostly utilize only one or two underlying RAID-5 arrays. Therefore, all three other schemes have a more than 4-fold speedup over RAID-5C, and RAID+ has minor advantage over RAID-50 by using slightly more disks, and over RAID H by having better spatial locality. With TPC-C, which has both random and sequential accesses, RAID+ slightly outperforms both RAID-50 and RAID H . Again RAID-5C clearly underperforms. Multi-workload system throughput Now we examine normal performance with multiple workloads sharing the   underlying disk pool, using the 29-disk, 4-volume setting similar to that in online reconstruction tests <ref type="figure">(Fig- ure 6</ref>). We evaluated all unique 4-workload combinations from the 8 MSR/SPC traces, executing each of these 70 workload mixes on 4 RAID volumes built with RAID-5, RAID H , and RAID+.</p><p>Here we adopt weighted speedup <ref type="bibr" target="#b13">[14]</ref>, a widely-used multi-workload performance metric in computer architecture, to measure the overall system throughput. As each workload is replayed at fixed speed (by timestamps given in traces), we use 1/latency to replace the typical IPC (Instructions Per Cycle) measurement in architecture studies, calculating the weighted speedup as</p><formula xml:id="formula_3">1 n ∑ n i=1 (L c i /L i ) for an n-application workload mix. Here L c</formula><p>i and L i denote the average latency of the ith workload using conventional RAID (RAID-5) and the system to be evaluated, respectively. I.e., RAID-5 is used as the baseline for measuring performance speedup.</p><p>To summarize the results, both RAID H and RAID+ deliver considerable weighted speedup in all 70 test cases, demonstrating their capability of consistently improving the overall system throughput by utilizing more disks simultaneously. More specifically, RAID H obtains an average weighted speedup of 1.83 (over 1.33 in 80% of cases) over the 4-volume RAID-5 baseline, while RAID+ performs better, with average weighted speedup of 2.05 (over 1.5 in 80% of cases). <ref type="figure" target="#fig_6">Figure 9</ref> showcases one sample test case (running src1 1, usr 1, prn 0, and Fin2), showing the speedup (based on average latency in each one-minute episode) of RAID+ over RAID-5 along the timeline. All but one speedup data points are above 1, with prn 0 reaching over 25 at one point. By zooming into the request patterns, we find such large profit comes from the burstiness in most workloads. <ref type="figure" target="#fig_6">Figure 9</ref>(b) illustrates this for a 100ms-long window, 200ms into the execution, showing the per-ms request count for each workload. At this granularity, one clearly sees that the workloads have sporadic requests and often form interleaving bursts. The most bursty workloads, prn 0 and src1 1, benefit more from RAID+ and RAID H , which use all disks to serve each volume. In particular, during request peaks these workloads see faster processing and lower I/O queue wait time, as confirmed by our detailed profiling, hence achieving the 25× (transient) speedup.</p><p>While the majority of the 70 mixes possess such "complementary" request patterns, there are cases where two or more workloads have sustained simultaneously intensive I/O activities, leading to slowdown of individual workload. However, among the total of 280 executions (70 mixed runs with 4 workloads per mix), there are only 39 cases of slowdown. Again, if a workload has to be guaranteed stronger performance isolation, RAID+ pools can be physically partitioned (such as building 41+19 volumes within a 60-disk enclosure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Sensitivity to Internal Parameters</head><p>Finally, we study the impact of RAID+'s key parameters. <ref type="figure" target="#fig_7">Figure 10</ref>(a) shows both the aggregate random read and write throughput (left y axis) and the offline rebuild time (right y axis) with RAID+ pool size n, increased from 41 to 59, while fixing k at 7 and block size at 2MB.</p><p>These results show that the random read performance increases linearly with n (by up to 39%), due to uniform load distribution to all disks in the pool. The write throughput, though also growing steadily (by up to 24%), is much lower, as each of these 64KB write will bring at least four underlying I/O operations, for reading and writing back both the concerned data and parity blocks. In addition, such read-modify-write operations are synchronized, further lowering the aggregate throughput. The offline rebuild time, unsurprisingly, decreases as n grows and conforms to the model shown in <ref type="table">Table 1</ref>. <ref type="figure" target="#fig_7">Figure 10</ref>(b) shows similar experiments, with n fixed at 59 and varying k. With regard to user I/O performance, as modeled in <ref type="table">Table 1</ref>, the aggregate throughput is mostly independent of k and the rebuild time grows linearly with it. One unexpected exception is with k = 3, where the write bandwidth appears considerably higher than any other k values. By using the iostat tool, we find that with k = 3 there are significantly fewer disk reads for parity calculation. Here with the 2+1 dataparity setup, there are higher chances for parity data to be reconstructed from cached data blocks.</p><p>Next, in <ref type="figure" target="#fig_7">Figure 10</ref>(c) we fix both n (59) and k (7) and change the block size. As expected, the block size has little impact on the random read/write performance. Meanwhile, the rebuild time decreases significantly, though not linearly. As RAID+'s rebuild access pattern introduces less regular access patterns compared with those of conventional RAID systems, larger block sizes improve performance by promoting sequential accesses.</p><p>Last, to examine the effect of throughput-friendly block addressing (Section 4.3), we run the fio sequential read and write workloads, with I/O sizes of 2MB. <ref type="figure" target="#fig_7">Figure 10(d)</ref> shows the results using four stripe ordering strategies: 1) "native", original stripe ordering from a RAID+ template (stripes a to t in <ref type="figure" target="#fig_0">Figure 2</ref>), 2) "random", randomized stripe ordering using a pseudo-random function, 3) "read-opt", our proposed read-friendly ordering, and 4) "write-opt", our proposed write-friendly ordering. Bandwidths shown are normalized to "native". While the native and randomized strategies almost perform identically, the read-friendly strategy does generate a 28% improvement with sequential reads. Write-friendly ordering, on the other hand, brings a much smaller profit (4%). Again, unlike the "pure" sequential streams with reads, writes are not exactly sequential due to read-modifywrite of both data and parity blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Data Layout Optimization Existing RAID layout optimizations roughly form two categories: 1) distributing either data blocks or parity blocks evenly across all the disks (e.g., RAID-5 vs. RAID-4), and 2) exploiting spatial data locality (e.g., left-symmetric RAID-5 <ref type="bibr" target="#b29">[30]</ref>). Inspired by them, RAID+ spreads data and parity blocks in a much larger shared pool, with its throughput-friendly block addressing promoting access locality.</p><p>The parity declustering layout <ref type="bibr" target="#b35">[36]</ref> utilizes as few disks as possible in data reconstruction, further analysed and extended/optimized by many <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b18">20]</ref>. However, unlike with RAID+, rebuild is still capped by the write speed of the replacement disk, though these solutions do spread rebuild reads to remaining disks.</p><p>ZFS <ref type="bibr" target="#b5">[6]</ref> uses "dynamic striping" to distribute load across "virtual devices", dynamically adjusting striping width and device selection to facilitate fast out-ofplace updates and balanced capacity utilization. Systems such as IBM XIV <ref type="bibr" target="#b24">[25]</ref> and the Flat Datacenter Storage (FDS) <ref type="bibr" target="#b37">[38]</ref>  Work also exists on designing data organizations sensitive to workload characteristics or application scenarios. E.g., Disk Caching Disk (DCD) <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b38">39]</ref> uses an additional disk as a cache to convert small random writes into large log appends. HP's AutoRAID <ref type="bibr" target="#b51">[53]</ref> partitions RAID storage and differentiates the handling of hot and cold data. ALIS <ref type="bibr" target="#b20">[22]</ref> and BORG <ref type="bibr" target="#b4">[5]</ref> reorganize frequently accessed blocks (and block sequences) to place them sequentially in a dedicated area. These techniques are orthogonal to ours and can be incorporated by RAID+ to improve application performance. Optimizations on RAID Reconstruction Prior studies have targeted improving reconstruction performance. Many of them focus on designing better data layout in a disk group <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b54">56]</ref>, to minimize I/O for recovery or distribute rebuild I/O as evenly as possible. Other approaches optimize the RAID reconstruction workflow to make full use of higher sequential bandwidth, such as DOR (Disk-Oriented Reconstruction) <ref type="bibr" target="#b18">[20]</ref>, PR <ref type="bibr" target="#b30">[31]</ref>, and others <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b54">56]</ref>. In addition, PRO <ref type="bibr" target="#b44">[45]</ref> rebuilds frequently-accessed areas first and S 2 -RAID [48] optimizes reads and writes separately for faster recovery. Finally, task scheduling techniques optimize reconstruction rate control <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">47]</ref>.</p><p>Except for WorkOut <ref type="bibr" target="#b52">[54]</ref>, which outsources part of user requests to surrogate disks during reconstruction, existing studies focus on improvement within one RAID group. RAID+ takes a different path from all, with builtin "backup" layouts to utilize all disks in a larger pool in reconstruction, while maintaining the fault tolerance and flexibility of smaller, logical RAID arrays. RAID Scaling Adding disks to an array requires data movement to regain uniform distribution. <ref type="bibr">Zhang et al.</ref> proposed batch movement and lazy metadata update to speed up data redistribution <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref>. FastScale <ref type="bibr" target="#b57">[59]</ref> uses a deterministic function to minimize data migration while balancing data distribution. CRAID <ref type="bibr" target="#b33">[34]</ref> uses a dedicated caching partition to capture and redistribute only hot data to incremental devices.</p><p>Another approach is randomized RAID, which randomly chooses a fraction of blocks to be moved to newly added disks. Prior work to this end <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b40">41]</ref> reduces migration, but produces unbalanced distribution after several expansions <ref type="bibr" target="#b33">[34]</ref>. Also, existing randomized RAID systems require extra book keeping and look-up.</p><p>RAID+, in contrast, allows large disk enclosures to directly host user volumes, each using its own RAID configuration, with templates stamping out allocations in all shapes and sizes. Meanwhile, it is not designed for dynamic, heterogeneous distributed environments targeted by methods like CRUSH <ref type="bibr" target="#b49">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This paper proposes RAID+, a new RAID architecture that breaks the resource isolation between multiple colocated RAID volumes and allows the decoupling of stripe width k from disk group size n. It uses a novel Latin-square-based data template to guarantee uniform and deterministic data distribution of k-block stripes to all n disks, where n could be much larger than k. It also delivers near-uniform distribution in both user data and RAID reconstruction content even after one or several disk failures, as well as fast RAID rebuild.</p><p>With RAID+, users can deploy large disk pools with virtual RAID volumes constructed and configured dynamically, according to different application demands. By utilizing all disks evenly while maintaining spatial locality, it enhances both multi-tenant system throughput and single-workload application performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RAID+ layout (n = 5, k = 3) Definition 1. A Latin square of order n is an n × n array filled with n different items, each occurring exactly once in each row and column.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 2 .</head><label>2</label><figDesc>Let L 1 and L 2 be two n-order Latin squares. L 1 and L 2 are mutually orthogonal if, when su- USENIX Association 16th USENIX Conference on File and Storage Technologies 281</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Stripe order for a 5-disk array (k = 3). Gray indicates parity blocks. The head of each stripe equals the tail of the previous one added by 0 (read-friendly) or 1 (write-friendly).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Interim data layout for a 5-disk RAID+ array when one disk fails. RAID+ uses the fourth MOLS, L 3 , to generate uniform data distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: TPC-C online rebuild w. single-disk failure Single-workload online rebuild Next, we examine online rebuild by creating a single-disk failure and performing reconstruction without stopping the execution of application(s). Figure 5 illustrates one sample test case (TPC-C). It plots the number of transactions committed per 10-second episode along the timeline, with a disk failure incurred at 300 seconds into the execution. First, results demonstrate that RAID+ matches the TPC-C throughput of RAID-50 and RAID H (all beating RAID-5C, unsurprisingly) in normal operation. Second, RAID+ offers much shorter online rebuild time than conventional RAID (396 seconds vs. RAID-5C's 1137 and RAID-50's 858). RAID H comes closer, but still takes 11.4% longer than RAID+. Third, RAID+, RAID-50, and RAID H bring similar degrades to TPC-C performance during rebuild. Although RAID-5C sees smaller relative performance impact, its degraded performance still lags behind due to its lower baseline. Overall,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 8 :</head><label>68</label><figDesc>Figure 6: Sample multi-workload performance w. online rebuild</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Case study with sample 4-workload mix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Impact of several factors on RAID+'s performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : Offline rebuild time comparison</head><label>2</label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Overall, App RAID-50 RAID-5C RAID H RAID+</head><label></label><figDesc>). RAID H comes closer, but still takes 11.4% longer than RAID+. Third, RAID+, RAID-50, and RAID H bring similar degrades to TPC- C performance during rebuild. Although RAID-5C sees smaller relative performance impact, its degraded perfor- mance still lags behind due to its lower baseline.</figDesc><table>FaceBook 
app perf 
1 
1.02 
1.41 
1.42 
reb perf 
1 
1.05 
2.29 
2.36 

TPC-C 
app perf 
1 
0.61 
1.00 
1.03 
reb perf 
1 
0.75 
1.94 
2.17 

GridGraph 
app perf 
1 
0.27 
1.22 
1.23 
reb perf 
1 
3.14 
2.05 
2.06 

MongoDB 
app perf 
1 
0.89 
0.99 
1.01 
reb perf 
1 
1.05 
1.60 
2.08 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 : Normal application performance</head><label>4</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="2"> Here RAID R has more even read distribution due to larger read volume than write in rebuild. RAID H however exhibits more skewed distribution of blocks to read involved in recovery, with CoV level appearing to be dependent on the hash function used. 286 16th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="3"> Considering the moderate request levels in test programs/traces, the smaller pool size allows us to test smaller (and higher number of) workload mixes, with results easier to plot and analyze.</note>

			<note place="foot" n="288"> 16th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="290"> 16th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="292"> 16th USENIX Conference on File and Storage Technologies USENIX Association</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Acknowledgment</head><p>We thank all reviewers for their insightful comments, and especially our shepherd Gala Yadgar for her guidance during our camera-ready preparation. We also thank Mu Lin and Xiaokang Sang for helpful discussions. This work was partially supported by the National Natural Science Foundation of China (under Grant 61672315) and the National Grand Fundamental Research 973 Program of China (under Grant 2014CB340402).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://traces.cs.umass.edu/index.php/Storage/Storage" />
		<title level="m">Umass trace repository</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Declustered disk array architectures with optimal and near-optimal parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">A</forename><surname>Burkhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">J</forename><surname>Stockmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flaviu</forename><surname>Cristian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 25th International Symposium on Computer Architecture (ISCA&apos;98)</title>
		<meeting>25th International Symposium on Computer Architecture (ISCA&apos;98)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of methods for scheduling low priority disk drive tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Bachmat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Measurements and Modeling of Computer Systems (SIGMETRICS&apos;02)</title>
		<meeting>the International Conference on Measurements and Modeling of Computer Systems (SIGMETRICS&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding a needle in Haystack: Facebook&apos;s photo storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajgel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;10)</title>
		<meeting>the 9th USENIX Conference on Operating Systems Design and Implementation (OSDI&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="47" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BORG: Block-reORGanization and selfoptimization in storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Medha</forename><surname>Bhadkamkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Useche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liptak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raju</forename><surname>Rangaswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vagelis</forename><surname>Hristidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on File and Storage Technologies (FAST&apos;09)</title>
		<meeting>the 7th USENIX Conference on File and Storage Technologies (FAST&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">ZFS: The last word in file systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bonwick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Moore</surname></persName>
		</author>
		<ptr target="https://wiki.illumos.org/download/attachments/1146951/zfs_last.pdf" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the construction of sets of mutually orthogonal Latin squares and the falsity of a conjecture of Euler. Transactions of the</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharadchandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrikhande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="191" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient, distributed data placement strategies for storage area networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Brinkmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kay</forename><surname>Salzwedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Scheideler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA&apos;00)</title>
		<meeting>the 12th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA&apos;00)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="119" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ceph</surname></persName>
		</author>
		<ptr target="https://github.com/ceph/libcrush" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A gracefully degradable declustered RAID architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheung</forename><surname>Siu-</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ada Wai-Chee</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cluster Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="105" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st ACM symposium on Cloud computing (SOCC&apos;10)</title>
		<meeting>the 1st ACM symposium on Cloud computing (SOCC&apos;10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="143" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">How long does it approximately take for a RAID reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Netapp</forename><surname>Corporation</surname></persName>
		</author>
		<ptr target="https://kb.netapp.com/support/s/article/ka21A0000000jOzQAI/how-long-does-it-approximately-take-for-a-raid-reconstruction?language=en_US" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Oracle Corporation. A better RAID strategy for high capacity drives in mainframe storage</title>
		<ptr target="http://www.oracle.com/technetwork/articles/systems-hardware-architecture/raid-strategy-hi-capacity-drives-170907.pdf" />
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">System-level performance metrics for multiprogram workloads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stijn</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SCADDAR: An efficient randomized technique to reorganize continuous media blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Didi</forename><surname>Shu Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Data Engineering (ICDE&apos;02)</title>
		<meeting>the 18th International Conference on Data Engineering (ICDE&apos;02)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="473" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crystal: Software-defined storage for multitenant object stores</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raúl</forename><surname>Gracia-Tinedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Sampé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Zamora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sánchez-Artigas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>García-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosef</forename><surname>Moatti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Rom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST&apos;17)</title>
		<meeting>the 15th USENIX Conference on File and Storage Technologies (FAST&apos;17)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="243" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parity declustering for continuous operation in redundant disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;92)</title>
		<meeting>the 5th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;92)</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="23" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast, on-line failure recovery in redundant disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Siewiorek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Twenty-Third International Symposium on Fault-Tolerant Computing (FTCS-23)</title>
		<meeting>The Twenty-Third International Symposium on Fault-Tolerant Computing (FTCS-23)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On-line data reconstruction in redundant disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Balancing I/O response time and disk rebuild time in a RAID5 disk array</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-sixth Hawaii International Conference on System Sciences (HICSS-26)</title>
		<meeting>the Twenty-sixth Hawaii International Conference on System Sciences (HICSS-26)</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The automatic improvement of locality in storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Windsor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Jay</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honesty</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (TOCS)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="424" to="473" />
			<date type="published" when="2005-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DCD -Disk Caching Disk: A new approach for boosting I/O performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Symposium on Computer Architecture (ISCA&apos;96)</title>
		<meeting>the 23rd International Symposium on Computer Architecture (ISCA&apos;96)</meeting>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robbert</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wyatt</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<title level="m">An analysis of USENIX Association 16th USENIX Conference on File and Storage Technologies</title>
		<imprint>
			<biblScope unit="page">291</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caching</forename><surname>Facebook Photo</surname></persName>
		</author>
		<title level="m">Proceedings of the TwentyFourth ACM Symposium on Operating Systems Principles (SOSP&apos;13)</title>
		<meeting>the TwentyFourth ACM Symposium on Operating Systems Principles (SOSP&apos;13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="167" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<ptr target="http://www.redbooks.ibm.com/redbooks/pdfs/sg247659.pdf" />
		<title level="m">IBM. IBM XIV storage system architecture and implementation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Hash functions for hash ta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Jenkins</surname></persName>
		</author>
		<ptr target="http://burtleburtle.net/bob/hash/evahash.html" />
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lifetime improvement of NAND flash-based storage systems using dynamic program and erase scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeyong</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Sangwook Shane Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Conference on File and Storage Technologies (FAST&apos;14)</title>
		<meeting>the 12th USENIX Conference on File and Storage Technologies (FAST&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Analysis of repair algorithms for mirrored-disk systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hannu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Heikki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nohpill</forename><surname>Saikkonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Reliability</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="200" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking erasure codes for cloud file systems: Minimizing I/O for recovery and degraded reads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osama</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randal</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Conference on File and Storage Technologies (FAST&apos;12)</title>
		<meeting>the 10th USENIX Conference on File and Storage Technologies (FAST&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="251" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The performance of parity placements in disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TOC)</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="651" to="664" />
			<date type="published" when="1993-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automatic recovery from disk failure in continuous-media servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards higher disk head utilization: Extracting free bandwidth from busy disk drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Lumb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Conference on Symposium on Operating System Design &amp; Implementation (OSDI&apos;00)</title>
		<meeting>the 4th Conference on Symposium on Operating System Design &amp; Implementation (OSDI&apos;00)</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed sparing in disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dick</forename><surname>Mattson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digest of Papers COMPCON Spring</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CRAID: online RAID upgrades using dynamic hot data reorganization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Cortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX conference on File and Storage Technologies (FAST&apos;14)</title>
		<meeting>the 12th USENIX conference on File and Storage Technologies (FAST&apos;14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="133" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mongodb</surname></persName>
		</author>
		<ptr target="https://www.mongodb.com" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Performance analysis of disk arrays under failure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C S</forename><surname>Muntz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Conference on Very Large Data Bases (VLDB&apos;90)</title>
		<meeting>the 16th International Conference on Very Large Data Bases (VLDB&apos;90)</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="162" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Write off-loading: Practical power management for enterprise storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyanth</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Donnelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Rowstron</surname></persName>
		</author>
		<idno>10:1-10:23</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Flat datacenter storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><forename type="middle">B</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinliang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Suzue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;12)</title>
		<meeting>the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The design and implementation of a DCD device driver for Unix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tycho</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tycho</forename><surname>Nightingale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 USENIX Technical Conference (ATC&apos;99)</title>
		<meeting>the 1999 USENIX Technical Conference (ATC&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="295" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A case for redundant arrays of inexpensive disks (RAID)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randy</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1988 ACM International Conference on Management of Data (SIGMOD&apos;88)</title>
		<meeting>the 1988 ACM International Conference on Management of Data (SIGMOD&apos;88)</meeting>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient disk replacement and data migration algorithms for large disk subsystems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="345" />
			<date type="published" when="2005-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving storage system availability with D-GRAID</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sivathanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Conference on File and Storage Technologies (FAST&apos;04)</title>
		<meeting>the USENIX Conference on File and Storage Technologies (FAST&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="15" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Post-RAID alternatives address RAID&apos;s shortcomings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Staimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Adshead</surname></persName>
		</author>
		<ptr target="http://www.computerweekly.com/feature/Post-RAID-alternatives-address-RAIDs-shortcomings" />
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A framework for building unobtrusive disk maintenance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eno</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Bucy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">R</forename><surname>Lumb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd USENIX Conference on File and Storage Technologies (FAST&apos;04)</title>
		<meeting>the 3rd USENIX Conference on File and Storage Technologies (FAST&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PRO: A popularity-based multi-threaded reconstruction optimization for RAID-structured storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zzhikun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenlei</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Conference on File and Storage Technologies (FAST&apos;07)</title>
		<meeting>the 5th USENIX Conference on File and Storage Technologies (FAST&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="277" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Argon: Performance insulation for shared storage servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Abd-El-Malek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eno</forename><surname>Thereska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename><forename type="middle">R</forename><surname>Ganger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th USENIX Conference on File and Storage Technologies (FAST&apos;07)</title>
		<meeting>the 5th USENIX Conference on File and Storage Technologies (FAST&apos;07)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">S 2 -RAID: Parallel RAID architecture for fast data recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiguang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1638" to="1647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Reliability analysis on RAID+</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhufan</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Ceph: A scalable, highperformance distributed file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename><forename type="middle">D E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maltzahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</title>
		<meeting>the 7th USENIX Symposium on Operating Systems Design and Implementation (OSDI &apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="307" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">CRUSH: Controlled, scalable, decentralized placement of replicated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maltzahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 ACM/IEEE Conference on Supercomputing (SC&apos;06)</title>
		<meeting>the 2006 ACM/IEEE Conference on Supercomputing (SC&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scalable performance of the Panasas parallel file system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Unangst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zainul</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garth</forename><forename type="middle">A</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Small</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Zelenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Conference on File and Storage Technologies (FAST&apos;08)</title>
		<meeting>the 6th USENIX Conference on File and Storage Technologies (FAST&apos;08)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The HP AutoRAID hierarchical storage system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wilkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Staelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer System (TOCS)</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="136" />
			<date type="published" when="1996-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">WorkOut: I/O workload outsourcing for boosting RAID reconstruction performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzhen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th USENIX Conference on File and Storage Technologies (FAST&apos;09)</title>
		<meeting>the 7th USENIX Conference on File and Storage Technologies (FAST&apos;09)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MICRO: A multilevel cachingbased reconstruction optimization for mobile storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TOC)</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1386" to="1398" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Evaluation of distributed recovery in large-scale storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">J E</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 13th International Symposium on High-Performance Distributed Computing (HPDC&apos;04)</title>
		<meeting>13th International Symposium on High-Performance Distributed Computing (HPDC&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">SLAS: An efficient approach to scaling roundrobin striped volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwu</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zheng</surname></persName>
		</author>
		<idno>3:1-3:39</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage (TOS)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">ALV: A new data redistribution approach to RAID-5 scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwu</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TOC)</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="345" to="357" />
			<date type="published" when="2010-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">FastScale: Accelerate RAID scaling by minimizing data migration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on File and Storage Technologies (FAST&apos;11)</title>
		<meeting>the 9th USENIX Conference on File and Storage Technologies (FAST&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="149" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">GridGraph: Large-scale graph processing on a single machine using 2-level hierarchical partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 USENIX Annual Technical Conference (ATC&apos;15)</title>
		<meeting>the 2015 USENIX Annual Technical Conference (ATC&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
