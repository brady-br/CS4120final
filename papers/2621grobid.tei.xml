<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:48+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sweet Storage SLOs with Frosting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">Randy Katz University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">Randy Katz University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Alspaugh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Ion Stoica</orgName>
								<orgName type="institution">Randy Katz University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sweet Storage SLOs with Frosting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Modern datacenters support a large number of applications with diverse performance requirements. These performance requirements are expressed at the application layer as high-level service-level objectives (SLOs). However , large-scale distributed storage systems are unaware of these high-level SLOs. This lack of awareness results in poor performance when workloads from multiple applications are consolidated onto the same storage cluster to increase utilization. In this paper, we argue that because SLOs are expressed at a high level, a high-level control mechanism is required. This is in contrast to existing approaches, which use block-or disk-level mechanisms. These require manual translation of high-level requirements into low-level parameters. We present Frosting , a request scheduling layer on top of a distributed storage system that allows application programmers to specify their high-level SLOs directly. Frosting improves over the state-of-the-art by automatically translating high-level SLOs into internal scheduling parameters and uses feedback control to adapt these parameters to changes in the workload. Our preliminary results demonstrate that our overlay approach can multiplex both latency-sensitive and batch applications to increase utilization, while still maintaining a 100ms 99th percentile latency SLO for latency-sensitive clients.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Modern datacenter architectures must support a variety of both user-facing and internal applications. Each application typically has an associated performance requirement, expressed in terms of a service-level objective (SLO), such as a latency or throughput target for end-to-end requests. SLOs reflect the expectations of the users of that application and violations can result in significant economic repercussions <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, today's distributed storage systems, such as HBase and BigTable <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>, are not directly aware of applications' high-level SLOs. Current solutions require manual tuning of low-level internal system parameters like weights, slots, or disk IOPS until the desired high-level SLO is satisfied <ref type="bibr" target="#b9">[10]</ref>. Manually setting static parameters is painful for application programmers, who are forced to translate their high-level SLOs into these foreign lowlevel system parameters. Moreover, static parameters fundamentally fail to adapt to the rapid and frequent workload changes in the datacenter. Critically, manual tuning inevitably becomes unmanageable as system complexity and the number of workloads increase. Frontend and MapReduce Frontend q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q <ref type="figure">Figure 1</ref>: A latency-sensitive frontend application issues get requests to a static HBase configuration. A MapReduce job begins issuing scan requests to the same HBase cluster at time t = 100 and finishes at t = 500. Frontend latency increases by 3x due to contention for the storage layer.</p><p>These shortcomings stymie the ability of datacenters to multiplex different workloads onto a single, shared storage layer. Because of this inability to multiplex, datacenters often contain multiple, physically distinct storage systems, each provisioned separately. This has significant economic costs. First, each separate storage system must be provisioned individually for peak load. This requires a higher degree of overprovisioning, and contributes to underutilization of the cluster <ref type="bibr" target="#b1">[2]</ref>. Second, segregation of data leads to data staleness. Analytics jobs typically must wait for the freshest data to be copied into a batch system. This lag results in delayed insights and suboptimal results <ref type="bibr" target="#b2">[3]</ref>. Copying also leads to duplication of data and the associated consistency and management concerns. Finally, there is increased operational complexity, requiring additional staff and expertise, and increasing exposure to software bugs and configuration errors <ref type="bibr" target="#b12">[13]</ref>.</p><p>Consolidating multiple workloads onto a single storage system avoids these problems. However, a na¨ıvena¨ıve approach to consolidation does not realize these benefits, since unmediated resource demands lead to SLO violations. In an initial experiment, we ran a latency-sensitive frontend application and a MapReduce job, both simultaneously issuing requests against the same HBase cluster. The results of this experiment <ref type="figure">(Figure 1)</ref> show that the latencysensitive frontend application can suffer a 3x increase in 99th percentile latency when run alongside a batch job. Companies like Amazon, Google, and Microsoft have identified that degradation in the 99th percentile latency is a major source of user dissatisfaction <ref type="bibr" target="#b11">[12]</ref>. Thus, to con-trol the end-to-end 99th percentile latency while consolidating workloads, we focus on high-level SLOs, which marks a shift from existing solutions based on low-level operations.</p><p>However, there are a number of challenges to overcome to achieve this goal.</p><p>Translating high-level SLOs. The storage system should allow programmers to specify their high-level SLOs directly, and automatically translate these SLOs into the correct set of internal system parameters.</p><p>Scheduling. The lack of preemption and skew in request size differentiates storage request scheduling from other domains. This is exacerbated by how distributed storage systems sit atop a deep software and hardware stack <ref type="figure">(Figure 2</ref>). Consolidating storage workloads requires choosing appropriate scheduling mechanisms at the right layers in the stack.</p><p>Dynamic SLO enforcement. Due to diurnal patterns and load fluctuations, the storage system needs to adjust request scheduling in response to the workload. This dynamism will allow the system to take advantage of slack in the workload to run low-priority requests, while also being able to gracefully adapt to load spikes.</p><p>To address these concerns, we present Frosting. Frosting adopts a new top-down approach to enforcing SLOs. Frosting acts as an overlay atop an existing distributed storage system, performing scheduling on requests from different applications to enforce SLOs. An application's high-level SLOs are translated into proportional share allocations by a feedback controller. These allocations are used to weight the admission of a client's requests to the underlying storage system. The feedback controller adapts to dynamic changes in the workload by monitoring each client's performance, and continually adjusting allocations to maximize overall SLO compliance of the system. Our preliminary results demonstrate that our overlay approach can multiplex both latency-sensitive and batch applications to increase utilization, while still maintaining a 100ms 99th percentile latency SLO for latency-sensitive clients. Finally, we discuss further improvements and potential avenues of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Design</head><p>Frosting is designed to overlay distributed storage systems which support get, put, and scan operations on rows or objects. An example architecture is shown in <ref type="figure">Fig- ure 2</ref>. Frosting is backwards-compatible with the API of the underlying storage system. Clients with an associated SLO tag their requests to Frosting with their client name. Legacy untagged requests are treated as low-priority, and are handled with a best-effort FIFO policy. High-level SLOs are defined as a percentile latency or throughput target for a given type of request from a client name. For example, an application could tag all of its requests with and use online performance measurements to build a linear performance model that predicts how a client's performance will change with changes in scheduler allocation. These performance models are used by the feedback controller to continually adjust each client's allocation to maximize SLO compliance of the system. Frosting's scheduling mechanism and dynamic control scheme are detailed in §2.2 and §2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">System Model</head><p>Frosting ensures predictable performance by limiting the number of outstanding requests in the underlying software and hardware stack. Limiting the number of outstanding requests minimizes the degree of queuing in lower lay- ers, which is important since queuing delay tends to dominate total latency in this type of system <ref type="bibr" target="#b10">[11]</ref>. In fact, as Frosting has no way of preempting or controlling a request once it has been issued to a lower layer, keeping the queue lengths in lower layers short is the only way to bound latency. However, limiting queue length means the system operates below peak utilization. <ref type="figure" target="#fig_2">Figure 4</ref> demonstrates the effect of queueing within the stack. We performed an experiment with two clients, the first issuing a constant rate of latency-sensitive reads, and the second issuing an increasing rate of large scans. As the rate of scans increases above 8 req/s, the length of the block I/O queue in the operating system (as measured by iostat) increases rapidly, along with 99th percentile read latency. To preserve latency, Frosting attempts to keep queue lengths below this operating point.</p><p>The overlay scheduling approach we chose with Frosting is simple and minimizes changes to existing code. A coordinated scheduling approach, which performs request scheduling at multiple layers of the stack, may be able to achieve better latency and throughput properties. However, out initial evaluations of Frosting show that even an overlay approach can result in meaningful improvements. We plan to explore the strengths and weaknesses of both approaches as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Scheduling</head><p>With the feedback controller enforcing policy in the form of user-stated SLOs, the scheduler becomes merely a mechanism. The feedback controller adjusts the scheduler's parameters to effect a corresponding change in a client's performance. The Frosting scheduler is thus unconcerned with traditional scheduling goals of maximizing fairness or interactivity. Instead, its primary requirement is to behave predictably to changes made by the feedback controller, with the secondary goal of simplicity of API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Storage request scheduling</head><p>Request scheduling for distributed storage systems is a challenging problem which differs from the classical domains of CPU and packet scheduling in two respects.</p><p>Lack of preemption. CPU schedulers use preemption to limit the amount of CPU time spent running a thread. A request scheduler is unable to preempt requests because it is impossible to "unsend" or cancel a request to a lower layer of the stack. This lack of preemption leads to greater degrees of skew in allocation in the system, which complicates allocating exact portions of system resources.</p><p>Size skew. In packet scheduling, packets are quantized units up to an MTU in size, and require processing time directly proportional to size. Storage requests to Frosting are not chunked into MTUs, and can take unknown amounts of processing time. A large 1000-row scan is viewed as a single request, resulting in a long blocking operation of unknown length. The actual processing time depends on the type of request, composition and magnitude of the workload, and underlying system properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Frosting Scheduler</head><p>Scheduling in Frosting can be modeled abstractly as a set of worker threads pulling requests from a shared queue. Incoming requests are pushed into this queue, and popped off by worker threads. Internal to this abstract queue, the Frosting scheduler separates requests into additional queues based on client name and request type. Proportional share scheduling is used to decide whose request to serve <ref type="bibr" target="#b22">[23]</ref>. Proportional share supports fine-grained, fractional allocation of resources. It is convenient for situations with a dynamic set of clients, since relative weightings always remain consistent.</p><p>The Frosting prototype implements proportional share via lottery scheduling <ref type="bibr" target="#b22">[23]</ref>. Each time a worker thread becomes inactive, a lottery is held among all queues with pending requests. The lottery is biased based on the proportional share assigned to each queue, such that a queue with twice the share will have twice as many of its requests dequeued.</p><p>This ignores the actual cost of handling a request, and is most predictable when all queues have outstanding requests. However, as shown in §3, even this simple scheme shows benefits under the assumption of closedloop clients. We discuss other potential scheduler improvements in §5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dynamic Control</head><p>The feedback controller in Frosting is used to adapt to changes in the workload. The controller actively monitors each client's normalized SLO performance, which is expressed as the ratio between a client's desired and actual performance. For example, a client with an SLO of 100ms experiencing 200ms of actual latency has a normalized SLO performance of 0.5. The feedback controller periodically adjusts each client's proportional share allocation to maximize overall SLO compliance of the system. These changes are made based on an online linear performance model of each client built from performance measurements collected in prior time intervals. Since these models are recomputed each time interval, they approximate the actual non-linear behavior of the system. Solving for client allocations a i , ..., a n for an approaching time interval t can be expressed as a linear program, originally formulated by Merchant, et al. <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_0">∀ i,j,i =j w i (1 − p i (a t i )) − w j (1 − p j (a t j )) &lt; The (1 − p i (a t i )</formula><p>) term expresses a "badness" factor for client i in terms of normalized SLO performance, with potential allocation a t i and the client's linear performance model p i . This badness factor is then weighted by a w i term that indicates the relative importance of the client.</p><p>The linear program attempts to minimize , the maximum difference in badness of all pairs of clients i, j. During overload, this has the effect of proportionally degrading each client's performance according to their weight.</p><p>Two additional constraints are necessary. Changes in allocation are limited to a step size σ, which prevents the feedback controller from solving for allocation points where the linear performance models do not apply. Total allocation by the system must also sum to 1.</p><formula xml:id="formula_1">∀ i |a t i − a t−1 i | ≤ σ a t 1 + ... + a t n = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>The prototype implementation of Frosting is built on top of HBase, a distributed column-store similar to Google's BigTable <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>. Modifications to existing code were minimal. It was straightforward to replace the existing FIFO request queue with our own abstract queue with overloaded enqueue and dequeue methods. The HBase RPC protocol was also modified to include the client's name with each request. No modifications were necessary to HDFS or the operating system. We evaluate two components of Frosting. First, we evaluate the suitability of the Frosting proportional share scheduler as a mechanism ( §2.2). We test how setting different proportional share values affect a latency-sensitive client's 99th percentile SLO when Frosting is under contention from a batch workload. Next, we use the same situation to evaluate the ability of the feedback controller to enforce policy by setting a high-level SLO for the latencysensitive client ( §2.3). All experiments were run on a 3-node cluster of EC2 c1.xlarge instances, with HDFS configured to use the 4 local disks on each instance. A Yahoo! Cloud Serving Benchmark <ref type="bibr" target="#b6">[7]</ref> (YCSB) client was used to issue latency-sensitive read requests to Frosting with a uniformly random key access pattern. A MapReduce word count job with 4 map tasks was used to generate batch scan requests to Frosting over the same key range as the YCSB client.</p><p>Proportional share scheduler. To provide a baseline, we measured YCSB latency with HBase's default single FIFO queue without MapReduce running concurrently. We then ran three experiments with concurrent MapReduce: the Frosting scheduler with share settings of 90:10 and 99:1 for YCSB to MapReduce, as well as HBase's default FIFO policy again. Each experiment was run 10 times. <ref type="figure" target="#fig_3">Figure 5(a)</ref> plots the mean and standard deviation in 99th percentile latency for each run. <ref type="figure" target="#fig_3">Figure 5(b)</ref> shows the average MapReduce throughput for each.</p><p>We see that the Frosting scheduler performs superiorly to FIFO when YCSB is under contention with MapReduce. Both proportional share settings show little fluctuation over the duration of the experiment, and are able to achieve better latencies than FIFO. While FIFO increases to as much as twice the baseline 99th percentile latency, the 99:1 policy remains within 10% of baseline while increasing utilization by allowing some MapReduce requests to run. 90:10 strikes a point in between FIFO and 99:1, allowing more MapReduce requests to run at the cost of increased latency. This demonstrates that proportional share allocations predictably map to 99th percentile latency values, and shows how Frosting can tradeoff between two competing clients.</p><p>Feedback control. Given that proportional share is an appropriate mechanism, we evaluate the ability of the feedback controller to converge on and maintain a highlevel SLO. We specified a 99th percentile read SLO of 100ms for YCSB to Frosting, with a weight of w 1 = 80. MapReduce was given a throughput SLO of 40 req/s with a lower weight of w 2 = 1. This throughput SLO allows MapReduce to balloon to use any extra capacity in the system as long as YCSB's SLO is being met.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Many others have looked at consolidating applications onto shared storage resources. Soundararajan et al. present an approach to controlling storage bandwidth contention in a server farm by passing application-level quality-of-service requirements across to a SAN and down to the OS <ref type="bibr" target="#b19">[20]</ref>. Their approach requires modification to the kernel.</p><p>Padala et al. also focus on controlling resource contention among applications, but use a hypervisor-level approach involving controlling CPU allocations to storage clients <ref type="bibr" target="#b16">[17]</ref>. Similarly, Soundararajan et al. present a technique for coordinated partitioning of storage and memory bandwidth in a shared, virtual storage environment to prioritize among latency-sensitive applications <ref type="bibr" target="#b20">[21]</ref>. We differ in that we support latency SLOs while also scheduling throughput-oriented requests to improve utilization.</p><p>Argon provides performance isolation for shared storage clients also using cache partitioning, request amortization, and quanta-based disk time scheduling, but focuses on providing guarantees for throughput rather than high-percentile latency <ref type="bibr" target="#b21">[22]</ref>. PARDA, mClock, and Maestro provide SLOs for virtual machines and storage arrays <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15]</ref>. Differentiated storage attaches semantic information to storage requests to associate specialized caching policies <ref type="bibr" target="#b15">[16]</ref>. These systems differ from ours by operating at the block device level and not incorporating high-level SLOs.</p><p>Many other systems have also focused on scheduling disk I/O for differentiated quality-of-service <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19]</ref>. They differ from Frosting in that they provide guarantees on operations to the block device, rather than requests to an application-level storage system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Future Work</head><p>We are considering multiple avenues of future work.</p><p>Additional scheduler parameters. Proportional share scheduling can be combined with allocation reservations and limits to express stronger policies for certain workload scenarios. Another potential parameter is adjusting the total number of worker threads in the system. However, these additional parameters are more difficult to model dynamically and fit within a linear program.</p><p>Coordinated stack scheduling. Controlling request scheduling at each layer of the software stack, while more complicated, should perform better than a pure overlay scheduler like Frosting. A coordinated approach can also better manage "multicast" effects, where a single request to a layer spawns multiple requests to lower layers.</p><p>Storage system interfaces. Consolidating onto a shared storage layer requires clients to use a common storage API. We chose to use HBase in Frosting, but a client might wish to also consolidate MapReduce running directly on HDFS, or MySQL directly on the block device. Settling on a common storage abstraction is necessary to support a broad range of clients.</p><p>Economic argument. There is an inherent tradeoff between throughput and latency in any queuing system. An open problem is understanding when consolidation reduces monetary provisioning costs and quantifying this economically.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Frosting running on a 3-node cluster. By scheduling get, put, and scan requests, controlling when and what order they are released to HBase, Frosting limits queuing in lower layers of the software and hardware stack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A fixed number of gets were issued to the HBase cluster, while the number of scans were slowly increased. At a rate of 8 or fewer scans per second, queue lengths in lower layers are short, keeping latency low.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 (</head><label>5</label><figDesc>Figure 5(c) depicts how the feedback controller adjusts YCSB's proportional share, and YCSB's 99th percentile latency. YCSB and MapReduce initially start off with equal share allocations. The feedback scheduler increases YCSB's allocation at the maximum step size (σ), until YCSB meets its 99th percentile latency SLO around t = 70. Since YCSB briefly exceeds its performance target here, share oscillates as the controller trades off between YCSB and MapReduce, before settling around a value of 90. The inherent noisiness in 99th percentile latency results in occasional variation in YCSB share and latency, even if share allocations do not change. We plan to explore further modifications to the feedback controller to mitigate this effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: In two experiments, a latency-sensitive frontend workload (YCSB) and a throughput-oriented batch workload (MapReduce) issue requests to the same HBase cluster. Left and middle: Here, the Frosting share parameters are set to a fixed known value. We compare the latency (left) and throughput (right) to those obtained by the baseline and FIFO. Performance of the two clients varies predictably as the share values are adjusted. Right: When a 100ms YCSB SLO is specified, the feedback controller rapidly increases YCSB's share until the SLO is met at t = 70. The controller converges to approximately a 90:10 setting.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported in part by an NSF CISE Expeditions award, gifts from Google, SAP, Amazon Web Services, Blue Goji, Cisco, Cloudera, Ericsson, General Electric, Hewlett Packard, Huawei, Intel, MarkLogic, Microsoft, NetApp, Oracle, Quanta, Splunk, VMware and by DARPA (contract #FA8650-11-C-7136).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hbase</surname></persName>
		</author>
		<ptr target="http://hbase.apache.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Warehouse-Scale Computing: Entering the Teenage Decade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Apache hadoop goes realtime at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borthakur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disk scheduling with quality of service guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMCS &apos;99</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Performance virtualization for large-scale storage systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chambliss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SRDS &apos;03</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bigtable: A distributed storage system for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>ACM TOCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Benchmarking cloud serving systems with YCSB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOCC &apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Start-time fair queueing: a scheduling algorithm for integrated services packet switching networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Networking</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">PARDA: Proportional allocation of resources for distributed storage access</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Waldspurger</surname></persName>
		</author>
		<idno>FAST &apos;09</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">mclock: Handling throughput variability for hypervisor io scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Varman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated IO load balancing across storage devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The cost of latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
		<ptr target="http://perspectives.mvdirona.com/2009/10/31/TheCostOfLatency.aspx" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Izrailevsky</surname></persName>
		</author>
		<ptr target="http://techblog.netflix.com/2011/01/nosql-at-netflix.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Facade: Virtual storage devices with performance guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Lumb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;03</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maestro: quality-of-service in large disk arrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICAC &apos;11</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Differentiated storage services. SIGOPS Operational System Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Mesnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Akers</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive control of virtualized resources in utility computing environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Padala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys &apos;07</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The user and business impact of server delays, additional bytes, and http chunking in web search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schurman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brutlag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cello: A disk scheduling framework for next generation operating systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Vin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS &apos;97</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards end-to-end quality of service: Controlling i/o interference in shared storage servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amza</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In Middleware &apos;08</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic resource allocation for database servers running on virtual storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Soundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;09</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Argon: performance insulation for shared storage servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAST &apos;07</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lottery scheduling: Flexible proportional-share resource management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Weihl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI &apos;94</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
