<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:56+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IOPriority: To The Device and Beyond</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Manzanares</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Western Digital Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Blagojevi´cblagojevi´c</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Western Digital Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyril</forename><surname>Guyot</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Western Digital Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">IOPriority: To The Device and Beyond</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>In large scale data centers, controlling tail latencies of IO requests keeps storage performance bounded and predictable, which is critical for infrastructure resource planning. This work provides a transparent mechanism for applications to pass prioritized IO commands to storage devices. As a consequence, we observe much shorter tail latencies for prioritized IO while impacting non-prioritized IO in a reasonable manner. We also provide a detailed description of the changes we made to the Linux Kernel that enable applications to pass IO priorities to a storage device. Our results show that passing priorities to the storage device is capable of decreasing tail latencies by a factor of 10x while decreasing IOPS minimally.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Hard disk drives (HDD) have been a part of the computer storage hierarchy since the 1950s. Although alternative technologies (such as flash) have been introduced, HDDs are still relevant to this day due to their capacity, performance, and cost properties, which place them firmly between tape and flash. HDD technology improvements have pushed the devices to greater densities, but the fact remains that the HDD is a mechanical device with limited opportunities for request parallelism. This has led to a decrease in the performance to capacity ratio of HDDs, measured in IOPS/TB, due to the fact that capacity has increased while IOPS have remained nearly flat.</p><p>In order to increase the IOPS from a single actuator arm that controls HDD heads, modern hard disk drives employ device level queues that are managed by drive firmware schedulers. These schedulers leverage drive internal information, such as magnetic head position, to determine the IO request that should be served next. Although these sophisticated schedulers are able to improve the throughput of a set of requests, this comes at a cost of increased tail latencies. Furthermore, a mechanism that would preserve the IO order between the drive and the hostside IO queues is currently non-existent. As a consequence, host-drive IO re-scheduling is a common occurrence, i.e. IO requests ordering established at the host side experience reordering once the requests reach the drive. A common effect caused by re-scheduling are unpredictable and long IO latencies. At the same time, with the rise in cloud based storage, and the fact that HDDs are the central element of this storage, HDD tail latency has become a critical performance factor <ref type="bibr" target="#b2">[7]</ref>.</p><p>To limit the worst case latency for performancecritical IO requests, two main approaches have been considered in production environments: (i) prioritizing the "real-time" over the "background" IO requests, and (ii) limiting the number of IO requests issued to a storage device. Although the first option appears to be a reasonable approach it is rarely used in production settings due to the host's inability to communicate the priority information to a storage device. Consequently any high priority IO request is likely to get rescheduled by the device scheduler, and possibly experience high delays. Currently, limiting the number of issued IO requests is the default option in many data centers. Submitting a low number of IOs to a storage device limits the effects of the device IO scheduler and reduces IO tail latencies. Unfortunately, the low number of outstanding IOs also prevents users from reaching the highest possible IO throughput.</p><p>In this work we provide a path of communication between a user application and a storage device that allows device-level IO request prioritization. Passing the application-level priorities to a storage device scheduler minimizes IO re-scheduling. Additionally this reduces the tail latency of performance-critical IOs without throttling IO requests and disabling the device scheduler. The communication path now includes user applications, Linux block schedulers, and the drive scheduler on the other end. It is important to note that our work leverages existing IO prioritization APIs without any changes and our implementation has been merged into the 4.10 series kernel <ref type="bibr">[4]</ref>. The rest of this paper includes a discussion of the host and drive level queue interactions as well as a set of results that stress the effectiveness of bridging the host and HDD schedulers. Although our work was motivated by HDDs, our solution is relevant to applications leveraging storage devices that employ internal queueing, such as solid-state drives and future storage-class memories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Host-Device Queue Interaction</head><p>Adding support for priority being passed to storage devices is a critical tuning knob when devices queue commands internally. The SATA, SCSI, and NVMe storage standards have support for queued commands and the storage device is allowed to reorder or delay these queued commands based on the device internal information that is not visible to the host. Typically HDD schedulers have favored throughput over latency, which is no longer prudent for cloud storage systems.</p><p>To demonstrate the performance impacts of device schedulers, we show the inversely proportional relationship of HDD throughput to IO request tail latencies by varying the number of IO requests in the device queue. It is important to understand this relationship because it is the main motivation for increasing the control over the HDD scheduler. We focus on small, random read IO commands because these commands are latency sensitive and not cacheable (writes are frequently buffered, caching avoids the storage device, large IOs have high latencies). <ref type="figure" target="#fig_0">Figure 1a</ref> captures a set of experiments where the fio tool <ref type="bibr">[1]</ref> issues 4KiB random read requests to an HGST Ultrastar HE8 drive. The number of outstanding requests ranges from 1-32 and performance is measured in terms of throughput and 99.99 percentile latency. To ensure the IO requests are not served from a cache, we run fio with the DIRECT IO option. Presented results clearly show an increase in drive throughput that correlates with the number of outstanding requests, i.e. the throughput nearly doubles as the number of outstanding requests grows from 1-32. <ref type="figure" target="#fig_0">Figure 1a</ref> also demonstrates that the increase in IOPS comes at a cost to the tail latency the application experiences. The tail latency increases from under 100ms to over a second which is a degradation of nearly 10x. With cloud storage providers pinpointing tail latency as a key metric <ref type="bibr">[2,</ref><ref type="bibr">3]</ref>, we are in strong need of mechanisms that allow controlling the tail latency of individual applications.</p><p>The IO schedulers in the kernel storage stack were designed to control the latency and performance characteristics of user applications. There are three common schedulers in the Linux Kernel, CFQ, Deadline and NOOP, and only CFQ is capable of handling IO priorities. The main idea behind introducing prioritization in the host scheduler is to control the IO duration and avoid high latencies for performance critical IOs (presented in <ref type="figure" target="#fig_0">Figure 1a</ref>). In our next set of experiments we examine the effects of the CFQ prioritization on the IO request latency and total number of IOPS. We simultaneously run two types of workload on a single HDD: (i) background workload, always with 32 outstanding requests, and (ii) foreground workload with varying number of outstanding requests from 1-32. The foreground workload we run either with or without prioritization. <ref type="figure" target="#fig_0">Figure 1b</ref> shows that CFQ achieves some level of fairness between the foreground and background workloads when priority is not used. The scheduler has the nice property that it does not starve the foreground workload in case of a low issue depth, and at the same time the background work is still able to achieve nearly half of the available IOPS. One thing to note is that the total IOPS is noticeably lower than the IOPS demonstrated in <ref type="figure" target="#fig_0">Fig- ure 1a</ref>. In addition the CFQ scheduler does not increase foreground performance as the issue depth increases, but the background workload performance does degrade with the increased workload from the foreground. This is an unusual result because one would expect the foreground performance to increase with a decrease in IOPS for the background work. In addition, the foreground and background latencies increase with increased foreground workload. While the experiments reveal IOPS performance problems with CFQ in certain cases, the overall conclusion is that CFQ does a nice job of keeping a relatively low tail latency for foreground work. CFQ provides reasonable isolation between the foreground and background process.</p><p>In the case when IO priority is set on the foreground work, <ref type="figure" target="#fig_0">Figure 1b</ref> presents somewhat unintuitive behavior. Foreground work has a lower total IOPs and much higher latencies, when we indicate that the foreground work is high priority. In addition the background workload starts to see much higher IOPs. At low foreground queue depths we observe priority inversion. Once the queue depth of the foreground requests starts to rise to 8 requests and beyond, we see that the background work nearly stops completely and the foreground work has much better tail latencies.</p><p>In conclusion, host-level schedulers that implement priority awareness aren't very intuitive in their behavior with mixed workloads. In addition host level schedulers implementing priority leverage idling to provide isolation between priority classes, but this has the negative effect of lowering drive throughput. Due to re-scheduling that occurs within the drive, the host scheduler cannot make any guarantees about performance when a request is dispatched to a drive queue. To combat this problem, the schedulers idle to guarantee that all outstanding prioritized commands are finished in a predictable manner. It is our belief that our work is complementary to hostschedulers leveraging priority. We believe there is now an opportunity to revisit host-schedulers to take advantage of the benefits of device level priorities. , where work is F for foreground, B for background, Priority On is P, and drive priority is D. <ref type="figure" target="#fig_0">Figure 1a</ref> shows the relationship that drive queue depth has on application performance. <ref type="figure" target="#fig_0">Figure 1b</ref> demonstrates that prioritized commands and schedulers sometimes produce unexpected results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Implementation in The Linux Kernel</head><p>This section describes the prioritization extensions we introduce into the Linux Kernel. As described in Section 2, IO prioritization is a currently supported and well documented feature of the Linux kernel. However, the only IO scheduler that leverages the IO prioritization features is the CFQ scheduler. IO priorities can be assigned to a single process or a group of processes, and are manipulated via the iopriority set() system call. IO requests in the Linux Kernel travel a fairly complex path from the application to the storage device. As the IO request travels from the application to the device, crossing multiple kernel layers, it is represented in several forms. To better understand the technical details of our work, and in order to describe the changes we introduce into the kernel, we further examine the IO path in the Linux Kernel and how IO prioritization fits into this path. After being dispatched by a user application through a read()/write() system call, an IO request is handled by three main layers within the kernel: VFS, block and the device driver layer. Upon reaching the kernel and the VFS layer, a bio structure is created from the initial read()/write() requests, which can then be used by stackable device mapper targets. Eventually the bio structure is passed to the block layer where it gets transformed into a request structure, which is used by block layer schedulers to reorder and merge requests. CFQ is the only priority aware scheduler and obtains the priority of the IO request from the iocontext structure that is associated with the task struct structure of the current running task. The iocontext can be manipulated with the iopriority set() / get() system calls. Based on the iocontext value, the request structure is placed in a high/low priority queue. After the scheduler dispatches a request the priority is no longer used, and the request optionally transforms into another form (typically a SCSI command) before eventually being converted by device drivers into device specific requests.</p><p>In order for IO prioritization to be a useful feature that is independent of block layer schedulers, we have made changes to the block layer of Linux. We now associate the iocontext priority information with the request when a bio is converted into a request. This allows request based device drivers to act on priority information, which includes SATA, SAS, and NVMe devices. In this work our focus was on enabling prioritization within SATA devices, because they are the dominant storage devices within the cloud storage stack. We have currently implemented priority support for the device in the libata layer, which is used by SATA devices and is capable of interfacing with several HBAs including AHCI (implemented in many motherboards). The reader should note that the kernel-device communication is performed through an HBA that takes requests from the kernel and internally converts them to device commands. Consequently, passing prioritized commands to a device depends on the HBA support. We also discovered that the Broadcom LSI 9300-4I4E HBA supported passing prioritized commands to SATA HDDs so we updated the driver for this HBA, mpt3sas, to support iopriorities.</p><p>To demonstrate the IO latency effects of host→device priority communication, we repeat the experiments used to generate <ref type="figure" target="#fig_0">Figure 1b</ref>, only this time we run the experiments with priority information passed to the storage device. In addition to CFQ, we also include results generated by the NOOP and DEADLINE schedulers because our work enables IO priority to be independent of the scheduler. The results were collected on an 8 core Intel E5-2640 with 256GiB of memory running the 4.10.1 Linux kernel. All results were collected on a HGST UltraStar HE8 HDD using DIRECT IO in order to observe storage device performance. <ref type="figure" target="#fig_1">Figure 2</ref> represents the performance results for multiple combinations of foreground fio workloads and available IO schedulers. As in Section 2, the background workload is fixed to 32 outstanding IO requests, while the foreground work varies this parameter from 1-32. <ref type="figure" target="#fig_1">Figure 2</ref> shows that passing priority to the drive can improve application IOPS and tail latencies under mixed workloads. This result is consistent across all schedulers, which enables an application greater flexibility to tune their performance requirements. When we look at the results for the NOOP and DEADLINE schedulers, <ref type="figure" target="#fig_1">Fig- ure 2a and 2b</ref> respectively, we can observe nearly identical behavior. Recall that neither of these host schedulers have priority support built into their scheduling decisions. Therefore, they do not attempt to reorder or delay requests based on the priority, instead these schedulers just pass the priority value to the drive. The internal HDD scheduler uses the passed priorities to decide about the command ordering. The results in these figures are nearly identical so we choose to examine them in tandem. In these results we see that the foreground IOPS goes up significantly for lower queue depths when prioritized requests are issued to the drive, but has diminishing returns with higher queue depths. This is expected because the drive scheduler can best perform when there is a lower ratio of high priority to default priority requests. When the ratio of prioritized commands rises then non-prioritized commands get delayed causing anti-starvation mechanisms to kick-in. This leads to the scheduler switching between requests from both the prioritized and non prioritized set of commands, reaching lower than optimal IOPS.</p><p>Figures 2a and 2b demonstrate that enabling prioritization within the device has a large positive impact on the tail latency of the foreground IO. For the foreground workload, QD 1 and NOOP scheduler, priorities reduce the tail latency from 1.4s down to 81ms, over 10x. It is interesting to note that these results are collected with the background workload fixed at 32 outstanding requests, which puts significant stress on the HDD resources. The lower tail latency is observable for all queue depths across both the DEADLINE and NOOP schedulers. For QD 1, the tail latency of the background workload increases by about 2x when priority is passed to the drive and this is also expected given the drive is forced to work on prioritized IO. Expectedly, the tail latency for the background workload further increases as the number of high priority requests is increased.</p><p>The last set of results we wish to examine are what happens to the CFQ scheduler when we pass iopriorities to the device. In <ref type="figure" target="#fig_1">Figure 2c</ref> we see that by passing iopriority to the drive, in addition to using priority in the scheduler, we are able to ensure the prioritized IO has lower latencies. Note that this figure is identical to <ref type="figure" target="#fig_0">Fig- ure 1b</ref> with the addition of performance results of passing priority to the drive. Recall that we discussed in Section 2 that CFQ had the surprising behavior of increasing tail latency and lowering IOPS of prioritized foreground workloads, but by passing priority to the drive this is no longer an issue. We also see that IOPS are increased for foreground work with QD 4 and lower. The tail latency of the prioritized commands are vastly improved when we pass them to the device across all queue depths.</p><p>In summary by passing iopriority to the storage device, foreground IO achieves much better latency numbers and also increased IOPS in many cases. This improvement comes at a cost to the IOPS and latency of the background workload. These properties are held independently of the host level scheduler that is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>IO scheduling has been the main topic of many studies in recent years. <ref type="bibr">Kim et al. [12]</ref> propose connecting the IO priorities across the storage stack layers, to address the problem of priority inversions. Their approach is request-centric and based on the IO dependencies. Yang et al. introduce split scheduling <ref type="bibr" target="#b13">[18]</ref>, a handler-based cross-layer scheduling that enables preserving the IO information. Split scheduling prevents host-side storage layers, from reordering an IO request. IOFlow <ref type="bibr" target="#b12">[17]</ref> provides IO differentiation information between storage hypervisors and servers. Our method can be seen as complementary to these approaches, because we extend prioritized host based-scheduling to a device.</p><p>Young Jin et al. <ref type="bibr" target="#b14">[19]</ref> address the issue of host-IO Device re-scheduling. They acknowledge the host-device IO re-scheduling and design a dynamic scheduler that allows switching on/off the host and the device schedulers at runtime. They do not attempt passing the IO priorities to the device. A set of real-time scheduling algorithms has been developed <ref type="bibr" target="#b11">[16,</ref><ref type="bibr" target="#b3">8,</ref><ref type="bibr" target="#b11">16]</ref>. While they rely on SCAN to deliver highest throughput, we allow our priority commands to be processed completely independently of the standard drive request processing algorithm. Other traditional schedulers <ref type="bibr" target="#b6">[11,</ref><ref type="bibr" target="#b9">14,</ref><ref type="bibr" target="#b8">13]</ref> allow IO priorities, but do not pass them to the IO device. Differentiated Storage <ref type="bibr" target="#b10">[15]</ref> proposed modifications to IO interfaces to provide application to device IO classification, whereas we reuse existing interfaces and infrastructure. Multiple studies <ref type="bibr" target="#b4">[9,</ref><ref type="bibr" target="#b0">5,</ref><ref type="bibr" target="#b5">10]</ref> investigate IO scheduling in distributed systems, by mixing IO-and computation--bound workloads on a single node and focusing on the effects of multi-level IO scheduling in a virtualized/cloud environment. These studies are limited to the host side and do not include priority-aware IO devices. Blagojevi´cBlagojevi´c et al. <ref type="bibr" target="#b1">[6]</ref> show the importance of IO scheduling in a cloud environment and address the device interaction with a distributed IO accesses. In our work we systematically examine the Linux kernel changes necessary to allow IO prioritization within the device. Our work allows transparent integration of priorities with distributed systems and local host software.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Directions</head><p>In this work we have shown that using iopriority within a storage device improves prioritized application tail latencies and IOPS. Our approach is complementary to existing priority handling in host level schedulers and we have implemented our changes in the Linux Kernel and these changes have been merged upstream. The results show that prioritized commands to the device improve foreground performance across three Linux schedulers. This work is a step in the direction of allowing finer grained control of storage device behavior. Although we have demonstrated our work on an HDD it is applicable to SSDs as well, because they also queue and delay requests internally. With cloud storage providers being heavily dependent on predictable tail latencies this work provides a new dimension of optimization which is critical for cost and performance sensitive applications.</p><p>Our future work will focus on examining more workloads and including mixed read/write workloads. Prioritization is currently implemented at the process level and we plan to investigate command level prioritization with aio interfaces in the Linux Kernel. In addition we will look at the internals of host level schedulers and identify areas where knowledge of device priority may lead to alternative design choices. This work has focused on HDD performance and another direction of the future work will be an SSD extension. In addition, the NVMe standards have command and queue level prioritization and we plan to investigate ioprioritization in this context.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Current Behaviors of HDD and Schedulers The two graphs demonstrate the performance impact of increasing QD as well as how host and device schedulers can produce unexpected behavior. In all our figures IOPS are represented as solid lines and dashed lines indicate tail latency. The legend is a tuple of [Scheduler.Work.Priority On.Drive Priority], where work is F for foreground, B for background, Priority On is P, and drive priority is D. Figure 1a shows the relationship that drive queue depth has on application performance. Figure 1b demonstrates that prioritized commands and schedulers sometimes produce unexpected results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Drive Priority &amp; Scheduler Impact These graphs demonstrate that priority information passed to the storage device improves latency across all host level schedulers.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coexisting scheduling policies boosting i/o virtual machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aragiorgis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koziris</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 international conference on Parallel Processing</title>
		<meeting>the 2011 international conference on Parallel Processing</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="407" to="415" />
		</imprint>
	</monogr>
	<note>Euro-Par&apos;11</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Priority IO scheduling in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blagojevic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guyot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma-Teescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bandic</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th USENIX Workshop on Hot Topics in Cloud Computing, HotCloud&apos;13</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Disks for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brewer</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Greenfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cypher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And T&amp;apos;so</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Google</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reschedulable-group-scan scheme for mixed real-time/nonreal-time disk scheduling in a multimedia system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-I</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-K</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Syst. Softw</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="143" to="152" />
			<date type="published" when="2001-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">I/o scheduling model of virtual machine based on multi-core dynamic partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing</title>
		<meeting>the 19th ACM International Symposium on High Performance Distributed Computing<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="142" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive disk i/o scheduling for mapreduce in virtualized environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Parallel Processing</title>
		<meeting>the 2011 International Conference on Parallel Processing<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Anticipatory scheduling: a disk scheduling framework to overcome deceptive idleness in synchronous i/o</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Druschel</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth ACM symposium on Operating systems principles</title>
		<meeting>the eighteenth ACM symposium on Operating systems principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="117" to="130" />
		</imprint>
	</monogr>
	<note>SOSP &apos;01, ACM</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enlightening the i/o path: A holistic approach for application performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Conference on File and Storage Technologies</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="345" to="358" />
		</imprint>
	</monogr>
	<note>USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Freeblock scheduling outside of disk firmware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumb</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganger</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st USENIX conference on File and storage technologies</title>
		<meeting>the 1st USENIX conference on File and storage technologies<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="20" to="20" />
		</imprint>
	</monogr>
	<note>FAST&apos;02, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards higher disk head utilization: extracting free bandwidth from busy disk drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumb</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riedel</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th conference on Symposium on Operating System Design &amp; Implementation -Volume</title>
		<meeting>the 4th conference on Symposium on Operating System Design &amp; Implementation -Volume<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="7" to="7" />
		</imprint>
	</monogr>
	<note>OSDI&apos;00, USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differentiated storage services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mesnier</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akers</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Third ACM Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="57" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Disk scheduling in a multimedia i/o system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reddy</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L N</forename><surname>Wyllie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wijayaratne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="37" to="59" />
			<date type="published" when="2005-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ioflow: A software-defined storage architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thereska</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ballani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>O&amp;apos;shea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rowstron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talpey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="182" to="196" />
		</imprint>
	</monogr>
	<note>SOSP &apos;13, ACM</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Split-level i/o scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kowsalya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Kiswany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpaci-Dusseau</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles</title>
		<meeting>the 25th Symposium on Operating Systems Principles<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="474" to="489" />
		</imprint>
	</monogr>
	<note>SOSP &apos;15, ACM</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ncq vs. i/o scheduler: Preventing unexpected misbehaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">I</forename><surname>Eom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>And</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeom</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Storage</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
