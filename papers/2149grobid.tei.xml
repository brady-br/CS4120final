<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast Software Cache Design for Network Appliances Fast Software Cache Design for Network Appliances</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>July 15-17, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brdgai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Andersen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Huacheng</roleName><forename type="first">Yu</forename><forename type="middle">♦</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kaminsky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<addrLine>Huacheng Yu</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Princeton University</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<orgName type="institution" key="instit4">Princeton University ♦</orgName>
								<address>
									<addrLine>BrdgAI ♥</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast Software Cache Design for Network Appliances Fast Software Cache Design for Network Appliances</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2020 USENIX Annual Technical Conference</title>
						<meeting>the 2020 USENIX Annual Technical Conference						</meeting>
						<imprint>
							<date type="published">July 15-17, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the 978-1-939133-14-4 Open access to the Proceedings of the 2020 USENIX Annual Technical Conference is sponsored by USENIX. https://www.usenix.org/conference/atc20/presentation/zhou</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>The high packet rates handled by network appliances and similar software-based packet processing applications place a challenging load on caches such as flow caches. In these environments, both hit rate and cache hit latency are critical to throughput. Much recent work, however, has focused exclusively on one of these two desiderata, missing opportunities to further improve overall system throughput. This paper introduces Bounded Linear Probing (BLP), a new cache design optimized for network appliances. BLP works well across different workloads and cache sizes by balancing between hit rate and lookup latency. To accompany BLP, we also present a new, lightweight cache eviction policy called Probabilistic Bubble LRU that achieves near-optimal cache hit rate (assum-ing the algorithm is offline) without using any extra space. We make three main contributions: a theoretical analysis of BLP, a comparison between existing and proposed cache designs using microbenchmarks, and an end-to-end evaluation of BLP in the popular Open vSwitch (OvS) system. Our end-to-end experiments show that BLP is effective in practice: replacing the microflow cache in OvS with BLP improves throughput by up to 15%.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Network virtualization is a core infrastructure component for cloud computing. In virtualized networks, virtual switches route packets between virtual machines (VMs) and between VMs and the outside world. Like the VMs themselves, the virtual switch resides in the hypervisor. The high speed of modern NICs-40Gb/s, 100Gb/s, and even 200Gb/s <ref type="bibr" target="#b1">[2]</ref>-makes virtual switches a critical network performance bottleneck.</p><p>Many software-based network systems, such as appliances, middleboxes, packet analytic frameworks, and virtual switches, rely on fast flow caches to achieve good average-case performance <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b36">38]</ref>. These environments impose challengingand, indeed, somewhat contradictory-requirements upon the caches they use. First, of course, they benefit from high hit rates. But, either to avoid wasting memory or to fit in faster levels of the CPU cache, they also strive to be compact. In addition, because of the high rates at which packet-centric systems operate, the flow cache lookups must have extremely low latency. * work started while at Carnegie Mellon University These competing requirements place such systems in an interesting middle ground compared to much of the prior work, which usually fall into one of the two extremes. Higher-level caching systems such as web caches and memcached often adopt comparatively expensive cache designs and replacement algorithms to maximize hit rate <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b24">26]</ref>. On the other hand, CPU caches have such tight timing requirements that they use very simple set associative designs that sacrifice hit rate for extremely low access time measured in clock cycles.</p><p>In this paper, we present the design, theoretical analysis, and empirical evaluation of a new cache design called Bounded Linear Probing, or BLP, that provides higher cache hit rates than simple set-associative designs, while remaining fast and hardware-friendly. BLP achieves low latency by ensuring purely local access to the cache data structure: Look-ups require a single read that spans at most two consecutive CPU cache lines. At the same time, BLP allows non-local propagation of full buckets. A basic set-associative cache provides only one location for a given set of objects. BLP allows those objects to creep into later bins, and over repeated inserts and evictions, this property allows high-occupancy bins to shift some of their load to nearby, less-occupied bins. To better serve skewed workloads, we accompany it with a cache eviction algorithm called Probabilistic Bubble LRU, or PBLRU, that fulfills the same design goals as BLP: It requires no extra space, adds little latency overhead and achieves near-optimal cache hit rate.</p><p>BLP is a simple and effective design for performancecritical software caches; despite its simplicity, we believe it to be a novel design point in the space of "cache table" designs, and provide a theoretical analysis of why it provides an improved hit rate over basic set-associative designs that access the same number of elements. The result is a design that performs nearly as well as the fastest set-associative designs, with hit rates that are closer to that of more advanced, yet expensive, designs such as cuckoo or hopscotch-based caches. We validate these results empirically using both microbenchmarks and by incorporating BLP into Open vSwitch <ref type="bibr" target="#b28">[30]</ref>, the most popular virtual switch, which is widely used in production. Replacing the microflow cache in OvS with BLP improves throughput by up to 15%: Its lookup latency is about 10 clock cycles longer than that of the basic set-associative design, but BLP's increased cache hit rate more than compensates for the higher latency. In contrast, many of the more expensive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Microflow Cache</head><p>Megaflow Cache</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Packet Classifier</head><p>Hash <ref type="table">Table   Hash  Table  with Mask   Hash  Table  with Mask   …   Hash  Table  with Mask</ref> Update Miss</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miss Update</head><p>Figure 1: Flow Caching Hierarchy in Open vSwitch cache designs that can achieve high cache hit rates do not justify their huge latency penalties. Our new cache eviction algorithm PBLRU further improves the throughput by up to 10% even if the workload is only modestly skewed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Flow Caching in Open vSwitch</head><p>Open vSwitch achieves high performance through extensive flow caching. Open vSwitch's caching hierarchy consists of three layers: a microflow cache, a megaflow cache, and a caching-aware packet classifier, as illustrated in <ref type="figure">Figure 1</ref>. The first cache that a packet encounters in OvS is the microflow cache, which caches forwarding decisions for each transport connection (or microflow). The microflow cache is a hash table that maps microflows to OpenFlow flows if there is an exact match using all the packet header fields. If the packet misses in the microflow cache, then OvS does a lookup in its megaflow cache. This cache supports wildcard matching but does not use flow priorities. The megaflow cache is a set of n hash tables, each with a unique wildcard mask. For each hash table, the lookup key is the packet header after applying the mask associated with the table. These hash tables are reactively created and populated by the packet classifier. Because looking up a packet in the megaflow cache searches all n hash tables, it is more expensive than a microflow cache lookup. Therefore, the cache hit rate of the microflow cache (the first cache) is critical to the performance of OvS.</p><p>The key observation that inspired our work is that although a microflow cache miss is expensive, it is not immensely more expensive than a microflow hit. In typical deployments, where the average number of hash table searches per megaflow lookup is small (as noted in Section 7.2 of Pfaff et al. <ref type="bibr" target="#b28">[30]</ref>), the microflow miss penalty is only hundreds to thousands of cycles on modern server CPUs. Hence, making the correct tradeoff between cache hit rate and lookup latency is crucial to the system throughput. In contrast, much of the previous work on software cache designs focuses primarily on improving cache hit rate <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b16">18,</ref><ref type="bibr" target="#b11">13]</ref>. In the situations studied by previous work, optimizing for hit rate makes sense: the cache misses in these systems were much more expensive than a hit because they often involved querying very slow backend services such as a database. The cache hit rate, therefore, determines not only the throughput, but also end-to-end request latency <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b25">27]</ref>.</p><p>The rest of this paper uses the OvS microflow cache as a case study to analyze and evaluate various design options and demonstrate the effectiveness of our new caching algorithm, Bounded Linear Probing (BLP). We show how BLP can balance cache hit rate (and thus miss penalty) and lookup latency to improve the throughput of Open vSwitch compared to alternate designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Packet and Flow Caching</head><p>Caching is a common and effective technique for speeding up network packet processing; existing solutions include hardware-based <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b27">29]</ref> and software-based <ref type="bibr" target="#b8">[10]</ref> approaches. Many early hardware routers used flow caching to achieve fast average-case performance. In the modern era, most hardware routers and switches have moved to more costly, but guaranteed-performance designs, such as TCAMs, to be able to provide their maximum forwarding rate under arbitrary (and possibly malicious) traffic. Software switches, however, broadly retain a cache-based design <ref type="bibr">[3,</ref><ref type="bibr" target="#b34">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hash Table Options For Caching</head><p>Caching is typically managed using a hash table as its basic data structure, but unlike the "full" problem of a general hash table, caches gain an extra degree of freedom: By definition, they do not need to store all possible keys and may choose to evict an existing item.</p><p>One of the contributions of this paper is to explore the tradeoff between the cache's hit rate and the lookup/insertion cost imposed by its hash table structure. To illustrate this tradeoff, we begin in Section 4 by describing points that operate at two extremes of the spectrum: First, a basic setassociative cache, in which an item can be stored only in one of m different slots shared by all other items that hash to the same bucket (row) of the hash table. This design is fast but achieves a relatively low hit rate. Next, we introduce two more advanced cache designs that incorporate ideas from cuckoo and hopscotch hashing, which can achieve much higher table occupancy (and thus hit rates), but at the cost of more expensive inserts and lookups. In the rest of this section, we present prior work on fast caches, including a brief introduction to cuckoo and hopscotch hashing. <ref type="bibr">Cuckoo [28]</ref> and hopscotch <ref type="bibr" target="#b19">[21]</ref> hashing both aim to achieve high table occupancy (upwards of 90%) in an "open-addressed" hash table design, i.e., one that does not need to use linked lists to store data items. The pointer chasing of a linked-list design adds substantial lookup latency, and the pointers themselves can add substantial memory overhead, especially when the entries in the table are small, which is the case for flow caches. that it allows a key to be placed in one of the H buckets starting from the one it is hashed to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cuckoo and hopscotch hashing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hardware Cache Designs</head><p>Although the focus of this work is on software caches, there are many parallels to related work on hardware caches.</p><p>Cache hit rate versus lookup latency Balancing the cache hit rate and lookup latency has been studied in the context of DRAM hardware caches. Alloy Cache <ref type="bibr" target="#b29">[31]</ref>, for example, improved performance over prior work by reducing the hit latency, even though doing so slightly reduced the hit rate.</p><p>Set-associative caches Hardware caches are often organized into rows (i.e., buckets) and ways (i.e., slots). An m-way set-associative cache uses a subset of the address bits to index into a row; the cache block (cache line) can be stored in one of the row's m ways. To balance the load across rows, researchers have proposed using a hash of the block address as the index <ref type="bibr" target="#b21">[23]</ref> as is commonly done in software-based hash tables and caches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skewed-associative caches and cuckoo-like cache designs</head><p>Skewed-associative caches <ref type="bibr" target="#b33">[35]</ref> extend this idea and allow each way to be indexed with a different hash function. In an m-way skewed-associative cache, a cache block B could be stored in row h i (B) for way i, for 0 ≤ i &lt; m.</p><p>Inspired by cuckoo hashing, zcache <ref type="bibr" target="#b32">[34]</ref> is an extension of skewed-associative caching. Instead of replacing one of the m existing blocks on a cache miss, it performs a breadth-first search to find additional eviction candidates. After picking a victim entry, it relocates blocks on the cuckoo path to accommodate the new block. These designs are not wellsuited for high speed, low latency software caches for packet processing, as they require several cache line reads per lookup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Cache Design and Eviction Policy</head><p>A large amount of prior work on caching <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b11">13]</ref> focuses on cache eviction policies. Improved policies, ranging from LRU and LFU to modern alternatives such as LHD <ref type="bibr" target="#b7">[9]</ref>, increase cache hit rate under skewed workload distributions by biasing eviction towards likely less-useful candidates.</p><p>The majority of prior cache eviction algorithms require additional tracking metadata to implement their eviction policies. In contrast, our new algorithm, PBLRU, adds no space overhead. The most related work to our algorithm is an earlier paper by Zhang and Xue <ref type="bibr" target="#b37">[39]</ref> that explores the same bubbling idea. We discuss the differences between PBLRU and their algorithm, DC-Bubble, in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Design and Analysis</head><p>We begin by presenting two baseline cache designs, a setassociative option and a "cuckoo-like" option, and analyze their expected hit rates. We then introduce bounded linear probing and its analysis using the same framework.</p><p>To understand the expected hit rate, we assume that the working set is fixed, and that each lookup key is drawn uniformly at random from that working set. We only analyze uniform distributions in this section, for the following two reasons: a) prior works studied caching performance on uniformly-distributed workloads <ref type="bibr" target="#b23">[25]</ref> and b) the expected hit rate under the uniform distribution is easier to analyze, yet it provides a lower bound on the hit rate under any other distribution (see Appendix C for a formal argument). We use α to denote the ratio of the working set size to the number of entries in the cache table, which we call the oversubscription factor. When α &lt; 1, the cache has more capacity than there are items in the working set. We determine hit rate in terms of α, and then provide numerical interpretations for some values observed in the OvS workloads, such as α = 0.95. In Section 7, we show empirical hit rate curves for real implementations across a range of α values.</p><p>All of the designs we evaluate use some amount of setassociativity. The caches are partitioned into n buckets, each containing m entries. To determine if an item is in a bucket, the implementation examines whether it is stored in any of the entries in the bucket. The table contains a total of n × m entries, and the working set has size α × n × m. To store a key-value pair (k, v), one hashes the key and determines in a table-specific way a set of (one or more) buckets that could hold the key, and stores both fingerprint(k) and v in an appropriate entry. In OvS, v is a pointer to a megaflow cache entry. Each design uses a different algorithm to decide which entry of the table will store a given pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analytical Framework for Hit Rate</head><p>To analyze the expected hit rate of a cache design, it suffices to estimate the expected number of keys the cache could hold after a sufficiently long warm-up period. This is because, in our formulation, each cache access is uniformly random, so the cache hit rate is equal to the total number of cached keys divided by the size of the working set. Moreover, the number of keys stored in all the cache designs we evaluate never decreases with an increasing number of cache accesses, and it has a maximum value of n × m. Therefore, it will eventually stop increasing. Denote the final number of occupied entries in bucket i by c i . The probability of a cache hit is equal to</p><formula xml:id="formula_0">c 0 + · · · + c n−1 αnm .<label>(1)</label></formula><p>By symmetry, all c i have the same expected value. Hence, by linearity of expectation, the expected cache hit rate is E[ci]/(αm). For each cache design, we describe how its hit rate is estimated from a high level, and leave all the details to appendices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Set-associative Cache</head><p>We start with a simple design-a set-associative cache. In an m-way set-associative cache, each item is mapped to a bucket by a hash function h, and each bucket has m slots. <ref type="figure">Figure 3a</ref> SIMD-optimized Lookup To accelerate lookups, we use SIMD instructions to compare multiple fingerprints at the same time <ref type="table">(similar to techniques used by Google's Swiss  Tables [6]</ref>). <ref type="figure">Figure 5</ref> shows how the lookup works in a 4-way set-associative cache. The stock OvS design does not use SIMD-accelerated reads for its microflow cache, so to ensure a fair basis for comparison, we implemented this optimization and use it as the baseline for comparison.</p><p>To search for fingerprint f in a bucket, we first duplicate it four times and store it in a 64-bit integer match. Then, we load the first 64 bits of the bucket into another 64-bit integer sig. We compare the packed 16-bit integers in sig and match for equality, storing the results in cmp. cmp consists of 4 16-bit integers r 0 ,r 1 ,r 2 and r 3 , where r i is 0xFFFF if f i = f and 0 otherwise. We can then count the number of trailing zeros in cmp to figure out which slot f matches in the bucket.</p><p>Lookup in an 8-way set-associative cache works similarly to the 4-way set-associative cache, but uses 128-bit integers instead of 64-bit integers. For 2-4 cuckoo-lite, because the eight candidate fingerprints are not consecutive, we have to first copy the fingerprints from two buckets into one 128-bit integer, then perform packed integer comparison.</p><p>SIMD-accelerated lookup in 2-4 BLP works as follows: The eight candidate fingerprints are not contiguous in memory (unlike cuckoo-lite), but are separated by the 64 packed value bits. Therefore, instead of copying fingerprints, we load both buckets into a wider 256-bit integer and mask off all the value bits. Eliminating the extra load instruction reduces the lookup latency by ∼ 10% and makes BLP more SIMD-friendly than cuckoo-lite.</p><p>Buffer Bucket In 2-4 BLP, if the lookup key hashes to the last bucket of the table, both the first and the last bucket are searched. This corner case requires both a second cache line read and, more importantly, an extra branch. To avoid incurring branch prediction misses, we added a buffer bucket following the original cache table. This buffer bucket has minimal impact on the cache miss rate but improves lookup speed: When the lookup key hashes to the last bucket, and that bucket is full, the new key spills to the buffer bucket instead of wrapping around the table. At lookup time, we search both the last bucket and this buffer bucket, which avoids the branch misprediction and allows for processor prefetching4. One thing which worth mentioning is that this optimization is specific to BLP and does not work well with other cache designs -it breaks the alignment of the number of buckets (typically a power of 2). Moreover, the extra space given by this buffer bucket is negligible compared to the size of the cache. We therefore only apply the optimization to BLP.</p><p>Batched Lookup with Prefetching We use batched lookup with prefetching to overlap bucket computation with memory reads, which minimizes the impact of DRAM access latency.</p><p>4Note that this optimization means that no keys will ever spill into the first bucket.</p><p>This technique is common in many existing packet processing applications and frameworks <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b9">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We present our evaluation top-down: We begin with a description of the experimental setup followed by a set of end-to-end benchmarks that compare the different cache table designs (described above) in the context of Open vSwitch. These results demonstrate the benefits and generality of BLP in a realistic packet processing application. Next, we use a set of microbenchmarks to understand more deeply the fundamental tradeoffs that each of the cache design brings to the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experiment Setup</head><p>Our experiments are conducted on c220g2 instances from CloudLab <ref type="bibr" target="#b31">[33]</ref>. Each of the instances is equipped with the following hardware:</p><formula xml:id="formula_1">Hardware Description CPU 2× Intel Xeon E5-2660v3 CPUs (2.60GHz) DRAM 160 GiB DDR4 Memory L3 Cache 2× 24 MiB NIC Intel X520 dual-port 10GbE</formula><p>We also controlled for the following factors, which otherwise had noticeable effects on our results:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Number Generator</head><p>Throughout the experiments, we use PCG-32 <ref type="bibr" target="#b3">[5]</ref>, a fast and statistically robust algorithm for our random number generation.5</p><p>Cache Warming As discussed in Section 4, 2-4 cuckoo-lite and 2-4-BLP do not displace keys. Instead, they depend purely on cache warming to reach the maximum hit rate. Therefore, in each experiment, we first warm the testing cache until it reaches a stable state, i.e., the cache hit rate stops increasing.</p><p>All experimental results reported below are the average of five runs. The variance was low, so we omit error bars from our graphs. Because the differences between many of the designs are small-in the range of 10% or so-while the absolute performance differences between a high cache hit rate (low alpha) and a low cache hit rate (high alpha) are relatively large, we deliberately choose not to start axes at 0; the graphs are "zoomed-in" to the regions of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">End-to-end Benchmarks</head><p>As a concrete end-to-end benchmark using an important application, we modified the microflow cache in Open vSwitch (v2.10.1) to use the various cache designs described above.</p><p>Open vSwitch was running on the a c220g2 instance with two 10Gb Ethernet ports, port 0 and port 1. To accurately 5The quality of the random number generator directly affects the cache hit rates. Unintended workload locality (i.e., back-to-back keys that hash to nearby buckets) produces higher than expected hit rates; poor random number generators exacerbate this effect. Earlier in the research process for this work, a bad, hand-crafted random number generator caused this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Expected Cache Hit Rate of Setassociative Caches</head><p>In a m-way set-associative cache with n buckets, for each bucket, the probability that there are exactly t keys mapped to it is</p><formula xml:id="formula_2">񮽙 αnm t 񮽙 · n −t · (1 − 1/n) αnm−t = αnm(αnm − 1) · · · (αnm − t + 1) t! · n t · (1 − 1/n) αnm−t which when t ≪ αnm, is approximately (αnm) t t! · n t · (1 − 1/n) αnm = (αm) t t! · (1 − 1/n) αnm</formula><p>which by the fact that 1 − ǫ ≈ e −ǫ for small ǫ, is approximately (αm) t t! · e −αnm/n = (αm) t e αm t! If t ≤ m, then all t keys will be cached; otherwise, only m will be cached. Therefore, the expected number of keys that are cached in a bucket is approximately</p><formula xml:id="formula_3">m 񮽙 t=0 t · (αm) t e αm t! + ∞ 񮽙 t=m+1 m · (αm) t e αm t! ,</formula><p>which by the fact that 񮽙 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Expected Cache Hit Rate of 2-4 BLP</head><p>Recall that a i is the number of keys from the working set that map to cache bucket i and b i is the number of keys spill from bucket i to i + 1 after a sufficiently long warm-up period. For j ≥ 0, we have:</p><formula xml:id="formula_4">Pr[a i = j] = 񮽙 αnm j 񮽙 · n −j · (1 − 1/n) αnm−j .</formula><p>By the law of total expectation and Equation <ref type="formula">(2)</ref> in Section 4.5, for 0 &lt; l &lt; m, we have</p><formula xml:id="formula_5">p l ≈        񮽙 m j ′ =0 񮽙 񮽙 m−j ′ j=0</formula><p>Pr</p><formula xml:id="formula_6">[a i = j] 񮽙 · p j ′ if l = 0, 񮽙 l+m j=l Pr[a i = j] · p l+m−j if 0 &lt; l &lt; m,</formula><p>and by the definition of probability distribution, </p><formula xml:id="formula_7">p 0 + · · · + p m = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Expected Hit Rate of BLP under Non-Uniform Distributions</head><p>In the following, we prove that the expected hit rate obtained in Section 4.5 for the uniform distribution is always a lower bound for any other distribution. Fix any distribution over the working set S, let p x be the probability of key x. Without loss of generality, we may assume p x &gt; 0 for all x ∈ S, since otherwise, we could simply remove all x with zero probability from the working set. Recall that c i denotes the final number of occupied entries in bucket i. Observe that c 0 , . . . , c n−1 are determined only by the hash function, i.e., how many keys are mapped to each bucket. They do not depend on the probability distribution of the keys (as long as all keys have non-zero probability), the distribution only affects how fast the final numbers are achieved.</p><p>After a sufficiently long warm-up period, all buckets achieved their final numbers of occupied entries. Now, consider all possible memory configurations after the warm-up. Further key lookups define a Markov chain over them, where the transition probability from memory configuration A to configuration B is the probability that A becomes B after one lookup. Observe that this Markov chain is aperiodic (i.e., there does not exist a t &gt; 1 and a state A such that A can only go back to itself after steps of multiples of t). It is well-known that for any aperiodic Markov chain and any initial state, as the number of steps (key lookups) increases, the distribution of the state will approach some final stationary distribution (note that the stationary distribution may not be unique, hence the final stationary distribution may depend on the initial state). The final hit rate is computed from this stationary distribution and the distribution of the keys. More specifically, let q x be the expectation, over a random hash function and random lookups in the warm-up period (which determine the Markov chain and the distribution of initial state), of the probability that key x is cached according to the final stationary distribution. Thus, the expected hit rate is equal to 񮽙 x ∈S p x q x . Next, by linearity of expectation, 񮽙 x ∈S q x is equal to the expected total number of occupied entries in the data structure, E[ci] · n. The key observation is that if p x ≥ p y then q x ≥ q y , i.e., if a key is more likely to occur, then it has a higher probability to appear in the final stationary distribution, over a random hash function and warm-up period.7 Let S high := {x : p x ≥ 1/|S|} be the set of keys that occur with at least the average probability and S low := {x : p x &lt; 1/|S|} be the set of keys that occur with probability lower than the average, and let q := min x ∈S high q x . Hence, q ≥ q x for all x ∈ S low . We have</p><formula xml:id="formula_8">񮽙 x ∈S p x q x = 񮽙 x ∈S q x |S| + 񮽙 x ∈S (p x − 1/|S|)q x = 1 |S| 񮽙 x ∈S q x + 񮽙 x ∈S high (p x − 1/|S|)q x + 񮽙 x ∈S low (p x − 1/|S|)q x</formula><p>which by the fact that p x ≥ 1/|S| and q x ≥ q for x ∈ S high , and the fact that p x &lt; 1/|S| and q x ≤ q for x ∈ S low , is at least</p><formula xml:id="formula_9">≥ 1 |S| 񮽙 x ∈S q x + 񮽙 x ∈S high (p x − 1/|S|)q + 񮽙 x ∈S low (p x − 1/|S|)q = E[ci] · n |S| + 񮽙 x ∈S p x · q − 񮽙 x ∈S 1 |S| · q = E[ci] · n |S| + q − q = E[ci] · n |S| . The last quantity E[ci ]·n |S |</formula><p>is precisely the expected hit rate under the uniform distribution, as we argued in Section 4. Therefore, the expected hit rate under any non-uniform distribution is always lower bounded by the hit rate under the uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis on Warm-up Time</head><p>In the following, we present an informal estimation on the relationship between the hit rate of BLP and its warm-up time. As we argued in Section 4.5, the hit rate is equal to the number of occupied entries in the BLP divided by the size of the working set. In each lookup in the warm-up period, the key may be either a) in the BLP already, or b) not in the BLP and the buckets are full, or c) not in the BLP and the bucket is not full. Only in case c), do we increase the number of occupied entries by one. Denote the final hit rate by r max . When 7Note that this is not true if the hash function is fixed. the current hit rate is r, we are going to approximate the probability of case c) by r max − r. That is, we assume there is a fixed set of (r max − r) · (αmn) keys in the working set such that they are the missing keys from the BLP in order to achieve the maximum hit rate of r max .</p><p>Therefore, let L be the length of the warm-up, we have dr dL = r max − r αmn , and when L = 0, r = 0. By solving this ordinary differential equation, we obtain r = r max (1 − e − L αmn ). That is, when the length of the warm-up is a large constant times the working set size, the estimated hit rate becomes very close to r max . For α = 0.95, we have verified by experiments that a warm-up period of length 20 times the working set size is sufficient to obtain a hit rate that is less than 1% lower than r max .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>t</head><label></label><figDesc>≥0 (αm) t e αm t! = 1, is equal to = m − m 񮽙 t=0 (m − t) · (αm) t e αm t! , Hence, the expected cache hit rate is (m − 񮽙 m t=0 (m − t) · (αm) t e αm t! )/(αm).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Solving the above system of linear equations for (p 0 , . . . , p m ) with m = 4, α = 0.95 gives us p 0 = 0.37889778, p 1 = 0.15160669, p 2 = 0.14369602, p 3 = 0.12041777 and p 4 = 0.20538175. By Equation (3), we get E[c i ] = 3.59.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The open vswitch* exact-match cache</title>
		<ptr target="https://software.intel.com/en-us/articles/the-open-vswitch-exact-match-cache" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mellanox ConnectX-6 product brief</title>
		<ptr target="https://www.mellanox.com/related-docs/prod_adapter_cards/PB_ConnectX-6_EN_Card.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Add smc cache after emc cache</title>
		<ptr target="https://mail.openvswitch.org/pipermail/ovs-dev/2018-July/349395.html" />
		<imprint/>
	</monogr>
	<note>ovs-dev. patch v5 1/2] dpif-netdev</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pcg</surname></persName>
		</author>
		<ptr target="http://www.pcg-random.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swiss</forename><surname>Tables</surname></persName>
		</author>
		<ptr target="https://abseil.io/blog/20180927-swisstables" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scavenger: A new last level cache architecture with global block priority</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="doi">doi:10.1109/MICRO.2007.42</idno>
	</analytic>
	<monogr>
		<title level="m">40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007)</title>
		<imprint>
			<date type="published" when="2007-12" />
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling cache performance beyond lru</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="doi">doi:10.1109/HPCA.2016.7446067</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="225" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Lhd: Improving cache hit rate by maximizing hit density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th USENIX NSDI</title>
		<meeting>15th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximate caches for packet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chang Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improving hash join performance through prefetching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
		<idno type="doi">0362-5915.doi:10.1145/1272743.1272747</idno>
		<ptr target="http://doi.acm.org/10.1145/1272743.1272747" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cache memory design for network processors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Chiueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pradhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Symposium on HighPerformance Computer Architecture</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynacache: Dynamic cloud caching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eisenman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Katti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 15). USENIX Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Memshare: a dynamic multi-tenant key-value cache</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rushton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stutsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference (USENIX ATC 17)</title>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="978" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A Paging Experiment with the Multics System. Defense Technical Information Center</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Corbato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I O T C P</forename><surname>Mac</surname></persName>
		</author>
		<ptr target="http://books.google.com/books?id=5wDQNwAACAAJ" />
		<imprint>
			<date type="published" when="1968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Moongen: A scriptable high-speed packet generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Emmerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gallenmüller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wohlfart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15nd ACM SIGCOMM conference on Internet measurement, IMC &apos;15</title>
		<meeting>the 15nd ACM SIGCOMM conference on Internet measurement, IMC &apos;15</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A cool and practical alternative to traditional hash tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh Workshop on Distributed Data and Structures (WDAS&apos;06)</title>
		<meeting>Seventh Workshop on Distributed Data and Structures (WDAS&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memc3: Compact and concurrent memcache with dumber caching and smarter hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<idno>978-1-931971-00-3</idno>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13)</title>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="371" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quickly generating billion-record synthetic databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Englert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Baclawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 1994 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="1994-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A fully associative softwaremanaged cache design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Hallnor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<idno type="doi">doi:10.1145/339647.339660</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 27th International Symposium on Computer Architecture</title>
		<meeting>27th International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000-06" />
			<biblScope unit="page" from="107" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hopscotch hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tzafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Symposium on Distributed Computing</title>
		<meeting>the 22Nd International Symposium on Distributed Computing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raising the bar for using GPUs in software packet processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th USENIX NSDI</title>
		<meeting>12th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using prime numbers for cache indexing to eliminate conflict misses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kharbutli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno>doi: 10. 1109/HPCA.2004.10015</idno>
	</analytic>
	<monogr>
		<title level="m">10th International Symposium on High Performance Computer Architecture (HPCA&apos;04)</title>
		<imprint>
			<date type="published" when="2004-02" />
			<biblScope unit="page" from="288" to="299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SILT: A memory-efficient, high-performance key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>23rd ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2011-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MICA: A holistic approach to fast in-memory key-value storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th USENIX NSDI</title>
		<meeting>11th USENIX NSDI</meeting>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Memcached: A distributed memory object caching system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Memcached</surname></persName>
		</author>
		<ptr target="http://memcached.org/" />
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scaling memcache at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nishtala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fugal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcelroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stafford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Venkataramani</surname></persName>
		</author>
		<idno>978-1-931971-00-3</idno>
	</analytic>
	<monogr>
		<title level="m">Presented as part of the 10th USENIX Symposium on Networked Systems Design and Implementation (NSDI 13)</title>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="385" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cuckoo hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rodler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="122" to="144" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A 50-Gb/s IP router</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="248" />
			<date type="published" when="1998-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The design and implementation of open vswitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pettit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koponen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajahalme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stringer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shelar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Casado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2015)</title>
		<meeting>the 12th USENIX Symposium on Networked Systems Design and Implementation (NSDI 2015)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fundamental latency trade-off in architecting dram caches: Outperforming impractical sramtags with a simple and practical design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MICRO</title>
		<meeting>ACM MICRO</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The v-way cache: demand-based associativity via global replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<idno type="doi">doi:10.1109/ISCA.2005.52</idno>
	</analytic>
	<monogr>
		<title level="m">32nd International Symposium on Computer Architecture (ISCA&apos;05)</title>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="544" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Introducing CloudLab: Scientific infrastructure for advancing cloud architectures and applications. USENIX ;login</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">The Cloudlab</forename><surname>Team</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The zcache: Decoupling ways and associativity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<idno type="doi">doi:10.1109/MICRO.2010.20</idno>
	</analytic>
	<monogr>
		<title level="m">43rd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2010-12" />
			<biblScope unit="page" from="187" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A case for two-way skewed-associative caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<idno type="doi">doi:10.1145/165123.165152</idno>
		<ptr target="http://doi.acm.org/10.1145/165123.165152" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual International Symposium on Computer Architecture, ISCA &apos;93</title>
		<meeting>the 20th Annual International Symposium on Computer Architecture, ISCA &apos;93</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Flow caching for high entropy packet fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koponen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rajahalme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the third workshop on Hot topcis in software defined networking, HotSDN &apos;14</title>
		<meeting>the third workshop on Hot topcis in software defined networking, HotSDN &apos;14</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Optimizing open vswitch to support millions of flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><forename type="middle">C</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gobriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Global Communications Conference (GLOBECOM 2017</title>
		<meeting>the 2017 IEEE Global Communications Conference (GLOBECOM 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel cache architecture to support layer-four packet classification at memory access speeds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degroat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Divide-and-conquer: A bubble replacement for low level caches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<idno type="doi">978-1-60558-498-0.doi:10.1145/1542275.1542291</idno>
		<ptr target="http://doi.acm.org/10.1145/1542275.1542291" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Supercomputing, ICS &apos;09</title>
		<meeting>the 23rd International Conference on Supercomputing, ICS &apos;09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="80" to="89" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">High Performance Ethernet Forwarding with CuckooSwitch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scalable</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 9th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)</title>
		<meeting>9th International Conference on emerging Networking EXperiments and Technologies (CoNEXT)</meeting>
		<imprint>
			<date type="published" when="2013-12" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
