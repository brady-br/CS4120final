<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T03:00+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coordinating Liquid and Free Air Cooling with Workload Allocation for Data Center Power Minimization Coordinating Liquid and Free Air Cooling with Workload Allocation for Data Center Power Minimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>June 18-20. 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaorui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaorui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="laboratory">Power-Aware Computer System (PACS) Laboratory</orgName>
								<orgName type="institution" key="instit1">The Ohio State University</orgName>
								<orgName type="institution" key="instit2">The Ohio State University</orgName>
								<address>
									<postCode>43210</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coordinating Liquid and Free Air Cooling with Workload Allocation for Data Center Power Minimization Coordinating Liquid and Free Air Cooling with Workload Allocation for Data Center Power Minimization</title>
					</analytic>
					<monogr>
						<title level="m">• Philadelphia, PA USENIX Association 11th International Conference on Autonomic Computing</title>
						<imprint>
							<biblScope unit="page">249</biblScope>
							<date type="published">June 18-20. 2014</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 11th International Conference on Autonomic Computing (ICAC &apos;14) is sponsored by USENIX. https://www.usenix.org/conference/icac14/technical-sessions/presentation/li_li</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Data centers are seeking more efficient cooling techniques to reduce their operating expenses, because cooling can account for 30-40% of the power consumption of a data center. Recently, liquid cooling has emerged as a promising alternative to traditional air cooling, because it can help eliminate undesired air recirculation. Another emerging technology is free air cooling, which saves chiller power by utilizing outside cold air for cooling. Some existing data centers have already started to adopt both liquid and free air cooling techniques for significantly improved cooling efficiency and more data centers are expected to follow. In this paper, we propose SmartCool, a power optimization scheme that effectively coordinates different cooling techniques and dynamically manages work-load allocation for jointly optimized cooling and server power. In sharp contrast to the existing work that addresses different cooling techniques in an isolated manner , SmartCool systematically formulates the integration of different cooling systems as a constrained optimization problem. Furthermore, since geo-distributed data centers have different ambient temperatures, SmartCool dynamically dispatches the incoming requests among a network of data centers with heterogeneous cooling systems to best leverage the high efficiency of free cooling. A lightweight heuristic algorithm is proposed to achieve a near-optimal solution with a low run-time overhead. We evaluate SmartCool both in simulation and on a hardware testbed. The results show that SmartCool out-performs two state-of-the-art baselines by having a 38% more power savings.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, high power consumption has become a serious concern in operating large-scale data centers. For example, a report from Environmental Protection Agency (EPA) estimated that the total energy consumption from data centers in the US was over 100 billion kWh in 2011. Among the total power consumed by a data center, cooling power can account for 30-40% <ref type="bibr" target="#b13">[14]</ref> <ref type="bibr" target="#b1">[2]</ref>. As new high-density servers (e.g., blade servers) are increasingly being deployed in data centers, it is important for the cooling systems to more effectively remove the heat. However, with the high-density servers being installed, the traditional computer room air conditioner (CRAC) system might not be efficient enough, as its Power Usage Effectiveness (PUE) is around 2.0 or higher. PUE is defined as the ratio of the total energy consumption of a data center over energy consumed by the IT equipment such as servers. With a high PUE, the cooling power consumption of a data center can grow tremendously as high-density servers being deployed, which not only increases the operating cost, but also causes negative environmental impact. Therefore, data centers are in an urgent need to find higher-efficient cooling techniques to reduce PUE.</p><p>Two new cooling techniques have recently been proposed to increase the cooling efficiency and lower the PUE of a data center. The first one is liquid cooling, which conducts coolant through pipes to some heat exchange devices that are attached to the IT equipments, such that the generated heat can be directly taken away by the coolant. The second one, which is referred to as free air cooling <ref type="bibr" target="#b6">[7]</ref>, exploits the relatively cold air outside the data center for cooling and thus saves the power of chilling the hot air returned from the IT equipment. Although both of the two cooling techniques highly increase the cooling efficiency of data centers, each technique has its own limitations. The liquid cooling approach requires additional ancillary facilities (e.g., the valves and pipes) and maintenance, which can increase the capital investment when being deployed in a large scale. The free air cooling technique requires a low outside air temperature, which might not be available all the time in a year. In order to mitigate the problems, hy-brid cooling system, which is composed with the liquid cooling, the free air cooling and the traditional CRAC air cooling, can be used to lower the cooling cost and ensure the cooling availability. Several hybrid-cooled data centers have been put into production. For example, the CERN data center located in Europe adopts a hybrid cooling system with both liquid-cooling and traditional CRAC cooling systems, in which about 9% of the servers are liquid-cooled <ref type="bibr" target="#b3">[4]</ref>.</p><p>However, efficiently operating such a hybrid cooling system is not a trivial task. Currently, existing data centers that adopt multiple cooling techniques commonly use some preset outside temperature thresholds to switch between different cooling systems, regardless of the time-varying workload. Such a simplistic solution can often lead to unnecessarily low cooling efficiencies. Although some previous studies <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b34">[35]</ref> have proposed to intelligently distribute the workload across the servers and manage the cooling system according to the real-time workload to avoid over-cooling, they address only one certain cooling technology and thus the resulted workload distribution might not be optimal for the hybrid cooling system. To the best of our knowledge, no prior research has been done for efficiently coordinating multiple cooling techniques in a hybrid-cooled data center. In addition, for a network of data centers that are geographically distributed, there exist some works focusing on balancing the workload or reducing the server power cost <ref type="bibr" target="#b15">[16]</ref> <ref type="bibr" target="#b26">[27]</ref>, but none of them minimize the cooling power consumption, especially when the data centers have heterogeneous cooling systems. In order to minimize the power consumption of a hybrid-cooled data center, we need to face several new challenges. First, the different characteristics of these three cooling systems (liquid cooling, free air cooling and the traditional CRAC air cooling) demand for a systematic approach to coordinate them effectively. Second, workload distribution in such a hybrid-cooled data center needs to be carefully planned in order to jointly minimize the cooling and server power consumption. Third, due to the different local temperatures, a novel workload distribution and cooling management approach is needed for data centers that are geographically distributed at different locations, in order to better utilize free air cooling in the hybrid cooling system more efficiently.</p><p>In this paper, we propose SmartCool, a power optimization scheme to optimize the total power consumption of a hybrid-cooled data center by intelligently managing the hybrid cooling system and distributing the workload. We first formulate the power optimization problem for a single data center, which can then be solved with a widely adopted optimization technique. We then extend the power optimization scheme to fit a network of geo-distributed data centers. To reduce the computational overhead, we propose a light-weight algorithm to solve the optimization problem for the geodistributed data centers. Specifically, this paper has the following major contributions:</p><p>• More and more data centers are on their way to adopt high-efficient cooling techniques, but many data centers heavily rely on simplistic solutions to separately manage their cooling systems, which often lead to an unnecessarily low cooling efficiency. In this paper, we propose to address an increasingly important problem: Intelligent coordination of cooling systems for jointly minimized cooling and server power in a data center.</p><p>• We formulate the cooling management in a hybridcooled data center with liquid cooling, free air cooling, and traditional CRAC air cooling, as a constrained power optimization problem to minimize the total power consumption. SmartCool features a novel air recirculation model developed based on computational fluid dynamics (CFD).</p><p>• To best leverage the high efficiency of free cooling in geo-distributed data centers that have different ambient temperatures, we extend our optimization formulation to dynamically dispatch the incoming requests among a network of data centers with heterogeneous cooling systems. A light-weight heuristic algorithm is proposed to achieve a near-optimal solution with a low run-time overhead.</p><p>• We evaluate SmartCool both in simulation and on a hardware testbed with real-world workload and temperature traces. The results show that SmartCool outperforms two state-of-the-art baselines by saving 38% more power consumption.</p><p>The rest of the paper is organized as follows. We review the related work in Section 2, and introduce the background of different cooling technologies in Section 3. Section 4 formulates power optimization problem for a single hybrid-cooled data center, which is extended for geo-distributed data centers in Section 5 with a lightweight algorithm. We present our simulation results in Section 6 and the hardware experiment results in Section 7. Finally, Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Minimizing the power consumption of data centers has recently received much attention, such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref>. In particular, a lot of work has been done to optimize the traditional CRAC air cooling in data centers. For example, Anto et al. <ref type="bibr" target="#b21">[22]</ref> construct a model of a single CRAC unit which offers flexible selection in different heat exchangers and coolants. Zhou et al. <ref type="bibr" target="#b22">[23]</ref> propose a computationally efficient multivariable model to capture the effects of CRAC fan speed and supplied air temperature on the rack inlet temperatures. Tang et al. <ref type="bibr" target="#b19">[20]</ref> propose a workload scheduling scheme to make the inlet temperatures of all servers as even as possible. A holistic approach is proposed by <ref type="bibr">Chen et al. [30]</ref> that integrates the management of IT, power and cooling infrastructures to improve the data center efficiency. In our work, we adopt the modeling process of traditional CRAC cooling system, but coordinate its work with the liquid cooling and free cooling systems as a hybrid-cooling system for the improvement of data center cooling efficiency.</p><p>Free air cooling and liquid cooling have also attracted wide research attentions. Christy et al.</p><p>[9] study two primary free cooling systems, the air economizer and the water economizer. Gebrehiwot et al. <ref type="bibr" target="#b2">[3]</ref> study the thermal performance of an air economizer for a modular data center using computational fluid dynamics. Coskun et al. <ref type="bibr" target="#b0">[1]</ref> provide a 3D thermal model for liquid cooling, with variable fluid injection rates. Hwang et al. <ref type="bibr" target="#b7">[8]</ref> develop an energy model for liquid-cooled data centers based on the thermo-fluid first principles. Differently, our work focuses on the power optimization of hybrid-cooled data centers, by managing the cooling modes and workload distribution.</p><p>For geo-distributed data centers, Adnan et al. <ref type="bibr" target="#b15">[16]</ref> save the cost of load balancing by utilizing the flexibility of the Service Level Agreements. Related algorithms are developed in <ref type="bibr" target="#b32">[33]</ref> <ref type="bibr" target="#b33">[34]</ref>[12] to minimize the total cost of geo-distributed data centers and the environmental impact. Our global workload dispatching strategy minimizes the total power consumption of all the distributed data center, by leveraging the temperature differences among different locations and maximizing the usage of free air cooling. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the cooling system of a hybrid-cooled data center, which includes traditional air cooling, liquid cooling and free air cooling. The liquid cooling system uses chiller and cooling tower to provide coolant. Either the CRAC system or the free cooling system can be selected for air cooling. The CRAC system also relies on chiller and cooling tower to provide the coolant, which is then used to absorb heat from the air in the data center. The free cooling system draws outside air into the data center through the Air Handling Unit (AHU) when the outside temperature can meet the cooling requirement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Different Cooling Technologies</head><p>Traditional CRAC air cooling is the most widely used cooling technology in existing data centers. This system deploys several CRAC units in the computer room to supply cold air. The cold air usually goes under the raised floor before joining in the cold aisle through perforated tiles to cool down the servers, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The hot air from the servers is output to the hot aisle and returned to the CRAC system to be cooled and reused. The deployment of cold aisle and hot aisle is used to form isolation between cold and hot airs. However, due to the existence of seams between servers and racks, as well as the space close to the ceiling where there is no isolation, cold air and hot air are often mixed to a certain extent, which decreases the cooling efficiency. The PUE of a data center using CRAC cooling is usually around 2.0 <ref type="bibr" target="#b5">[6]</ref>.</p><p>Liquid cooling technology usually uses coolant (e.g., water) to directly absorb heat from the servers. It can be divided into three categories: direct liquid cooling <ref type="bibr" target="#b18">[19]</ref>, rack-level liquid cooling <ref type="bibr" target="#b23">[24]</ref> and submerge cooling <ref type="bibr" target="#b25">[26]</ref>. In direct liquid cooling, the microprocessor in a server is directly attached with a cold plate that contains the coolant to absorb heat of the microprocessor, while the other components are still cooled by chilled air flow. Direct liquid cooling improves the cooling efficiency by enhancing the heat exchange process. Rack-level liquid cooling and submerge cooling adopt some other heat exchange devices instead of the cold plate. In this paper, we adopt the direct liquid cooling technology as an example to demonstrate the effectiveness of our solution, due to its low cost.</p><p>Free air cooling is a highly efficient cooling approach that uses the cold air outside the data center and saves power by shutting off the chiller system. It is usually utilized within a range of outside temperature and humidity. Within this range, the outside air can be used for cooling via an air handler fan. The traditional CRAC system is employed by these data centers as the backup cooling system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Power Optimization for a Local Hybridcooled Data Center</head><p>In this section, we introduce the power models and formulate the total power optimization problem for a local hybrid-cooled data center. We then present how we solve the problem by using computational fluid dynamics (CFD) modeling and optimization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Power Models of a Hybrid-cooled Data Center</head><p>We use the following models to calculate the server and cooling power consumption in the data center.</p><p>•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Server Power Consumption Model</head><p>For a server i, we adopt a widely accepted server power consumption model <ref type="bibr" target="#b10">[11]</ref> as:</p><formula xml:id="formula_0">P server i = W i × P compute i + P idle i (1)</formula><p>where W i is the workload handled by server i, in terms of CPU utilization. P compute i is the maximum computing power when the workload is 100%. P idle i represents the static idle power consumed by the server. If the workload is 0, the server can be shut down to save power and thus the power consumption is 0. Therefore, the total server power consumption of a data center with N server is</p><formula xml:id="formula_1">P server = N ∑ i=1 P server i (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Air Cooling Power Model</head><p>In a hybrid-cooled data center, the components of a liquid-cooled server except for the microprocessor, the rest server components, such as disk and memory, are cooled by the air cooling system, which contributes to the hot air coming out of the server. To characterize this relationship, we assume that in a liquid-cooled server, α percent of the power is consumed by the microprocessor. Therefore, assuming the first M servers among the total N servers are liquid-cooled, we can calculate the total power consumption of all the servers and components that are cooled by the air cooling as:</p><formula xml:id="formula_2">P server air = N ∑ i=M+1 P server i + M ∑ i=1 (1 − α) * P server i (3)</formula><p>The power consumption of the traditional CRAC air cooling system depends on the heat generation (i.e., P server air ) and the efficiency of the CRAC system:  </p><p>According to <ref type="bibr" target="#b10">[11]</ref>, the COP (coefficient of performance, characterizing the cooling efficiency) of a CRAC system can be calculated according to the supplied air temperature T sup :</p><formula xml:id="formula_4">COP air = 0.0068 * T 2 sup + 0.0008 * T sup + 0.458 (5)</formula><p>To avoid overheating the servers, the inlet air temperature of an air-cooled server needs to be bounded by a threshold. Based on a temperature model from <ref type="bibr" target="#b20">[21]</ref>, we use Equation 6 to first calculate the outlet air temperature, and then get the inlet air temperature with Equation 7:</p><formula xml:id="formula_5">K i T i out = N ∑ j=1 h i j K j T j out + (K i − N ∑ j=1 h i j K j )T sup + P air i<label>(6)</label></formula><formula xml:id="formula_6">T i in = N ∑ j=1 h ji * (T j out − T sup ) + T sup<label>(7)</label></formula><p>K i represents a multiplicative item, ρ f i C p , where C p is the specific heat of air. ρ represents the air density, and f i is the air flow rate to server i. h describes the air recirculation. In Equation 6, the first term characterizes the impact of the air recirculation from server j to server i and the second term models the cooling effect of the supplied air. The third term is the power consumption of server i that heats up the passing cold air. Equation 7 shows that inlet server temperature is determined by the supplied air temperature and the recirculation heat. We explain how to derive h using CFD in Section 4.3.</p><p>When the free air cooling method is chosen for the hybrid-cooled data center, the air cooling power is calculated in a different way, according to <ref type="bibr" target="#b6">[7]</ref> P air f ree = (PUE f ree − 1) * P server air</p><p>In our experiment, the free cooling PUE is modeled to be proportional to the ambient air temperature according to <ref type="bibr" target="#b9">[10]</ref>. This is because when the outside air temperature is relatively high, more air is needed to take away the heat generated by the servers, and the fan speed of AHU needs to be higher to draw more air.</p><p>In our paper, we assume that only one of the two air cooling systems can run at one time in a hybrid-cooled data center. Thus the total air cooling power consumption can be expressed as:</p><formula xml:id="formula_8">P air = β P air CRAC + (1 − β )P air f ree (9)</formula><p>where β is a binary variable indicating which air cooling system is activated.</p><p>•</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Liquid Cooling Power Model</head><p>With M liquid-cooled servers and the microprocessor consuming α percent of the server power, we have the liquid cooling power consumption as</p><formula xml:id="formula_9">P liquid = ∑ M i=1 αP server i COP liquid (10)</formula><p>where COP liquid is the COP of the chiller used in the liquid cooling system. Due to the high cooling capacity of liquid medium, the changes of the liquid temperature and the flow rate hardly affect the COP value, and thus COP liquid can be viewed as a constant. To derive a COP that can provide cooling guarantee for all the liquid-cooled servers, we run simple experiments with the worst-case setup by putting all servers to 100% utilization, and adjust the chiller set point and flow rate of the cold plate to ensure the microprocessor temperature is below the threshold. We then use the COP gained in this situation as a constant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Power Minimization</head><p>We now formulate the power minimization problem of the hybrid-cooled data center. N servers are deployed in the data center and M of them are liquid-cooled. Assuming that the total workload is W total , we minimize the total power consumption as:</p><formula xml:id="formula_10">min{P server + P air + P liquid }<label>(11)</label></formula><p>Subject to:</p><formula xml:id="formula_11">N ∑ i=1 W i = W total<label>(12)</label></formula><formula xml:id="formula_12">T mp i &lt; T mp th 1 ≤ i ≤ M<label>(13)</label></formula><formula xml:id="formula_13">T in i &lt; T in th M + 1 ≤ i ≤ N<label>(14)</label></formula><p>Equation 12 guarantees that all the workload W total is handled by the servers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">CFD-based Recirculation Matrix and Optimization Solution</head><p>We now explain how to get the CFD-based recirculation matrix H. In Equation 6, h i j is an element of the matrix H, indicating the percentage of heat flow recirculated from server i to server j. To simulate the thermal environment of the data center, we use Fluent <ref type="bibr" target="#b4">[5]</ref>, which is a CFD software package. <ref type="figure">Figure 3</ref> shows both the layout of the data center model used in this paper and an example of the thermal environment when all the servers are air-cooled. We set the CRAC supply temperature in CFD and use it to get the outlet temperature of each server, in different workload distribution scenarios. After getting the power consumption (P air i ), the outlet temperature of each server (T i out ,T j out ) and the CRAC supply temperature (T sup ) in all the scenarios, we use them to solve the linear equation shown in Equation 6 and get the recirculation matrix H.</p><p>To solve the optimization problems, we use LINGO <ref type="bibr" target="#b14">[15]</ref>, a comprehensive optimization tool. LINGO employs branch-and-cut methods to break a non-linear programming model down into a list of sub problems to enhance the computation efficiency. It is important to note that our scheme performs offline optimization to determine workload distribution, server on/off and the cooling mode of the data center at different outside temperatures. To dynamically determine those configurations, our scheme can conduct the optimization for different loading levels in an offline fashion and then apply the results online based on the current loading and the current outside temperature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Power Optimization for Geo-Distributed</head><p>Hybrid-cooled Data Centers  distributed data centers together. In this section, we extend the total power optimization problem for a single data center to fit geo-distributed data centers. We develop a two-layer light-weight optimization algorithm to lower the computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Global Optimization</head><p>At first, we formulate a global optimization problem for minimizing the total power consumption of geodistributed data centers, which is very similar to the optimization problem for a single data center. To simplify the notations, we assume that each data center has N servers, which is not a hard requirement for the formulation. We minimize the total power consumption of the data center system that contains K data centers to handle W geo total workload as:</p><formula xml:id="formula_14">min K ∑ j=1 P DC j<label>(15)</label></formula><p>Subject to:</p><formula xml:id="formula_15">K ∑ j=1 N ∑ i=1 W i, j = W geo total<label>(16)</label></formula><formula xml:id="formula_16">T mp i, j &lt; T mp th 1 ≤ i ≤ M 1 ≤ j ≤ K<label>(17)</label></formula><formula xml:id="formula_17">T in i, j &lt; T in th M + 1 ≤ i ≤ N 1 ≤ j ≤ K<label>(18)</label></formula><p>P DC j is the total power consumption of data center j. T mp i, j is the microprocessor temperature of liquid-cooled server and T mp i, j represents the inlet temperature of aircooled server in a data center. W i, j is the workload distributed to server i in data center j. Equation 16 guarantees that all the workload for geo-distributed data centers can be handled. Equation 17 and Equation 18 are the temperature constraints of liquid-cooled and air-cooled servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Two-layer Light-weight Optimization</head><p>To solve the global optimization problem for geodistributed hybrid-cooled data centers, a straightforward solution is to use LINGO directly, as the solution for the single data center power optimization in Section 4. However, as LINGO utilizes the branch-and-bound technique to solve the problem, the computational complexity increases significantly when LINGO solves the problem with geo-distributed data centers. Therefore, we design a two-layer light-weight optimization algorithm to lower the computation complexity.</p><p>We first define cPUE for a single data center as</p><formula xml:id="formula_18">cPUE = P server + P cooling P compute (W )<label>(19)</label></formula><p>where P compute is the total dynamic computing power consumed by the servers to handle a given workload W . As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the local optimizer uses the power optimization process of a single data center (discussed in Section 4) to derive an optimal cPUE for a data center with a given workload W and an outside temperature T outside as cPUE optimal (T outside ,W ),</p><formula xml:id="formula_19">cPUE optimal = f (T outside ,W )<label>(20)</label></formula><p>In fact, with different amounts of workload and different outside temperatures, cPUE optimal has different values. To get the cPUE optimal model for each data center, we need to obtain a set of sample values of the optimal power consumption with different workloads and outside temperatures. We change the workload from 0% to 100% with a 10% increment step each time, and also change the outside temperature from 0 • C to 20 • C with an increment of 1 • C. We get the power optimization solution of each single data center with different workload and outside temperature combinations. The obtained results are a set of sampled triplets as (cPUE optimal ,W, T outside ). We then use the Levenberg Marquardt (LM) algorithm to conduct the linear fitting to find out the cPUE optimal model:</p><formula xml:id="formula_20">cPUE optimal = a * T outside + b * W + c<label>(21)</label></formula><p>The coefficients a, b, c are determined by the data center cooling configuration (e.g., the number of liquid-cooled servers). We choose to do linear model fitting due to the consideration of calculation complexity. Its accuracy is adequate within the acceptable range as we will discuss in Section 6.4. With the cPUE optimal model, we can model the optimal power consumption of a single data center by combining Equations 19 and 21 as: </p><formula xml:id="formula_21">P DC = P compute (W ) * cPUE optimal (T outside ,W ) (22)</formula><p>Given a specific moment, the air temperature outside a data center is a constant, and thus P DC only depends on W. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, the local optimizer sends the P DC (W ) model to the global optimizer, which optimizes the total power consumption by manipulating the workload assigned to each data center. This optimization problem is also solved using LINGO.</p><p>Our algorithm successfully decouples the global power optimization problem to a global workload distribution problem and a power optimization problem of each local data center, and thus reduces the optimization overhead significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Maintaining Response Time</head><p>When the workload dispatching is managed by the global optimizer in <ref type="figure" target="#fig_4">Figure 4</ref>, the request response time needs to be maintained below a threshold. We consider two components of response time: the queuing delay within the data center, and the network delay outside the data center.</p><p>A data center can be modeled as a GI/G/m queue <ref type="bibr" target="#b10">[11]</ref>. Using the Allen-Cullen approximation for the GI/G/m model, the queuing delay and the number of servers needed to satisfy a given workload demand are related as follows:</p><formula xml:id="formula_22">R = 1 µ + P m µ(1 − ρ) ( C 2 A +C 2 B 2m ) (23)</formula><p>where R is the average queuing delay. 1 µ is the average processing time of a request. ρ is the average server utilization. m is the number of servers. P m = ρ m+1 2 for ρ &lt; 0.7. P m = ρ m +ρ 2 for ρ &gt; 0.7 and C 2 A and C 2 B represent the squared coefficients of the variation of request inter-arrival times and request sizes, respectively. The network delay d i j between the source i and the data center j is taken to be proportional to the geographical distances between them.</p><p>When dispatching workload among data centers, we have the constraint that</p><formula xml:id="formula_23">W + d i j &lt; T i j<label>(24)</label></formula><p>where T i j is the response time threshold of the requests dispatched from source i to data center j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Simulation Results</head><p>In this section, we present our evaluation results from the simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Evaluation Setup</head><p>To evaluate different power optimization schemes in a single hybrid-cooled data center, we use a data center model that employs the standard configuration of alternating hot and cold aisles, which is consistent with those used in the previous studies <ref type="bibr" target="#b10">[11]</ref>. <ref type="figure">Figure 3</ref> shows both the data center layout and a thermal environment example when all the servers are air-cooled. The data center consists of four rows of servers, where the first row is composed of liquid-cooled servers. Each row has eight racks, where each rack has 40 servers, adding up to 1,280 servers in the entire data center. The server in our data center model has a 100W idle power consumption and a 300W maximum power consumption when fully utilized. The volumetric flow rate of the intake air of each server is 0.0068m 3 /s. Each of the four CRAC units in the data center pushes chilled air into a raised floor plenum at a rate of 9000 f t 3 /min <ref type="bibr" target="#b10">[11]</ref>. There also exists a free cooling economizer system that uses outside air when suitable to meet the cooling requirements. For the liquid-cooled servers we use the rack CDU (coolant distribution unit), in which the CPU of every server is cooled by cold plate and the other components are cooled by the chilled air. We have two chiller systems, of which one is to supply cold water for the cold plates and the other one is to supply the coolant to the CRAC units.</p><p>To evaluate different power optimization schemes among different data centers, we consider about three data centers with different cooling configuration, a aircooled data center ( all the four rows of servers are air-cooled), a hybrid-cooled data center ( one row of servers are liquid-cooled as discussed in the previous paragraphes ) and a liquid-cooled data center ( all the servers are liquid-cooled ). The only difference between these three data centers are the number of liquid-cooled servers. Other settings of the data centers are the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison of Cooling Schemes</head><p>In this section, we compare our SmartCool scheme with two baselines: Load-Unaware and Liquid-First.</p><p>Load-Unaware determines the cooling mode by comparing the outside temperature to a fixed temperature threshold, which is equal to the highest CRAC supply temperature that can safely cool the servers when they are all fully utilized. When the outside temperature is below the threshold free air cooling is used, otherwise the traditional air cooling system with chillers and pumps is selected. Load-Unaware prefers to distribute the workload to the liquid-cooled servers. If they are fully utilized, the remaining workload is then distributed to the air-cooled servers. The servers in the middle of each row and at the bottom of each rack are prior, as servers located at those places have less recirculation impact and lower inlet temperature <ref type="bibr" target="#b10">[11]</ref>.</p><p>In contrast, Liquid-First dynamically adjusts the temperature threshold for free air cooling, based on the realtime workload. It first distributes workload to the servers in the same way as Load-Unaware, and then uses the highest CRAC supply temperature that can safely cool the servers as the temperature threshold. <ref type="figure">Figure 5</ref> shows the total power consumption of the three different schemes at different loadings with different outside temperature. We can see from the results that all the three cooling schemes achieve a low power consumption when the outside temperature is low, because all of them can use free air cooling. Compared with Load-Unaware and Liquid-First, SmartCool shows the lowest power consumption because it considers the heat recirculation among air-cooled servers when distributing workload.</p><p>When the outside temperature increases, we can see that Load-Unaware is the first to have a jump in the power consumption curve among the three schemes. This is because Load-Unaware uses a fixed temperature threshold to decide whether to use free air cooling or not. The temperature threshold of Load-Unaware is determined with the data center running a 100% percent workload. Therefore, it is unnecessarily low for less workload such as 30%. Liquid-First is the second one to have a power consumption jump due to switching from free air cooling to CRAC cooling. It can use free air cooling more than Load-Unaware when the outside temperature is higher, because its temperature threshold is determined based on the real-time workload (e.g., 30%, 50% or 70%) rather than the maximum workload (100%). Hence Liquid-First saves power compared with LoadUnaware. SmartCool scheme is the last one to have the power consumption jump, because SmartCool optimizes the workload distribution among liquid-cooled and aircooled servers, while the two baselines concentrate the workload on a small number of air-cooled servers and result in some hot spots when air cooling is necessary. Those hot spots require a lower temperature of the supplied air for cooling and thus increase the power consumption. Therefore, SmartCool is the most power efficient scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Power Breakdown of Cooling Schemes</head><p>In <ref type="figure">Figure 6</ref>, we break the cooling power consumption of the three schemes (including Load-Unaware,LiquidFirst and SmartCool) into Air Free (free cooling power consumption), Air Chiller (traditional air cooling power consumption) and Liquid Cooling (liquid cooling power  <ref type="figure">Figure 6</ref> (a) shows the cooling power under different outside temperatures at 30% loading. We can see that when the outside temperature is 0 (Celsius), LoadUnaware and Liquid-First consume the same amount of cooling power. SmartCool consumes less cooling power than the two baselines because it distributes all the workload to the air-cooled servers, which are cooled by the more efficient free air, while the baselines prefer to use liquid-cooled servers.</p><p>When the outside temperature raises to 10 • C, LoadUnaware switches to the traditional air cooling mode, which starts to use the chiller system and leads to the increase of the cooling power consumption. Differently, the other two schemes show similar results as those at 0 • C. When the outside temperature is 20 • C, all the three schemes switch to traditional air cooling mode. However, SmartCool still consumes less cooling power because it considers the impact of air recirculation and optimizes the workload distribution.</p><p>Figures 6 (b) and 6 (c) show the breakdown of cooling power consumption when the data center is at 50% and 70% loading. They show the same trends as 6 (a) though the total cooling power increases due to the increase of the workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Comparison between SmartCool and Global Optimal Solution</head><p>In this section, we evaluate our two-layer power optimization algorithm in the geo-distributed data center settings and compare it with the global optimization scheme in terms of optimization performance, including the optimized total power consumption and the time overhead. The global optimal scheme solves the geo-distributed power optimization problem as a whole including deciding the global and local workload distribution as well as the cooling mode management of each data center. SmartCool uses a two-layer optimization algorithm as discussed in Section 5. Due to the long computation time of the global optimization scheme, we use three smaller scale data centers in this set of experiments. Each data center has two rows of racks. Each row contains 4 racks and each rack contains four blocks. There exists 10 servers in each block. <ref type="figure" target="#fig_6">Figure 7</ref>(a) shows the total power consumption of the two schemes at different loadings. We can see that SmartCool has very close optimization result to the global optimal solution. The performance difference is due to the model fitting error introduced from the cPUE modeling process as discussed in Section 5. However, our SmartCool consumes much less time than the Global Optimal solution according to <ref type="figure" target="#fig_6">Figure 7</ref>(b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results with Real Workload and Temperature Traces</head><p>We now evaluate different power management schemes on three geo-distributed data centers with real workload and temperature traces. Each data center contains 1,280 servers. To show the diversity of different data centers, we configure them with different cooling system, which are air cooling, liquid cooling and hybrid cooling, respectively. The outside temperatures are shown in <ref type="figure" target="#fig_7">Fig- ure 8</ref> (a) which are one week temperature traces of three different locations, Geneva, Hamina and Chicago <ref type="bibr" target="#b24">[25]</ref>. We compare the total power consumption under three workload dispatching schemes: Liquid-First, Low-TempFirst and SmartCool. Liquid-First dispatches workload to the three data centers according to their cooling efficiencies, which are ranked from high to low as the liquidcooled data center, the hybrid-cooled data center and the air-cooled data center. Thus workload is first dispatched to the liquid-cooled data center and if it can not handle all the workload, the rest part is dispatched to the hybridcooled data center and then the air-cooled data center. For Low-Temp-First, workload is first distributed to the data center with the lowest outside temperature since it has the highest possibility to use free cooling system which will consume the least cooling power. For SmartCool, workload is distributed according to the approach discussed in Section 5. <ref type="figure" target="#fig_7">Figure 8</ref> (b) shows a one week trace of the average CPU utilization from an IBM production data center <ref type="bibr" target="#b28">[29]</ref>. We use this trace to generate the total workload in our experiment. <ref type="figure" target="#fig_7">Figure 8 (c)</ref> shows the power consumption of the three different schemes. We can see that our SmartCool consumes the least power because it considers the impacts of both the outside tempera- ture and the workload on the data center PUE. Liquid First consumes more power than SmartCool but less than the Low-Temp-First solution, because it first concentrates workload on liquid-cooled data center, which has relatively high cooling efficiency. Low-Temp-First consumes the most power among the three schemes because when the outside temperature is not low enough or the workload is relatively high, Low-Temp-First first dispatches all the workload to the data center with the lowest outside temperature and then traditional air cooling system with chiller and pump must be used to cool down the servers, which will cause high cooling power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Hardware Experiment</head><p>In addition to the simulations, we also conduct experiments on our hardware testbed to evaluate SmartCool by comparing it with the baselines, i.e., Load-Unaware and Liquid-First. The testbed includes one liquid-cooled server and three air-cooled servers. A heater is used to set the ambient temperature to be 22 • C, 26 • C and 30 • C. To compare the total power consumptions of the three schemes, we use power meters to measure the power consumed by the servers and the cold plate used for liquid cooling. For the reason that we do not have an air handler and just use the ambient air to take away heat generated by the server, we assume that the air cooling power is zero under free cooling mode and use Equation 4 and 5 to estimate the power consumption of traditional air cooling. <ref type="figure" target="#fig_8">Figure 9</ref> shows the power consumption of different schemes with different ambient temperatures and loadings. We can see that when the ambient temperature is 22 • C, the air cooling power of all the three schemes are zero at different loadings, because they can all adopt free air cooling. SmartCool consumes less cooling power than Load-Unaware and Liquid-First, for it does not consume liquid cooling power when the ambient temperature is at 22 • C as all the workload is distributed to the air-cooled servers. In contrast, the two baselines prefer to distribute workload to the liquid-cooled servers, no matter how cold the ambient air is. When the ambient temperature is 22 • C and the workload is at 30% or 50%, Load-Unaware consumes the most cooling power, because it begins to use traditional air cooling since the ambient temperature exceeds its fixed temperature threshold for cooling mode decision, and thus cause more cooling power. Liquid-First still uses free cooling at 30% and 50% loadings, and begins to use traditional air cooling when the workload is 70%. At 26 • C SmartCool still consumes less cooling power than the other two schemes. The results show the same trend when the outside temperature is 30 • C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we have presented SmartCool, a power optimization scheme that effectively coordinates different cooling techniques and dynamically manages workload allocation for jointly optimized cooling and server power. In sharp contrast to the existing work that addresses different cooling techniques in an isolated manner, SmartCool systematically formulates the integration of different cooling systems as a constrained optimization problem. Furthermore, since geo-distributed data centers have different ambient temperatures, SmartCool dynamically dispatches the incoming requests among a network of data centers with heterogeneous cooling systems to best leverage the high efficiency of free cooling. A light-weight heuristic algorithm is proposed to achieve a near-optimal solution with a low time overhead. SmartCool has been evaluated both in simulation and on a hardware testbed with real-world workload and temperature traces. The results show that SmartCool outperforms two state-of-the-art baselines by having a 38% more power savings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Cooling System of Hybrid-Cooled Data Center. The cold plates used for liquid cooling are installed inside the liquid-cooled servers. The air cooling mode is decided based on the outside temperature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Air circulation in an air-cooled data center. Some hot air can be recirculated to the inlet and mixed with the cold air, degrading the cooling efficiency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 3: Data center model used in evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: System diagram for geo-distributed data centers. The local optimizer optimizes the cooling configuration and workload distribution locally. The global optimizer optimizes the global workload distribution based on the data center power models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Total power consumption with Load-Unaware, Liquid-First and SmartCool at different loadings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Power consumption and required time comparison between Global Optimal and SmartCool</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Total Power Consumption comparison of different workload dispatching schemes consumption). We choose three outside temperature points, 0 • C, 10 • C and 20 • C to discuss the impact of outside temperature. Figure 6 (a) shows the cooling power under different outside temperatures at 30% loading. We can see that when the outside temperature is 0 (Celsius), LoadUnaware and Liquid-First consume the same amount of cooling power. SmartCool consumes less cooling power than the two baselines because it distributes all the workload to the air-cooled servers, which are cooled by the more efficient free air, while the baselines prefer to use liquid-cooled servers. When the outside temperature raises to 10 • C, LoadUnaware switches to the traditional air cooling mode, which starts to use the chiller system and leads to the increase of the cooling power consumption. Differently, the other two schemes show similar results as those at 0 • C. When the outside temperature is 20 • C, all the three schemes switch to traditional air cooling mode. However, SmartCool still consumes less cooling power because it considers the impact of air recirculation and optimizes the workload distribution. Figures 6 (b) and 6 (c) show the breakdown of cooling power consumption when the data center is at 50% and 70% loading. They show the same trends as 6 (a) though the total cooling power increases due to the increase of the workload.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Hardware results under different ambient temperatures at different loadings (x-axis: a is Load-Unaware, b is Liquid-First, c is SmartCool)</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling and dynamic management of 3d multicore systems with liquid cooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coskun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ayala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Atienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">T S</forename><surname>Rosing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Very Large Scale Integration</title>
		<meeting>International Conference on Very Large Scale Integration</meeting>
		<imprint>
			<publisher>VLSI-SoC</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cost of a cloud: research problems in data center networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">P</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGCOMM CCR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CFD analysis of free cooling of modular data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gebrehiwot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aurangabadkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">D</forename><surname>Agonafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</title>
		<meeting>Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cern</forename><forename type="middle">Data</forename><surname>Centre</surname></persName>
		</author>
		<ptr target="http://home.web.cern.ch/about/computing" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://www.caeai.com/cfd-software.php" />
		<title level="m">COMPUTATIONAL FLUID DYNAMICS (CFD) SOFTWARE BY ANSYS INC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The uptime institute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Chemicoff</forename></persName>
		</author>
		<ptr target="http://symposium.uptimeinstitute.com" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Energy efficient free cooling system for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">S</forename><surname>Abimannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<publisher>CloudCom</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Energy savings achievable through liquid cooling: A rack level case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">P</forename><surname>Manno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">G J</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITherm)</title>
		<meeting>Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITherm)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Energy efficient free cooling system for data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sujatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">S</forename><surname>Abimannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Cloud Computing Technology and Science (CloudCom</title>
		<meeting>International Conference on Cloud Computing Technology and Science (CloudCom</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DSE precision cooling system sales brochure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emerson</forename><surname>Liebert</surname></persName>
		</author>
		<ptr target="http://www.emersonnetworkpower.com" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint optimization of idle and cooling power in data centers while maintaining response time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">T N</forename><surname>Vijaykumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parasol and greenswitch: Managing datacenters powered by renewable energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Katsak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">R</forename><surname>Bianchini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint power optimization of data center network and servers with correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd IEEE International Conference on Computer Communications (INFOCOM)</title>
		<meeting>the 33rd IEEE International Conference on Computer Communications (INFOCOM)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The datacenter as a computer: An introduction to the design of warehouse-scale machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clidaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">U</forename><surname>Holzle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Morgan and Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindo</forename><surname>Systems Inc</surname></persName>
		</author>
		<ptr target="http://www.lindo.com" />
		<title level="m">LINDO software products</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Energy efficient geographical load balancing via dynamic deferral of workload</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Adnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sugihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th IEEE</title>
		<meeting>5th IEEE</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint management of data centers and electric vehicles for maximized regulation profits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brocanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth IEEE International Greeen Computing Conference (IGCC)</title>
		<meeting>the fourth IEEE International Greeen Computing Conference (IGCC)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Server liquid cooling with chiller-less data center design to enable significant energy savings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">V</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</title>
		<meeting>Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Server liquid cooling with chiller-less data center design to enable significant energy savings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Parida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">V</forename><surname>Kamath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</title>
		<meeting>Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Thermalaware task scheduling for data centers through minimizing heat recirculation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">G</forename><surname>Varsamopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Cluster Computing</title>
		<meeting>IEEE International Conference on Cluster Computing</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sensorbased fast thermal evaluation model for energy efficient highperformance data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">P</forename><surname>Cayton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Intelligent Systems and Image Processing</title>
		<meeting>International Conference on Intelligent Systems and Image Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling of air conditioning systems for cooling of data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">B</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITHERM)</title>
		<meeting>Intersociety Conference on Thermal and Thermomechanical Phenomena in Electronic Systems (ITHERM)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data center cooling management and analysis-a model based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">A</forename><surname>Mcreynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</title>
		<meeting>Semiconductor Thermal Measurement and Management Symposium (SEMI-THERM)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Data center rack level cooling utilizing watercooled, passive rear door heat exchangers as a cost effective alternative to crash air cooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Movotny</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">TIMEANDDATE. Weather around the World. www.timeanddate.com</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Treehugger</forename><surname>Inc</surname></persName>
		</author>
		<ptr target="http://www.treehugger.com" />
	</analytic>
	<monogr>
		<title level="j">Data Center Cooling Energy Reduction Thanks to Fluid Submerged Servers</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TAPO: Thermal-aware power optimization techniques for servers and data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allen-Ware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">E</forename><surname>Elnozahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Greeen Computing Conference (IGCC)</title>
		<meeting>IEEE International Greeen Computing Conference (IGCC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting thermal energy storage to reduce data center capital and operating expenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>the 20th International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SHIP: Scalable hierarchical power control for large-scale data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lefurgy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">T W</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Parallel and Distributed Systems (PACT)</title>
		<meeting>International Conference on Parallel and Distributed Systems (PACT)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Integrated management of application performance, power and cooling in data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hyser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">S</forename><surname>Singhal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Network Operations and Management Symposium (NOMS)</title>
		<meeting>Network Operations and Management Symposium (NOMS)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Electricity bill capping for cloud-scale data centers that impact the power markets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Parallel Processing</title>
		<meeting>the International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting thermal and energy storage to cut the electricity bill for datacenter cooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">X Wang</forename><surname>Testore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Network and Service Management (CNSM)</title>
		<meeting>the 8th International Conference on Network and Service Management (CNSM)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Greening geographical load balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">L</forename><surname>Andrew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems</title>
		<meeting>the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Renewable and cooling aware workload management for sustainable data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wierman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gmach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marwah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">C</forename><surname>Hyser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems</title>
		<meeting>the ACM SIGMETRICS joint international conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Integrated management of cooling resources in air-cooled data centers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">And</forename><forename type="middle">A</forename><surname>Mcreynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Automation Science and Engineering (CASE)</title>
		<meeting>IEEE Conference on Automation Science and Engineering (CASE)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
