<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharya</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep2">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep3">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep4">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep5">Max Planck Institute for Informatics</orgName>
								<orgName type="department" key="dep6">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep7">CISPA Helmholtz Center for Information Security</orgName>
								<orgName type="department" key="dep8">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<addrLine>Apratim Bhattacharya</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Machine learning (ML) has progressed rapidly during the past decade and the major factor that drives such development is the unprecedented large-scale data. As data generation is a continuous process, this leads to ML model owners updating their models frequently with newly-collected data in an online learning scenario. In consequence, if an ML model is queried with the same set of data samples at two different points in time, it will provide different results. In this paper, we investigate whether the change in the output of a black-box ML model before and after being updated can leak information of the dataset used to perform the update, namely the updating set. This constitutes a new attack surface against black-box ML models and such information leakage may compromise the intellectual property and data privacy of the ML model owner. We propose four attacks following an encoder-decoder formulation, which allows inferring diverse information of the updating set. Our new attacks are facilitated by state-of-the-art deep learning techniques. In particular , we propose a hybrid generative model (CBM-GAN) that is based on generative adversarial networks (GANs) but includes a reconstructive loss that allows reconstructing accurate samples. Our experiments show that the proposed attacks achieve strong performance.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning (ML) has progressed rapidly during the past decade. A key factor that drives the current ML development is the unprecedented large-scale data. In consequence, collecting high-quality data becomes essential for building advanced ML models. Data collection is a continuous process, which in turn transforms the ML model training into a continuous process as well: Instead of training an ML model for once and keeping on using it afterwards, the model's owner needs to keep on updating the model with newly-collected data. As training from scratch is often prohibitive, this is often achieved by online learning. We refer to the dataset used to perform model update as the updating set.</p><p>In this paper, our main research question is: Can different outputs of an ML model's two versions queried with the same set of data samples leak information of the corresponding updating set?. This constitutes a new attack surface against machine learning models. Information leakage of the updating set may compromise the intellectual property and data privacy of the model owner.</p><p>We concentrate on the most common ML applicationclassification. More importantly, we target black-box ML models -the most difficult attack setting where an adversary does not have access to her target model's parameters but can only query the model with her data samples and obtain the corresponding prediction results, i.e., posteriors in the case of classification. Moreover, we assume the adversary has a local dataset from the same distribution as the target model's training set, and the ability to establish the same model as the target model with respect to model architecture. Finally, we only consider updating sets which contain up to 100 newly collected data samples. Note that this is a simplified setting and a step towards real-world setting.</p><p>In total, we propose four different attacks in this surface which can be categorized into two classes, namely, singlesample attack class and multi-sample attack class. The two attacks in the single-sample attack class concentrate on a simplified case when the target ML model is updated with one single data sample. We investigate this case to show whether an ML model's two versions' different outputs indeed constitute a valid attack surface. The two attacks in the multisample attack class tackle a more general and complex case when the updating set contains multiple data samples.</p><p>Among our four attacks, two (one for each attack class) aim at reconstructing the updating set which are the first attempts in this direction. Compared to many previous attacks inferring certain properties of a target model's training set <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b16">20]</ref>, a dataset reconstruction attack leads to more severe consequences.</p><p>Our experiments show that indeed, the output difference of the same ML model's two different versions can be exploited to infer information about the updating set. We detail our contributions as the following.</p><p>General Attack Construction. Our four attacks follow a general structure, which can be formulated into an encoderdecoder style. The encoder realized by a multilayer perceptron (MLP) takes the difference of the target ML model's outputs, namely posterior difference, as its input while the decoder produces different types of information about the updating set with respect to different attacks.</p><p>To obtain the posterior difference, we randomly select a fixed set of data samples, namely probing set, and probe the target model's two different versions (the second-version model is obtained by updating the first-version model with an updating set). Then, we calculate the difference between the two sets of posteriors as the input for our attack's encoder.</p><p>Single-Sample Attack Class. The single-sample attack class contains two attacks: Single-sample label inference attack and single-sample reconstruction attack. The first attack predicts the label of the single sample used to update the target model. We realize the corresponding decoder for the attack by a two-layer MLP. Our evaluation shows that our attack is able to achieve a strong performance, e.g., 0.96 accuracy on the CIFAR-10 dataset <ref type="bibr">[1]</ref>.</p><p>The single-sample reconstruction attack aims at reconstructing the updating sample. We rely on autoencoder (AE). In detail, we first train an AE on a different set of data samples. Then, we transfer the AE's decoder into our attack model as its sample reconstructor. Experimental results show that we can reconstruct the single sample with a performance gain (with respect to mean squared error) of 22% for the MNIST dataset <ref type="bibr">[2]</ref>, 107.1% for the CIFAR-10 dataset, and 114.7% for the Insta-NY dataset <ref type="bibr" target="#b2">[6]</ref>, over randomly picking a sample affiliated with the same label of the updating sample.</p><p>Multi-Sample Attack Class. The multi-sample attack class includes multi-sample label distribution estimation attack and multi-sample reconstruction attack. Multi-sample label distribution estimation attack estimates the label distribution of the updating set's data samples. It is a generalization of the label inference attack in the single-sample attack class. We realize this attack by setting up the attack model's decoder as a multilayer perceptron with a fully connected layer and a softmax layer. Kullback-Leibler divergence (KL-divergence) is adopted as the model's loss function. Our experiments demonstrate the effectiveness of this attack. For the CIFAR-10 dataset, when the updating set's cardinality is 100, our attack model achieves a 0.00384 KL-divergence which outperforms random guessing by a factor of 2.5. Moreover, the accuracy of predicting the most frequent label is 0.29 which is almost 3 times higher than random guessing.</p><p>Our last attack, namely multi-sample reconstruction attack, aims at generating all samples in the updating set. This is a much more complex attack than the previous ones. The decoder for this attack is assembled with two components. The first one learns the data distribution of the updating set samples. In order to achieve coverage and accuracy of the reconstructed samples, we propose a novel hybrid generative model, namely CBM-GAN. Different from the standard generative adversarial networks (GANs), our Conditional Best of Many GAN (CBM-GAN) introduces a "Best Match" loss which ensures that each sample in the updating set is reconstructed accurately. The second component of our decoder relies on machine learning clustering to group the generated data samples by CBM-GAN into clusters and take the central sample of each cluster as one final reconstructed sample. Our evaluation shows that our approach outperforms all baselines when reconstructing the updating set on all MNIST, CIFAR-10, and Insta-NY datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we start by introducing online learning, then present our threat model, and finally introduce the datasets used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Online Learning</head><p>In this paper, we focus on the most common ML task -classification. An ML classifier M is essentially a function that maps a data sample x ∈ X to posterior probabilities y ∈ Y , i.e., M : X → Y . Here, y ∈ Y is a vector with each entry indicating the probability of x being classified to a certain class or affiliated with a certain label. The sum of all values in y is 1. To train an ML model, we need a set of data samples, i.e., training set. The training process is performed by a certain optimization algorithm, such as ADAM, following a predefined loss function. A trained ML model M can be updated with an updating set denoted by D update . The model update is performed by further training the model with the updating set using the same optimization algorithm on the basis of the current model's parameters. More formally, given an updating set D update and a trained ML model M , the updating process F update can be defined as F update : D update , M → M where M is the updated version of M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Threat Model</head><p>For all of our four attacks, we consider an adversary with black-box access to the target model. This means that the adversary can only query the model with a set of data samples, i.e., her probing set, and obtain the corresponding posteriors. This is the most difficult attack setting for the adversary <ref type="bibr" target="#b37">[40]</ref>. We also assume that the adversary has a local dataset which comes from the same distribution as the target model's training set following previous works <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b37">40]</ref>. Moreover, we consider the adversary to be able to establish the same ML model as the target ML model with respect to model architecture. This can be achieved by performing model hyperparam-eter stealing attacks <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b44">47]</ref>. The adversary needs these two information to establish a shadow model which mimics the behavior of the target model to derive data for training her attack model (see Section 3). Also, part of the adversary's local dataset will be used as her probing set. Finally, we assume that the target ML model is updated only with new data, i.e., the updating set and the training set are disjoint.</p><p>We later show in Section 6 that the two assumptions, i.e., the adversary's knowledge of the target model's architecture and her possession of a dataset from the same distribution as the target model's training set, can be further relaxed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Datasets Description</head><p>For our experimental evaluation, we use three datasets: MNIST, CIFAR-10, and Insta-NY. Both MNIST and CIFAR-10 are benchmark datasets for various ML security and privacy tasks. MNIST is a 10-class image dataset, it consists of 70,000 28×28 grey-scale images. Each image contains in its center a handwritten digit. Images in MNIST are equally distributed over 10 classes. CIFAR-10 contains 60,000 32×32 color images. Similar to MNIST, CIFAR-10 is also a 10-class balanced dataset. Insta-NY <ref type="bibr" target="#b2">[6]</ref> contains a sample of Instagram users' location check-in data in New York. Each check-in represents a user visiting a certain location at a certain time. Each location is affiliated with a category. In total, there are eight different categories. Our ML task for Insta-NY is to predict each location's category. We use the number of check-ins happened at each location in each hour on a weekly base as the location's feature vector. We further filter out locations with less than 50 check-ins, in total, we have 19,215 locations for the dataset. In Section 6, we further use Insta-LA <ref type="bibr" target="#b2">[6]</ref> which contains the check-in data from Los Angeles for our threat model relaxation experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">General Attack Pipeline</head><p>Our general attack pipeline contains three phases. In the first phase, the adversary generates her attack input, i.e., posterior difference. In the second phase, our encoder transforms the posterior difference into a latent vector. In the last phase, the decoder decodes the latent vector to produce different information of the updating set with respect to different attacks. <ref type="figure">Figure 1</ref> provides a schematic view of our attack pipeline.</p><p>In this section, we provide a general introduction for each phase of our attack pipeline. In the end, we present our strategy of deriving data to train our attack models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attack Input</head><p>Recall that we aim at investigating the information leaked from posterior difference of a model's two versions when queried with the same set of data samples. To create this posterior difference, the adversary first needs to pick a set of data samples as her probing set, denoted by D probe . In this work, the adversary picks a random sample of data samples (from her local dataset) to form D probe . Choosing or crafting <ref type="bibr" target="#b30">[33]</ref> a specific set of data samples as the probing set may further improve attack efficiency, we leave this as a future work. Next, the adversary queries the target ML model M with all samples in D probe and concatenates the received outputs to form a vector y probe . Then, she probes the updated model M with samples in D probe and creates a vector y probe accordingly. In the end, she sets the posterior difference, denoted by δ, to the difference of both outputs:</p><formula xml:id="formula_0">δ = y probe − y probe</formula><p>Note that the dimension of δ is the product of D probe 's cardinality and the number of classes of the target dataset. For this paper, both CIFAR-10 and MNIST are 10-class datasets, while Insta-NY is an 8-class dataset. As our probing set always contains 100 data samples, this indicates the dimension of δ is 1,000 for CIFAR-10 and MNIST, and 800 for Insta-NY.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder Design</head><p>All our attacks share the same encoder structure, we model it with a multilayer perceptron. The number of layers inside the encoder depends on the dimension of δ: Longer δ requires more layers in the encoder. As our δ is a 1,000-dimension vector for the MNIST and CIFAR-10 datasets, and 800-dimension vector for the Insta-NY dataset, we use two fully connected layers in the encoder. The first layer transforms δ to a 128-dimension vector and the second layer further reduces the dimension to 64. The concrete architecture of our encoder is presented in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder Structure</head><p>Our four attacks aim at inferring different information of D update , ranging from sample labels to the updating set itself.</p><p>Thus, we construct different decoders for different attacks with different techniques. The details of these decoders will be presented in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Shadow Model</head><p>Our encoder and decoder need to be trained jointly in a supervised manner. This indicates that we need ground truth data for model training. Due to our minimal assumptions, the adversary cannot get the ground truth from the target model. To solve this problem, we rely on shadow models following previous works <ref type="bibr" target="#b9">[13,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b37">40]</ref>. A shadow model is designed to mimic the target model. By controlling the training process of the shadow model, the adversary can derive the ground truth data needed to train her attack models.</p><p>As presented in Section 2, our adversary knows (1) the architecture of the target model and <ref type="formula" target="#formula_12">(2)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Single-sample Attacks</head><p>In this section, we concentrate on the case when an ML model is updated with a single sample. This is a simplified attack scenario and we aim to examine the possibility of using posterior difference to infer information about the updating set. We start by introducing the single-sample label inference attack, then, present the single-sample reconstruction attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-sample Label Inference Attack</head><p>Attack Definition. Our single-sample label inference attack takes the posterior difference as the input and outputs the label of the single updating sample. More formally, given a posterior difference δ, our single-sample label inference attack is defined as follows:</p><formula xml:id="formula_1">A LI : δ →</formula><p>where is a vector with each entry representing the probability of the updating sample affiliated with a certain label.</p><p>Methodology. To recap, the general construction of the attack model consists of an MLP-based encoder which takes the posterior difference as its input and outputs a latent vector µ. For this attack, the adversary constructs her decoder also with an MLP which is assembled with a fully connected layer and a softmax layer to transform the latent vector to the corresponding updating sample's label. The concrete architecture of our A LI 's decoder is presented in Appendix C.</p><p>To obtain the data for training A LI , the adversary generates ground truth data by creating a shadow model as introduced in Section 3 while setting the shadow updating set's cardinality to 1. Then, the adversary trains her attack model A LI with a cross-entropy loss. Our loss function is,</p><formula xml:id="formula_2">L CE = ∑ i i log( ˆ i )</formula><p>where i is the true probability of label i andîandˆandî is our predicted probability of label i. The optimization is performed by the ADAM optimizer.</p><p>To perform the label inference attack, the adversary constructs the posterior difference as introduced in Section 3, then feeds it to the attack model A LI to obtain the label.</p><p>Experimental Setup. We evaluate the performance of our single-sample label inference attack using the MNIST, CIFAR-10, and Insta-NY datasets. First, we split each dataset into three disjoint datasets: The target dataset D target , the shadow dataset D shadow , and the probing dataset D probe . As mentioned before, D probe contains 100 data samples. We then split D shadow to D train shadow and D update shadow to train the shadow model as well as updating it (see Section 3). The same process is applied to train and update the target model with D target .</p><p>As mentioned in Section 3, we build 10,000 and 1,000 updated models for shadow and target models, respectively. This means the training and testing sets for our attack model contain 10,000 and 1,000 samples, respectively. We use convolutional neural network (CNN) to build shadow and target models for both CIFAR-10 and MNIST datasets, and a multilayer perceptron (MLP) for the Insta-NY dataset. The CIFAR-10 model consists of two convolutional layers, one max pooling layer, three fully connected layers, and a softmax layer. The MNIST model consists of two convolutional layers, two fully connected layers, and a softmax layer. Finally, the Insta-NY model consists of three fully connected layers and a softmax layer. The concrete architectures of the models are presented in Appendix A.</p><p>All shadow and target models' training sets contain 10,000 images for CIFAR-10 and MNIST, and 5,000 samples for Insta-NY. We train the CIFAR-10, MNIST and Insta-NY models for 50, 25, and 25 epochs, respectively, with a batch size of 64. To create an updated ML model, we perform a singleepoch training. Finally, we adopt accuracy to measure the performance of the attack. All of our experiments are implemented using Pytorch <ref type="bibr">[3]</ref>. For reproducibility purposes, our code will be made available.</p><p>Results. <ref type="figure" target="#fig_0">Figure 2</ref> depicts the experimental results. As we can see, A LI achieves a strong performance with an accuracy of 0.97 on the Insta-NY dataset, 0.96 on the CIFAR-10 dataset, and 0.68 on the MNIST dataset. Moreover, our attack significantly outperforms the baseline model, namely Random, which simply guesses a label over all possible labels. As both CIFAR-10 and MNIST contain 10 balanced classes, the baseline model's result is approximately 10%. For the Insta-NY dataset, since it is not balanced, we randomly sample a label for each sample to calculate the baseline which results in approximately 29% accuracy. Our evaluation shows that the different outputs of an ML model's two versions indeed leak information of the corresponding updating set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Single-sample Reconstruction Attack</head><p>Attack Definition. Our single-sample reconstruction attack takes one step further to construct the data sample used to update the model. Formally, given a posterior difference δ, our single-sample reconstruction attack, denoted by A SSR , is defined as follows:</p><formula xml:id="formula_3">A SSR : δ → x update</formula><p>where x update denotes the sample used to update the model</p><formula xml:id="formula_4">(D update = {x update }).</formula><p>Methodology. Reconstructing a data sample is a much more complex task than predicting the sample's label. To tackle this problem, we need an ML model which is able to generate a data sample in the complex space. To this end, we rely on autoencoder (AE). Autoencoder is assembled with an encoder and a decoder. Different from our attacks, AE's goal is to learn an efficient encoding for a data sample: Its encoder encodes a sample into a latent vector and its decoder tries to decode the latent vector to reconstruct the same sample. This indicates AE's decoder itself is a data sample reconstructor. For our attack, we first train an AE, then transfer the AE's decoder to our attack model as the initialization of the attack's decoder. <ref type="figure" target="#fig_1">Figure 3</ref> provides an overview of the attack methodology. The concrete architectures of our AEs' encoders and decoders are presented in Appendix D.</p><p>After the autoencoder is trained, the adversary takes its decoder and appends it to her attack model's encoder. To establish the link, the adversary adds an additional fully connected layer to its encoder which transforms the dimensions of the latent vector µ to the same dimension as µ AE .</p><p>We divide the attack model training process into two phases. In the first phase, the adversary uses her shadow dataset to train an AE with the previously mentioned model architecture. In the second phase, she follows the same procedure for singlesample label inference attack to train her attack model. Note that the decoder from AE here serves as the initialization of the decoder, this means it will be further trained together with the attack model's encoder. To train both autoencoder and our attack model, we use mean squared error (MSE) as the loss function. Our objective is,   wherê x update is our predicted data sample. We again adopt ADAM as the optimizer.</p><formula xml:id="formula_5">L MSE = ˆ x update − x</formula><p>Experimental Setup. We use the same experimental setup as the previous attack (see Section 4.1) except for the evaluation metric. In detail, we adopt MSE to measure our attack's performance instead of accuracy.</p><p>We construct two baseline models, namely Label-random and Random. Both of these baseline models take a random data sample from the adversary's shadow dataset. The difference is that the Label-random baseline picks a sample within the same class as the target updating sample, while the Random baseline takes a random data sample from the whole shadow dataset of the adversary. The Label-random baseline can be implemented by first performing our single-sample label inference attack to learn the label of the data sample and then picking a random sample affiliated with the same label.</p><p>Results. First, our single-sample reconstruction attack achieves a promising performance. As shown in <ref type="figure">Figure 4</ref>, our attack on the MNIST dataset outperforms the Random baseline by 36% and more importantly, outperforms the Labelrandom baseline by 22%. Similarly, for the CIFAR-10 and Insta-NY datasets, our attack achieves an MSE of 0.014 and 0.68 which is significantly better than the two baseline models, i.e., it outperforms the Label-random (Random) baselines by a factor of 2.1 (2.2) and 2.1 (2.3), respectively. The difference between our attack's performance gain over the baseline models on the MNIST and on the other datasets is expected as the MNIST dataset is more homogeneous compared to the other two. In other words, the chance of picking a random data sample similar to the updating sample is much higher in the MNIST dataset than in the other datasets.</p><p>Secondly, we compare our attack's performance against the results of the autoencoder for sample reconstruction. Note that AE takes the original data sample as input and outputs the reconstructed one, thus it is considered as an oracle, since the adversary does not have access to the original updating sample. Here, we just use AE's result to show the best possible result for our attack. From <ref type="figure">Figure 4</ref>, we observe that AE achieves 0.042, 0.0043, and 0.51 MSE for the MNIST, CIFAR-10, and Insta-NY datasets, respectively, which indeed outperforms our attack. However, our attack still has a comparable performance.</p><p>Finally, <ref type="figure" target="#fig_3">Figure 5</ref> visualizes some randomly sampled reconstructed images by our attack on MNIST. The first row depicts the original images used to update the models and the second row shows the result of our attack. As we can see, our attack is able to reconstruct images that are visually similar to the original sample with respect to rotation and shape. We also show the result of AE in the third row in <ref type="figure" target="#fig_3">Figure 5</ref> which as mentioned before, is the upper bound for our attack. The results from <ref type="figure">Figure 4</ref> and <ref type="figure" target="#fig_3">Figure 5</ref> demonstrate the strong performance of our attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Multi-sample Attacks</head><p>After demonstrating the effectiveness of our attacks against the updating set with a single sample, we now focus on a more general attack scenario where the updating set contains multiple data samples that are never seen during the training. We introduce two attacks in the multi-sample attack class: Multi-sample label distribution estimation attack and multisample reconstruction attack.  <ref type="table">)</ref> together with the baseline model and transfer attack. KL-divergence and accuracy are adopted as the evaluation metric. Accuracy here is used to measure the prediction of the most frequent label over samples in the updating set. Transfer 10-100 means each of the training sample for the attack model corresponds to an updating set containing 10 data samples and each of the testing sample for the attack model corresponds to an updating set containing 100 data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-sample Label Distribution Estimation Attack</head><p>Attack Definition. Our first attack in the multi-label attack class aims at estimating the label distribution of the updating set's samples. It can be considered as a generalization of the label inference attack in the single-sample attack class. Formally, the attack is defined as:</p><formula xml:id="formula_6">A LDE : δ → q</formula><p>where q as a vector denotes the distribution of labels over all classes for samples in the updating set.</p><p>Methodology. The adversary uses the same encoder structure as presented in Section 3 and the same decoder structure of the label inference attack (Section 4.1). Since the label distribution estimation attack estimates a probability vector q instead of performing classification, we use Kullback--Leibler divergence (KL-divergence) as our objective function:</p><formula xml:id="formula_7">L KL = ∑ i ( ˆ q ) i log ( ˆ q ) i (q ) i</formula><p>wherê q and q represent our attack's estimated label distribution and the target label distribution, respectively, and (q ) i corresponds to the ith label.</p><p>To train the attack model A LDE , the adversary first generates her training data as mentioned in Section 3. She then trains A LDE with the posterior difference δ 1 shadow · · · δ m shadow as the input and the normalized label distribution of their corresponding updating sets as the output. We assume the adversary knows the cardinality of the updating set. We try to relax this assumption later in our evaluation. Experimental Setup. We evaluate our label distribution estimation attack using updating set of cardinalities 10 and 100. For the two different cardinalities, we build attack models as mentioned in the methodology. All data samples in each updating set for the shadow and target models are sampled uniformly, thus each sample (in both training and testing set) for the attack model, which corresponds to an updating set, has the same label distribution of the original dataset. We use a batch size of 64 when updating the models.</p><p>For evaluation metrics, we calculate KL-divergence for each testing sample (corresponding to an updating set on the target model) and report the average result over all testing samples (1,000 in total). Besides, we also measure the accuracy of predicting the most frequent label over samples in the updating set. We randomly sample a dataset with the same size as the updating set and use its samples' label distribution as the baseline, namely Random.</p><p>Results. We report the result for our label distribution estimation attack in <ref type="figure" target="#fig_4">Figure 6</ref>. As shown, A LDE achieves a significantly better performance than the Random baseline on all datasets. For the updating set with 100 data samples on the CIFAR-10 dataset, our attack achieves 3 and 2.5 times better accuracy and KL-divergence, respectively, than the Random baseline. Similarly, for the MNIST and Insta-NY datasets, our attack achieves 1.5 and 4.8 times better accuracy, and 2 and 7.9 times better KL-divergence. Furthermore, A LDE achieves a similar improvement over the Random baseline for the updating set of size 10.</p><p>Recall that the adversary is assumed to know the cardinality of the updating set in order to train her attack model, we further test whether we can relax this assumption. To this end, we first update the shadow model with 100 samples while updating the target model with 10 samples. As shown in <ref type="figure" target="#fig_4">Fig- ure 6a</ref> and <ref type="figure" target="#fig_4">Figure 6c</ref> Transfer 100-10, our attack still has a similar performance as the original attack. However, when the adversary updates her shadow model with 10 data samples while the target model is updated with 100 data samples <ref type="figure" target="#fig_4">(Figure 6b</ref> and <ref type="figure" target="#fig_4">Figure 6d</ref> Transfer 10-100), our attack performance drops significantly, in particular for KL-divergence on the CIFAR-10 dataset. We believe this is due to the 10 samples not providing enough information for the attack model to generalize to a larger updating set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>USENIX Association 29th USENIX Security Symposium 1297</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-sample Reconstruction Attack</head><p>Attack Definition. Our last attack, namely multi-sample reconstruction attack, aims at reconstructing the updating set. This attack can be considered as a generalization of the singlesample reconstruction attack, and a step towards the goal of reconstructing the training set of a black-box ML model. Formally, the attack is defined as follows:</p><formula xml:id="formula_8">A MSR : δ → D update where D update = {x 1 update , . . . , x |D update |</formula><p>update } contains the samples used to update the model. Methodology. The complexity of the task for reconstructing an updating set increases significantly when the updating set size grows from one to multiple. Our single-sample reconstruction attack (Section 4.2) uses AE to reconstruct a single sample. However, AE cannot generate a set of samples. In fact, directly predicting a set of examples is a very challenging task. Therefore, we rely on generative models which are able to generate multiple samples rather than a single one.</p><p>We first introduce the classical Generative Adversarial Networks (GANs) and point out why classical GANs cannot be used for our multi-sample reconstruction attack. Next, we propose our Conditional Best of Many GAN (CBM-GAN), a novel hybrid generative model and demonstrate how to use it to execute the multi-sample reconstruction attack. Generative Adversarial Networks. Samples from a dataset are essentially samples drawn from a complex data distribution.</p><p>Thus, one way to reconstruct the dataset D update is to learn this complex data distribution and sample from it. This is the approach we adopt for our multi-sample reconstruction attack. Mainly, the adversary starts the attack by learning the data distribution of D update , then she generates multiple samples from the learned distribution, which is equivalent to reconstructing the dataset D update . In this work, we leverage the state-of-theart generative model GANs, which has been demonstrated effective on learning a complex data distribution.</p><p>A GAN consists of a pair of ML models: a generator (G) and a discriminator (D). The generator G learns to transform a Gaussian noise vector z ∼ N (0, 1) to a data samplê x,</p><formula xml:id="formula_9">G : z → ˆ x</formula><p>such that the generated samplê x is indistinguishable from a true data sample. This is enabled by the discriminator D which is jointly trained. The generator G tries to fool the discriminator, which is trained to distinguish between samples from the Generator (G) and true data samples. The objective function maximized by GAN's discriminator D is,</p><formula xml:id="formula_10">L D = E x∈D update log(D(x)) + E ˆ x log(1 − D( ˆ x))<label>(1)</label></formula><p>The GAN discriminator D is trained to output 1 ("true") for real data and 0 ("false") for fake data. On the other hand, the generator G maximizes:</p><formula xml:id="formula_11">L G = E ˆ x log(D( ˆ x))</formula><p>Thus, G is trained to produce samplesˆxsamplesˆ samplesˆx = G(z) that are classified as "true" (real) by D.</p><p>However, our attack aims to reconstruct D update for any given δ, which the standard GAN does not support. Therefore, first, we change the GAN into a conditional model to condition its generated samplesˆxsamplesˆ samplesˆx on the posterior difference δ. Second, we construct our novel hybrid generative model CBM-GAN, by adding a new "Best Match" loss to reconstruct all samples inside the updating set accurately. CBM-GAN. The decoder of our attack model is casted as our CBM-GAN's generator (G). To enable this, we concatenate the noise vector z and the latent vector µ produced by our attack model's encoder (with posterior different as input), and use it as CBM-GAN's generator's input, as in Conditional GANs <ref type="bibr" target="#b27">[30]</ref>. This allows our decoder to map the posterior difference δ to samples in D update .</p><p>However, Conditional GANs are severely prone to mode collapse, where the generator's output is restricted to a limited subset of the distribution <ref type="bibr" target="#b3">[7,</ref><ref type="bibr" target="#b48">51]</ref>. To deal with this, we introduce a reconstruction loss. This reconstruction loss forces our GAN to cover all the modes of the distribution (set) of data samples used to update the model. However, it is unclear, given a posterior difference δ and a noise vector z pair, which sample in the data distribution we should force CBM-GAN to reconstruct. Therefore, we allow our GAN full flexibility in learning a mapping from posterior difference and noise vector z pairs to data samples -this means we allow it to choose the data sample to reconstruct. We realize this using a novel "Best Match" based objective in the CBM-GAN formulation,</p><formula xml:id="formula_12">L BM = ∑ x∈D update minˆx∼Gˆx minˆ minˆx∼G minˆx∼Gˆ minˆx∼Gˆx − x 2 2 + ∑ ˆ x log(D( ˆ x))<label>(2)</label></formula><p>wherê x ∼ G represents samples produced by our CBM-GAN given a latent vector µ and noise sample z. The first part of the L BM objective is based on the standard MSE reconstruction loss and forces our CBM-GAN to reconstruct all samples in  Here, the K-means algorithm is adopted to perform clustering where we set K to |D update |. In the end, for each cluster, the adversary calculates its centroid, and takes the nearest sample to the centroid as one reconstructed sample. Experimental Setup. We evaluate the multi-sample reconstruction attack on the updating set of size 100 and generate 20,000 samples for each updating set reconstruction with CBM-GAN. For the rest of the experimental settings, we follow the one mentioned in Section 5.1 except for evaluation metrics and baseline.</p><p>We use MSE between the updating and reconstructed data samples to measure the multi-sample reconstruction attack's performance. We construct two baselines, namely Shadowclustering and Label-average. For Shadow-clustering, we perform K-means clustering on the adversary's shadow dataset. More concretely, we cluster the adversary's shadow dataset into 100 clusters and take the nearest sample to the centroid of each cluster as one reconstructed sample. For Label-average, we calculate the MSE between each sample in the updating set and the average of the images with the same label in the adversary's shadow dataset.</p><p>Results. In <ref type="figure" target="#fig_6">Figure 8</ref>, we first present some visualization of the intermediate result of our attack, i.e., the CBM-GAN's output before clustering, on the CIFAR-10 dataset. For each randomly sampled image in the updating set, we show the 5 nearest reconstructed images with respect to MSE generated by CBM-GAN. As we can see, our attack model tries to generate images with similar characteristics to the original images. For instance, the 5 reconstructed images for the airplane image in <ref type="figure" target="#fig_6">Figure 8b</ref>   blurry version of the airplane itself. The similar result can be observed from the boat image in <ref type="figure" target="#fig_6">Figure 8a</ref>, the car image in <ref type="figure" target="#fig_6">Figure 8c</ref>, and the boat image in <ref type="figure" target="#fig_6">Figure 8d</ref>. It is also interesting to see that CBM-GAN provides different samples for the two different horse images in <ref type="figure" target="#fig_6">Figure 8b</ref>. The blurriness in the results is expected, due to the complex nature of the CIFAR-10 dataset and the weak assumptions for our adversary, i.e., access to black-box ML model. We also quantitatively measure the performance of our intermediate results, by calculating the MSE between each image in the updating set and its nearest reconstructed sample. We refer to this as one-to-one match. <ref type="figure">Figure 9</ref> shows for the CIFAR-10, MNIST, and Insta-NY datasets, we achieve 0.0283, 0.043 and 0.60 MSE, respectively. It is important to note that the adversary cannot perform one-to-one match as she does not have access to ground truth samples in the updating set, i.e., one-to-one match is an oracle. <ref type="figure">Figure 9</ref> shows the mean squared error of our full attack with clustering for all datasets. To match each of our reconstructed samples to a sample in D update , we rely on the Hungarian algorithm <ref type="bibr" target="#b21">[24]</ref>. This guarantees that each reconstructed sample is only matched with one ground truth sample in D update and vice versa. As we can see, our attack outperforms both baseline models on the CIFAR-10, MNIST and Insta-NY datasets (20%, 22%, and 25% performance gain for Shadow-clustering and 60.1%, 5.5% and 14% performance gain for Label-average, respectively). The different performance gain of our attack over the label-average baseline for different datasets is due to the different complexity of these datasets. For instance, all images inside MNIST have black background and lower variance within each class compared to the CIFAR-10 dataset. The different complexity results in some datasets having a more representative label-average, which leads to a lower performance gain of our attack over them.</p><p>These results show that our multi-sample reconstruction attack provides a more useful output than calculating the average from the adversary's dataset. In detail, our attack achieves an MSE of 0.036 on the CIFAR-10 dataset, 0.051 on the MNIST dataset, and 0.64 on the Insta-NY dataset. As expected, the MSE of our final attack is higher than one-to-one match, i.e., the above mentioned intermediate results.</p><p>We further visualize our full attack's result on the MNIST dataset. <ref type="figure" target="#fig_9">Figure 10</ref> shows a sample of a full MNIST updating set reconstruction, i.e., the CBM-GAN's reconstructed images for the 100 original images in an updating set. We observe that our attack model reconstructs diverse digits of each class that for most of the cases match the actual ground truth data very well. This suggests CBM-GAN is able to capture most modes in a data distribution well. Moreover, comparing the results of this attack ( <ref type="figure" target="#fig_9">Figure 10</ref>) with the results of the single-sample reconstruction attack ( <ref type="figure" target="#fig_3">Figure 5</ref>), we can see that this attack produces sharper images. This result is due to the discriminator of our CBM-GAN, as it is responsible for making the CBM-GAN's output to look real, i.e., sharper in this case.</p><p>One limitation of our attack is that CBM-GAN's sample generation and clustering are performed separately. In the future, we plan to combine them to perform an end-to-end training which may further boost our attack's performance.</p><p>From all these results, we show that our attack does not generate a general representation of data samples affiliated with the same label, but tries to reconstruct images with similar characteristics as the images inside the updating set (as shown by the different shapes of the same numbers in <ref type="figure" target="#fig_9">Figure 10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relaxing The Knowledge of Updating Set Cardinality.</head><p>One of the above attack's main assumptions is the adversary's knowledge of the updating set cardinality, i.e., |D update |. Next, we show how to relax this assumption. To recap, the adversary needs the updating set cardinality when updating her shadow model and clustering CBM-GAN's output. We address the former by using updating sets of different cardinalities. For the latter, we use the silhouette score to find the optimal k for K-means, i.e., the most likely value of the target updating set's cardinality. The silhouette score lies in the range between -1 and 1, it reflects the consistency of the clustering. Higher silhouette score leads to more suitable k.</p><p>Specifically, the adversary follows the previously presented methodology in Section 5.2 with the following modifications. First, instead of using updating sets with the same cardinality, the adversary uses updating sets with different cardinalities to update the shadow model. Second, after the adversary generates multiple samples from CBM-GAN, she uses the silhouette score to find the optimal k. The silhouette score is used here to identify the target model's updating set cardinality from the different updating sets cardinalities used to update  <ref type="table">Table 1</ref>: Evaluation of the data transferability attacks. The first column shows all different attacks, the second and third shows the performance of the attacks using similar and different distributions, respectively. Where A LI performance is measured in accuracy, A MSR and A SSR measured in MSE, and A LDE (10) and A LDE (100) measured in accuracy (KL-divergence).</p><p>the shadow model. We evaluate the effectiveness of this attack on all datasets. We use a target model updated with 100 samples and create our shadow updated models using updating sets with cardinality 10 and 100. Concretely, we update the shadow model half of the time with updating sets of cardinality 10 and the other half with cardinality 100.</p><p>Our evaluation shows that our attack consistently produces higher silhouette score -by at least 20%-for the correct cardinality in all cases. In another way, our method can always detect the right cardinality of the updating set in this setting. Moreover, the MSE for the final output of the attack only drops by 1.6%, 0.8%, and 5.6% for the Insta-NY, MNIST, and CIFAR-10 datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this section, we analyze the effect of different hyperparameters of both the target and shadow models on our attacks' performance. Furthermore, we investigate relaxing the threat model assumptions and discuss the limitations of our attacks. Relaxing The Attacker Model Assumption. Our threat model has two main assumptions: Same data distribution for both target and shadow datasets and same structure for both target and shadow models. We relax the former by proposing data transferability attack and latter by model transferability attack.</p><p>Data Transferability. In this setting, we locally train and update the shadow model with a dataset which comes from a different distribution from the target dataset. For our experiments, we use Insta-NY as the target dataset and Insta-LA as the shadow dataset. <ref type="table">Table 1</ref> depicts the evaluation results. As expected, the performance of our data transferability attacks drops; however, they are still significantly better than corresponding baseline models. For instance, the performance of the multi-sample reconstruction attack drops by 14% but is still 10% better than the baseline (see <ref type="figure">Figure 9)</ref>. Moreover, the multi-sample label distribution attack's accuracy (KL-divergence) only drops by 6.8% (18.9%) and 0% (63%), which is still significantly better than the baseline (see <ref type="figure" target="#fig_4">Figure 6</ref>) by 6.5x (2x) and 4.6x (4.8x) for updating set sizes of 10 and 100, respectively.</p><p>Model Transferablity. Now we relax the attacker's knowledge on the target model's architecture, i.e., we use different architectures for shadow and target models. In our experiments on Insta-NY, we use the same architecture mentioned previously in Section 4.1 for the target model, and remove one hidden layer and use half of the number of neurons in other hidden layers for the shadow model.</p><p>The performance drop of our model transferability attack is only less than 2% for all of our attacks, which shows that our attacks are robust against such changes in the model architectures. We observe similar results when repeating the experiment using different architectures and omit them for space restrictions.</p><p>Effect of The Probing Set Cardinality. We evaluate the performance of our attacks on CIFAR-10 when the probing set cardinality is 10, 100, 1,000, or 10,000. As our encoder's input size relies on the probing set cardinality (see Section 3), we adjust its input layer size accordingly.</p><p>As expected, using a probing set of size 10 reduces the performance of the attacks. For instance, the single-sample label inference and reconstruction attacks' performance drops by 9% and 71%, respectively. However, increasing the probing set cardinality from 100 to 1,000 or 10,000 has a limited effect (up to 3.5% performance gain). It is also important to mention that the computational requirement for our attacks increases with an increasing probing set cardinality, as the cardinality decides the size of the input layer for our attack models. In conclusion, using 100 samples for probing the target model is a suitable choice.</p><p>Effect of Target Model Hyperparameters. We now evaluate our attacks' performance with respect to two hyperparameters of the target model. Target Model's Training Epochs Before Updating. We use the MNIST dataset to evaluate the multi-sample label distribution estimation attack's performance on target models trained for 10, 20, and 50 epochs. For each setting, we update the model and execute our attack as mentioned in Section 5.1.</p><p>The experiments show that the difference in the attack's performance for the different models is less than 2%. That is expected as gradients are not monotonically decreasing during the training procedure. In other words, information is not necessarily vanishing <ref type="bibr" target="#b11">[15]</ref>.</p><p>Target Model's Updating Epochs. We train target and shadow models as introduced in Section 5.1 with the Insta-NY dataset, but we update the models using different number of epochs. More concretely, we update the models using from 2 to 10 epochs and evaluate the multi-sample label distribution estimation attack's performance on the updated models. We report the results of our experiments in <ref type="figure" target="#fig_11">Figure 11</ref>. As expected, the multi-sample label distribution estimation attack's performance improves with the increase of the number of epochs used to update the model. For instance, the attack performance improves by 25.4 % when increasing the number of epochs used to update the model from 2 to 10.</p><p>Limitations of Our Attacks. For all of our attacks, we assume a simplified setting, in which, the target model is solely updated on new data. Moreover, we perform our attacks on updating sets of maximum cardinality of 100. In future work, we plan to further investigate a more complex setting, where the target model is updated using larger updating sets of both new and old data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Possible Defenses</head><p>Adding Noise to Posteriors. All our attacks leverage posterior difference as the input. Therefore, to reduce our attacks' performance, one could sanitize posterior difference. However, the model owner cannot directly manipulate the posterior difference, as she does not know with what or when the adversary probes her model. Therefore, she has to add noise to the posterior for each queried sample independently. We have tried adding noise sampled from a uniform distribution to the posteriors. Experimental results show that the performance for some of our attacks indeed drops to a certain degree. For instance, the single-sample label inference attack on the CIFAR-10 dataset drops by 17% in accuracy. However, the performance of our multi-sample reconstruction attack stays stable. One reason might be the noise vector z is part of CBM-GAN's input which makes the attack model more robust to the noisy input.</p><p>Differential Privacy. Another possible defense mechanism against our attacks is differentially private learning. Differential privacy <ref type="bibr" target="#b6">[10]</ref> can help an ML model learn its main tasks while reducing its memory on the training data. If differentially private learning schemes <ref type="bibr" target="#b0">[4,</ref><ref type="bibr" target="#b5">9,</ref><ref type="bibr" target="#b36">39]</ref> are used when updating the target ML model, this by design will reduce the performance of our attacks. However, it is also important to mention that depending on the privacy budget for differential privacy, the utility of the model can drop significantly.</p><p>We leave an in-depth exploration of effective defense mechanisms against our attacks as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Works</head><p>Membership Inference. Membership inference aims at determining whether a data sample is inside a dataset. It has been successfully performed in various settings, such as biomedical data <ref type="bibr" target="#b14">[18,</ref><ref type="bibr" target="#b17">21]</ref> and location data <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b34">37]</ref>. Shokri et al. <ref type="bibr" target="#b37">[40]</ref> propose the first membership inference attack against machine learning models. In this attack, an adversary's goal is to determine whether a data sample is in the training set of a blackbox ML model. To mount this attack, the adversary relies on a binary machine learning classifier which is trained with the data derived from shadow models (similar to our attacks). More recently, multiple membership inference attacks have been proposed with new attacking techniques or targeting on different types of ML models <ref type="bibr" target="#b15">[19,</ref><ref type="bibr" target="#b24">27,</ref><ref type="bibr" target="#b25">28,</ref><ref type="bibr" target="#b28">31,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b39">42,</ref><ref type="bibr" target="#b50">53]</ref>.</p><p>In theory, membership inference attack can be used to reconstruct the dataset, similar to our reconstruction attacks. However, it is not scalable in the real-world setting as the adversary needs to obtain a large-scale dataset which includes all samples in the target model's training set. Though our two reconstruction attacks are designed specifically for the online learning setting, we believe the underlying techniques we propose, i.e., pretrained decoder from a standard autoencoder and CBM-GAN, can be further extended to reconstruct datasets from black-box ML models in other settings.</p><p>Model Inversion. <ref type="bibr">Fredrikson et al. [12]</ref> propose model inversion attack first on biomedical data. The goal of model inversion is to infer some missing attributes of an input feature vector based on the interaction with a trained ML model. Later, other works generalize the model inversion attack to other settings, e.g." reconstructing recognizable human faces <ref type="bibr" target="#b7">[11,</ref><ref type="bibr" target="#b16">20]</ref>. As pointed out by other works <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b37">40]</ref>, model inversion attack reconstructs a general representation of data samples affiliated with certain labels, while our reconstruction attacks target on specific data samples used in the updating set. The former has been demonstrated to be effective on simple ML models, such as logistic regression, while the latter is designed specifically for decision trees, a class of machine learning classifiers. Moreover, relying on an active learning based retraining strategy, the authors show that it is possible to steal an ML model even if the model only provides the label instead of posteriors as the output. More recently, Orekondy et al. <ref type="bibr" target="#b31">[34]</ref> propose a more advanced attack on stealing the target model's functionality and show that their attack is able to replicate a mature commercial machine learning API. In addition to model parameters, several works concentrate on stealing ML models' hyperparameters <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b44">47]</ref>.</p><p>Besides the above, there exist a wide range of other attacks and defenses on machine learning models <ref type="bibr">[4, 5, 8, 9, 13, 14, 16, 17, 22, 23, 25, 26, 35, 41, 43, 44, 46, 48-50, 52, 54-56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>Large-scale data being generated at every second turns ML model training into a continuous process. In consequence, a machine learning model queried with the same set of data samples at two different time points will provide different results. In this paper, we investigate whether these different model outputs can constitute a new attack surface for an adversary to infer information of the dataset used to perform model update. We propose four different attacks in this surface all of which follow a general encoder-decoder structure. The encoder encodes the difference in the target model's output before and after being updated, and the decoder generates different types of information regarding the updating set.</p><p>We start by exploring a simplified case when an ML model is only updated with one single data sample. We propose two different attacks for this setting. The first attack shows that the label of the single updating sample can be effectively inferred. The second attack utilizes an autoencoder's decoder as the attack model's pretrained decoder for single-sample reconstruction.</p><p>We then generalize our attacks to the case when the updating set contains multiple samples. Our multi-sample label distribution estimation attack trained following a KL-divergence loss is able to infer the label distribution of the updating set's data samples effectively. For the multi-sample reconstruction attack, we propose a novel hybrid generative model, namely CBM-GAN, which uses a "Best Match" loss in its objective function. The "Best Match" loss directs CBM-GAN's generator to reconstruct each sample in the updating set. Quantitative and qualitative results show that our attacks achieve promising performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10 model:</head><p>Sample → conv2d <ref type="bibr" target="#b1">(5,</ref><ref type="bibr" target="#b2">6)</ref> max <ref type="formula" target="#formula_12">(2)</ref> conv2d <ref type="bibr" target="#b1">(5,</ref><ref type="bibr" target="#b12">16)</ref> max <ref type="formula" target="#formula_12">(2)</ref> FullyConnected <ref type="formula" target="#formula_10">(120)</ref> FullyConnected <ref type="formula">(84)</ref> FullyConnected <ref type="formula" target="#formula_10">(10)</ref> Softmax → Insta-NY Model:</p><p>Sample → FullyConnected <ref type="formula" target="#formula_12">(32)</ref> FullyConnected <ref type="formula" target="#formula_10">(16)</ref> FullyConnected <ref type="formula">(9)</ref> Softmax → Here, max(2) denotes a max-pooling layer with a 2×2 kernel, FullyConnected(x) denotes a fully connected layer with x hidden units, Conv2d(k',s') denotes a 2-dimension convolution layer with kernel size k × k and s filters, and Softmax denotes the Softmax function. We adopt ReLU as the activation function for all layers for the MNIST, CIFAR-10 and Location models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Encoder Architecture</head><p>Encoder architecture:</p><formula xml:id="formula_13">δ → FullyConnected(128) FullyConnected(64) → µ</formula><p>Here, µ denotes the latent vector which serves as the input for our decoder. Furthermore, we use LeakyReLU as our encoder's activation function and apply dropout on both layers for regularization. Sample → FullyConnected <ref type="formula">(64)</ref> FullyConnected <ref type="formula" target="#formula_12">(32)</ref> FullyConnected <ref type="formula" target="#formula_10">(16)</ref> FullyConnected(16) → µ AE</p><p>Here, µ AE is the latent vector output of the encoder. Moreover, k i , s i , and f i represent the kernel size, number of filters, and number of units in the ith layer. The concrete values of these hyperparameters depend on the target dataset, we present our used values in <ref type="table">Table 2</ref>. We adopt ReLU as the activation function for all layers for the MNIST and CIFAR-10 encoders. For the Insta-NY decoder, we use ELU as the activation function for all layers except for the last one. Finally, we apply dropout after the first fully connected layer for MNIST and CIFAR-10. For Insta-NY, we apply dropout and batch normalization for the first three fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 AE's Decoder Architecture</head><p>Autoencoder's decoder architecture for MNIST and CIFAR-10:</p><formula xml:id="formula_14">µ AE → FullyConnected( f 1 ) FullyConnected( f 2 ) ConvTranspose2d(k 1 , s 1 ) ConvTranspose2d(k 2 , s 2 ) ConvTranspose2d(k 3 , s 3 ) → Sample</formula><p>Autoencoder's decoder architecture for Insta-NY:</p><p>µ AE → FullyConnected <ref type="formula" target="#formula_10">(16)</ref> FullyConnected <ref type="formula" target="#formula_12">(32)</ref> FullyConnected <ref type="formula">(64)</ref> FullyConnected(168) → Sample</p><p>Here, ConvTranspose2d(k',s') denotes a 2-dimension transposed convolution layer with kernel size k × k and s i specifies the number of units in the ith fully connected layer. The concrete values of these hyperparameters are presented in <ref type="table">Table 2</ref>. For MNIST and CIFAR-10 decoders, we again use ReLU as the activation function for all layers except for the last one where we adopt tanh. For the Insta-NY decoder, we adopt ELU for all layers except for the last one. We also apply dropout after the last fully connected layer for regularization for MNIST and CIFAR-10, and dropout and batch normalization on the first three fully connected layers for Insta-NY.  <ref type="formula" target="#formula_10">(512)</ref> FullyConnected <ref type="formula" target="#formula_12">(256)</ref> FullyConnected <ref type="formula" target="#formula_10">(128)</ref> FullyConnected <ref type="formula" target="#formula_10">(1)</ref> Sigmoid →{1,0}</p><p>Here, for both generators and discriminators, Sigmoid is the Sigmoid function, batch normalization is applied on the output of each layer except the last layer, and LeakyReLU is used as the activation function for all layers except the last one, which uses tanh.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: [Higher is better] Performance of the single-sample label inference attack (A LI ) on MNIST, CIFAR-10, and Insta-NY datasets together with the baseline model. Accuracy is adopted as the evaluation metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Methodology of the single-sample reconstruction attack (A SSR ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 4: [Lower is better] Performance of the single-sample reconstruction attack (A SSR ) together with autoencoder and two baseline models. Mean squared error is adopted as the evaluation metric. Autoencoder (AE) serves as an oracle as the adversary cannot use it for her attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualization of some generated samples from the single-sample reconstruction attack (A SSR ) on the MNIST dataset. Samples are fair random draws, not cherry-picked. The first row shows the original samples. The second row shows the reconstructed samples by A SSR . The third shows row the reconstructed samples by autoencoder, i.e., the upper bound of our reconstruction attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: [Lower is better for (a) and (b), higher is better for (c) and (d)] Performance of the multi-sample label distribution estimation attack (A LDE ) together with the baseline model and transfer attack. KL-divergence and accuracy are adopted as the evaluation metric. Accuracy here is used to measure the prediction of the most frequent label over samples in the updating set. Transfer 10-100 means each of the training sample for the attack model corresponds to an updating set containing 10 data samples and each of the testing sample for the attack model corresponds to an updating set containing 100 data samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Methodology of the multi-sample reconstruction attack (A MSR ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of some generated samples from the multi-sample reconstruction attack (A MSR ) before clustering on the CIFAR-10 dataset. Samples are fair random draws, not cherry-picked. The left column shows the original samples and the next 5 columns show the 5 nearest reconstructed samples with respect to mean squared error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 presents</head><label>7</label><figDesc>a schematic view of our multi-sample reconstruction attack's methodology. The concrete architec- ture of CBM-GAN's generator and discriminator for the three datasets used in this paper are listed in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 9: [Lower is better] Performance of the multi-sample reconstruction attack (A MSR ) together with one-to-one match and the two baseline models. Mean squared error (MSE) is adopted as the evaluation metric. The match between the original and reconstructed samples is performed by the Hungarian algorithm for both A MSR and Shadow-clustering. For Label-average, each sample is matched within the average of samples with the same class in the shadow dataset. One-to-one match serves as an oracle as the adversary cannot use it for her attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Visualization of a full MNIST updating set together with the output of the multi-sample reconstruction attack (A MSR ) after clustering. Samples are fair random draws, not cherry-picked. The left column shows the original samples and the right column shows the reconstructed samples. The match between the original and reconstructed samples is performed by the Hungarian algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: [Lower is better] The performance of the multisample label distribution estimation attack (A LDE ) with different number of epochs used to update the target model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Model Stealing.</head><label></label><figDesc>Another related line of work is model steal- ing. Tramèr et al. [45] are among the first to introduce the model stealing attack against black-box ML models. In this attack, an adversary tries to learn the target ML model's pa- rameters. Tramèr et al. propose various attacking techniques including equation-solving and decision tree path-finding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>C</head><label></label><figDesc>Single-sample Label Inference Attack's De- coder Architecture A LI 's decoder architecture: µ → FullyConnected(n) Softmax → Here, n is equal to the size of , i.e., n = ||. D Single-sample Reconstruction Attack D.1 AE's Encoder Architecture AE's encoder architecture for MNIST and CIFAR-10: Sample → conv2d(k 1 , s 1 ) max(2) conv2d(k 2 , s 2 ) max(2) FullyConnected( f 1 ) FullyConnected( f 2 ) → µ AE AE's encoder architecture for Insta-NY:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>ESigmoid</head><label></label><figDesc>Multi-sample Reconstruction Attack's De- coder Architecture CBM-GAN's generator architecture for MNIST: µ, z → FullyConnected(2048) FullyConnected(2048) FullyConnected(2048) FullyConnected(784) → Sample CBM-GAN's discriminator architecture for MNIST:→{1,0} CBM-GAN's generator architecture for Insta-NY: µ, z → FullyConnected(512) FullyConnected(512) FullyConnected(256) FullyConnected(168) → Sample CBM-GAN's discriminator architecture for Insta-NY: µ, z → FullyConnected</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers, and our shepherd, David Evans, for their helpful feedback and guidance.</p><p>The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ ERC grant agreement no. 610150-imPACT.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>A Target Models Architecture</p><p>Sample → conv2d <ref type="bibr" target="#b1">(5,</ref><ref type="bibr" target="#b6">10)</ref> max <ref type="formula">(2)</ref> conv2d <ref type="bibr" target="#b1">(5,</ref><ref type="bibr" target="#b16">20)</ref> max <ref type="formula">(2)</ref> FullyConnected <ref type="formula">(50)</ref> FullyConnected <ref type="formula">(10)</ref> Softmax → </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning with Differential Privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Machine Learning (ICML)</title>
		<meeting>the 2018 International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inferring Social Links from Mobility Profiles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2-19 International Conference on Learning Representations (ICLR</title>
		<meeting>the 2-19 International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page" from="2" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Evaluating the Robustness of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 2017 IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacypreserving Logistic Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Monteleoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 2009 Annual Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Security Symposium (USENIX Security)</title>
		<meeting>the 2014 USENIX Security Symposium (USENIX Security)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Property Inference Attacks on Fully Connected Neural Networks using Permutation Invariant Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Ganju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Borisov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Distributed Linear Regression on High-Dimensional Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrià</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Schoppmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana</forename><surname>Raykova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Doerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samee</forename><surname>Zahur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Privacy Enhancing Technologies Symposium</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explaining and Harnessing Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2015 International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LEMNA: Explaining Deep Learning based Security Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Purui</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang Abd Xinyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="364" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MBeacon: Privacy-Preserving Beacons for DNA Methylation Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inken</forename><surname>Hagestedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Berrang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS). Internet Society</title>
		<meeting>the 2019 Network and Distributed System Security Symposium (NDSS). Internet Society</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating Privacy Leakage of Generative Models Using Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Danezis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano De</forename><surname>Cristofaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Privacy Enhancing Technologies Symposium</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Briland</forename><surname>Hitaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Ateniese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Perezcruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Homer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szabolcs</forename><surname>Szelinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margot</forename><surname>Redman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duggan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waibhav</forename><surname>Tembe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Muehling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">V</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dietrich</forename><forename type="middle">A</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><forename type="middle">F</forename><surname>Nelson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Resolving Individuals Contributing Trace Amounts of DNA to Highly Complex Mixtures Using High-Density SNP Genotyping Microarrays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Craig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS Genetics</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Nita-Rotaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 2018 IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">Zhenqiang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="259" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Hungarian Method for the Assignment Problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable Optimization of Randomized Operational Decisions in Adversarial Classification Settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeniy</forename><surname>Vorobeychik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<meeting>the 2015 International Conference on Artificial Intelligence and Statistics (AISTATS)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="599" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanqing</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Annual Computer Security Applications Conference</title>
		<meeting>the 2019 Annual Computer Security Applications Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards Measuring Membership Privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bindschaedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<idno>CoRR abs/1712.09136</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding Membership Inferences on Well-Generalized Learning Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Bindschaedler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diyue</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR abs/1802.04889</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploiting Unintended Feature Leakage in Collaborative Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 2019 IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Conditional Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno>CoRR abs/1411.1784</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Machine Learning with Membership Privacy using Adversarial Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 2019 IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards Reverse-Engineering Black-Box Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Augustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2018 International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knockoff Nets: Stealing Functionality of Black-Box Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tribhuvanesh</forename><surname>Orekondy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Practical Black-Box Attacks Against Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Berkay Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security (ASIACCS)</title>
		<meeting>the 2017 ACM Asia Conference on Computer and Communications Security (ASIACCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Knock Knock, Who&apos;s There? Membership Inference on Aggregate Location Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><surname>Pyrgelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmela</forename><surname>Troncoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano De</forename><surname>Cristofaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Network and Distributed System Security Symposium (NDSS). Internet Society</title>
		<meeting>the 2018 Network and Distributed System Security Symposium (NDSS). Internet Society</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Under the Hood of Membership Inference Attacks on Aggregate Location Time-Series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apostolos</forename><surname>Pyrgelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmela</forename><surname>Troncoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emiliano De</forename><surname>Cristofaro</surname></persName>
		</author>
		<idno>CoRR abs/1902.07456</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Salem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Berrang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Network and Distributed System Security Symposium (NDSS). Internet Society</title>
		<meeting>the 2019 Network and Distributed System Security Symposium (NDSS). Internet Society</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2015 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Membership Inference Attacks Against Machine Learning Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 2017 IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Machine Learning Models that Remember Too Much</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="587" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The Natural Auditor: How To Tell If Someone Used Your Words To Train Their Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno>CoRR abs/1811.00513</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Intriguing Properties of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>CoRR abs/1312.6199</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ensemble Adversarial Training: Attacks and Defenses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>2017. 13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 International Conference on Learning Representations (ICLR</title>
		<meeting>the 2017 International Conference on Learning Representations (ICLR</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stealing Machine Learning Models via Prediction APIs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tramér</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Juels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ristenpart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 USENIX Security Symposium (USENIX Security)</title>
		<meeting>the 2016 USENIX Security Symposium (USENIX Security)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="601" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimal Randomized Classification in Adversarial Settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgeniy</forename><surname>Vorobeychik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems (AAMAS)</title>
		<meeting>the 2014 International Conference on Autonomous Agents and Multi-agent Systems (AAMAS)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stealing Hyperparameters in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">Zhenqiang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Symposium on Security and Privacy (S&amp;P)</title>
		<meeting>the 2018 IEEE Symposium on Security and Privacy (S&amp;P)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">With Great Training Comes Great Vulnerability: Practical Attacks against Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bimal</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 USENIX Security Symposium (USENIX Security)</title>
		<meeting>the 2018 USENIX Security Symposium (USENIX Security)</meeting>
		<imprint>
			<publisher>USENIX</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1281" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Network and Distributed System Security Symposium (NDSS). Internet Society</title>
		<meeting>the 2018 Network and Distributed System Security Symposium (NDSS). Internet Society</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Disparate Vulnerability: on the Unfairness of Privacy Attacks Against Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Yaghini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Kulynych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carmela</forename><surname>Troncoso</surname></persName>
		</author>
		<idno>CoRR abs/1906.00389</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diversity-Sensitive Conditional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2019 International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automated Crowdturfing Attacks and Defenses in Online Review Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bimal</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenna</forename><surname>Cryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1143" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Yeom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Giacomelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 IEEE Computer Security Foundations Symposium (CSF)</title>
		<meeting>the 2018 IEEE Computer Security Foundations Symposium (CSF)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cost-Sensitive Robustness against Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Learning Representations (ICLR)</title>
		<meeting>the 2019 International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Tagvisor: A Privacy Advisor for Sharing Hashtags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahleen</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Te</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Web Conference (WWW)</title>
		<meeting>the 2018 Web Conference (WWW)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards Plausible Graph Anonymization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Humbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartlomiej</forename><surname>Surma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Manoharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilles</forename><surname>Vreeken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Backes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Network and Distributed System Security Symposium (NDSS). Internet Society</title>
		<meeting>the 2020 Network and Distributed System Security Symposium (NDSS). Internet Society</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
