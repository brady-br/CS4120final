<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Say Goodbye to Off-heap Caches! On-heap Caches Using Memory-Mapped I/O</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacovos</forename><forename type="middle">G</forename><surname>Kolokasis</surname></persName>
							<email>kolokasis@ics.forth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science (ICS)</orgName>
								<orgName type="laboratory">Foundation for Research and Technology -Hellas (FORTH)</orgName>
								<address>
									<region>Inc</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Papagiannis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science (ICS)</orgName>
								<orgName type="laboratory">Foundation for Research and Technology -Hellas (FORTH)</orgName>
								<address>
									<region>Inc</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polyvios</forename><surname>Pratikakis</surname></persName>
							<email>polyvios@ics.forth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science (ICS)</orgName>
								<orgName type="laboratory">Foundation for Research and Technology -Hellas (FORTH)</orgName>
								<address>
									<region>Inc</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Bilas</surname></persName>
							<email>bilas@ics.forth.gr</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science (ICS)</orgName>
								<orgName type="laboratory">Foundation for Research and Technology -Hellas (FORTH)</orgName>
								<address>
									<region>Inc</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Foivos</forename><surname>Zakkak</surname></persName>
							<email>fzakkak@redhat.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Computer Science (ICS)</orgName>
								<orgName type="laboratory">Foundation for Research and Technology -Hellas (FORTH)</orgName>
								<address>
									<region>Inc</region>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Say Goodbye to Off-heap Caches! On-heap Caches Using Memory-Mapped I/O</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Many analytics computations are dominated by iterative processing stages, executed until a convergence condition is met. To accelerate such workloads while keeping up with the exponential growth of data and the slow scaling of DRAM capacity, Spark employs off-memory caching of intermediate results. However, off-heap caching requires the serialization and de-serialization (serdes) of data, which add significant overhead especially with growing datasets. This paper proposes TeraCache, an extension of the Spark data cache that avoids the need of serdes by keeping all cached data on-heap but off-memory, using memory-mapped I/O (mmio). To achieve this, TeraCache extends the original JVM heap with a managed heap that resides on a memory-mapped fast storage device and is exclusively used for cached data. Preliminary results show that the TeraCache prototype can speed up Machine Learning (ML) workloads that cache intermediate results by up to 37% compared to the state-of-the-art serdes approach.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Analytics applications often use ML <ref type="bibr" target="#b13">[16]</ref> algorithms to process massive amounts of data. Such applications iterate over one or more computation steps until a convergence condition is met. To speed up such workloads, Spark <ref type="bibr" target="#b33">[35]</ref> caches large intermediate results of complex computation pipelines and reuses them in each step. Intermediate results are stored as Resilient Distributed Datasets (RDDs) <ref type="bibr" target="#b31">[34]</ref> in an LRU cache. <ref type="figure" target="#fig_0">Figure 1</ref>(a) shows the performance impact of RDD caching for three well-known ML workloads: Linear Regression (LR), Logistic Regression (LgR), and Support-Vector Machines (SVM). All workloads run with a 64GB dataset and a single Spark executor using 32GB DRAM and 30 CPU cores (we report the average of three runs; deviation was minimal and is omitted). Caching RDDs in both memory and disk (hybrid) * Also with the Department of Computer Science, University of Crete. † Work performed while employed by The University of Manchester. improves performance up to 90%, compared to recomputing intermediate results on demand at every stage.</p><p>DRAM-only caching is not a long-term solution, as the amount of data generated and processed increases at a high rate <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b24">27]</ref>, while DRAM scaling reaches its limits <ref type="bibr" target="#b10">[13,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b17">20]</ref>. Cached data increasingly exceed physical DRAM size, making workloads prone to recomputing intermediate results at each step. Therefore, current practice is to use fast storage devices to increase Spark's effective cache size for intermediate RDDs <ref type="bibr" target="#b35">[37]</ref>. NAND flash storage devices such as SSD and NVMe block devices <ref type="bibr" target="#b0">[1]</ref>, as well as NVMs, have higher density and capacity than DRAM <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b22">25]</ref>. SSD and NVMe devices scale to terabytes per PCIe slot <ref type="bibr" target="#b16">[19]</ref> at a lower cost <ref type="bibr" target="#b5">[8]</ref>, while DRAM scales to GBs per DIMM. Moreover, NVMe block devices provide higher capacity at a lower cost compared to NVM, while still having access latency in the order of few µs and perform hundreds of thousands IOPS for both reads and writes.</p><p>During execution, Spark initially places RDDs in the onheap (DRAM) cache. If an RDD block does not fit in memory, it is serialized to the off-heap (disk) cache and its memory is collected during the next garbage collection (GC) cycle. Similarly, if there is not enough memory, the LRU cache evicts older entries to the off-heap cache. When Spark refers to an RDD that is stored off-heap, it deserializes the serialized block from disk back into memory. Every block is serialized at most once, since RDDs are immutable. However, deserialization can occur multiple times per block in each iterative stage.</p><p>Today, despite outperforming the recomputation strategy, caching in off-heap device caches incurs high serdes overhead. According to <ref type="bibr">Zhang et al. [36]</ref>, serdes rather than disk I/O dominates overhead. <ref type="figure" target="#fig_0">Figure 1(b)</ref> shows the performance impact of off-heap RDD caches on LR, LgR, and SVM, when storing RDDs only off-heap (disk). Note the large impact of serdes overhead with off-heap caching. Serdes accounts for 27% on average of total execution time using only disk storage. Deserialization accounts for 80%-90% of the total serdes overhead because the workload retrieves immutable cached RDDs from the device in every iteration. Serdes overhead becomes worse as storage device speed improves and the gap to CPU and memory performance narrows down. The total execution time also increases in the case of disk-only caching, mainly due to reduced parallelism and idle CPU time, as disk throughput cannot keep 30 CPU cores at full load.</p><p>Using an on-heap cache (DRAM) together with an off-heap cache (disk) reduces serdes cost, but it also incurs significant GC overhead. <ref type="figure" target="#fig_0">Figure 1(b)</ref> shows the performance impact of storing RDDs in a hybrid cache, both on-heap (DRAM) and off-heap (disk), as is currently common practice. Using a relatively large on-heap cache (Spark reserves 60% of the heap as cache), serdes overhead decreases considerably, by 20% on average, by keeping some RDDs in memory compared to storing them exclusively on disk. However, such a large on-heap cache increases GC time between 13x (SVM) and 36x (LgR), compared to disk-only caching. Cached RDDs are initially placed in the heap, resulting in a higher ratio of long-lived objects to short-live objects. Hence, GC consumes more time marking live objects in the JVM heap <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b29">32]</ref> and ends up reclaiming a smaller percentage of the heap, since a big portion is occupied by cached RDDs. In essence, Spark uses the DRAM-only JVM heap both for execution and cache memory. This can lead to unpredictable performance or even failures, because caching large data causes extra GC pressure during execution time.</p><p>In this work we argue that RDD caching should be performed only in a large, managed, on-heap cache, in a part of the heap that is memory-mapped onto a fast storage device and is not garbage collected. TeraCache divides the JVM heap into an execution and a cache part, locating the execution heap solely in DRAM and the cache part in a DRAM-mapped block device. This inherently eliminates serdes and all associated overheads, prevents GC on cached data, and is inline with device technology trends and server power limitations.</p><p>Next, we observe a trade-off on how DRAM should be divided between execution and cache memory in Spark. Clearly, the execution part of the heap should use enough DRAM to not cause GC pressure during task execution. Conversely, the more DRAM used the cache heap, the faster the access to the cached data. We propose a design where the JVM monitors memory pressure for execution and caching, and dynamically adjusts the use of DRAM between the two parts.</p><p>Our approach, TeraCache, is a co-design approach for onheap RDD caching over memory-mapped fast storage devices. TeraCache spans the Spark cache, JVM memory management and GC, and mmio. The main benefits of TeraCache are:</p><p>• It removes the need for serdes by caching only in a large memory-mapped heap, thus, allowing the JVM to access cached data directly using load/store operations.</p><p>• It reduces the frequency and length of GC cycles, despite keeping cached RDDs in the heap, by reclaiming RDD cached objects separately from the rest of the JVM heap.</p><p>• It dynamically adjusts the DRAM used for mmio and execution heap, to balance execution speed and I/O overhead.</p><p>We implement an early prototype of TeraCache that targets caching in Spark and investigate the dynamic resizing of DRAM between the two parts of the heap, evaluating its effectiveness on iterative ML workloads. Our evaluation shows performance improvement by up to 37% compared to the current state-of-the-art hybrid approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TeraCache: Caching Over a Device Heap</head><p>Spark consists of one driver and multiple executor processes. The driver is the main process run by Spark users, which generates all tasks, while the executors are responsible for executing tasks of the driver. <ref type="figure" target="#fig_1">Figure 2</ref>(a) shows how Spark divides executor memory (top layer) into two logical parts: (1) execution memory, for storing temporary data during computation and (2) storage memory, for caching intermediate RDDs in an LRU cache. Each executor runs in a JVM instance and allocates memory from the JVM heap, which resides in DRAM. When an RDD does not fit in storage memory it gets serialized <ref type="bibr">[2]</ref> and moved to disk.</p><p>Our work takes advantage of this dual use of executor memory for computation and caching. We physically partition the JVM heap to serve these two roles. Then, we map each part of the JVM Heap to specific resources in the memory hierarchy, as follows <ref type="figure" target="#fig_1">(Figure 2</ref>(b)): (1) a JVM Heap (H1) allocated exclusively on DRAM (DR1) and which can be divided into generations <ref type="bibr" target="#b11">[14]</ref>, e.g., New and Old; and (2) a custom managed heap (TeraCache Heap) that contains all cached RDDs and its size is limited by device capacity (S C ).</p><p>The memory mappings for pages used by mmap reside in the remaining part of DRAM (DR2). S DR1 and S DR2 are dynamically adjusted by TeraCache using an adaptive policy at run-time. To do this, TeraCache requires a number of extensions in the Spark Block Manager and the JVM garbage collector, as described below. TeraCache, on-heap RDD cache over a memory-mapped fast storage device S DR1 + S DR2 = S DRAM (S DRAM =DRAM size, S DR1 =DR1 size, S DR2 =DR2 size, S C =Capacity of fast storage device). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design Challenges</head><p>TeraCache heap allocation using mmap on NVMe is practical: We modify the JVM to allocate an additional heap, memory-mapped onto a fast storage device, e.g., NAND-Flash SSD or NVMe, using Linux mmap <ref type="bibr" target="#b7">[10]</ref>. Fast SSDs and NVMe devices, as opposed to HDDs, are amenable to mmio, due to the characteristics of the device and the access patterns produced. <ref type="figure" target="#fig_2">Figure 3</ref> (left) shows the performance of LgR on HDD <ref type="bibr" target="#b4">[6]</ref> and NVMe <ref type="bibr" target="#b1">[3]</ref>, for both serdes and mmap. In both cases we use a 18GB dataset and a single Spark executor using 8GB DRAM and 30 cores. The actual working set that needs to be cached is 10x the DRAM cache size. mmap produces small -due to the small (4KB) page size-and relatively random I/Os compared to serdes, as shown in <ref type="figure" target="#fig_2">Figure 3 (right)</ref>, which shows the average request size. HDDs do not perform well for this access pattern. Serdes with 3x larger request size is more than 3x better than mmap, despite the high serdes CPU overhead. However, the NVMe achieves high throughput and low latency for small request sizes regardless of the access pattern <ref type="bibr" target="#b19">[22]</ref>, resulting in mmap performing 36% better compared to serdes. We believe that using an optimized mmio path, such as FastMap <ref type="bibr" target="#b20">[23]</ref>, can further improve performance.</p><p>Another way to grow the JVM heap over a fast storage device to avoid serdes, would be to use the OS virtual memory system use the NVMe as swap space <ref type="bibr" target="#b7">[10]</ref>. Although this would enable storing very large JVM heaps in an NVMe, it cannot be used to target solely the RDD cache objects, and cannot avoid garbage collection of the resulting large heap. As TeraCache uses two separate heaps for execution and caching, in order to explicitly avoid GC in the cache, mmap is a better fitting mechanism to place the caching heap on the storage device. <ref type="figure" target="#fig_3">Figure 4</ref>(a) shows that being able to maintain separate heaps for the execution and caching parts of the heap, strictly use the NVMe for caching, and avoid GC in the cache, are all vital to performance. TeraCache yields up to 2x improvement compared to simply swapping a large, yet garbage-collected, single JVM heap onto the device. TeraCache heap management avoids costly GC: Since the JVM is unaware of execution-storage memory separation, all objects get allocated on the JVM heap (middle layer of <ref type="figure" target="#fig_1">Fig- ure 2(a)</ref>). This increases GC time, for two reasons: (1) cached RDDs are long-lived collection of objects and are managed by explicit persist and unpersist actions of Spark applications. Therefore, they rarely get collected and the garbage collector spends a significant part of time traversing live objects; and (2) more GC cycles are required to reclaim enough space in execution memory, since each GC cycle is able to free little memory due to long-lived cached RDDs. As the cached objects' life-time is clearly defined by how long they remain in the Spark cache, we can avoid GCs in TeraCache. Thus, our design uses a custom allocator to manage TeraCache and reduces object reclamation cost, as follows.</p><p>We augment the garbage collector and make it aware of the differences between H1 and TeraCache. H1 is treated as a standard JVM heap and is collected using standard GC algorithms. TeraCache uses a custom region-based allocator <ref type="bibr" target="#b21">[24]</ref> and conforms with the Java memory model <ref type="bibr" target="#b15">[18]</ref>. Specifically, we organize TeraCache into regions corresponding to RDDs. A region is a collection of pages of memory and contains objects of the same RDD. Consequently, TeraCache can free batches of objects with identical lifetimes allocated in the same region at once, similarly to Broom <ref type="bibr" target="#b9">[12]</ref>. To maintain memory safety, we do not allow references from TeraCache to H1. All objects residing in TeraCache may only reference objects residing in TeraCache. We achieve this by migrating each RDD object and all objects reachable from it to TeraCache. Then, the GC does not need to traverse TeraCache to identify live-objects in H1. RDDs are immutable and always safe to move to TeraCache without corrupting other objects.</p><p>Caching RDDs in TeraCache: Since RDD caching is explicitly managed by developers through the Spark RDD API <ref type="bibr" target="#b3">[5]</ref>, we introduce two Java annotations, @cache and @uncache, to annotate the corresponding code points in the Spark Block Manager. The Block Manager is the Spark component within the executor that manages caching, serialization, data transfers, etc. The annotations we introduce are syntactic metadata that communicate to the JVM that an RDD is cached or uncached by Spark. At a @cache annotation, which implies the caching of an RDD, TeraCache performs a traversal of the RDD data similar to the marking phase of a mark-sweep GC, marking all objects that can be reached from it and migrating them to an appropriate TeraCache region. We move the data from H1 to TeraCache instead of directly allocating them there, as RDD objects will have already been created when the application requests the Spark Block Manager to cache them. Respectively, at an @uncache annotation, the JVM can reclaim the RDD block and its space from TeraCache. In addition, to annotating the user facing API we also annotate, with @uncache the Spark Block Manager function that handles eviction of RDDs -based on an LRU policy when the Storage memory becomes full-to reclaim RDDs when TeraCache reaches its capacity limit. Division of DRAM between DR1 and DR2: <ref type="figure" target="#fig_1">Figure 2(b)</ref> shows that TeraCache divides the physical DRAM (bottom layer) into two regions: (1) DR1 used for H1, and (2) DR2 used as a cache for the memory mappings of TeraCache. To reduce the time spent in GC during task execution, DR1 needs to be large enough to accommodate as much of the newly created objects as possible. At the same time, the size of DR2 determines the number of page faults causing mmap I/O, which will have a direct effect on the average access times for the cached data. Ideally, we need sufficient space for H1 when tasks create new objects and sufficient space for mmap when tasks use cached data. However, since DR1 and DR2 share the physical DRAM, the larger the size of DR1 the smaller the size of DR2 and vice versa. Thus, we propose a mechanism that dynamically resizes DR1 and DR2.</p><p>We use two metrics to determine whether such resizing is needed. For DR1, we measure the frequency of minor GCs/s, since a higher frequency indicates that the application needs more heap space (DR1). Similarly, for DR2 we measure the rate of pagefaults for a single memory-map since a high frequency of page faults indicates that more space needed for memory mappings. Overall, if the rate of pagefaults increases and minor GC frequency is low, then we decrease the size of DR1 and increase the size of DR2 and vice versa. Section 3 shows that dynamic resizing of DR1 and DR2 is necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prototype Implementation</head><p>We implement an early prototype of TeraCache based on the ParallelGC <ref type="bibr" target="#b11">[14]</ref>. ParallelGC splits the JVM heap into two generations: (1) NewGen for keeping short-lived objects, and (2) OldGen for keeping long-lived objects. NewGen is divided into an Eden space and two equally divided survivor spaces. New objects are allocated in the Eden space, while objects that survive a minor GC get moved to the survivor spaces. Finally, objects surviving enough GCs and reach a predefined tenuring threshold are further moved to the OldGen. In our prototype, we place the NewGen in DRAM (H1) and mmap OldGen onto an NVMe device. This implementation uses OldGen as cache, containing all long-lived objects, including cached data. The Spark Block Manager does not notify the JVM when a cache operation is performed, however, the garbage collector promotes cached data objects in OldGen after several minor GCs. To avoid GC on cached data we explicitly disable GC in OldGen.</p><p>Our prototype targets only caching and does not support reclamation of cached RDDs during uncache operations. Note that our prototype, allows for long-lived data other than the cached ones to slip in the Old Gen as well. To ensure that OldGen will primarily contain cached data and only a low number of non-cached data we set the tenured threshold of the GC to 25. Using this threshold we avoid in OldGen allocation of long-lived objects which are not related to cached data. As a result only 5% of the allocated objects in OldGen are irrelevant long-lived data. This allows us to evaluate the GC time and I/O traffic, as they would be achieved by a more complete prototype of TeraCache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Using the TeraCache prototype implementation we perform a set of experiments to estimate: (1) the overall performance benefit using TeraCache compared to hybrid (baseline), (2) the impact of using TeraCache on GC overhead, and (3) the effect of DRAM division between DR1 and DR2. Evaluation Setup: We ran all experiments on a dual-socket server with two Intel(R) Xeon(R) E5-2630 v3 CPUs at 2.4GHz, with 8 physical cores and 16 hyper-threads each (32 total hyper-threads), 32 GB of DDR4 DRAM and CentOS v7.3, with Linux kernel 4.14.72. As storage device we use a PCIe-attached Samsung SSD 970 PRO with 500GB capacity. In our experiments we use OpenJDK v8u250-b70 and Spark v2.3.0, using one Spark executor with 30 threads. We evaluate TeraCache against hybrid using KMeans (KM), LR, LgR, and SVM workloads from the Spark-Bench Suite <ref type="bibr" target="#b12">[15]</ref>. Each workload ran for 100 iterations on a 64GB dataset. Also, for TeraCache we ran each workload with the different configurations shown in <ref type="table">Table 1</ref>, while for hybrid we use a 32GB heap that leverages 60% of the heap total heap space for on-heap cache and the full storage device as the off-heap RDD cache. Overall Performance Benefits Using TeraCache: <ref type="figure" target="#fig_3">Fig- ure 4(a)</ref> shows the total execution time of each benchmark when using hybrid (middle) and TeraCache (right). For TeraCache we plot the best performing configurations in <ref type="table">Table 1</ref> for the corresponding workloads i.e., configuration C for LR, LgR and configuration D for KM and SVM. For hybrid, we use the maximum heap size to allocate more RDDs on-heap to reduce the number of evictions in the storage device. We observe that TeraCache improves the overall performance by 7%, 32%, 37%, and 20% for KM, LR, LgR, and SVM, respectively compared to hybrid. To better understand the source of the performance improvement we break down the execution time in Other, Serdes+I/O, and GC time. Impact of TeraCache on GC overhead: As discussed in Section 2, <ref type="figure" target="#fig_3">Figure 4</ref>(a) shows that GC time decreases by 43%, 50%, and 45% for LR, LgR, and SVM respectively in hybrid. Part of the reduction is attributed to TeraCache not having to mark cached RDDs, while another part is attributed to TeraCache using the DRAM heap only for ephemeral objects thus performing fewer collections. In contrast, the original Spark keeps both short-lived objects and RDDs in the DRAM heap and only evicts serialized RDDs to the device. Specifically for KM, we observe a 5% increase in GC time. We attribute this increase to the fact that, in KM, tasks do not access large cached data, but instead create large amounts of shuffle data (short-lived objects) in H1. This, for smaller sizes of H1 <ref type="figure" target="#fig_3">(Fig- ure 4</ref> (c) configurations A and B), increases GC pressure and causes frequent collections. Finally, note that even in E <ref type="figure" target="#fig_3">(Fig- ure 4(c)</ref>), where GC time is minimal, execution time is not necessarily minimum, as accessing TeraCache produces a large number of page faults (e.g., for LR), as discussed below. Effect of DRAM Division Between DR1 and DR2: As discussed in Section 2, the DRAM division between DR1 and DR2 affects the overall performance of the applications. <ref type="figure" target="#fig_3">Fig- ures 4(b)</ref>, (c), and (d) show the execution time, the GC time, and the total number of page faults respectively for each workload, for configurations A-E in <ref type="table">Table 1</ref>. KM has the minimum number of page faults using D but the GC time is higher by 16% than E. KM generates shuffle data between stages, requiring more space for the execution memory, as shown by the great improvement in total execution time between B and C. SVM has the minimum number of page faults and the lowest GC and execution time using D, while LR and LgR using C. Conversely, LR and LgR access the cached data frequently, and hence respond better to increasing the DRAM available for mmap pages. In general, variability in the execution time between configurations can be attributed to the application pattern. Benchmark stages with more accesses to cached data benefit more from more mmap pages in DR2, while stages that create many temporary objects benefit more from increasing H1. Thus, resizing DR1 and DR2 at runtime between stages is beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>TeraCache essentially relies on certain properties in JVMbased data processing frameworks, requiring a modified JVM that is aware of the annotation notifications. TeraCache takes advantage of applications with the following properties:</p><p>• They create objects that can be grouped in sets of similar and preferably long lifetimes. For example, Spark workflows with iterative computations use exist long-lived objects, such as accumulated records and shuffle data. Such groups of objects allow TeraCache to free regions in its TeraCache heap without the need for extensive garbage collection. Our early TeraCache prototype relies on the runtime system to move these objects to its TeraCache heap, i.e., by annotating Spark caching code, which is transparent to the program and user.</p><p>• They create objects whose transitive closure does not cover the entire heap. To ensure safety, we avoid pointers from the TeraCache to H1, by computing the transitive closure of references (i.e., all reachable objects) and move them in a single region in TeraCache. This implies that the transitive closure of such objects should not be the entire heap, otherwise using TeraCache will result in high overheads.</p><p>Note that although TeraCache's approach could conceptually cope with mutable objects, e.g., by re-scanning their transitive closure after they are modified for new pointers, in Spark cached objects are immutable. This simplifies further the management of the TeraCache heap as a cache.</p><p>Recently, there is a fair amount of research activity towards extending DRAM in two directions: (1) the use of transparently caching NVMe (or NVM) devices to DRAM, an approach we follow as well, and (2) extending (but not caching) the system physical address space with byte-addressable NVM <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b26">29]</ref> or block-oriented NVMe devices <ref type="bibr">[7]</ref>. In both cases, employing a large heap occurs large GC overheads. Our approach shows that there are significant performance benefits in managing properly short-lived and group of objects with similar properties in a co-design fashion by reducing GC overheads. Therefore, we believe that TeraCache could be used in both design alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Caching in Spark: Neutrino <ref type="bibr" target="#b27">[30]</ref> proposes a fine-grain, offheap caching mechanism that performs serdes for blocks that belong to the same RDD, based on executor available memory at runtime. LCS and LRC <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b30">33]</ref> improve the management of on-heap memory caching, evicting RDD blocks that lead to minimum recomputation time in subsequent stages, without dealing with GC overhead. MemTune <ref type="bibr" target="#b28">[31]</ref> dynamically tunes executor caching space at runtime, based on data center workloads. MemTune provides task-level data prefetching with a configurable window size to overlap computation with serdes operation. <ref type="bibr">Zhang et al. [36]</ref> modify the re-caching algorithm to avoid moving blocks from memory back to disk at the end of each task, reducing serdes cost. Tungsten <ref type="bibr" target="#b2">[4]</ref> uses off-heap computation to eliminate serdes. However, Tungsten applies to known object schema (e.g. Spark SQL) while TeraCache can be employed for arbitrary schema object data. These works reduce serdes cost by managing cached data inmemory while paying significant GC cost to traverse cached data on every GC cycle. Instead, TeraCache completely removes serdes, providing direct access to the cached data and enables caches that exceed DRAM size, over memory-mapped fast storage devices. Finally, TeraCache reduces GC overhead by preventing GC traversals of cached data. JVM heaps over byte-addressable NVM: Espresso <ref type="bibr" target="#b26">[29]</ref> takes advantage of the larger capacity of NVM by introducing a new programming model to persist long-lived objects. Espresso does not provide a GC policy to avoid frequent traversals of long-lived objects, increasing GC overhead, especially in big data analytics frameworks. Panthera <ref type="bibr" target="#b25">[28]</ref> places YoungGen in DRAM and divides OldGen into DRAM and NVM. Panthera is integrated in ParallelGC and traverses all cached RDDs on every major GC resulting in significant performance degradation. Both Espresso and Panthera remove serdes overhead by increasing heap size at the expense of GC overhead. However, they are agnostic to application specific characteristics, such as caching. Panthera statically divides DRAM between the two heaps, regardless of job requirements. TeraCache is a co-design approach that uses application knowledge to explicitly and transparently manage cached objects and avoid unnecessary traversals over cached data on every GC cycle, significantly reducing CPU overhead. Additionally, TeraCache dynamically resizes the DRAM space used for memory-mapped I/O and execution memory in Spark according to job requirements, improving GC time and cache access time. Finally, the design of TeraCache is generic; it can be implemented on top of different garbage collector. Region-based memory management for big data systems: Broom <ref type="bibr" target="#b9">[12]</ref> show that big-data systems generate objects with the predefined life-times. They use region-based memory management to locate in objects in shared regions improving GC time. The downside of Broom is that add an additional complexity to the end user to be aware of regions, while TeraCache leverages the cache architecture of the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Spark applications often cache intermediate data, especially when performing iterative computations. However, the repeated serialization/deserialization of Spark RDDs creates significant CPU overhead that cannot currently be reduced without increasing GC overhead. We believe such overheads can be eliminated by extending the JVM heap over fast storage devices. We propose TeraCache, a co-design of the JVM and Spark that uses an on-heap RDD cache, memory-mapped over a fast storage device. TeraCache provides direct access over cached RDDs, removing both serdes and GC overheads for cached objects. Our preliminary results show that ML workload performance improves by up to 37% using TeraCache compared to serdes. We expect that TeraCache can also improve performance for other frameworks making use of very large immutable object caches (e.g., Apache Flink).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Hybrid caching outperforms on-demand recomputation. (b) Disk-only caching incurs high serdes overhead and lower GC time, whereas hybrid caching exhibits the reverse behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Off-memory caching via serdes. (b) TeraCache, on-heap RDD cache over a memory-mapped fast storage device S DR1 + S DR2 = S DRAM (S DRAM =DRAM size, S DR1 =DR1 size, S DR2 =DR2 size, S C =Capacity of fast storage device).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LgR on HDD vs NVMe SSD storage devices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Applications performance using Linux swap, hybrid and TeraCache. (b) Execution Time, (c) Total GC time, and (d) Total number of page faults for each TeraCache configuration with 64GB dataset.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>We thankfully acknowledge the support of the Evolve Project (Grant Agreement N o 825061), funded by the European Union Horizon 2020 Research and Innovation Programme. Anastasios Papagiannis is also supported by the Facebook Graduate Fellowship. Finally, we thank Yannis Sfakianakis, Giorgos Xanthakis, Fotis Nikolaides, and the anonymous reviewers for their insightful comments and constructive feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Intel</forename><surname>Optane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ssd</forename><surname>Dc P4800x</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Series</surname></persName>
		</author>
		<ptr target="https://www.intel.com/content/www/us/en/products/memory-storage/solid-state-drives/data-center-ssds/optane-dc-ssd-series/optane-dc-p4800x-series.html.Accessed" />
		<imprint>
			<date type="published" when="2020-03-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="https://www.cnet.com/products/wd-caviar-se-80gb/.Accessed" />
		<title level="m">SAMSUNG 970 EVO Plus NVMe M.2 SSD 500GB</title>
		<imprint>
			<date type="published" when="2020-03-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Project tungsten: Bringing apache spark closer to bare metal</title>
		<ptr target="https://databricks.com/blog/2015/04/28/project-tungsten-bringingspark-closer-to-bare-metal.html" />
		<imprint>
			<date type="published" when="2015-05-25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rdd Programming Guide</surname></persName>
		</author>
		<ptr target="https://spark.apache.org/docs/2.3.0/rdd-programming-guide.html" />
		<imprint>
			<date type="published" when="2018-03-15" />
		</imprint>
	</monogr>
<note type="report_type">Accessed</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="https://www.samsung.com/us/computing/memory-storage/solid-state-drives/ssd-970-evo-plus-nvme-m-2-500gb-mz-v7s500b-am/" />
		<title level="m">Western Digital Caviar SE WD800JD</title>
		<imprint>
			<date type="published" when="2018-03-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ssd</forename><surname>Nvme</surname></persName>
		</author>
		<ptr target="https://www.samsung.com/semiconductor/minisite/ssd/product/consumer/ssd960/,2019.Accessed" />
		<imprint>
			<date type="published" when="2020-03-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A study on garbage collection algorithms for big data environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulo</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linux Device Drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Corbet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rubini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Kroahhartman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Reilly Media, Inc</publisher>
		</imprint>
	</monogr>
	<note>3rd Edition</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lcs: An efficient data eviction strategy for spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhen</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanhua</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Parallel Program</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1285" to="1297" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Broom: Sweeping out garbage collection from big data systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ionel</forename><surname>Gog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Giceva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malte</forename><surname>Schwarzkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kapil</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Vytiniotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesan</forename><surname>Ramalingan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Conference on Hot Topics in Operating Systems, HOTOS&apos;15</title>
		<meeting>the 15th USENIX Conference on Hot Topics in Operating Systems, HOTOS&apos;15<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Who limits the resource efficiency of my datacenter: An analysis of alibaba datacenter traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yungang</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Quality of Service, IWQoS &apos;19</title>
		<meeting>the International Symposium on Quality of Service, IWQoS &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The garbage collection handbook: the art of automatic memory management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antony</forename><surname>Hosking</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliot</forename><surname>Moss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparkbench: A comprehensive benchmarking suite for in memory data analytic platform spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentina</forename><surname>Salapura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM International Conference on Computing Frontiers, CF &apos;15</title>
		<meeting>the 12th ACM International Conference on Computing Frontiers, CF &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed graphlab: A framework for machine learning and data mining in the cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Makarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Sverdlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Selberherr</surname></persName>
		</author>
		<title level="m">Emerging Memory Technologies: Trends, Challenges, and Modeling Methods. Microelectronics Reliability</title>
		<imprint>
			<date type="published" when="2012-04" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="628" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Manson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarita</forename><forename type="middle">V</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Java Memory Model. SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="378" to="391" />
			<date type="published" when="2005-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Samsung drops 128TB SSD and kinetic-type flash drive bombshells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mellor</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memory scaling: A systems architecture perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th IEEE International Memory Workshop, IMW 2013</title>
		<imprint>
			<date type="published" when="2013-05" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The case for ramcloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parag</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Leverich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mazières</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhasish</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ongaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Parulkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mendel</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Rumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Stratmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Stutsman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tucana: Design and implementation of a fast and efficient scale-up key-value store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Papagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Saloustros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilar</forename><surname>González-Férez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Bilas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 USENIX Annual Technical Conference (USENIX ATC 16)</title>
		<meeting><address><addrLine>Denver, CO</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="537" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimizing Memory-mapped I/O for Fast Storage Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Papagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Xanthakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC &apos;20</title>
		<meeting>the 2020 USENIX Conference on Usenix Annual Technical Conference, USENIX ATC &apos;20<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-07" />
		</imprint>
	</monogr>
	<note>Giorgos Saloustros, Manolis Marazakis, and Angelos Bilas</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical parallel dynamic dependence analysis for recursively task-parallel programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Foivos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Polyvios</forename><surname>Zakkak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pratikakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling trends in nand flash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Parat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Electron Devices Meeting (IEDM)</title>
		<imprint>
			<date type="published" when="2018-12" />
		</imprint>
	</monogr>
	<note>pages 2.1.1-2.1.4</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">DataAge 2025 -The Evolution of Data to Life-Critical</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Reinsel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Gantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Rydning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-11" />
			<pubPlace>Seagate</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rajesh Kumar Aggarwal, and Vishal Passricha. A global survey on data deduplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubhanshi</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pooja</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Grid High Perform. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="43" to="66" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fang Lv, Xiaobing Feng, and Guoqing Harry Xu. Panthera: Holistic memory management for big data processing over hybrid memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haris</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI&apos;19)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Espresso: Brewing java for more non-volatility with non-volatile memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;18</title>
		<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neutrino: Revisiting memory caching for iterative data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erci</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th USENIX Conference on Hot Topics in Storage and File Systems, HotStorage&apos;16</title>
		<meeting>the 8th USENIX Conference on Hot Topics in Storage and File Systems, HotStorage&apos;16<address><addrLine>Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="16" to="20" />
		</imprint>
	</monogr>
<note type="report_type">USENIX Association</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Memtune: Dynamic memory management for inmemory data analytic platforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Butt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)</title>
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="page" from="383" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An experimental evaluation of garbage collectors on big data applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="570" to="583" />
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Lrc: Dependency-aware cache management for data analytics clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khaled Ben</forename><surname>Letaief</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2017 -IEEE Conference on Computer Communications</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: A fault-tolerant abstraction for inmemory cluster computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;12</title>
		<meeting>the 9th USENIX Conference on Networked Systems Design and Implementation, NSDI&apos;12<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud&apos;10</title>
		<meeting>the 2nd USENIX Conference on Hot Topics in Cloud Computing, HotCloud&apos;10</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="10" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding and improving disk-based intermediate data caching in spark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tanimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ogawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2508" to="2517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making sense of performance in inmemory computing frameworks for scientific data analysis: A case study of the spark system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ujjwal</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Ficklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="369" to="382" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
