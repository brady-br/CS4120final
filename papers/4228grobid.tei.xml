<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T04:05+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Preech: A System for Privacy-Preserving Speech Transcription Prεεch: A System for Privacy-Preserving Speech Transcription</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>August 12-14, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimaa</forename><surname>Ahmed</surname></persName>
							<email>ahmed27@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><forename type="middle">Roy</forename><surname>Chowdhury</surname></persName>
							<email>roychowdhur2@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kassem</forename><surname>Fawaz</surname></persName>
							<email>kfawaz@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parmesh</forename><surname>Ramanathan</surname></persName>
							<email>parmesh.ramanathan@wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimaa</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><forename type="middle">Roy</forename><surname>Chowdhury</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kassem</forename><surname>Fawaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parmesh</forename><surname>Ramanathan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of Wisconsin-Madison</orgName>
								<orgName type="institution" key="instit2">University of Wisconsin-Madison</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Preech: A System for Privacy-Preserving Speech Transcription Prεεch: A System for Privacy-Preserving Speech Transcription</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th USENIX Security Symposium</title>
						<meeting>the 29th USENIX Security Symposium						</meeting>
						<imprint>
							<date type="published">August 12-14, 2020</date>
						</imprint>
					</monogr>
					<note>This paper is included in the Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>New advances in machine learning have made Automated Speech Recognition (ASR) systems practical and more scal-able. These systems, however, pose serious privacy threats as speech is a rich source of sensitive acoustic and textual information. Although offline and open-source ASR eliminates the privacy risks, its transcription performance is inferior to that of cloud-based ASR systems, especially for real-world use cases. In this paper, we propose Prεεch, an end-to-end speech transcription system which lies at an intermediate point in the privacy-utility spectrum. It protects the acoustic features of the speakers&apos; voices and protects the privacy of the textual content at an improved performance relative to offline ASR. Additionally, Prεεch provides several control knobs to allow customizable utility-usability-privacy trade-off. It relies on cloud-based services to transcribe a speech file after applying a series of privacy-preserving operations on the user&apos;s side. We perform a comprehensive evaluation of Prεεch, using diverse real-world datasets, that demonstrates its effectiveness. Prεεch provides transcription at a 2% to 32.25% (mean 17.34%) relative improvement in word error rate over Deep Speech, while fully obfuscating the speakers&apos; voice biometrics and allowing only a differentially private view of the textual content.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>New advances in machine learning and the abundance of speech data have made Automated Speech Recognition (ASR) systems practical and reliable <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b15">17]</ref>. ASR systems have achieved a near-human performance on standard datasets <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b15">17]</ref>, at a scale. This scalability is desirable in many domains, such as journalism <ref type="bibr" target="#b23">[25]</ref>, law, business, education, and health care, where cost, delay, and third-party legal implications <ref type="bibr" target="#b27">[29]</ref> prohibit the application of manual transcription services <ref type="bibr" target="#b10">[12]</ref>. For example, recent research has identified private voice transcription as one of the challenges journalists face when interviewing sensitive sources <ref type="bibr" target="#b23">[25]</ref>.</p><p>Several companies, such as Google and Amazon, provide online APIs for speech transcription. This convenience, however, comes at the cost of privacy. A speech recording contains acoustic features that can reveal sensitive information about the user, such as age, gender <ref type="bibr" target="#b37">[39]</ref>, emotion <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b38">40]</ref>, accent, and health conditions <ref type="bibr" target="#b39">[41]</ref>. The acoustic features are also biometric identifiers of the speakers <ref type="bibr" target="#b24">[26]</ref>, enabling speaker identification and impersonation <ref type="bibr" target="#b18">[20]</ref>. Additionally, the textual content of speech can be sensitive <ref type="bibr" target="#b27">[29]</ref>. For example, medical recordings can contain private health information about patients <ref type="bibr" target="#b10">[12]</ref>, and business recordings can include proprietary information. Current cloud services already support several speech processing APIs like speaker identification and diarization. They also support text analysis APIs, such as topic modeling, document categorization, sentiment analysis, and entity detection ( <ref type="bibr">Sec. 3.2)</ref>, that can extract sensitive information from text. Applying these APIs to the recorded speech can significantly undermine the user's privacy.</p><p>Offline and open-source transcription services, like Deep Speech <ref type="bibr" target="#b16">[18]</ref>, solve these privacy challenges as the speech files never leave the user's trust boundary. However, we find that their performance does not match that of a cloud service provider <ref type="bibr" target="#b43">[45]</ref>, especially on real-world conversations and different accents (Sec. 2.2). Thus, the primary goal of this paper is to: provide an intermediate solution along the utilityprivacy spectrum that uses cloud services while providing a formal privacy guarantee.</p><p>We present Prεεch (Privacy-Preserving Speech) as a means to achieve this goal; it is an end-to-end speech transcription system that: (1) protects the users' privacy along the acoustic and textual dimensions; (2) improves the transcription performance relative to offline ASR; and (3) provides the user with control knobs to customize the trade-offs between utility, usability, and privacy. Textual Privacy: Prεεch segments and shuffles the input speech file to break the context of the text, effectively transforming it into a bag-of-words. Then, it injects dummy (noise) segments to provide the formal privacy guarantee of differential privacy (DP) <ref type="bibr" target="#b11">[13]</ref>.</p><p>Acoustic Privacy: Prεεch applies voice conversion to protect the acoustic features of the input speech file and ensure noise indistinguishability.</p><p>We evaluate Prεεch over a set of real-world datasets covering diverse demographics. Our evaluation shows that Prεεch provides a superior transcription accuracy relative to Deep Speech, the state-of-the-art offline ASR. Also, Prεεch prevents cloud services from extracting any user-specific acoustic features from the speech. Finally, applying Prεεch thwarts the learning of any statistical models or sensitive information extraction from the text via natural language processing tools.</p><p>In summary, the main contributions of this paper are: (1) End-to-end practical system: We propose Prεεch, a new end-to-end system that provides privacy-preserving speech transcription at an improved performance relative to offline transcription. Specifically, Prεεch shows a relative improvement of 2% to 32.52% (mean 17.34%) in word error rate (WER) on real-world evaluation datasets over Deep Speech, while fully obfuscating the speakers' voice biometrics and allowing only a DP view of the textual content.</p><p>(2) Non-standard use of differential privacy: Prεεch uses DP in a non-standard way, giving rise to a set of new challenges. Specifically, the challenges are (1) "noise" corresponds to concrete words, and need to be added in the speech domain (2) "noise" has to be indistinguishable from the original speech (details in Sec. 4.5). (3) Customizable Design: Prεεch provides several control knobs for users to customize the functionality based on their desired levels of utility, usability, and privacy (Sec. 7.4). For example, in a relaxed privacy setting, Prεεch's relative improvement in WER ranges from 44% to 80% over Deep Speech (Sec. 7.4.1).</p><p>The full version of this paper is available online <ref type="bibr" target="#b1">[3]</ref>, and some demonstrations of Prεεch are available at this link <ref type="bibr">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Speech Transcription Services</head><p>We first provide some background on online and offline speech transcription services. Next, we present a utility evaluation using standard and real-world speech datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background</head><p>Speech transcription refers to the process of extracting text from a speech file. ASR systems are available to the users either through cloud-based online APIs or offline software.</p><p>(1) Cloud-Based Transcription: We utilize two cloud-based speech transcription services -Google's Cloud Speech-toText and Amazon Transcribe.</p><p>(2) Offline Transcription: We consider the Deep Speech architecture from <ref type="bibr">Baidu [18]</ref>, which is trained using Mozilla's 1 Common Voice dataset as a representative offline transcription <ref type="bibr" target="#b0">1</ref> https://voice.mozilla.org/en/datasets service. This dataset is crowdsourced and open-source. Specifically, we use the Deep Speech 0.4.1 model 2 (released in January 2019). Note that we do not consider offline transcribers that are not open for general use. For example, Google's ondevice speech recognizer <ref type="bibr" target="#b0">[1]</ref> is an offline transcriber that is currently only supported on Google's Pixel devices and does not allow an API or open-source access, limiting its usability.</p><p>Notations: Let S denote the input speech file associated with a ground truth transcript T g S . The user can either use a cloud service provider (CSP) or an offline service provider (OSP) to obtain the transcript (denoted by T CSP S or T OSP S , respectively). Transcription Accuracy: The standard metric for quantifying the accuracy loss from transcription is the word error rate (WER) <ref type="bibr" target="#b16">[18]</ref>. WER treats the transcript as a sequence of words. It models the difference between the two sequences by counting the number of deleted words (D), the number of substituted words (U), and the number of injected words (I). If the number of words in T g S is W , WER is given as: D+U+I W .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Utility Comparison</head><p>In this section, we empirically evaluate the utility gap between the CSP and the OSP over a wide range of standard and realworld datasets. We use these datasets throughout the paper. Standard Datasets: These datasets include (1) the TIMIT-TEST subset <ref type="bibr" target="#b14">[16]</ref>, (2) a subset from Librispeech dev-clean dataset <ref type="bibr" target="#b29">[31]</ref>, and (3) the DAPS dataset <ref type="bibr" target="#b26">[28]</ref>. TIMIT-TEST <ref type="bibr" target="#b1">3</ref> subset comprises of 1344 utterances by 183 speakers from eight major dialect regions of the United States. The LibriSpeech subset consists of eleven speakers, 20 utterances each. For DAPS, we use the evaluation subset prepared for the 2018 voice conversion challenge <ref type="bibr" target="#b22">[24]</ref> that consists of five scripts read by ten speakers: five males and five females. Real-world Datasets: We also assess the real-world performance of both transcription services on non-American accent datasets and real conversations among speakers of different demographics. For the accented datasets, we evaluate 200 utterances of two speakers from the VCTK dataset <ref type="bibr" target="#b44">[46]</ref>: speaker p262 of a Scottish accent and speaker p266 of an Irish accent. For the real-world datasets, we evaluate 20 minutes of speech from the "Facebook, Social Media Privacy, and the Use and Abuse of Data" hearing before the U.S. Senate 4 . We construct the 20 minutes by selecting three continuous chunks of speech from the hearing such that they include nine speakers: 8 senators and Mark Zuckerberg. Another real-world dataset is the Supreme Court of the United States case "Carpenter v. United States" <ref type="bibr" target="#b3">5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Privacy Threat Analysis</head><p>We study the privacy threats that a cloud-based transcription service poses while processing private speech data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Voice Analysis</head><p>The biometric information embedded in S can leak sensitive information about the speakers, including their emotional status <ref type="bibr" target="#b2">[4,</ref><ref type="bibr" target="#b38">40]</ref>, health condition <ref type="bibr" target="#b39">[41]</ref>, sex <ref type="bibr" target="#b37">[39]</ref>, and even identity <ref type="bibr" target="#b24">[26]</ref>. Furthermore, extracting this information enables critical attacks like voice cloning and impersonation attacks <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b45">47]</ref>. In this section, we showcase a few representative examples of how cloud-based APIs can pose serious privacy threats to the acoustic features within S. Speaker Diarization: CSPs utilize advanced diarization capabilities to cluster the speakers within a speech file, even if they have not been observed before. The basic idea is to (1) segment the speech file into segments of voice activity, and (2) extract a speaker-specific embedding from each segment, such that (3) segments with close enough embeddings should belong to the same speaker. We verified the strength of the diarization threat over three multi-speaker datasets: VCTK (mixing p266 and p262), Facebook, and Carpenter. We measure the performance of the IBM diarization service using Watson's Speech-to-Text API 6 via Diarization Error Rate (DER). DER estimates the fraction of time the speech file segments are not attributed to the correct speaker cluster. The DER values are 0%, 4.85%, and 1.32% for the three <ref type="bibr" target="#b4">6</ref> https://www.ibm.com/cloud/watson-speech-to-text datasets, respectively. Hence, the API can correctly distinguish between, and cluster, the different speakers, more than 95% of the entire dataset duration despite lacking any prior information about the individual speakers.</p><p>Speaker Identification: A speaker identification task maps the speech segments in a speech file to an individual. We use the Azure Identification API, which consists of two stages: (1) user enrollment and (2) identification (whether a given voice sample matches any of the enrolled users). The enrollment stage requires only 30 seconds of speech from each user to extract their voice-print. We enrolled 22 speakers as follows: 10 from DAPS, two from VCTK, two from Carpenter, and eight from Facebook. The identification accuracy was nearly 100% for all speakers.</p><p>Speaker Cloning and Impersonation: Lastly, we applied a Tacotron-based speech synthesizer from Google <ref type="bibr" target="#b18">[20]</ref>; a network that can synthesize speech in the voice of any speaker. The network generates a target speaker's embedding, which it uses to synthesize speech on a given piece of text. In our setting, we used the network to generate the speakers' embedding in our evaluation datasets. Then, we synthesized eight speech utterances using the embeddings of each speaker. We enrolled the speakers in Azure's Speech Identification API using their natural voice samples and tested whether the API will map the synthesized segments to the corresponding speaker. Except for the second speaker in Carpenter, the cloned samples were successfully identified as the true speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Text Analysis</head><p>CSPs possess natural language processing (NLP) capabilities that enable automated statistical analyses on large sets of documents. Those analyses fall into two broad categories. The first type involves identifying specific words from the transcript that correspond to sensitive information such as an address, name, and SSN using named-entity extraction <ref type="bibr" target="#b12">[14]</ref>. The other type of analysis involves statistically analyzing the entire transcript on the whole to extract some semantic or user-identifying information. This analysis uses two types of information: the set of words (i.e., bag-of-words representation of the transcript) and their order of appearance (to capture the context).</p><p>Bag-of-Words Analysis: One of the most commonplace analysis that treats a document as a bag-of-words is topic modeling <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b41">43]</ref>. Topic modeling is an unsupervised machine learning technique that identifies clusters of words that best characterize a set of documents. Another popular technique is stylometry analysis, which aims at attributing authorship (in our case, the speaker) of a document based on its literary style. It is based on computing a set of stylistic features like mean word length, words histogram, special character count, and punctuation count from the disputed document <ref type="bibr" target="#b28">[30]</ref>.</p><p>Context-based Analysis: An example of context-based analysis is sentiment analysis (understanding the overall attitude in a block of text). Text categorization is another example; it refers to classifying a document according to a set of predetermined labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prεεch</head><p>Our discussion in the previous sections highlights a trade-off between privacy and utility. The OSP provides perfect privacy at the cost of higher error rates, especially for non-standard speech datasets. On the other hand, clear privacy violations accompany revealing the speech recording to the CSP. Motivated by this trade-off, we present Prεεch, a practical system that lies at an intermediate point along the utility-privacy spectrum of speech transcription.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">System and Threat Models</head><p>We consider the scenario where users have audio recordings of private conversations that require high transcription accuracy. For example, a journalist with recordings of confidential interviews is a paradigmatic user for Prεεch. Other examples include a therapist with recordings of patient therapy sessions or a course instructor with oral examination records of students. Prεεch, however, does not target real-time transcription applications. For example, voice assistants and online transcription (e.g. a live-streaming press conference) are out-of-scope. Thus, for our target use cases, the latency of transcription is not a critical concern. The adversary is the CSP or any other entity having direct or indirect access to the stored speech at the CSPs. This adversary is capable of the aforementioned voice-and text-based analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prεεch Overview</head><p>Prεεch provides an end-to-end tunable system which aims at satisfying the following design goals:</p><p>1. protect the users' privacy along the acoustic and textual dimensions; 2. improve on the transcription accuracy compared to offline models; and 3. provide the users with control knobs to customize Prεεch's functionality according to their desired level of utility, usability, and privacy.</p><p>To this end, Prεεch applies a series of privacy-preserving operations to the input speech file before sending it to the CSP. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the high-level overview of Prεεch. Below, we briefly describe Prεεch's privacy-preserving operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Preserving Textual Privacy</head><p>Prεεch protects the privacy of the textual content of an input speech file S through the following three operations:</p><p>Segmentation and shuffling: Prεεch breaks S into a sequence of segments, denoted by S. This is followed by shuffling the segments to remove all ordering information. Thus, segmenting and shuffling S transform its textual content into a bag-of-words representation.</p><p>Sensitive word scrubbing (SWS): First, Prεεch applies the OSP to identify the list of sensitive keywords that contain numbers, proper nouns, or any other user-specified words. Next, Prεεch applies keyword spotting, KWS, (identify portions of the speech that correspond to a keyword) to each of the segments in S. Only the segments that do not contain a keyword pass to the CSP for transcription.</p><p>Dummy word injection to ensure differential privacy:</p><p>The bag-of-words representation of a transcript corresponds to its word histogram (Sec. 4.5). As discussed in Sec. 3.2, several statistical analyses can be built on the word histogram of the transcript T CSP S such as topic modeling or stylometry analysis. Thus, protecting the privacy of this word histogram is a primary focus of Prεεch, and the privacy guarantee we choose is that of differential privacy. To this end, Prεεch ensures DP by adding a suitable amount of dummy words to S before sending it to the CSP. This way, the CSP is allowed only a differentially private view of the word histogram and any subsequent statistical model built over it (by Thm. 4.1 in Sec. 4.5).</p><p>The main challenge in this setting is that the dummy words must be added in the speech domain, which Prεεch addresses as follows. First, Prεεch estimates the general domain of the text for S (specifically its vocabulary, details in Sec. 4.5) from T OSP S . Next, it generates dummy text segments using a state-of-the-art NLP language model. Finally, Prεεch applies text-to-speech (TTS) transforms to these dummy segments and adds them to S. However, leaving it just at this would be insufficient as the CSP can potentially distinguish between the two different sources of speech (TTS generated dummy segments and segments in S) based on their acoustic features. Therefore, Prεεch provides the user with multiple options to synthesize indistinguishable dummy segments, namely (1) voice cloning <ref type="bibr" target="#b18">[20]</ref>, and (2) voice conversion <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b42">44]</ref>. These options offer different trade-offs between utility, usability, and privacy (Secs. 4.5.2 and 4.6). As stated in Sec. 3.2, textbased attacks exploit individual sensitive words or the order of the words or the word histogram. Thus, from the above discussion, Prεεch protects privacy along all three dimensions (evaluation results in Sec. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Preserving Voice Privacy</head><p>Voice conversion, VC, is a standard speech processing technique that transforms the voice of a source speaker of a speech utterance to that of another speaker. Prεεch applies voice conversion to fulfill a two-fold agenda. First, it obfuscates the sensitive voice biometric features in S. Second, VC ensures that the dummy segments (noise added to ensure differential privacy) are acoustically indistinguishable from the original speech file segments. There are two main categories in voice conversion: one-to-one VC, and many-to-one VC ( Sec. 4.6). <ref type="figure" target="#fig_0">Fig. 1</ref> depicts the workflow of Prεεch. Given a speech file S, the first step (1) is to break S into a sequence of disjoint and short speech segments, S. This is followed by (2) sensitive word scrubbing where speech segments containing numbers, proper nouns, and user-specified keywords are removed from S. Next, (3) given the domain of S's textual content (its vocabulary), Prεεch generates a set of text segments (as is suitable for satisfying the DP guarantee as discussed in Sec. 4.5), and subjects it to TTS transformation (4). At this point, Prεεch has audio segments for the input speech, S, as well as the dummy segments, S d . If the user also wants to hide the voice biometric information in S, Prεεch applies (5) voice conversion over all the segments in S S d to convert them to the same target speaker. This process hides the acoustic features of S and ensures that the segments in S and S d are indistinguishable. This is followed by Prεεch partitioning S across N &gt; 0 non-colluding CSPs (Sec. 4.5). This partitioning reduces the number of dummy segments that are required to achieve the DP guarantee (Sec. 4.5). Next, Prεεch adds a suitable amount of dummy segments from S d to each partition S i , i ∈ [N] and shuffles them. Additionally, Prεεch keeps track of time-stamps of the dummy segments, T S i and order of shuffling, Order i for each such partition (6). After obtaining the transcript (7) for each partition from the N CSPs, Prεεch removes S d 's transcripts and de-shuffles the remaining portion of the transcript using T S i and Order i , and outputs the final transcript to the user <ref type="bibr" target="#b6">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">End-to-End System Description</head><p>In what follows, we elaborate on the key components of Prεεch, namely segmentation, sensitive word scrubbing, DP word histogram release, and voice conversion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Segmentation Algorithm</head><p>A key component of Prεεch is breaking the textual context by segmenting S. We represent S as a sequence of segments S, where each segment can contain multiple words. Prεεch applies a hierarchical segmentation approach that starts with a stage of silence detection based on the energy level, followed by pitch detection to detect speech activity for finer segmentation. The mechanism is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>We define a period of silence as the time duration when the RMS power of the speech signal drops below -35 dB for at least 500ms. The initial segmentation stage detects such silence periods from S resulting in coarse segments. A human speech signal can be viewed as a modulated periodic signal where the signal period is referred to as the glottal cycle <ref type="bibr" target="#b25">[27]</ref>. In the second stage, Prεεch uses the existence of glottal cycles <ref type="bibr" target="#b5">[7]</ref> to detect human voice, which breaks down the coarse segments into finer ones. A time duration of at least 20 ms without the presence of glottal cycles is regarded as non-speech.</p><p>As some segments might be abrupt or too short to allow for correct speech recognition, Prεεch performs two additional optimization steps. First, it merges nearby fine segments to ensure a minimum length per segment. Second, it does not partition segments at the boundaries of the identified human speech and allows 40 ms of non-speech to be included at the beginning and the end of each segment.</p><p>Control Knob: Segmenting S presents with a trade-offsmaller segments result in better privacy guarantee at the expense of deteriorated transcription accuracy due to semantic context loss. Prεεch allows the user to tune the minimum length of the segments as a means to control this trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitive Word Scrubbing</head><p>Prεεch performs sensitive word scrubbing (SWS) as follows. First, it obtains the offline transcript of S, T OSP S . Next, it applies named entity recognition (NER) on T OSP S . NER is an NLP technique that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, monetary values, etc. Prεεch also gives the option for users to specify some keywords of their choice. This allows customization of the sensitive keyword list as users have subjective ideas of what they might consider sensitive.</p><p>After the list of sensitive words is finalized, Prεεch applies keyword spotting (KWS) on the segments. KWS is needed for the following three reasons. First, KWS is used to spot the userdefined keywords which cannot be identified by NER. Second, the initial T OSP S is generated on S without segmentation to achieve the highest estimation accuracy. However, for Prεεch, we need to identify the segments containing the keywords. Finally, the OSP might not transcribe the named-entities correctly at all locations. For example, the name "Carpenter" might be repeated 20 times in S, while the OSP transcribes it accurately only five times. KWS has higher accuracy in spotting keywords than the OSP's transcription accuracy.</p><p>Control Knob: KWS takes the list of keywords and matches them phonetically to a speech file based on a sensitivity score. This sensitivity score sets a threshold for the phonetic similarity required for a keyword to be spotted. A low score results in false positives by flagging phonetically similar words as keywords which degrades the utility by transcribing non-sensitive segments using the OSP. Conversely, a high score could result in some keywords being missed and revealed to the CSP. Hence, the sensitivity score is a trade-off parameter between privacy and utility (Sec. 7.3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Differentially Private Word Histogram</head><p>We define vocabulary, V , to be the domain of non-stop and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Privacy Definition</head><p>As discussed in Sec. 3.2, the aforementioned word histogram is sensitive and can only be released to the CSP in a privacypreserving manner. Our privacy guarantee of choice is DP which is the de-facto standard for achieving data privacy <ref type="bibr" target="#b9">[11,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b13">15]</ref>. DP provides provable privacy guarantees and is typically achieved by adding noise to the sensitive data.</p><formula xml:id="formula_0">Definition 4.1 ((ε, δ)-differentially private d-distant his- togram release). A randomized mechanism A : N |V | → N |V | ,</formula><p>which maps the original histogram into a noisy one, satisfies (ε, δ)-DP if for any pair of histograms H 1 and H 2 such that</p><formula xml:id="formula_1">||H 1 − H 2 || 1 = d and any set O ⊆ N |V | , Pr[A(H 1 ) ∈ O] ≤ e ε · Pr[A(H 2 ) ∈ O] + δ.</formula><p>(1)</p><p>In our context, the DP guarantee informally means that from the CSP's perspective, the observed noisy histogram, ˜ H, could have been generated from any histogram within a distance d from the original histogram, H. We define the set of all such histograms to be the ε-indistinguishability neighborhood for H. In other words, from˜Hfrom˜ from˜H the CSP will not be able to distinguish between T CSP S and any other transcript that differs from T CSP S in d words from V .</p><p>An important result for differential privacy is that any postprocessing computation performed on the output of a differentially private algorithm does not cause any loss in privacy.</p><p>Theorem 4.1. (Post-Processing) Let A : X → R be a randomized algorithm that is (ε, δ)-DP. Let f : R → R be an arbitrary randomized mapping. Then f • A : X → R is (ε, δ)-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DP.</head><p>Another result is that the privacy of DP-mechanism can be amplified if it is preceded by a sampling step.</p><p>Theorem 4.2. Let A be an (ε, δ)-DP algorithm and D is an input dataset. Let A be another algorithm that runs A on a random subset of D obtained by sampling it with probability β. Algorithm A will satisfy (ε , δ )-DP where ε = ln(1 + β(e ε − 1)) and δ &lt; βδ.</p><p>Additionally, we define a DP mechanism namely the truncated Laplace mechanism <ref type="bibr" target="#b4">[6]</ref> which is used in Prεεch. </p><formula xml:id="formula_2">p = e ε/d −1 e ε/d +1 and η 0 = − d·ln((e ε/d +1)δ) ε + d.</formula><p>Theorem 4.3. The truncated Laplace mechanism satisfies (ε, δ)-DP for d-distant histogram releases <ref type="bibr" target="#b4">[6]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Discussion</head><p>Prεεch's use of DP is different from the most standard usecase of DP (like numeric datasets). It deals with concrete units like words instead of numeric statistics -introducing new challenges; we discuss these challenges and how Prεεch circumvents them in this section. Vocabulary definition: The foremost task for defining the word histogram is defining the vocabulary, V . The most conservative approach to define V is to consider the total set of all English stemmed and non-stop words. Such a vocabulary would be prohibitively large for efficient and practical usage.</p><p>However, note that such a definition of V is an overestimate as no real-world document would contain all possible English words. Recall that our objective of adding noise is to obfuscate any statistical analysis built on top of the document's BoW (histogram), such as a topic modeling and stylometry analysis. Typically, BoW based statistical analyses are concerned only with the set of most frequent words. For example, any standard topic model captures only the top m percentile most frequent words in a transcript <ref type="bibr" target="#b35">[37,</ref><ref type="bibr" target="#b41">43]</ref>. The same applies to stylometry analysis, which is based on measures of the unique distribution of frequently used words of different individuals.</p><p>Thus, as long as the counts of the most common words of the transcript are protected (via DP), the subsequent statistical model (like topic model) built over the word histogram will be privacy-preserving too (by Thm. 4.1). However, highfrequency words might not be the only ones that contain important information about T S . To tackle this, we also include words with large Term Frequency-Inverse Document Frequency (TF-IDF) weight to our vocabulary. This weight is a statistical measure used to evaluate how significant a word is to a document relative to a baseline corpus. The weight increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the baseline corpus. This offset adjusts for the fact that some words appear more frequently in general. To this end, Prεεch makes an estimate of the vocabulary from T OSP S . Although existing offline transcribers have high WER, we found (empirically) that they can identify the set of domain words of S with high accuracy (details in Sec. }}. Note that V should be devoid of all sensitive words which are scrubbed off from S in step 2 of <ref type="figure" target="#fig_0">Fig. 1</ref>. Additionally, the vocabulary can be extended to contain out-of-domain words, i.e., random English words that are not necessarily part of the original document. This helps in protecting against text classification attacks (Sec. 7.3). Specificities of the word histogram: As discussed above, the goal of the DP mechanism is to generate noisy counts for each w i ∈ V . An artifact of our setting is that this noise has to be non-negative and integral. This is because dummy words (for the noisy counts) can only be added to S; removing any word from S is not feasible as this would entail in recognizing the word directly from S, which would require accurate transcription. Hence, Prεεch uses the truncated Laplace mechanism to ensure non-negative and integral noise. Setting privacy parameters: The parameters ε and δ quantify the privacy provided by a DP-mechanism; lower the values higher is the privacy guarantee achieved. The distance parameter d, intuitively, connects the privacy definition in the word histogram, which is purely a formal representation, to a semantic privacy notion. For example, it can quantify how much the noisy topic models computed by the CSP (from T CSP S ) should differ from that of T g S . Thus, the user can tune d depending on the target statistical analysis. In the following, we detail a mechanism, as a guide for the user, for choosing d when the target statistical analysis is topic modeling.</p><p>Let us assume that the user has a set of speech files {S j } to be transcribed. Let D j denote the ground truth transcript corresponding to speech file S j . The objective is to learn t topics from the corpus j D j with at least k words per topic (a topic is a distribution over a subset of words from the corpus).</p><formula xml:id="formula_3">Let T = {T 1 , · · · , T t } represent the original topic model built on j D j = j T g S j and T = T 1 , · · · , T</formula><p>t represent the noisy topic model computed by the CSP.</p><p>The following theorem (Thm. 4.4) provides a lower bound on the pairwise 1 distance between the true and noisy topics as a function of the privacy parameters of the DP word histogram release mechanism (specifically, the term C min is a function of (d, ε, δ)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4.4. For any pair of topics</head><formula xml:id="formula_4">(T, T ) ∈ T × T , ||T − T || 1 ≥ 2 1 1−(t−1) k max j |D j | C min t − 1 2 1 − t k max j |D j || , where C min = min j,l v·(|D j |−|w l, j |ω j ) |D j |·(|D j |+v·ω j )</formula><p>, |D j | is the total number of words in D j , ω j is the total number of unique words, v is the variance of the distribution Lp(ε , δ , d), δ = βδ and |w l, j | is the number of times the word w l ∈ V appears in D j .</p><p>The proof of this theorem and the descriptions of the parameters are presented in the full paper <ref type="bibr" target="#b1">[3]</ref> . Dummy word injection: As discussed earlier, achieving differential privacy requires adding dummy words to S. Prεεch generates the dummy text corpus using an NLP language model (Sec. 6). The model takes in a short text sample from the required topic and generates an entire document of any required length based on that input. In some scenarios, the user can also provide a corpus of non-publicly available documents with the same vocabulary. This scenario is valid in many practical settings. For instance, in an educational institution, the sensitive speech files requiring transcription might be the interviews/oral exams of the students conducted on a specific subject, and the noise corpus can be the lecture notes of the same subject.</p><p>Next, Prεεch generates a set of dummy segments, S d , from the dummy corpus above. Let us assume that each of the true segments contains at most k non-stop words (depends on the segment length). Prεεch ensures that each dummy segment also contains no more than k non-stop words. Additionally, each such segment must contain only one word from the vocabulary V . This means that although the physical noise addition is carried at the segment level, it is still equivalent to adding noise at the level of words (belonging to V ) as we only care about w i ∈ V . Each dummy segment is injected only once per CSP. Since the dummy segments have to be added in the speech domain, Prεεch applies TTS transforms to the segments in S d such that they have the same acoustic features as S. This condition ensures that S d are indistinguishable from S in terms of their acoustic features. Prεεch provides the user with two broad options to satisfy this conditionvoice cloning or voice conversion.</p><p>Voice cloning is a TTS system that generates speech in a target speaker voice. Given a speech sample from the target speaker, the system generates an embedding of the speaker's voice biometric features. It uses this embedding to synthesize new utterances of any linguistic content in the target speaker's voice. Prεεch utilizes such a technology to clone the original speaker's voice and uses it to generate acoustically similar dummy segments S d . Prεεch applies a state-of-the-art voice cloning system <ref type="bibr" target="#b18">[20]</ref>, which generates a close-to-natural synthetic voice using a short (∼ 5 sec.) target voice sample.</p><p>We evaluate this cloning system in Sec. 3.1, and the cloned samples are successfully identified as the true speakers. However, voice cloning does not protect the speakers' voice biometrics, and can be potentially thwarted by a stronger adversary. Hence, Prεεch provides voice conversion (VC) as a stronger privacy-preserving option for the user. VC transforms the voice of a source speaker to sound like a target speaker. Prεεch utilizes VC to obfuscate the true speakers' voice biometrics as well as to mitigate the DP noise indistinguishability concern by converting the true and dummy segments into a single target speaker voice (Sec. 4.6). We discuss the utility-privacy trade-offs of both options in Sec. 7.</p><p>It is important to note that the dummy segments do not affect the WER of T CSP S . It is so because Prεεch can exactly identify all such dummy segments (from their timestamps) and remove them from T CSP S . Additionally, since the transcription is done one segment at a time, the dummy segments do not affect the accuracy of the true segments (S) either. Segmentation and voice conversion are the culprits behind the WER degradation, as will be evident in Sec. 7. Thus in Prεεch, the noise (in the form of dummy segments) can ensure differential privacy without affecting the utility. This is in contrast to standard usage of differential privacy for releasing numeric statistics where the noisy statistics result in a clear loss of accuracy. However, the addition of the dummy segments in Prεεch does increase the monetary cost of using the online service that has to transcribe more speech data than needed. We analyze this additional cost in Sec. 7.</p><p>In practice, we have multiple well-known cloud-based transcription services with low WER like Google Cloud Speechto-Text, Amazon Transcribe, etc. Prεεch uses them to its advantage in the following way. Prεεch splits the set of segments S into N different sets (step 3 in Sec. 4.5.3)</p><formula xml:id="formula_5">S i , i ∈ [N]</formula><p>where N is the number of CSPs with low WER. Then, Prεεch sends each subset to a different CSP (after adding suitable noise segments to each set and shuffling them). Since each engine is owned by a different, often competing corporation, it is reasonable to assume that the CSPs are non-colluding. Thus, assuming that each segment contains at most one word in V , each subset of segments S i can be viewed as randomly sampled sets from S with sampling probability β = 1/N. From Thm. 4.2, this partitioning results in a privacy amplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Mechanism</head><p>We summarize the DP mechanism by which Prεεch generates the dummy segments for S. The inputs for the mechanism are (1) S -the short segments of the speech file S, (2) the privacy parameters ε and δ and (3) N -the number of non-colluding CSPs to use. This mechanism works as follows:</p><p>• Identify the vocabulary V = {w|w ∈ { top m percentile of the most frequent words in T OSP S } ∪ { words with TF-IDF value ≥ ∆ in T OSP S }} through running an offline transcriber over S.</p><p>• Tune the value of d based on the lower bound from Thm. 4.4, ε and δ.</p><p>• Generate N separate noise vectors,</p><formula xml:id="formula_6">η i ∼ [Lp((ln(1 + 1 β (e ε − 1)), βδ, d)] |V | , i ∈ [N].</formula><p>Thus for every partition i, Prεεch associates each word in V with a noise value, a non-negative integer.</p><p>• From the NLP generated text, extract all the text segments that contain words from V . For each partition i, sample the text segments from this corpus to match the noise vector η i . This is the set of noise (dummy) segments for partition i, S d,i . Iterate on generating text from the NLP language model until the required noise count is satisfied.</p><p>•</p><note type="other">Randomly partition S into N sets S i , i ∈ [N] where Pr[segment s goes to partition i</note><formula xml:id="formula_7">] = β = 1/N, s ∈ S.</formula><p>• For each partition i ∈ [N], shuffle the dummy segments in S d,i (after applying TTS and VC) with the segments in S i (after applying VC), and send it to the CSP i .</p><p>The first 4 steps in the above mechanism are performed in stage 3 in Prεεch <ref type="figure" target="#fig_0">(Fig. 1</ref>) while steps 5-6 are performed in stage 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4.5. Any topic model computed by CSP</head><formula xml:id="formula_8">i , i ∈ [N] from T CSP i S is (ε, δ)-DP.</formula><p>Proof. From Thm. 4.2 and Thm. 4.3, we conclude that the word histogram˜Hhistogram˜ histogram˜H i computed from T CSP i S is (ε, δ) -DP for distance d. Thm. 4.1 proves that the topic model from˜Hfrom˜ from˜H i is still (ε, δ)-DP as it is a post-processing computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4">Novelty of Prεεch's Use of Differential Privacy</head><p>Here, we summarize the key novelty in Prεεch's use of DP: (1) Typically, DP is applied to statistical analysis of numerical data where "noise" corresponds to numeric values. In contrast, in Prεεch, "noise" corresponds to concrete units -words. To tackle this challenge, we applied a series of operations (segmentation, shuffling, and partitioning) to transform the speech transcription into a BoW model, where the DP guarantee can be achieved. Moreover, the noise addition has to be done in the speech domain. This constraint results in new challenges: the lack of a priori access to the word histogram domain V , and generating indistinguishable dummy speech segments. (2) In our setting, the use of a DP mechanism does not introduce a privacy-utility trade-off from the speech transcription standpoint. Prεεch performs transcription one segment at a time. It keeps track of the timestamps of the dummy segments and completely removes their corresponding text from the final transcription (Sec. 4.2.3). This filtration step is achievable in Prεεch, unlike numeric applications of DP, because of the atomic nature of transcription. However, the dummy segments increase the monetary cost of transcription, resulting in a privacy-monetary cost trade-off as shown in <ref type="table">Table 3</ref>. To tackle this issue, Prεεch takes advantage of the presence of multiple CSPs (Sec. 4.5.2). Thus, the idea of utilizing multiple CSPs for cost reduction (Thm. 4.2) is a novel contribution.</p><p>(3) We introduce an additional parameter d, the distance between the pair of histograms, in our privacy definition (Defn. 4.1). Intuitively, d connects the privacy definition in the word histogram model, which is purely a formal representation, to a semantic privacy notion (e.g., <ref type="bibr" target="#b0">1</ref> distance between true and noisy topic models, Thm. 4.4) as shown in <ref type="figure" target="#fig_8">Fig. 6 and  7</ref>. This contribution builds on ideas like group privacy <ref type="bibr" target="#b11">[13]</ref> and generalized distance metrics <ref type="bibr" target="#b8">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5">Control Knobs</head><p>The construction of the DP word histogram provides the user with multiple control knobs for customization: Parameter d: According to Def. 4.1, from˜Hfrom˜ from˜H the CSP will not be able to distinguish between T CSP S and any other transcript that differs from T CSP S in d words from V . Thus, higher the value of d, larger is the ε-indistinguishability neighborhood for˜Hfor˜ for˜H and hence, better is the privacy guarantee. But it results in an increased amount of noise injection (hence, increased monetary cost -details in Sec. 7.3).</p><p>Vocabulary: The size of V is a control knob, specifically, the parameters m and ∆ and the number of out-of-domain words.</p><p>The trade-off here is: the larger the size of V , the greater is the scope of the privacy guarantee. However, the noise size scales with |V | and hence incurs higher cost (details in Sec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.3).</head><p>Voice transformation for noisy segments: Prεεch provides two options for noise synthesis -voice cloning and voice conversion. Voice cloning does not affect the transcription utility, measured in WER, because it does not apply any transformations on the original speaker's voice. However, it fails to protect the sensitive biometric information in S. Moreover, there is no guarantee that a strong adversary cannot develop a system that can distinguish the cloned speech segments from the original ones. This puts Prεεch's effectiveness at the risk of the arms race between the voice cloning system's performance and the adversary's strength. This limitation is addressed by voice conversion at the cost of transcription utility. We quantify these utility-privacy trade-offs in Sec. 7. Number of CSPs used for transcription: As discussed above, employing multiple CSPs lowers the monetary cost incurred. However, as shown in <ref type="table">Table 1</ref>, AWS has a higher WER than Google. Hence, using both the CSPs results in lower overall utility than just using Google's cloud service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Voice Conversion</head><p>Below, we discuss the two main categories of VC systems, highlighting their privacy-utility trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">One-to-One Voice Conversion</head><p>One-to-one VC maps a predefined source speaker voice to a target speaker voice. In Prεεch, we use sprocket <ref type="bibr" target="#b19">[21]</ref>, which is based on spectral conversion using a Gaussian mixture  model (GMM). Sprocket's training phase takes three steps: (1) acoustic features extraction of the source and target speakers samples, (2) time-alignment of the source and target features, and (3) GMM model training. During conversion, sprocket extracts the acoustic features of the new utterances, converts them using the learned GMM model, and generates the target waveform. Prεεch applies sprocket to convert the voice of all source speakers, including the synthesized dummy segments, into the same target speaker voice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Many-to-One Voice Conversion</head><p>For perfect voice privacy, the VC system should (1) map any voice (even if previously unseen) to the same target voice, (2) not leak any distinguishing acoustic features, and (3) operate on speech containing multiple speakers. To this end, Prεεch deploys the two-stage many-to-one VC <ref type="bibr" target="#b42">[44]</ref> mechanism. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the first stage is a phoneme classifier that transfers the speech utterance into phonetic posterior grams (PPG) matrix. A PPG is a time-aligned phonetic class <ref type="bibr" target="#b42">[44]</ref>, where a phoneme is the visual representation of a speech sound. Thus, the phoneme classifier removes the speakeridentifying acoustic features by mapping the spoken content into speaker-independent labels. In the second stage, a speech synthesizer converts the PPGs into the target voice.</p><p>The PPGs intermediate stage is irreversible and speakerindependent. It guarantees that the converted dummy segments S d and converted original segments S cannot be distinguished from each other. However, the actual implementation of the system carries many challenges. The first stage is a performance bottle-neck as it needs large phonetically aligned training data to generalize to new unseen voices. We overcome this challenge by generating a custom training speech dataset with aligned phonemes as described in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.3">Control Knobs</head><p>The aforementioned VC techniques present an interesting utility-usability-privacy trade-off. The one-to-one VC technique gives better accuracy than many-to-one VC since it is trained for a specific predefined set of source speakers (details in Sec. 7.4.1). However, this utility gain comes at the price of usability and privacy. First, unlike many-to-one VC, sprocket needs parallel training data -a set of utterances spoken by both the source and target speakers. Hence, it requires an enrollment phase to get the source speaker's voice samples, thereby limiting the scalability of Prεεch for previously unseen speakers. Second, one-to-one VC does not provide perfect indistinguishability. These two limitations are mitigated by applying many-to-one VC (Sec. 7.4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">End-to-End Threat Analysis</head><p>In this section, we go over the end-to-end system design of Prεεch and identify potential privacy vulnerabilities.</p><p>Voice Privacy: Many-to-one VC removes all the identifying features from S, like the speakers' voices, background noise, and recording hardware, thereby protecting voice privacy.</p><p>Textual Privacy: For sensitive word scrubbing, the best-case scenario from a privacy point of view is to have the user spell out the entire keyword list. However, due to its high usability overhead, Prεεch uses NER instead to identify named entities automatically from T OSP S . In Sec. 7.3.1, we empirically show that Prεεch can achieve near-perfect true positive rate in identifying the segments containing sensitive words. However, this is only an empirical result and is dataset dependent.</p><p>Our main defense against statistical analysis on the text is the DP guarantee on the word histogram. This DP guarantee would break down if the adversary can distinguish the dummy segments from the true segments. Many-to-one VC technique, by design, ensures that both sets of segments have the same acoustic features. However, the possibility of distinguishing them based on their textual features still remains. To address this threat, we rely on state-of-the-art NLP models with low perplexity (log-likelihood) scores to generate the dummy text corpus. The low perplexity scores ensure that the auto-generated text is as close as possible to the natural language generated by humans <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b34">36]</ref>. Although there is no formal guarantee about the adversary's ability to distinguish dummy and true segments based on their textual features, we have empirically analyzed this threat in Sec. 7.3.3 and Sec. 7.3.4. We leverage state-of-the-art NLP techniques to mount attacks on the dummy segments. Our results show that the adversary fails to distinguish between the dummy and true segments. However, the extent of such robustness is based on the efficacy of state-of-the-art NLP techniques.</p><p>Word correlations can also weaken the DP guarantee (d −w, if w is the maximum size of word groups with high correlation). This can be addressed by either increasing d or considering n-gram (n = w) word histograms. However, this would increase the requisite amount of dummy segments.</p><p>Long segments can also be a source of privacy vulnerability as each segment contains more contextual information. Hence, in the prototype Prεεch presented in the paper, we use short segments that contain at most two non-stop words.</p><p>Another weakness is related to vocabulary estimation, especially if some of the distribution-tail words are deemed to be sensitive. Prεεch provides no formal guarantees on the words that do not belong to V . Although our empirical evaluation shows that the OSP has a very high accuracy for the weighted estimation of V (Sec. 7.3.2), some sensitive distribution-tail words might still be missed due to the OSP's transcription errors. Additionally, our formal DP guarantee holds only for the word histogram (BOW ) on V . Textual analysis models other than BOW are empirically evaluated in Sec. 7.3.3 and Sec. 7.3.4.</p><p>Finally, if the CSP can reorder the segments (even partially since the speech file it receives contains dummy segments as well), it will be able to distinguish the dummy segments from the true ones and hence, learn the textual content of the file. For this again, we show empirically that current NLP techniques fail to reorder the segments (Sec. 7.3.4) even in the worst-case setting where all the segments go to one CSP. However, as before, this is an empirical result only. Formal Privacy Guarantee: For a speech file S, Prεεch provides perfect voice privacy (when using many-to-one VC) and an (ε, δ)-DP guarantee on the word histogram for the vocabulary considered (BOW ), under the assumption that the dummy segments are indistinguishable from the true segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Implementation</head><p>In this section, we describe the implementation details of Prεεch's building blocks (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>).</p><p>Segmentation: We implement the two-level hierarchical segmentation algorithm described in Sec. 4.3. The silence detection based segmentation is implemented using the Python pydub package <ref type="bibr" target="#b5">7</ref> . We used Praat 8 to extract the pitch information required for the second level of the segmentation algorithm. Sensitive Keyword Scrubbing: We use the NLP Python framework spaCy 9 for named entity recognition (NER) from the text. The keyword lists per each dataset can be found in the full paper <ref type="bibr" target="#b1">[3]</ref> . We employ PocketSphinx 10 for keyword spotting, a lightweight ASR that can detect keywords from continuous speech. It takes a list of words (in the text) and their respective sensitivity thresholds and returns segments that contain speech matching the words. PocketSphinx is a generic system that can detect any keyword specified in runtime; it is not trained on a pre-defined list of keywords and requires no per-user training or enrollment. Generating Dummy Segments: We use the open source implementation 11 of OpenAI's state-of-the-art NLP language model, <ref type="bibr">GPT 2 [36]</ref>, to generate the noise corpus.</p><p>Using this predictive model, we generate a large corpus representing the vocabulary of the evaluation datasets. An example of the generated text is available in the full paper <ref type="bibr" target="#b1">[3]</ref> . To generate the dummy segments, we segment each document at the same level as the speech segmentation algorithm. We build a hash table associating each vocabulary word with the segments that contain it. Prεεch uses a dummy segment only once per CSP to prevent it from identifying repetitions.</p><p>Text-to-Speech: We use the multi-speaker (voice cloning) TTS synthesizer <ref type="bibr" target="#b18">[20]</ref> to generate the speech files corresponding to the dummy segments. We use a pre-existing system implementation and pretrained models 12 .</p><p>One-to-One Voice Conversion: We use the open-source sprocket software <ref type="bibr" target="#b11">13</ref> . As described in Sec. 4.6.1, sprocket requires a parallel training data and the target voice should be unified for all source speakers. For the VCTK datasets, we use speaker p306 as the target voice. Since we also evaluate Prεεch on non-standard datasets (Facebook and Carpenter cases), we had to construct the parallel training data for their source speakers. For this, we use TTS to generate the required target voice training utterances in a single synthetic voice.</p><p>Many-to-One Voice Conversion: We utilize pre-existing architectures and hyperparameters 14 for the two-stage many-toone VC <ref type="bibr" target="#b42">[44]</ref> mechanism, shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. The first network, net 1 , is trained on a set of {raw speech, aligned phoneme labels} samples from a multi-speaker corpus, where the labels are the set of 61 phonemes from the TIMIT dataset. The only corpus that has a manual transcription of speech to the phonemes' level is the TIMIT dataset -a limited dataset. We found that training net 1 on TIMIT alone results in an inferior WER performance. For better generalization, we augment the training set by automatically generating phoneme-aligned transcriptions of standard ASR corpora. We use the Montreal Forced Aligner 15 to generate the aligned phonemes on LibriSpeech and TED-LIUM <ref type="bibr" target="#b36">[38]</ref> datasets. The second network, net 2 , synthesizes the phonemes into the target speaker's voice. It is trained on a set of {PPGs, raw speech} pairs from the target speaker's voice. We use the trained net 1 to generate the PPGs data for training net 2 . As such, we only need speech samples of the target speaker to train net 2 . This procedure also allows net 2 to account for net 1 's errors. We use Ljspeech <ref type="bibr" target="#b14">16</ref> as the target voice for its relatively large size -24 hours of speech from a single female.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Evaluation</head><p>We evaluate how well Prεεch meets the design objectives of Sec. 4. Specifically, we aim to answer the following questions:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cloning</head><p>One   We answer the first three questions for a prototype implementation of Prεεch that provides the maximum degree of formal privacy and hence, the least utility. For evaluating Q4, we relax the privacy guarantee to obtain utility and usability improvements.</p><p>Prototype Prεεch: For the prototype Prεεch presented in the paper: (1) segmentation length is adjusted to ensure that each segment contains at most two non-stop words (2) noisy segments are generated via the GPT2 language model (3) a single CSP (Google) is utilized (4) many-to-one VC is applied to both the dummy and true segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Q1. Transcription Utility</head><p>We assess the transcription WER after deploying end-to-end Prεεch on the non-standard datasets. Recall that <ref type="table">Table 1</ref> in Sec. 2.2 shows the baseline WER performance of the CSP and OSP before applying Prεεch. WER Analysis: Column 4 in <ref type="table" target="#tab_3">Table 2</ref> shows the end-to-end WER for the prototype Prεεch which represents the accumulative effect of segmentation, SWS, and many-to-one VC. Although VC is the main contributor to Prεεch's WER, as is evident from Sec. 7.4.1 and Sec. 7.3.1, there are two main observations. First, many-to-one VC is superior to Deep Speech. Specifically, Prεεch's relative improvement over Deep Speech ranges from 11.91% to 32.25% over the evaluation datasets (except for Carpenter2). Recall that we trained the VC system using standard ASR corpora, while we evaluate the WER on non-standard cases. Still, Prεεch's WER is superior to that of Deep Speech, which has been trained through hundreds of hours of speech data. Second, Prεεch does not have the same performance for all the datasets. This observation arises again from the lack of diversity in our VC training set. For example, the speaker in Carpenter 1 speaks loudly, allowing VC to perform well. On the other hand, the second speaker (Carpenter 2) is not as clear or loud, which results in an inferior VC performance. This observation is consistent with Deep Speech as well. Our experiments show that these results can be improved by adding samples of the source speaker voice to the training pipeline of net 1 and net 2 . We chose not to go with this approach as this limits the usability of the system, and in such a case sprocket (Sec. 7.4.1) would be a better choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Q2. Voice Biometric Privacy</head><p>To test the voice biometric privacy, we conduct two experiments using the voice analysis APIs (details in Sec. 3.1). In the first experiment, we assess the CSP's ability to separate speech belonging to different speakers after Prεεch applies the VC system. On our multi-speaker datasets, IBM diarization API concludes that there is only one speaker present.</p><p>Furthermore, we run the diarization API after adding the dummy segments (after TTS and VC). Again, the API detects the presence of only one speaker. Thus, not only does Prεεch hide the speaker's biometrics and map them to a single target speaker but also ensures noise indistinguishability, which is key to its privacy properties.</p><p>The second experiment tests Prεεch's privacy properties against a stronger adversary, who has access to samples from the true speakers. We enroll segments from the true speakers as well as the fake target speaker to Azure's Speaker Identification API. We pass the segments from Prεεch (after adding dummy segments and applying VC) to the API. When manyto-one VC is applied, in all evaluation cases, the API identifies the segments as belonging to the fake target speaker. Not a single segment was matched to the original speaker. Both experiments show that prototype Prεεch is effective in sanitizing the speaker's voice and ensuring noise indistinguishability.  <ref type="table">Table 3</ref>: Number of extra words due to dummy segments and the additional monetary cost in USD with varying d, at ε = 1 and δ = 0.05. <ref type="figure">Figure 6</ref>: Sentiment scores heatmap of 10 documents with varying d, at ε = 1 and δ = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Q3. Textual Privacy</head><p>We perform an extensive evaluation of the textual privacy, including sensitive word scrubbing, analysis of the DP mechanism, and defense against statistical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Sensitive Words Scrubbing:</head><p>We run PocketSphinx keyword spotting on each dataset at different sensitivity scores ranging from 0.2 to 1 17 . <ref type="figure" target="#fig_7">Fig. 5</ref> shows the detection true positive rate (TPR) versus the false positive rate (FPR) at different sensitivity scores. As the figure shows, the sensitivity score is a trade-off knob between privacy (high TPR) and utility (low FPR). We observe that Prεεch is able to achieve almost perfect TPR with low FPR values. Next, we evaluate the impact of SWS on the transcription utility. We set a sensitivity score of 0.95 for all the datasets to have a near-perfect TPR while minimizing the FPR. Our experiments show that the total duration of the segments flagged with sensitive keywords at this score is: 0.13%, 0.06%, 0.18%, 0.20%, and 0.08% of the total duration of each dataset in <ref type="figure" target="#fig_7">Fig. 5</ref>. Then, we transcribe the sensitive-flagged segments using Deep Speech. The overall transcription accuracy after SWS (i.e., equivalent to choosing voice cloning in Prεεch as cloning results in no addition WER) is presented in the second column of <ref type="table" target="#tab_3">Table 2</ref>. Since the segments are short, the portion of speech transcribed locally is limited. Hence, the impact of the OSP transcription errors is not significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">DP Mechanism Analysis:</head><p>We follow the DP mechanism described in Sec. 4.5.3.</p><p>Vocabulary Estimation: We estimate the vocabulary V using the OSP transcript. Let W represent the set of unique words in T g S . We define the accuracy of the vocabulary estimation, D acc , as the ratio between the count of the correctly identified unique words from T OSP S , |W | est , and the count of the unique words in T g S , |W |. For our datasets, the domain estimation accuracy is at least 75.54%. We also calculate the weighted estimation accuracy defined as: <ref type="bibr" target="#b15">17</ref> The sensitive keywords list for each dataset is in the full paper <ref type="bibr" target="#b1">[3]</ref> .</p><formula xml:id="formula_9">D weighted = ∑ P(w est ).1 w est ∈W |W |</formula><p>where P(w est ) is the weight of the estimated word w est in T g S . D weighted is more informative since it gives higher weights to the most frequent words in T g S . The weighted estimation accuracy is 99.989% in our datasets.</p><p>From W est we select V over which we apply the DP mechanism. Additionally, we extend our vocabulary to contain a set of random words from the English dictionary.</p><p>Histogram Distance: We analyze the distance between the original and noisy histograms (after applying Prεεch) and its impact on the cost of online transcription. Because of the nature of Prεεch's DP mechanism, the noise addition depends on four values only: |V |, ε, δ, and d.</p><p>For all our experiments, we fix the values of ε = 1 and δ = 0.05. <ref type="table">Table 3</ref> shows the amount of noise (dummy words) and their transcription cost in USD 18 for each of the evaluation datasets at different values of d. Each dataset has a different vocabulary size |V | and word count. The increase in the vocabulary size requires adding more dummy segments to maintain the same privacy level. In Prεεch, adding more noise comes at an increased monetary cost, instead of a utility loss. The table highlights the trade-off between privacy and the cost of adding noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Statistical Analysis</head><p>In this section, we evaluate the statistical analyses (details in Sec. 3.2) performed by the adversary to extract textual information on the noisy transcripts obtained from Prεεch.</p><p>Topic Model: We generate the topic models from the documents corresponding to the original and noisy word histograms, and evaluate their 1 distance. The topic model operates on a corpus of documents; hence we include eight more Supreme Court cases to our original evaluation datasets <ref type="bibr">(Face- book and Carpenter)</ref>. In this evaluation, we treat all these ten documents as one corpus; we aim to generate the topic model before and after applying Prεεch to the whole corpus.</p><p>We use AWS Comprehend API to generate the topic model. The API needs the number of topics as a hyperparameter that ranges from 1 to 100. Based on our apriori knowledge of the </p><formula xml:id="formula_10">T = {T 1 , · · · , T t } is a set of t topics where each T i , i ∈ [t]</formula><p>is a word distribution. We use the Hungarian algorithm to match each noisy topic T i ∈ T to its closest match in T , the true topic model. We evaluate the topics 1 distance for 21 runs. At each run, we generate a random noise vector per document, select the corresponding dummy segments, and evaluate the topic model on the set of original and noisy documents. <ref type="figure" target="#fig_8">Fig. 7</ref> shows the empirical CDF of the topics 1 distance at different values of d. As the figure shows, the higher the distance parameter d, the larger is the 1 distance between true and noisy topics.</p><p>Stylometry: In this experiment, we assume that the CSP applies stylometry analysis on T CSP S in an attempt to attribute it to an auxiliary document whose authors are known to the CSP. To evaluate the worst-case scenario, we assume the adversary possesses the original document T g S , and we compute the <ref type="bibr">2</ref>  &amp; Government. Running the API on Prεεch processed documents, using an extended-vocabulary (i.e., contains random words), dropped the classification accuracy to 0%. None of the documents got identified as legal, law, or government even at the smallest distance parameter value d = 2. Although a portion of the noise words belongs to the original Law &amp; Government category, segmentation, shuffling, and the out-ofdomain noise words successfully confuse the classifier. Sentiment Analysis: Sentiment analysis generates a score in the [−1, 1] range, which reflects the positive, negative, or neutral attitude in the text. First, we evaluate the sentiment scores of the original ten documents. For all of them, the score falls between −0.2 and −0.9, which is expected as they represent legal documents. Next, we evaluate the scores from Prεεch processed documents considering an extended-vocabulary. We find that all scores increase towards a more positive opinion. <ref type="figure">Fig. 6</ref> shows a heatmap of the sentiment scores as we change the distance parameter d for the then evaluation documents. Thus, Prεεch's two-pronged approach -1) addition of extended-vocabulary noise 2) removal of ordering information via segmentation and shuffling, proves to be effective. In a setting where the adversary has no apriori knowledge about the general domain of the processed speech, the noise addition mechanism gains extend from DP guarantee over the histogram to other NLP analyses as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">Indistinguishability Of Dummy Segments</head><p>The indistinguishability of the dummy segments is critical for upholding the DP guarantee in Prεεch. We perform two experiments to analyze whether current state-of-the-art NLP models can distinguish the dummy segments from their textual content. Most Probable Next Segment: In this experiment, the adversary has the advantage of knowing a true segment S t that is at least a few sentences long from the Facebook dataset. We use the state-of-the-art GPT 20 language model by OpenAI <ref type="bibr" target="#b33">[35]</ref> to determine the most probable next segment following S t using the model's perplexity score. In NLP, the perplexity score measures the likelihood that a piece of text follows the language model. We get the perplexity score of stitching S t to each of the other segments at the CSP. The segment with the lowest perplexity score is selected as the most probable next segment. We iterate over all the true segments of the Facebook dataset, selecting them as S t . We observed that a dummy segment is selected as the most probable next segment in 53.84% of the cases. This result shows that the language model could not differentiate between the true and dummy segments even when part of the true text is known to the adversary. Segments Re-ordering: Next, we attempt to re-order the segments based on the perplexity score. We give the adversary the advantage of knowing the first true segment S 0 . We get the perplexity score of S 0 , followed by each of the other segments. The segment with the lowest score is selected as the second segment S 1 and so on. We use the normalized Kendall tau rank distance K τ to measure the sorted-ness of the re-ordered segments. The normalized K τ distance measures the number of pairwise disagreements between two ranking lists, where 0 means perfect sorting, and 1 means the lists are reversed. The K τ score for running this experiment on the Facebook dataset is 0.512, which means that the re-ordered list is randomly shuffled w.r.t the true order. Hence, our attempt to re-order the segments has failed. These empirical results show that it is hard to re-order the segments or distinguish the dummy segments. This is expected due to three reasons: (1) the segments are very short; (2) the dummy segments are generated using a state-of-the-art language model; and (3) we observed that most of the transcription errors happen in the first and last words of a segment due to breaking the context. These errors add to the difficulty of re-ordering. Moreover, if the user partitions S among multiple CSP's (Sec.4.5.3), then consecutive segments would not go to the same CSP with high probability. This setting would increase Prεεch's protection against re-ordering attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Q4: Flexibility of the Control Knobs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.1">Utility-Privacy Trade-off</head><p>In this section, we empirically evaluate the controls knobs that provide a utility-privacy trade-off.</p><p>Minimum segment length: <ref type="figure" target="#fig_9">Fig. 8</ref> shows the trade-off between the number of words per segment and WER as function of the minimum segment length. As expected, increasing the minimum duration of a segment results in an increase in the number of words per segment. The WER in turn drops when the number of words per segment increase as the transcription service has more textual context. However, it can lead to potential privacy leakage. The results in <ref type="figure" target="#fig_9">Fig. 8</ref> indicate that for two real-world datasets, the number of words per segment can be kept between 2 and 3 with an acceptable degradation of the WER.</p><p>Voice Cloning: Voice cloning does not affect the true segments (it is only applied to dummy segments), resulting in no additional WER degradation. The WER for deploying voice cloning is incurred only due to segmentation and SWS. Thus, as shown in column 2 of <ref type="table" target="#tab_3">Table 2</ref>, the relative improvement in WER ranges from 44% to 80% over Deep Speech. This approach, however, has two limitations. First, the speaker's voice biometrics from S are not protected. Second, there is no guarantee that an adversary would not be able to distinguish the cloned speech segments from the original ones.</p><p>Sensitivity score of KWS: As shown in <ref type="figure" target="#fig_7">Fig. 5</ref>, lower the sensitivity score, higher is the TPR and hence greater is the privacy (most prominent in the Carpenter2 dataset). However, this also increases the FPR, which means a larger number of non-sensitive segments are transcribed via the OSP resulting in reduced accuracy.</p><p>One-To-One VC: <ref type="table" target="#tab_3">Table 2</ref>, column 3, shows that one-to-one VC outperforms many-to-one VC on most of the datasets. This result is expected since sprocket is trained and tested on the same set of source speakers while the many-to-one VC system generalizes to previously unseen speakers. We observe that the improvement for the VCTK dataset is more significant than others. Recall that in our one-to-one VC implementation in Sec. 6, the target voice for VCTK is a natural voice -speaker p306. The target voice for the other datasets is a synthetic one, which hinders the quality of the converted voice and the transcription accuracy. We investigate this observation by training sprocket for VCTK on a synthetic target voice as well. The WER then increased to 19.33% and 9.21% for p266 and p262. Hence, we attribute the difference in the relative improvement to the target voice naturalness. In practice, the target voice could easily be a natural pre-recorded voice, and the users are asked to repeat the same utterances at the enrollment phase. However, the one-to-one VC technique suffers from some privacy loss. The one-to-one VC system translates the acoustic features from a source to a target speaker's voice. Hence, it may leak some features from the source speaker. We observed that one-to-one VC is vulnerable to speaker identification analysis. Specifically, using Azure's Speaker Identification API, 10% of the voice-converted segments using sprocket were identified to their true speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.2">Usability-Privacy Trade-off</head><p>In our setting, usability can be measured along three dimensions: latency, monetary cost, and implementation overhead. However, we would like to stress that Prεεch is not designed for real-time speech transcription. Hence, latency is not a primary concern for Prεεch. Nevertheless, we include it in the following discussion for the sake of completeness. Latency Evaluations: Note that all the operations of Prεεch are performed on speech segments. Hence, the latency is linear in the number of segments. We evaluate the end-to-end system latency per segment (with length ∼ 6s) for the OSP, the CSP, and Prεεch; the latency values are 2.17s, 1.70s, and 14.90s, respectively. We observe that the overhead of Prεεch is mostly attributed to the many-to-one VC (11s per segment on average). When voice cloning (or one-to-one VC) is applied instead, Prεεch's end-to-end per segment latency reduces to 3.90s (or 11.47s) at the expense of a privacy loss as discussed in Sec.7.4.1.</p><p>Vocabulary Size: Considering a larger V (Sec. 4.5.3) increases the scope of the DP guarantee. For example, adding external words provides protection against statistical analysis like text classification (Sec.7.3). However, larger V results in increased amount of dummy segments and hence, increased monetary cost <ref type="table">(Table 3)</ref>. For example, extending V by ∼ 1000 out-of-domain words for the Carpenter dataset incurred a total cost of $25 at d = 15.</p><p>Distance Parameter d: As explained in Sec. 4.5.2, larger the value of d, greater is the scope of privacy. However, the amount of required noise increases by d. For example, for the dataset VCTK p266, increasing d from 2 to 15 increases the cost by roughly $5 <ref type="table">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.3">Utility-Usability Trade-off</head><p>The following control knobs provide a venue for customizing the utility-usability trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of CSPs:</head><p>As discussed in Sec. 4.5.2, using multiple CSPs reduces the amount of dummy segments (and hence, the monetary cost) in Prεεch. However, it comes at the price of utility; the transcription accuracy of the different available CSPs varies. For example, from <ref type="table">Table 1</ref>, we observe that AWS has a higher WER than Google. Thus, using multiple CSPs may result in a lower mean utility.</p><p>One-to-One VC: As discussed above, one-to-one VC technique has lower WER than many-to-one VC technique <ref type="table" target="#tab_3">(Table  2)</ref>. However, it requires access to representative samples of the source speaker voice for parallel training thereby limiting scalability for previously unseen speakers (Sec. 4.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>In this section, we provide a summary of the related work.</p><p>Privacy by Design: One class of approaches redesigns the speech recognition pipeline to be private by design. For example, Srivastava et al. proposes an encoder-decoder architecture for speech recognition <ref type="bibr" target="#b40">[42]</ref>. Other approaches address the problem in an SMC setting by representing the basic operations of a traditional ASR system using cryptographic primitives <ref type="bibr" target="#b30">[32]</ref>. VoiceGuard is a system that performs ASR in the trusted execution environment of a processor <ref type="bibr" target="#b6">[8]</ref>. However, these approaches require redesigning the existing systems.</p><p>Speech Sanitization: Recent approaches have considered the problem from a similar perspective as ours. They sanitize the speech before sending it to the CSP. One such approach randomly perturbs the MFCC, pitch, tempo, and timing features of a speech before applying speech recognition <ref type="bibr" target="#b43">[45]</ref>. Others sanitize the speaker's voice using vocal tract length normalization (VTLN) <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b32">34]</ref>. A recent approach modifies the features relevant to emotions from an audio signal, makes them less sensitive through a GAN <ref type="bibr" target="#b2">[4]</ref>. Last, adversarial attacks against speaker identification systems can provide some privacy properties. These approaches apply minimal perturbations to the speech file to mislead a speaker identification network <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b20">22]</ref>.</p><p>These approaches are different from ours in two ways. First, they do not consider the textual content of the speech signal. The only exception is the approach by Qian et al. <ref type="bibr" target="#b32">[34]</ref>, which addresses the problem of private publication of speech datasets. This approach requires a text transcript with the audio file, which is not the case for the speech transcription task. In addressing the textual privacy of a speech signal, Prεεch adds indistinguishable noise to the speech file. The proposed techniques fail to provide this property. Second, the approaches above only consider voice privacy against a limited set of features, such as speaker identification or emotion recognition. Prεεch applies many-to-one VC to provide perfect voice privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this paper, we have proposed Prεεch, an end-to-end system for speech transcription that (1) protects the users' privacy along the acoustic and textual dimensions at (2) an improved performance relative to offline ASR, (3) while providing customizable utility, usability, and privacy trade-offs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: High-level overview of Prεεch, showing the knobs where a user can tune the associated trade-offs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of Prεεch's segmentation algorithm. The coarse segments in light gray. The absence of pitch information indicate non-speech instances, which further breaks down the coarse segments into finer segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>stemmed words from which T g S is constructed. Let c i denote the frequency of the word w i ∈ V in T g S . As is typical in the NLP literature, we model the transcription as a bag of words: BoW = {w i : c i |w i ∈ V }. Additionally, let H represent [c i ] -the count vector of BoW . In other words, the bag of words model represents a histogram on the vocabulary, i.e., a mapping from V to N |V | .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 4 .</head><label>4</label><figDesc>2 (Truncated Laplace mechanism for his- togram). Given a histogram H, the truncated Laplace mech- anism, Lp(ε, δ, d), adds a non-negative integer noise vector [max(η, 0)] |V | to H, where η follows a distribution, denoted by L(ε, δ, d) with a p.d.f Pr[η = x] = p · e −(ε/d)|x−η 0 | , where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The word cloud of the Facebook dataset visualizing the histogram as it changes after adding different levels of noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>7.3). For computing the TF-IDF values, IDF is computed using an external NLP corpus like Wikipedia articles. Thus formally, V = {w|w ∈ { top m per- centile of the most frequent words in T OSP S } ∪ { words with TF-IDF value ≥ ∆ in T OSP S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An illustration of the many-to-one VC pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: ROC curve for sensitive words detection at different values of the sensitivity score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Topics 1 distance CDF at d = 2, 5, and 15 for t = 8, 10, 12, and 14</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Segmentation trade-off between utility and privacy. WER(%) is measured using Google Cloud Speech-to-Text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. For this dataset, we evaluate a total of 40 minutes of speech from the advocates in the case.</figDesc><table>Datasets 

Google AWS Deep Speech 

Standard 
LibriSpeech 
9.14 
8.83 
9.37 
DAPS 
6.70 
7.53 
10.65 
TIMIT TEST 
6.27 
7.11 
20.08 

Non-Standard 

VCTK p266 
5.15 
10.09 
26.72 
VCTK p262 
4.53 
7.87 
15.97 
Facebook 1 
5.76 
7.45 
24.72 
Facebook 2 
3.07 
8.19 
26.61 
Facebook 3 
8.32 
9.42 
30.72 
Carpenter 1 
9.44 
9.44 
25.85 
Carpenter 2 
9.22 
11.53 
39.71 

Table 1: WER (%) comparison of cloud services, Google and 
AWS, versus the state-of-the-art offline system, Deep Speech. 

Accuracy Comparison: Table 1 presents the WER compar-
ison results. The results show that the CSPs are superior to 
the OSP on all the datasets. The performance gap, however, 
is more significant on the non-standard datasets; the CSP 
outperforms Deep Speech by 60% to 80% in WER. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>WER (%) of end-to-end Prεεch which represents the accumulative 
effect of segmentation, SWS, and different settings of voice privacy and its 
relative improvement in (%) over OSP (Deep Speech). 

0.95 
í¿. 
í¿. 
í¿. í¿ í¿. í¿, í¿. í¿ 

</table></figure>

			<note place="foot" n="7"> https://pypi.org/project/pydub/ 8 http://www.fon.hum.uva.nl/praat/ 9 https://github.com/explosion/spaCy 10 https://github.com/cmusphinx/pocketsphinx 11 https://github.com/huggingface/transformers</note>

			<note place="foot" n="12"> https://github.com/CorentinJ/Real-Time-Voice-Cloning 13 https://github.com/k2kobayashi/sprocket 14 https://github.com/andabi/deep-voice-conversion 15 https://montreal-forced-aligner.readthedocs.io/en/latest/ 16 https://keithito.com/LJ-Speech-Dataset/</note>

			<note place="foot" n="18"> The pricing model of Google Speech-to-Text is: $0.009 / 15 seconds.</note>

			<note place="foot" n="20"> https://github.com/huggingface/transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The work reported in this paper was supported in part by the NSF under grants <ref type="bibr">1661036, 1838733, 1942014, and 1931364</ref>. We also acknowledge Google for providing us with Google Cloud Platform credits and NVIDIA Corporation with the donation of the Quadro P6000 GPU used for this research. We would like to thank the anonymous reviewers for their useful comments and Micah Sherr for shepherding this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An all-neural on-device speech recognizer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramanathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04198</idno>
		<title level="m">Preech: A system for privacy-preserving speech transcription</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Emotionless: Privacy-preserving speech analysis for voice assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aloufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haddadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boyle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03632</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ehrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01816</idno>
		<title level="m">Shrinkwrap: Differentially-private query processing in private data federations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Accurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
		<respStmt>
			<orgName>Institute of Phonetic SciencesUniversity of Amsterdam</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Voiceguard: Secure and private speech processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Frassetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weinert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1303" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Attacking speaker recognition with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02384</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Broadening the scope of differential privacy using metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatzikokolakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Andrés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Bordenabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Palamidessi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Privacy Enhancing Technologies</title>
		<editor>E. De Cristofaro and M. Wright</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="82" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Publishing set-valued data via differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1087" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Assessing privacy risk in outsourcing. Assessing Privacy Risk in Outsourcing/AHIMA, American Health Information Management Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Davino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised named-entity extraction from the web: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data mining with differential privacy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="493" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Darpa timit acoustic-phonetic continous speech corpus cd-rom</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
	<note>nist speech disc 1-1.1. NASA STI/Recon technical report n, 93</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.6705</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">Lopez</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="4480" to="4490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">sprocket: Open-source voice conversion software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Odyssey</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="203" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fooling endto-end speaker verification with adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vulnerability in speaker verification-a study of technical impostor techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blomberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth European Conference on Speech Communication and Technology</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lorenzo-Trueba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04262</idno>
		<title level="m">The voice conversion challenge 2018: Promoting development of parallel and nonparallel methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Investigating the computer security practices and needs of journalists</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Holliday</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Roesner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th {USENIX} Security Symposium ({USENIX} Security 15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="399" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards directly modeling raw speech signal for speaker verification using cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Muckenhirn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Doss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Marcell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4884" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Characterization of glottal activity from speech signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S R</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yegnanarayana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing letters</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="469" to="472" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Can we automatically transform speech recorded on common consumer devices in realworld environments into professional production quality speech?-a dataset, insights, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Mysore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1006" to="1010" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The gdpr &amp; speech data: Reflections of legal and technology communities, first steps towards a common understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03458</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A scalable framework for stylometric analysis query processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nutanong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM 2016</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1125" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2015</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Privacy-preserving speech processing: cryptographic and string-matching frameworks show promise. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="62" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11460</idno>
		<title level="m">Anonymize and sanitize voice input on mobile devices</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving speech data publishing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2018-IEEE Conference on Computer Communications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1079" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Labeled lda: A supervised topic model for credit attribution in multi-labeled corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2009</title>
		<meeting><address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ted-lium: an automatic speech recognition dedicated corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="125" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Automatic speaker, age-group and gender identification from children&apos;s speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jančovič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="141" to="156" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Computational paralinguistics: emotion, affect and personality in speech and language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Paralinguistics in speech and language-state-of-the-art and the challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="39" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH 2019</title>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Probabilistic topic models. Handbook of latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">427</biblScope>
			<biblScope unit="page" from="424" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Phonetic posteriorgrams for many-to-one voice conversion without parallel data training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME 2016</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">You talk too much: Limiting privacy exposure via voice input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sherr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Privacy Engineering (IWPE)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
		<respStmt>
			<orgName>University of Edinburgh. The Centre for Speech Technology Research (CSTR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spoofing and countermeasures for speaker verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="130" to="153" />
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
