<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/atharsefid/Desktop/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2020-09-29T02:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This paper is included in the Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST &apos;17). Open access to the Proceedings of the 15th USENIX Conference on File and Storage Technologies is sponsored by USENIX. Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February 27-March 2, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqin</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Hao</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqin</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaicheng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Hao</forename><surname>Tong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminatahan</forename><surname>Sundararaman</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Parallel Machines</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Swaminathan Sundararaman</orgName>
								<address>
									<country>Parallel Machines</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">University of Chicago</orgName>
								<orgName type="institution" key="instit2">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">This paper is included in the Proceedings of the 15th USENIX Conference on File and Storage Technologies (FAST &apos;17). Open access to the Proceedings of the 15th USENIX Conference on File and Storage Technologies is sponsored by USENIX. Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February 27-March 2, 2017</date>
						</imprint>
					</monogr>
					<note>https://www.usenix.org/conference/fast17/technical-sessions/presentation/yan</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>TTFLASH is a &quot;tiny-tail&quot; flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GC-blocked I/Os with four novel strategies: plane-blocking GC, rotating GC, GC-tolerant read, and GC-tolerant flush. It is built on three SSD internal advancements: powerful controllers, parity-based RAIN, and capacitor-backed RAM, but is dependent on the use of intra-plane copyback operations. We show that TTFLASH comes significantly close to a &quot;no-GC&quot; scenario. Specifically, between 99-99.99th percentiles, TTFLASH is only 1.0 to 2.6× slower than the no-GC case, while a base approach suffers from 5-138× GC-induced slowdowns.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Flash storage has become the mainstream destination for storage users. The SSD consumer market continues to grow at a significant rate <ref type="bibr" target="#b6">[8]</ref>, SSD-backed cloud-VM instances are becoming the norm <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b10">13]</ref>, and flash/SSD arrays are a popular solution for high-end storage servers <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b42">44]</ref>. From the users side, they demand fast and stable latencies <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b26">28]</ref>. However, SSDs do not always deliver the performance that users expect <ref type="bibr" target="#b12">[15]</ref>. Some even suggest that flash storage "may not save the world" (due to the tail latency problem) <ref type="bibr" target="#b3">[5]</ref>. Some recent works dissect why it is hard to meet SLAs with SSDs <ref type="bibr" target="#b34">[36]</ref> and reveal high performance variability in 7 million hours of SSDs deployments <ref type="bibr" target="#b28">[30]</ref>.</p><p>The core problem of flash performance instability is the well-known and "notorious" garbage collection (GC) process. A GC operation causes long delays as the SSD cannot serve (blocks) incoming I/Os. Due to an ongoing GC, read latency variance can increase by 100× <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b22">24]</ref>. In the last decade, there is a large body of work that reduces the number of GC operations with a variety of novel techniques <ref type="bibr" target="#b27">[29,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b46">48]</ref>. However, we find almost no work in literature that attempts to eliminate the blocking nature of GC operations and deliver steady SSD performance in long runs.</p><p>We address this urgent issue with "tiny-tail" flash drive (TTFLASH), a GC-tolerant SSD that can deliver and guarantee stable performance. The goal of TTFLASH is to eliminate GC-induced tail latencies by circumventing GC-blocked I/Os. That is, ideally there should be no I/O that will be blocked by a GC operation, thus creating a flash storage that behaves close to a "no-GC" scenario. The key enabler is that SSD internal technology has changed in many ways, which we exploit to build novel GC-tolerant approaches.</p><p>Specifically, there are three major SSD technological advancements that we leverage for building TTFLASH. First, we leverage the increasing power and speed of today's flash controllers that enable more complex logic (e.g., multi-threading, I/O concurrency, fine-grained I/O management) to be implemented at the controller. Second, we exploit the use of Redundant Array of Independent NAND (RAIN). Bit error rates of modern SSDs have increased to the point that ECC is no longer deemed sufficient <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b43">45]</ref>. Due to this increasing failure, modern commercial SSDs employ parity-based redundancies (RAIN) as a standard data protection mechanism <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b9">12]</ref>. By using RAIN, we can circumvent GCblocked read I/Os with parity regeneration. Finally, modern SSDs come with a large RAM buffer (hundreds of MBs) backed by "super capacitors" <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b11">14]</ref>, which we leverage to mask write tail latencies from GC operations.</p><p>The timely combination of the technology practices above enables four new strategies in TTFLASH: (a) plane-blocking GC, which shifts GC blocking from coarse granularities (controller/channel) to a finer granularity (plane level), which depends on intra-plane copyback operations, (b) GC-tolerant read, which exploits RAIN parity-based redundancy to proactively generate contents of read I/Os that are blocked by ongoing GCs, (c) rotating GC, which schedules GC in a rotating fashion to enforce at most one active GC in every plane group, hence the guarantee to always cut "one tail" with one parity, and finally (d) GC-tolerant flush, which evicts buffered writes from capacitor-backed RAM to flash pages, free from GC blocking.</p><p>One constraint of TTFLASH is its dependency on intraplane copybacks where GC-ed pages move within a plane without the data flowing through the SSD controller, hence skipping ECC checks for garbage collected pages, which may reduce data reliability. The full extent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GC-Induced Tail Latency</head><p>We present two experiments that show GC cascading impacts, which motivate our work. Each experiment runs on a late-2014 128GB Samsung SM951, which can sustain 70 "KWPS" (70K of 4KB random writes/sec).</p><p>In <ref type="figure">Figure 1a</ref>, we ran a foreground thread that executes 16-KB random reads, concurrently with background threads that inject 4-KB random-write noises at 1, 2.5, and 5 KWPS (far below the max 70 KWPS) across three experiments. We measure L i , the latency of every 16-KB foreground read. <ref type="figure">Figure 1a</ref> plots the CDF of L i , clearly showing that more frequent GCs (from moreintense random writes) block incoming reads and create longer tail latencies. To show the tail is induced by GC, not queueing delays, we ran the same experiments but now with random-read noises (1, 2.5, and 5 KRPS. The read-noise results are plotted as the three overlapping thin lines marked "ReadNoise," which represents a  <ref type="figure">Figure 1</ref>: GC-Induced Tail Latencies (Section 2). perfect no-GC scenario. As shown, with 5 KWPS noise, read operations become 15×, 19×, and 96× slower compared to no-GC scenarios, at 90 th , 95 th and 99 th percentiles, respectively.</p><p>In <ref type="figure">Figure 1b</ref>, we keep the 5-KWPS noise and now vary the I/O size of the foreground random reads <ref type="bibr" target="#b6">(8,</ref><ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr">64</ref>, and 128 KB across five experiments). Supposedly, a 2× larger read should only consume 2× longer latency. However, the figure shows that GC induces more tail latencies in larger reads. For example, at 85 th percentile, a 64-KB read is 4× slower than a 32-KB read. The core of the problem is this: if a single page of a large read is blocked by a GC, the entire read cannot complete; as read size increases, the probability of one of the pages being blocked by GC also increases, as we explain later ( §3, §4.1). The pattern is more obvious when compared to the same experiments but with 5-KRPS noises (the five thin gray lines marked "ReadNoise").</p><p>For a fairer experiment, because flash read latency is typically 20× faster than write latency, we also ran read noises that are 20× more intense and another where read noises is 20× larger in size. The results are similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SSD Primer: GC Blocking</head><p>Before presenting TTFLASH, we first need to describe SSD internals that are essential for understanding GC blocking. This section describes how GC operates from the view of the physical hardware. SSD Layout: <ref type="figure" target="#fig_0">Figure 2</ref> shows a basic SSD internal layout. Data and command transfers are sent via parallel channels (C 1 ..C N ). A channel connects multiple flash planes; 1-4 planes can be packaged as a single chip (dashed box). A plane contains blocks of flash pages. In every plane, there is a 4-KB register support; all flash reads/writes must transfer through the plane register. The controller is connected to a capacitor-backed RAM used for multiple purposes (e.g., write buffering). For clarity, we use concrete parameter values shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RAM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C0</head><p>... GC operation (4 main steps): When used-page count increases above a certain threshold (e.g., 70%), a GC will start. A possible GC operation reads valid pages from an old block, writes them to a free block, and erases the old block, within the same plane. <ref type="figure" target="#fig_0">Figure 2</ref> shows two copybacks in a GC-ing plane (two valid pages being copied to a free block). Most importantly, with 4-KB register support in every plane, page copybacks happen within the GC-ing plane without using the channel <ref type="bibr">[11,</ref><ref type="bibr" target="#b15">18]</ref>.</p><p>The controller then performs the following for-loop of four steps for every page copyback: (1) send a flash-toregister read command through the channel (only 0.2µs) to the GC-ing plane, (2) wait until the plane executes the 1-page read command (40µs without using the channel), (3) send a register-to-flash write command, and (4) wait until the plane executes the 1-page write command (800µs without using the channel). Steps 1-4 are repeated until all valid pages are copied and then the old block is erased. The key point here is that copyback operations (steps 2 and 4; roughly 840µs) are done internally within the GC-ing plane without crossing the channel.</p><p>GC Blocking: GC blocking occurs when some resources (e.g., controller, channel, planes) are used by a GC activity, which will delay subsequent requests, similar to head-of-line blocking. Blocking designs are used as they are simple and cheap (small gate counts). But because GC latencies are long, blocking designs can produce significant tail latencies.</p><p>One simple approach to implement GC is with a blocking controller. That is, even when only one plane is performing GC, the controller is busy communicating with the GC-ing plane and unable to serve outstanding I/Os that are designated to any other planes. We refer to this as controller-blocking GC, as illustrated in <ref type="figure" target="#fig_1">Fig- ure 3a</ref>. Here, a single GC (the striped plane) blocks the controller, thus technically all channels and planes are blocked (the bold lines and dark planes). All outstanding I/Os cannot be served (represented by the non-colored I/Os). OpenSSD <ref type="bibr" target="#b2">[4]</ref>, VSSIM <ref type="bibr" target="#b48">[50]</ref>, and low-cost systems such as eMMC devices adopt this implementation.</p><p>Another approach is to support multi-threaded/multi-CPU with channel queueing. Here, while a thread/CPU is communicating to a GC-ing plane (in a for-loop) and blocking the plane's channel (e.g., bold line in <ref type="figure" target="#fig_1">Figure  3b</ref>), other threads/CPUs can serve other I/Os designated to other channels (the colored I/Os in bright planes).</p><p>We refer this as channel-blocking GC (i.e., a GC blocks the channel of the GC-ing plane). SSDSim <ref type="bibr" target="#b30">[32]</ref> and disksim +SSD <ref type="bibr" target="#b15">[18]</ref> adopt this implementation. Commodity SSDs do not come with layout specifications, but from our experiments ( §2), we suspect some form of channel-blocking (at least in client SSDs) exists. <ref type="figure">Figure 1</ref> also implicitly shows how blocked I/Os create cascading queueing delays. Imagine the "Outstanding I/Os" represents a full device queue (e.g., typically 32 I/Os). When this happens, the host OS cannot submit more I/Os, hence user I/Os are blocked in the OS queues. We show this impact in our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TTFLASH Design</head><p>We now present the design of TTFLASH, a new SSD architecture that achieves guaranteed performance close to a no-GC scenario. We are able to remove GC blocking at all levels with the following four key strategies:</p><p>1. Devise a non-blocking controller and channel protocol, pushing any resource blocking from a GC to only the affected planes. We call this fine-grained architecture plane-blocking GC ( §4.1).</p><p>2. Exploit RAIN parity-based redundancy ( §4.2) and combine it with GC information to proactively regenerate reads blocked by GC at the plane level, which we name GC-tolerant read ( §4.3). 3. Schedule GC in a rotating fashion to enforce at most one GC in every plane group, such that no reads will see more than one GC; one parity can only "cut one tail." We name this rotating GC ( §4.4).</p><p>4. Use capacitor-backed write buffer to deliver fast durable completion of writes, allowing them to be evicted to flash pages at a later time in GC-tolerant manner. We name this GC-tolerant flush ( §4.5). For clarity of description, the following sections will use concrete parameter values shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Plane-Blocking GC (PB)</head><p>Controller-and channel-blocking GC are often adopted due to their simplicity of hardware implementation; a GC is essentially a for-loop of copyback commands. This simplicity, however, leads to severe tail latencies as independent planes are unnecessarily blocked. Channelblocking is no better than controller-blocking GC for large I/Os; as every large I/O is typically striped across multiple channels, one GC-busy channel still blocks the entire I/O, negating the benefit of SSD parallelism. Furthermore, as SSD capacity increases, there will be more planes blocked in the same channel. Worse, GC period can be significantly long. A GC that copybacks 64 valid pages (25% valid) will lead to 54 ms (64×840µs) of blocked channel, which potentially leaves hundreds of other I/Os unservable. An outstanding read operation that supposedly only takes less than 100 µs is now delayed longer by order(s) of magnitude <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b22">24]</ref>. To reduce this unnecessary blocking, we introduce plane-blocking GC, as illustrated in <ref type="figure" target="#fig_1">Figure 3c</ref>. Here, the only outstanding I/Os blocked by a GC are the ones that correspond to the GC-ing plane ( labels). All I/Os to non-GCing planes (non-labels) are servable, including the ones in the same channel of the GC-ing plane. As a side note, plane-blocking GC can be interchangeably defined as chip-blocking GC; in this paper, we use 1 plane/chip for simplicity of presentation.</p><p>To implement this concept, the controller must perform a fine-grained I/O management. For illustration, let us consider the four GC steps ( §3). In TTFLASH, after a controller CPU/thread sends the flash-to-register read/write command (Steps 1 and 3), it will not be idle waiting (for 40µs and 800µs, respectively) until the next step is executable. (Note that in a common implementation, the controller is idling due to the simple for-loop and the need to access the channel to check the plane's copyback status). With plane-blocking GC, after Steps 1 and 3 (send read/write commands), the controller creates a future event that marks the completion time. The controller can reliably predict how long the intra-plane read/write commands will finish (e.g., 40 and 800 µs on average, respectively). To summarize, with planeblocking GC, TTFLASH overlaps intra-plane copyback and channel usage for other outstanding I/Os. As shown in <ref type="figure" target="#fig_1">Figure 3c</ref>, for the duration of an intra-plane copyback (the striped/GC-ing plane), the controller can continue serving I/Os to other non-GCing planes in the corresponding channel (▲ I/Os).</p><p>Plane-blocking GC potentially frees up hundreds of previously blocked I/Os. However, there is an unsolved GC blocking issue and a new ramification. The unsolved GC blocking issue is that the I/Os to the GC-ing plane ( labels in <ref type="figure" target="#fig_1">Figure 3c</ref>) are still blocked until the GC completes; in other words, with only plane-blocking, we cannot entirely remove GC blocking. The new ramification of plane-blocking is a potentially prolonged GC operation; when the GC-ing plane is ready to take another command (end of Steps 2 and 4), the controller/channel might still be in the middle of serving other I/Os, due to overlaps. For example, the controller cannot start GC write (Step 3) exactly 40 µs after GC read completes (Step 1), and similarly, the next GC read (Step 1) cannot start exactly 800 µs after the previous GC write. If GC is prolonged, I/Os to the GC-ing plane will be blocked longer. Fortunately, the two issues above can be masked with RAIN and GC-tolerant read.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RAIN</head><p>To prevent blocking of I/Os to GC-ing planes, we leverage RAIN, a recently-popular standard for data integrity <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b9">12]</ref>. RAIN introduces the notion of parity pages inside the SSD. Just like the evolution of disk-based RAIDs, many RAIN layouts have been introduced <ref type="bibr" target="#b31">[33,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42]</ref>, but they mainly focus on data protection, write optimization, and wear leveling. On the contrary, we design a RAIN layout that also targets tail tolerance. This section briefly describes our basic RAIN layout, enough for understanding how it enables GC-tolerant read ( §4.3); our more advanced layout will be discussed later along with wear-leveling issues ( §7). <ref type="figure" target="#fig_2">Figure 4</ref> shows our RAIN layout. For simplicity of illustration, we use 4 channels (C 0 −C 3 ) and the RAIN stripe width matches the channel count (N =4). The planes at the same position in each channel form a plane group (e.g., G 1 ). A stripe of pages is based on logical page numbers (LPNs). For every stripe (N −1 consecutive LPNs), we allocate a parity page. For example, for LPNs 0-2, we allocate a parity page P 012 .</p><p>Regarding the FTL design (LPN-to-PPN mapping), there are two options: dynamic or static. Dynamic map- ping, where an LPN can be mapped to any PPN, is often used to speed-up writes (flexible destination). However, in modern SSDs, write latency issues are absorbed by capacitor-backed RAM ( §4.5); thus, writes are spread across multiple channels. Second, dynamic mapping works well when individual pages are independent; however, RAIN pages are stripe dependent. With dynamic mapping, pages in a stripe can be placed behind one channel, which will underutilize channel parallelism.</p><formula xml:id="formula_0">... G0 G1 G.. ... ... ...</formula><p>Given the reasons above, we create a page-level hybrid static-dynamic mapping. The static allocation policies are: (a) an LPN is statically mapped to a plane (e.g., LPN 0 to plane G 0 C 0 in <ref type="figure" target="#fig_2">Figure 4</ref>), (b) N −1 consecutive LPNs and their parity form a stripe (e.g., 012P 012 ), and (c) the stripe pages are mapped to planes across the channels within one plane group (e.g., 012P 012 in G 0 ). Later, we will show how all of these are crucial for supporting GC-tolerant read ( §4.3) and rotating GC ( §4.4).</p><p>The dynamic allocation policy is: inside each plane/chip, an LPN can be dynamically mapped to any PPN (hundreds of thousands of choices). An overwrite to the same LPN will be redirected to a free page in the same plane (e.g., overwrites to LPN 0 can be directed to any PPN inside G 0 C 0 plane).</p><p>To prevent parity-channel bottleneck (akin to RAID-4 parity-disk bottleneck), we adopt RAID-5 with a slightly customized layout. First, we treat the set of channels as a RAID-5 group. For example, in <ref type="figure" target="#fig_2">Figure 4</ref>, P 012 and P 345 are in different channels, in a diagonal fashion. Second, as SSD planes form a 2-dimensional layout (G i C j ) with wearout issues (unlike disk's "flat" LPNs), we need to ensure hot parity pages are spread out evenly. To handle this, we will present dynamic migrations later ( §7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">GC-Tolerant Read (GTR)</head><p>With RAIN, we can easily support GC-tolerant read (GTR). For a full-stripe read (which uses N −1 channels), GTR is straightforward: if a page cannot be fetched due to an ongoing GC, the page content is quickly regenerated by reading the parity from another plane. In <ref type="figure" target="#fig_2">Fig- ure 4</ref>, given a full-stripe read of LPNs 0-2, and if LPN 1 is unavailable temporarily, the content is rapidly regenerated by reading the parity (P 012 ). Thus, the full-stripe read is not affected by the ongoing GC. The resulting latency is order(s) of magnitude faster than waiting for GC completion; parity computation overhead only takes less than 3µs for N ≤8 and the additional parity read only takes a minimum of 40+100µs (read+transfer latencies; <ref type="table" target="#tab_2">Table 1</ref>) and does not introduce much contention.</p><p>For a partial-stripe read (R pages where R&lt;N −1), GC-tolerant read will generate in total N −R extra reads; the worst case is when R=1. These N −R extra parallel reads will add contention to each of the N −R channels, which might need to serve other outstanding I/Os. Thus, we only perform extra reads if T GCtoComplete &gt;B× (40+100)µs where B is the number of busy channels in the N −R extra reads (for non-busy channels the extra reads are free). In our experience, this simple policy cuts GC tail latencies effectively and fairly without introducing heavy contention. In the opposing end, a "greedy" approach that always performs extra reads causes high channel contention.</p><p>We emphasize that unlike tail-tolerant speculative execution, often defined as an optimization task that may not be actually needed, GC-tolerant read is affirmative, not speculative; the controller knows exactly when and where GC is happening and how long it will complete. GTR is effective but has a limitation: it does not work when multiple planes in a plane group perform GCs simultaneously, which we address with rotating GC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Rotating GC (RGC)</head><p>As RAIN distributes I/Os evenly over all planes, multiple planes can reach the GC threshold and thus perform GCs simultaneously. For example, in <ref type="figure" target="#fig_2">Figure 4</ref>, if planes of LPNs 0 and 1 (G 0 C 0 and G 0 C 1 ) both perform GC, reading LPNs 0-2 will be delayed. The core issue is: one parity can only cut "one tail". Double-parity RAIN is not used due to the larger space overhead.</p><p>To prevent this, we develop rotating GC (RGC), which enforces that at most one plane in each plane group can perform one GC at a time. Concurrent GCs in different plane groups are still allowed (e.g., one in each G i as depicted in <ref type="figure" target="#fig_2">Figure 4</ref>). Note that rotating GC depends on our RAIN layout that ensures every stripe to be statically mapped to a plane group.</p><p>We now emphasize our most important message: there will be zero GC-blocked I/Os if rotating GC holds true all the time. The issue here is that our rotating approach can delay a plane's GC as long as (N −1)× T gc (the GC duration). During this period, when all the free pages are exhausted, multiple GCs in a plane group must execute concurrently. This could happen depending on the combination of N and the write intensity. Later in Appendix A, we provide a proof sketch showing that with stripewidth N ≤26, rotating GC can always be enforced under realistic write-intensive scenarios.</p><p>Employing a large stripe width (e.g., N =32) is possible but can violate rotating GC, implying that GC tail latencies cannot be eliminated all the time. Thus, in manychannel (e.g., 32) modern SSDs, we can keep N =8 or 16 (e.g., create four 8-plane or two 16-plane groups across the planes within the same vertical position). Increasing N is unfavorable not only because of rotating GC violation, but also due to reduced reliability and the more extra I/Os generated for small reads by GTR ( §4.3). In our evaluation, we use N =8, considering 1/8 parity overhead is bearable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">GC-Tolerant Flush (GTF)</head><p>So far, we only address read tails. Writes are more complex (e.g., due to write randomness, read-and-modify parity update, and the need for durability). To handle write complexities, we leverage the fact that flash industry heavily employs capacitor-backed RAM as a durable write buffer (or "cap-backed RAM" for short) <ref type="bibr" target="#b11">[14]</ref>. To prevent data loss, the RAM size is adjusted based on the capacitor discharge period after power failure; the size can range from tens to hundreds of MB, backed by "super capacitors" <ref type="bibr" target="#b8">[10]</ref>.</p><p>We adopt cap-backed RAM to absorb all writes quickly. When the buffer occupancy is above 80%, a background flush will run to evict some pages. When the buffer is full (e.g., due to intensive large writes), a foreground flush will run, which will block incoming writes until some space is freed. The challenge to address here is that foreground flush can induce write tails when the evicted pages must be sent to GC-ing planes.</p><p>To address this, we introduce GC-tolerant flush (GTF), which ensures that page eviction is free from GC blocking, which is possible given rotating GC. For example, in <ref type="figure" target="#fig_2">Figure 4</ref>, pages belonging to 3', 4' and P 3 ′ 4 ′ 5 ′ can be evicted from RAM to flash while page 5' eviction is delayed until the destination plane finishes the GC. With proven rotating GC, GTF can evict N −1 pages in every N pages per stripe without being blocked. Thus, the minimum RAM space needed for the pages yet to be flushed is small. Appendix A suggests that modern SSD RAM size is sufficient to support GTF.</p><p>For partial-stripe writes, we perform the usual RAID read-modify-write eviction, but still without being blocked by GC. Let us imagine a worst-case scenario of updates to pages 7' and 8' in <ref type="figure" target="#fig_2">Figure 4</ref>. The new parity should be P 67 ′ 8 ′ , which requires read of page 6 first. Despite page 6 being unreachable, it can be regenerated by reading the old pages P 678 , 7, and 8, after which pages 7', 8', and P 67 ′ 8 ′ can be evicted.</p><p>We note that such an expensive parity update is rare as we prioritize the eviction of full-stripe dirty pages to non-GCing planes first and then full-stripe pages to mostly non-GCing planes with GTF. Next, we evict partial-stripe dirty pages to non-GCing planes and finally partial-stripe pages to mostly non-GCing planes with GTF. Compared to other eviction algorithms that focus on reducing write amplification <ref type="bibr" target="#b33">[35]</ref>, our method adds GC tail tolerance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Implementation</head><p>This section describes our implementations of TTFLASH, which is available on our website <ref type="bibr">[1]</ref>.</p><p>• ttFlash-Sim (SSDSim): To facilitate accurate latency analysis at the device level, we first implement TTFLASH in SSDSim <ref type="bibr" target="#b30">[32]</ref>, a recently-popular simulator whose accuracy has been validated against a real hardware platform. We use SSDSim due to its clean-slate design. We implemented all the TTFLASH features by adding 2482 LOC to SSDSim. This involves a substantial modification (+36%) to the vanilla version (6844 LOC). The breakdown of our modification is as follow: plane-blocking (523 LOC), RAIN (582), rotating GC (254), GC-tolerant read (493) and write (630 lines).</p><p>• ttFlash-Emu ("VSSIM++"): To run Linux kernel and file system benchmarks, we also port TTFLASH to VSSIM, a QEMU/KVM-based platform that "facilitates the implementation of the SSD firmware algorithms" <ref type="bibr" target="#b48">[50]</ref>. VSSIM emulates NAND flash latencies on RAM disk. Unfortunately, VSSIM's implementation is based on 5-year old QEMU-v0.11 IDE interface, which only delivers 10K IOPS. Furthermore, as VSSIM is a single-threaded design, it essentially mimics a controllerblocking SSD (1K IOPS under GC). These limitations led us to make major changes. First, we migrated VSSIM's single-threaded logic to a multithreaded design within the QEMU AIO module, which enables us to implement channel-blocking. Second, we migrated this new design to a recent QEMU release (v2.6) and connected it to the PCIe/NVMe interface. Our modification, which we refer as "VSSIM++", can sustain 50K IOPS. Finally, we port TTFLASH features to VSSIM++, which we refer as ttFlash-Emu, for a total of 869 LOC of changes.</p><p>• Other attempts (OpenSSD and LightNVM): We attempted implementing TTFLASH on real hardware platforms (2011 Jasmine and 2015 Cosmos OpenSSD boards <ref type="bibr" target="#b2">[4]</ref>). After a few months trying, we hit many limitations of OpenSSD: single threaded (no pthread support), single logic for all channels (cannot control channel queues), no accessible commands for data transfer from flash RAM to host DRAM (preventing parity regeneration), no support for wall-clock time (preventing GC time prediction), inaccessible request queues and absence of GC queues (OpenSSD is whole-blocking). We would like to reiterate that these are not hardware limitations, but rather, the ramifications of the elegant simplicity of OpenSSD programming model (which is its main goal). Nevertheless, our conversations with hardware architects suggest that TTFLASH is implementable on a real firmware (e.g., roughly a 1-year development and testing project on a FPGA-based platform).</p><p>Finally, we also investigated the LightNVM (OpenChannel SSD) QEMU test platform <ref type="bibr" target="#b13">[16]</ref>. LightNVM <ref type="bibr" target="#b18">[21]</ref> is an in-kernel framework that manages OpenChannel SSD (which exposes individual flash channels to the host, akin to Software-Defined Flash <ref type="bibr" target="#b42">[44]</ref>). Currently, neither OpenChannel SSD nor LightNVM's QEMU test platform support intra-SSD copy-page command. Without such support and since GC is managed by the host OS, GC-ed pages must cross back and forth between the device and the host. This creates heavy background-vsforeground I/O transfer contention between GC and user I/Os. For example, the user's maximum 50K IOPS can downgrade to 3K IOPS when GC is happening. We leave this integration for future work after the intra-SSD copypage command is supported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We now present extensive evaluations showing that TTFLASH significantly eliminates GC blocking ( §6.1), delivers more stable latencies than the state-of-the-art preemptive GC ( §6.2) and other GC optimization techniques ( §6.3), and does not significantly increase P/E cycles beyond the RAIN overhead ( §6.4).</p><p>Workloads: We evaluate two implementations: ttFlash-Sim (on SSDSim) and ttFlash-Emu (on VS-SIM++), as described in Section 5. For ttFlash-Sim evaluation, we use 6 real-world block-level traces from Microsoft Windows Servers as listed in the figure titles of <ref type="figure">Figure 5</ref>. Their detailed characteristics are publicly reported <ref type="bibr" target="#b1">[3,</ref><ref type="bibr" target="#b32">34]</ref>. By default, for each trace, we chose the busiest hour (except the 6-minute TPCC trace). For ttFlash-Emu evaluation, we use filebench <ref type="bibr" target="#b0">[2]</ref> with six personalities as listed in the x-axis of <ref type="figure" target="#fig_4">Figure 8</ref>.</p><p>Hardware parameters: For ttFlash-Sim, we use the same 256-GB parameter values provided in with 64 MB cap-backed RAM and a typical device queue size of 32. ttFlash-Emu uses the same parameters but its SSD capacity is only 48 GB (limited by the machine's DRAM). We use a machine with 2.4GHz 8-core Intel Xeon Processor E5-2630-v3 and 64-GB DRAM. The simulated and emulated SSD drives are pre-warmed up with the same workload.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main Results</head><p>• Tiny tail latencies: <ref type="figure">Figure 5</ref> shows the CDF of read latencies from the six trace-driven experiments run on ttFlash-Sim. Note that we only show read latencies; write latencies are fast and stable as all writes are absorbed by cap-backed RAM ( §4.5). As shown in <ref type="figure">Figure  5</ref>, the base approach ("Base" = the default SSDSim with channel-blocking and its most-optimum FTL <ref type="bibr" target="#b30">[32]</ref>   nificant improvements are observed. When all features are added (RGC+GTR+PB), the tiny tail latencies are close to those of the no-GC scenario, as we explain later. <ref type="figure">Figure 6</ref> plots the average latencies of the same experiments. This graph highlights that although the latencies of TTFLASH and Base are similar at 90 th percentile <ref type="figure">(Fig- ure 5)</ref>, the Base's long tail latencies severely impact its average latencies. Compared to Base, TTFLASH's average latencies are 2.5-7.8× faster.</p><p>• TTFLASH vs. NoGC: To characterize the benefits of TTFLASH's tail latencies, we compare TTFLASH to a perfect "no-GC" scenario ("NoGC" = TTFLASH without GC and with RAIN). In NoGC, the same workload runs without any GC work (with a high GC threshold), thus all I/Os observe raw flash performance. <ref type="table" target="#tab_4">Table 2</ref> shows the slowdown from NoGC to TTFLASH at various high percentiles. As shown, TTFLASH significantly reduces GC blocking. Specifically, at 99-99.9 th percentiles, TTFLASH's slowdowns are only 1.00 to 1.02×. Even at 99.99 th percentile, TTFLASH's slowdowns are only 1.0 to 2.6×. In comparison, Base suffers from 5.6-138.2× slowdowns between 99-99.99 th percentiles (as obvious in <ref type="figure">Figure 5</ref>); for readability, NoGC lines are not plotted in the <ref type="figure">figure.</ref> In terms of average latencies, <ref type="figure">Figure 6</ref> shows that TTFLASH performs the same with or without GC.</p><p>• GC-blocked I/Os: To show what is happening inside the SSD behind our speed-ups, we count the percentage of read I/Os that are blocked by GC ("%GC-blocked I/Os"), as plotted in <ref type="figure" target="#fig_3">Figure 7</ref>. As important, we emphasize that GC-blocked I/Os fill up the device queue, creat- ing queueing delays that prevent new host I/Os from entering the device, which we count as "%queue-blocked I/Os." Thus, each bar in the figure has two parts: %GC-blocked (bottom, bold edge) and %queue-blocked I/Os (top), divided with a small horizontal borderline. <ref type="figure" target="#fig_3">Figure 7</ref> shows that with Base, without GC tolerance, 2-5% of reads are blocked by GC. As they further cause queueing delays, in total, there are 2-7% of blocked I/Os that cannot be served. As each TTFLASH feature is added, more I/Os are unblocked. With all the features in place ("All" bars), there are only 0.003-0.05% of blocked I/Os, with the exception of MSNFS (0.7%). The only reason why it is not 0% is that for non-fullstripe reads, TTFLASH will wait for GC completion only if the remaining time is shorter than the overhead of the extra reads (as explained in §4.3). We still count these I/Os as blocked, albeit only momentarily.</p><p>We next evaluate ttFlash-Emu with filebench <ref type="bibr" target="#b0">[2]</ref>. <ref type="figure" target="#fig_4">Figure 8</ref> shows the average latencies of filebenchlevel read operations (including kernel, file-system, and QEMU overheads in addition to device-level latencies) and the percentage of GC-blocked reads measured inside ttFlash-Emu. We do not plot latency CDF as filebench only reports average latencies. Overall, ttFlash-Emu shows the same behavior as ttFlash-Sim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">TTFLASH vs. Preemptive GC</head><p>As mentioned before, many existing work optimize GC, but does not eliminate its impact. One industry standard in eliminating ("postponing") GC impact is preemptive GC <ref type="bibr" target="#b7">[9]</ref>. We implement preemptive GC in SSDSim based on existing literature <ref type="bibr" target="#b38">[40]</ref>. The basic idea is to interleave user I/Os with GC operations. That is, if a user I/O arrives while a GC is happening, future copybacks should be postponed. <ref type="figure" target="#fig_5">Figure 9a</ref> compares ttFlash-Sim, preemptive, and NoGC scenarios for the DTRS workload (other workloads lead to the same conclusion). As shown, TTFLASH is closer to NoGC than preemptive GC. The reason is that preemptive GC must incur a delay from waiting for the block erase (up to 2 ms) or the current page copyback to finish (up to 800 µs delay), mainly because the finest-grained preemption unit is a page copyback ( §3). TTFLASH on the other hand can rapidly regenerate the delayed data. Most importantly, TTFLASH does not postpone GC indefinitely. In contrast, preemptive GC piles up GC impact to the future, with the hope that there will be idle time. However, with a continuous I/O stream, at one point, the SSD will hit a GC high water mark (not enough free pages), which is when preemptive GC becomes nonpreemptive <ref type="bibr" target="#b38">[40]</ref>. To create this scenario, we run the same workload but make SSDSim GC threshold hit the high water mark. <ref type="figure" target="#fig_5">Figure 9b</ref> shows that as preemptive GC becomes non-preemptive, it becomes GC-intolerant and creates long tail latencies.</p><p>To be more realistic with the setup, we perform a similar experiment as in the Semi-Preemptive GC paper <ref type="bibr">[40, §IV]</ref>. We re-rate DTRS I/Os by 10× and re-size them by 30×, in order to reach the high GC water mark (which we set to 75% to speed up the experiment). <ref type="figure" target="#fig_5">Figure 9c</ref> shows the timeline of observed latencies with TTFLASH and preemptive GC. We also run a synthetic workload with continuous I/Os to prevent idle time ( <ref type="figure" target="#fig_5">Figure 9d)</ref>; the workload generates 28-KB I/Os (full-stripe) every 130µs with 70% read and 30% write). Overall, <ref type="figure" target="#fig_5">Figures 9c-d</ref> highlight that preemptive GC creates backlogs of GC activities, which will eventually cause SSD "lock-down" when page occupancy reaches the high watermark. On the other hand, TTFLASH can provide stable latencies without postponing GC activities indefinitely. The last two experiments above create high intensity of writes, and within the same experiments, our GCtolerant flush (GTF; §4.5) provides stable latencies, as implicitly shown in <ref type="figure" target="#fig_5">Figures 9c-d</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">TTFLASH vs. GC Optimizations</head><p>GC can be optimized and reduced with better FTL management, special coding, novel write buffer scheme or SSD-based log-structured file system. Contrary to these efforts, our approach is fundamentally different. We do not focus in reducing the number of GCs, but instead, we eliminate the blocking nature of GC operations. With reduction, even if GC count is reduced by multiple times, it only makes GC-induced tail latencies shorter, but not disappear (e.g., as in <ref type="figure">Figure 5)</ref>. Nevertheless, the techniques above are crucial in extending SSD lifetime, hence orthogonal to TTFLASH. <ref type="figure" target="#fig_7">Figure 10</ref> compares the number of GCs (P/E cycles) completed by the Base approach and TTFLASH within the experiments in <ref type="figure">Figure 5</ref>. We make two observations. First, TTFLASH does not delay GCs; it actively performs GCs at a similar rate as in the base approach, but yet still delivers predictable performance. Second, TTFLASH introduces 15-18% of additional P/E cycles (in 4 out of 6 workloads), which mainly comes from RAIN; as we use N =8, there are roughly 15% (1/7) more writes in USENIX Association 15th USENIX Conference on File and Storage Technologies 23 minimum, from one parity write for every seven (N −1) consecutive writes. The exceptions are 53% of additional P/E cycles in MSNFS and TPCC, which happen because the workloads generate many small random writes, causing one parity write for almost every write. For this kind of workload, large buffering does not help. Overall, higher P/E cycles is a limitation of TTFLASH, but also a limitation of any scheme that employs RAIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Write (P/E Cycle) Overhead</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">TTFLASH vs. No RAIN</head><p>Earlier, in <ref type="figure">Figure 6</ref>, we show that TTFLASH has about the same average latencies as NoGC (TTFLASH without GC and with RAIN). In further experiments (not shown due to space), we also compare TTFLASH to "NoGC !R " (i.e., Base without GC and without RAIN). We observed TTFLASH's average latencies are 1.09-1.33× of NoGC !R 's. The RAIN-less NoGC !R is faster because it can utilize all channels. This is a limitation of TTFLASH; that is, as TTFLASH (or any SSD that) employs RAIN, the channels experience a slight contention. In <ref type="figure" target="#fig_2">Figure 4</ref> for example, reading LPNs 0-3 will incur contention on channel-0 (from LPNs 0 and 3). In a RAIN-less setup, the same read will utilize all four channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">TTFLASH under Write Bursts</head><p>TTFLASH can circumvent GC blocking when rotating GC is enforced ( §4.4). A limitation of TTFLASH is that under heavy write bursts, multiple GCs per plane group must be allowed to keep the number of free pages stable. <ref type="figure" target="#fig_8">Figure 11a</ref> shows the limit of our 256GB drive setup <ref type="table" target="#tab_2">(Table 1</ref>) with N =8. As shown, at 6 DWPD (55 MB/s), there is almost no GC-blocked reads, hence tiny tail latencies. 1 DWPD ("Drive Writes Per Day") implies 256GB/8hours (9.1 MB/s) of writes; we generously use 8 hours to represent a "Day" (Appendix A). However, at 7 DWPD (64 MB/s), TTFLASH exhibits some tail latencies, observable at the 90 th percentile. We emphasize that this is still much better than the Base approach, where the tail latencies are observed starting at 20 th percentile (not shown). We also believe that such intensive writes are hopefully rare; for 3-5yr lifespans, modern MLC/TLC drives must conform to 1-5 DWPD <ref type="bibr" target="#b14">[17]</ref>. <ref type="figure" target="#fig_8">Figure 11b</ref> shows that if we force only one GC  per plane group all the time ("F-RGC"), at 7 DWPD, the percentage of free pages (the y-axis) continuously drops over time (the x-axis). That is, RGC cannot keep up with the write bursts. Thus, to keep the number of free pages stable, under write bursts, we must allow multiple GCs to happen per plane group (the "RGC, 7DWPD" line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations and Discussions</head><p>We now summarize the limitations of TTFLASH. First, TTFLASH depends on RAIN, hence the loss of one channel per N channels (as evaluated in §6.5). Increasing N will reduce channel loss but cut less tail latencies under write bursts (Appendix A). Under heavy write bursts, TTFLASH cannot cut all tails (as evaluated in §6.6 and discussed in Appendix A). Finally, TTFLASH requires intra-plane copybacks, skipping ECC checks, which requires future work as we address below.</p><p>• ECC checking (with scrubbing): ECC-check is performed when data pass through the ECC engine (part of the controller). On foreground reads, before data is returned to the host, ECC is always checked (TTFLASH does not modify this property). Due to increasing bit errors, it is suggested that ECC checking runs more frequently, for example, by forcing all background GC copyback-ed pages read out from the plane and through the controller, albeit reduced performance.</p><p>TTFLASH, however, depends on intra-plane copybacks, which implies no ECC checking on copyback-ed pages, potentially compromising data integrity. A simple possible solution to compensate this problem is periodic idle-time scrubbing within the SSD, which will force flash pages (user and parity) flow through the ECC engine. This is a reasonable solution for several reasons. First, SSD scrubbing (unlike disk) is fast given the massive read bandwidth. For example, a 2 GB/s 512-GB client SSD can be scrubbed under 5 minutes. Second, scrubbing can be easily optimized, for example, by only selecting blocks that are recently GC-ed or have higher P/E counts and history of bit flips, which by implication can also reduce read disturbs. Third, periodic background operations can be scheduled without affecting foreground performance (a rich literature in this space <ref type="bibr" target="#b16">[19]</ref>). However, more future work is required to evaluate the ramifications of background ECC checks.</p><p>• Wear leveling (via horizontal shifting and vertical migration): Our static RAIN layout ( §4.2) in general does not lead to wear-out imbalance in common cases. However, rare cases such as random-write transactions (e.g., MSNFS) cause imbalanced wear-outs (at chip/plane level).</p><p>Imbalanced wear-outs can happen due to the two following cases: (1) There is write imbalance within a stripe (MSNFS exhibits this pattern). In <ref type="figure" target="#fig_2">Figure 4</ref> for example, if in stripe S 0 {012P 012 }, LPN 1 is more frequently updated than the rest, the planes of LPN 1 and P 012 will wear out faster than the other planes in the same group. (2) There is write imbalance across the stripes. For example, if stripes in group G 0 (e.g., stripe {012P 012 }) are more frequently updated than stripes in other groups, then the planes in G 0 will wear out faster.</p><p>The two wear-out problems above can be fixed by dynamic horizontal shifting and vertical migration, respectively. With horizontal shifting, we can shift the parity locations of stripes with imbalanced hot pages. For example, S 0 can be mapped as {12P 012 0} across the 4 planes in the same group; LPN 1 and P will now be directed to colder planes. With vertical migration, hot stripes can be migrated from one plane group to another ("vertically"), balancing the wear-out across plane groups.</p><p>As a combined result, an LPN is still and always statically mapped to a stripe number. A stripe, by default, is statically mapped to a plane group and has a static parity location (e.g., S 0 is in group G 0 with P 012 behind channel C 3 ). However, to mark dynamic modification, we can add a "mapping-modified" bit in the standard FTL table <ref type="figure">(LPN-PPN mapping)</ref>. If the bit is zero, the LPN-PPN translation performs as usual, as the stripe mapping stays static (the common case). If the bit is set (the rare case in rare workloads), the LPN-PPN translation must consult a new stripe-information table that stores the mapping between a stripe (S k ) to a group number (G i ) and parity channel position (C j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>We now discuss other works related to TTFLASH.</p><p>GC-impact reduction: Our work is about eliminating GC impacts, while many other existing works are about reducing GC impacts. There are two main reduction approaches: isolation and optimization, both with drawbacks. First, isolation (e.g., OPS isolation <ref type="bibr" target="#b34">[36]</ref>) only isolates a tenant (e.g., sequential) from another one (e.g., random-write). It does not help a tenant with both random-write and sequential workloads on the same dataset. OPS isolation must differentiate users while TTFLASH is user-agnostic. Second, GC optimization, which can be achieved by better page layout management (e.g., value locality <ref type="bibr" target="#b27">[29]</ref>, log-structured <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b41">43]</ref>) only helps in reducing GC period but does not eliminate blocked I/Os. GC-impact elimination: We are only aware of a handful of works that attempt to eliminate GC impact, which fall into two categories: without or with redundancy. Without redundancy, one can eliminate GC impact by preemption <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b45">47]</ref>. We already discussed the limitations of preemptive GC ( §6.2; <ref type="figure" target="#fig_5">Figure 9</ref>). With redundancy, one must depend on RAIN. To the best of our knowledge, our work is the first one that leverages SSD internal redundancy to eliminate GC tail latencies. There are other works that leverage redundancy in flash array (described later below).</p><p>RAIN: SSD's internal parity-based redundancy (RAIN) has become a reliability standard. Some companies reveal such usage but unfortunately without topology details <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b9">12]</ref>. In literature, we are aware of only four major ones: eSAP <ref type="bibr" target="#b35">[37]</ref>, PPC <ref type="bibr" target="#b31">[33]</ref>, FRA <ref type="bibr" target="#b40">[42]</ref> and LARS <ref type="bibr" target="#b39">[41]</ref>. These efforts, however, mainly concern about write optimization and wear leveling in RAIN but do not leverage RAIN to eliminate GC tail latencies.</p><p>Flash array: TTFLASH works within a single SSD. In the context of SSD array, we are aware of two published techniques on GC tolerance: Flash on Rails <ref type="bibr" target="#b44">[46]</ref> and Harmonia <ref type="bibr" target="#b36">[38]</ref>. Flash on Rails <ref type="bibr" target="#b44">[46]</ref> eliminates read blocking (read-write contention) with a ring of multiple drives where 1-2 drives are used for write logging and the other drives are used for reads. The major drawback is that read/write I/Os cannot utilize the aggregate bandwidth of the array. In Harmonia <ref type="bibr" target="#b36">[38]</ref>, the host OS controls all the SSDs to perform GC at the same time (i.e., it is better that all SSDs are "unavailable" at the same time, but then provide stable performance afterwards), which requires more complex host-SSD communication.</p><p>Storage tail latencies: A growing number of works recently investigated sources of storage-level tail latencies, including background jobs <ref type="bibr" target="#b16">[19]</ref>, file system allocation policies <ref type="bibr" target="#b29">[31]</ref>, block-level I/O schedulers <ref type="bibr" target="#b47">[49]</ref>, and disk/SSD hardware-level defects <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b28">30]</ref>. An earlier work addresses load-induced tail latencies with RAID parity <ref type="bibr" target="#b17">[20]</ref>. Our work specifically addresses GC-induced tail latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>SSD technologies have changed rapidly in the last few years; faster and more powerful flash controllers are cable of executing complex logic; parity-based RAIN has become a standard means of data protection; and capacitor-backed RAM is a de-facto solution to address write inefficiencies. In our work, we leverage a combina-tion of these technologies in a way that has not been done before. This in turn enables us to build novel techniques such as plane-blocking GC, rotating GC, GC-tolerant read and flush, which collectively deliver a robust solution to the critical problem of GC-induced tail latencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">Acknowledgments</head><p>We thank Sam H. Noh (our shepherd), Nisha Talagala, and the anonymous reviewers for their tremendous feedback. This material is based upon work supported by the NSF (grant Nos. CCF-1336580, CNS-1350499, CNS-1526304, CNS-1405959, and CNS-1563956) as well as generous donations from EMC, Google, Huawei, NetApp, and CERES Research Center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof Sketch</head><p>Limitation of maximum stripe width (N ): We derive the maximum stripe width allowable (N ) such that rotating GC ( §4.4) is always enforced. That is, as we can only cut one tail, there should be at most one GC per plane group at all time. Thus, a plane might need to postpone its GC until other planes in the same group complete their GCs (i.e., delayed by (N −1)× T gc ). We argue that N should be at least 8 for a reasonable parity space overhead (12.5%); a lower stripe width will increase space overhead. Below we show that N =8 is safe even under intensive write. <ref type="table">Table 3</ref> summarizes our proof, which is based on a per-plane, per-second analysis. We first use concrete values and later generalize the proof.</p><p>• <ref type="table">Table 3a</ref>: We use typical parameters: 4-KB page (S page ), 4-KB register size (S reg ), 25% valid pages (% validP g ), 840µs of GC copyback time per page (T copyback ), and 900µs of user write latency per page (T usrW rt ). Due to intensive copybacks (tens of ms), the 2ms erase time is set to "0" for proving simplicity.</p><p>• <ref type="table">Table 3b</ref>: Each plane's bandwidth (BW pl ) defines the maximum write bandwidth, which is 4.5 MB/s, from the register size (S reg ) divided by the user-write latency (T usrW rt ); all writes must go through the register.</p><p>• <ref type="table">Table 3c</ref>: With the 4.5 MB/s maximum plane bandwidth, there are 1152 pages written per second (#W pg/s ), which will eventually be GC-ed.</p><p>• <ref type="table">Table 3d</ref>: Intensive writes imply frequent overwrites; we assume 25% valid pages (% validP g ) to be GC-ed, resulting in 288 pages copybacked per second (#CB pg/s ). The % validP g can vary depending on user workload.</p><p>• <ref type="table">Table 3e</ref>: With 288 page copybacks, the total GC time per second per plane (T gc/s ) is 242 ms.</p><p>• <ref type="table">Table 3f</ref>: N planes in each group must finish their GCs in rotating manner. As each plane needs T gc time every second, the constraint is: N &lt;1/T gc . With our concrete values above, for rotating GC to hold true all the time, N must be less than 4 (T gc of 242 ms). Fortunately, N can a. Spage=4KB; Sreg=4KB; % validP g =25%;</p><p>Tprog=800µs; T read =40µs; T channel =100µs; T copyback =Tprog+T read =840µs; (Terase="0"); S page BW plane ×% validP g ×T copyback h. DW P D=5; P W P D=5; S pl =4GB; day=8hrs i.</p><p>BW pl = S pl × DW P D/day (in practice) = 4GB × 5 /8hrs =0.7 MB/s j.</p><p>T gc/s = plug (i) to (c,d,e) =38 ms N &lt; 1 / Tgc &lt; 26 <ref type="table">Table 3</ref>: Proof Sketch (Appendix A).</p><p>be larger in practice <ref type="table">(Table 3g</ref>-j). To show this, below we first generalize the proof.</p><p>• <ref type="table">Table 3g</ref>: We combine all the equations above to the equation in <ref type="table">Table 3g</ref>, which clearly shows that N goes down if BW pl or % validP g is high. Fortunately, we find that the constant 4.5 MB/s throughput (BW pl ) in <ref type="table">Ta- ble 3b</ref> is unrealistic in practice, primarily due to limited SSD lifetime. MLC block is only limited to about 5000-10,000 erase cycles and TLC block 3000 erase cycles. To ensure multi-year (3-5) lifespan, users typically conform to the Drive Writes Per Day (DWPD) constraint (1-5 DWPD for MLC/TLC drives) <ref type="bibr" target="#b14">[17]</ref>.</p><p>• <ref type="table">Table 3h</ref>: Let us assume a worst-case scenario of 5 DWPD, which translates to 5 P W P D (planes write per day) per plane. To make it worse, let us assume a "day" is 8 hours. We set plane size (S pl ) to 4 GB ( §3).</p><p>• <ref type="table">Table 3i</ref>: The more realistic parameters above suggest that a plane only receives 0.7 MB/s (4GB*5/8hrs), which is 6.5× less intense than the raw bandwidth (3b).</p><p>• <ref type="table">Table 3j</ref>: If we plug in 0.7 MB/s to the equations in <ref type="table">Table 3c</ref>-e, the GC time per plane (T gc ) is only 38 ms, which implies that N can be as large as 26.</p><p>In conclusion, N =8 is likely to always satisfy rotating GC in practice. In 32-channel SSD, N =32 can violate rotating GC; GC-tolerant read ( §4.3) cannot always cut the tails. Overall, <ref type="table">Table 3g</ref> defines the general constraint for N . We believe the most important value is BW pl . The other parameters relatively stay the same; S page is usually 4 KB, % validP g is low with high overwrites, and T copyback can increase by 25% in TLC chips (vs. MLC).</p><p>Minimum size of cap-backed RAM: With rotating GC, the RAM needs to only hold at most 1/N of the pages whose target planes are GC-ing ( §4.5). In general, the minimum RAM size is 1/N of the SSD maximum write bandwidth. Even with an extreme write bandwidth of the latest datacenter SSD (e.g., 2 GB/s) the minimum RAM size needed is only 256 MB.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SSD Internals (Section 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Various levels of GC blocking. Colored I/Os in bright planes are servable while non-colored I/Os in dark planes are blocked. (a) In controller-blocking ( §3), a GC blocks the controller/entire SSD. (b) In channel-blocking ( §3), a GC blocks the channel connected to the GC-ing plane. (c) In plane-blocking ( §4.1), a GC only blocks the GC-ing plane.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: TTFLASH Architecture. The figure illustrates our RAIN layout ( §4.2), GC-tolerant read ( §4.3), rotating GC ( §4.4), and GC-tolerant flush ( §4.5). We use four channels (C0−C3) for simplicity of illustration. Planes at the same "vertical" position form a plane group (G0, G1, etc.). A RAIN stripe is based on N −1 LPNs and a parity page (e.g., 012P012).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: %GC-blocked read I/Os. The figure above corresponds to the results in Figure 5. The bars represent the ratio (in percent) of read I/Os that are GC-blocked (bottom bar) and queue-blocked (top bar) as explained in §6.1. "All" implies PB+GTR+RGC (please see Figure 5's caption).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Filebench on ttFlash-Emu. The top and bottom figures show the average latencies of read operations and the percentage of GC-blocked reads, respectively, across six filebench personalities. "Base" represents our VSSIM++ with channel-blocking ( §5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: TTFLASH vs. Preemptive GC. The figures are explained in Section 6.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>For example, in comparison to base approaches, Value locality reduces erase count by 65% [29, Section 5], flash-aware RAID by 40% [33, Figure 20], BPLRU by 41% [35, Section 4 and Figure 7], eSAP by 10-45% [37, Figures 11-12], F2FS by 10% [39, Section 3], LARS by 50% [41, Figure 4], and FRA by 10% [42, Figure 12], SFS by 7.5× [43, Section 4], WOM codes by 33% [48, Section 6].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: GC Completions (P/E Cycles). The figure is explained in Section 6.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: TTFLASH under Write Bursts. The figure is explained in Section 6.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1</head><label>1</label><figDesc></figDesc><table>.95 

.96 

.97 

.98 

.99 

1 

0 
20 
40 
60 
80 

Display Ads Server (DAPPS) 

.95 

.96 

.97 

.98 

.99 

1 

0 
20 
40 
60 
80 

Dev. Tools Release (DTRS) 

.95 

.96 

.97 

.98 

.99 

1 

0 
20 
40 
60 
80 

Exchange Server (Exch) 

.95 

.96 

.97 

.98 

.99 

1 

0 
20 
40 
60 
80 

Live Maps Server (LMBE) 

.95 

.96 

.97 

.98 

.99 

1 

0 
20 
40 
60 
80 

Latency (ms) 

MSN File Server (MSNFS) 

(All) RGC+GTR+PB 
GTR+PB 

.95 

.96 

.97 

.98 

.99 

1 

0 
20 
40 
60 
80 

Latency (ms) 

TPC-C 

+PB 
Base 

Figure 5: Tail Latencies. The figures show the CDF of 

read latencies (x=0-80ms) in different workloads as we add 
each TTFLASH strategy: +PB ( §4.1), +GTR ( §4.3), and +RGC 
( §4.4). The y-axis shows 95-100 th percentiles. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 : TTFLASH vs. NoGC (almost no tail). The numbers above represent the slowdown ratio of TTFLASH read latencies compared to NoGC at high percentiles. For example, in DTRS, at 99.99 th percentile, TTFLASH's read latency is only 1.24× slower than NoGC's read latency.</head><label>2</label><figDesc></figDesc><table></table></figure>

			<note place="foot" n="18"> 15th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="20"> 15th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="22"> 15th USENIX Conference on File and Storage Technologies USENIX Association</note>

			<note place="foot" n="28"> 15th USENIX Conference on File and Storage Technologies USENIX Association</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Filebench</surname></persName>
		</author>
		<ptr target="http://filebench.sourceforge.net/wiki/index.php/Main_Page" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snia</forename><surname>Iotta</surname></persName>
		</author>
		<ptr target="http://iotta.snia.org" />
		<title level="m">Storage Networking Industry Association&apos;s Input/Output Traces, Tools, and Analysis</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<ptr target="http://www.openssd-project.org" />
	</analytic>
	<monogr>
		<title level="j">The OpenSSD Project</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<ptr target="http://highscalability.com/blog/2012/3/12/google-taming-the-long-latency-tail-when-more-machines-equal.html" />
		<title level="m">Google: Taming The Long Latency Tail -When More Machines Equals Worse Results</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="http://www.crucial.com/usa/en/storage-ssd-m550" />
		<title level="m">The Crucial M550 SSD</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="https://aws.amazon.com/blogs/aws/new-ssd-backed-elastic-block-storage/" />
		<title level="m">New SSD-Backed Elastic Block Storage</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="http://www.myce.com/news/report-ssd-market-doubles-optical-drive-shipment-rapidly-down-70415/" />
		<title level="m">Report: SSD market doubles, optical drive shipment rapidly down</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pre-emptive garbage collection of memory blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sandisk</surname></persName>
		</author>
		<ptr target="https://www.google.com/patents/US8626986" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<ptr target="http://www.theregister.co.uk/2014/09/24/storage_supercapacitors/" />
		<title level="m">Supercapacitors have the power to save you from data loss</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<ptr target="http://www.storagereview.com/micron_p420m_enterprise_pcie_ssd_review" />
		<title level="m">Micron P420m Enterprise PCIe SSD Review</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<ptr target="http://www.eweek.com/cloud/microsoft-rolls-out-ssd-backed-azure-premium-cloud-storage.html" />
		<title level="m">Microsoft Rolls Out SSD-Backed Azure Premium Cloud Storage</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">What Happens Inside SSDs When the Power Goes Down?</title>
		<ptr target="http://www.army-technology.com/contractors/data_recording/solidata-technology/presswhat-happens-ssds-power-down.html" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Why SSDs don</title>
		<ptr target="http://www.zdnet.com/article/why-ssds-dont-perform/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<ptr target="http://lightnvm.io/" />
		<title level="m">Solid State Drives</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">What&apos;s the state of DWPD? endurance in industry leading enterprise SSDs</title>
		<ptr target="http://www.storagesearch.com/dwpd.html" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Design Tradeoffs for SSD Performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijayan</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Wobber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rina</forename><surname>Panigrahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the USENIX Annual Technical Conference (ATC)</title>
		<meeting>the USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Opportunistic Storage Maintenance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Amvrosiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><forename type="middle">Demke</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashvin</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 25th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Random RAIDs with Selective Exploitation of Redundancy for High Performance Video Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitzhak</forename><surname>Birk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 7th International Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV)</title>
		<meeting>7th International Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV)</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LightNVM: The Linux Open-Channel SSD Subsystem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matias</forename><surname>Bjørling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 15th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time Garbage Collection for Flash-memory Storage Systems of Real-time Embedded Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Pin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tei-Wei</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Wu</forename><surname>Lo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Embedded Computing Systems (TECS)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Purity: Building Fast, Highly-Available Enterprise Flash Storage from Commodity Components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Colgrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cary</forename><surname>Sandvig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Sears</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Tamches</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the</title>
		<meeting>the</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">ACM SIGMOD International Conference on Management of Data (SIGMOD)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Tail at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz Andr</forename><surname>Barroso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s Highly Available Key-value Store</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madan</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunavardhan</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaminathan</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Werner</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 21st ACM SIGOPS symposium on Operating systems principles (SOSP)</title>
		<meeting>21st ACM SIGOPS symposium on Operating systems principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Limplock: Understanding the Impact of Limpware on Scale-Out Cloud Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiratat</forename><surname>Patana-Anake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 4th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What Bugs Live in the Cloud? A Study of 3000+ Issues in Cloud Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanakorn</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiratat</forename><surname>Leesatapornwongsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Patana-Anake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffry</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adityatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kurnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agung</forename><surname>Eliazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Laksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincentius</forename><surname>Lukman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anang</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Satria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 5th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Why Does the Cloud Stop Computing? Lessons from Hundreds of Service Outages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haryadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Gunawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riza</forename><forename type="middle">O</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agung</forename><surname>Suminto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anang</forename><forename type="middle">D</forename><surname>Laksono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffry</forename><surname>Satria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurnia</forename><forename type="middle">J</forename><surname>Adityatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eliazar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Symposium on Cloud Computing (SoCC)</title>
		<meeting>the 7th ACM Symposium on Cloud Computing (SoCC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Leveraging Value Locality in Optimizing NAND Flash-based SSDs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Pisolkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuvan</forename><surname>Urgaonkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 9th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Tail at Store: A Revelation from Millions of Hours of Disk and SSD Deployments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gokul</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Kenchammana-Hosekote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haryadi</forename><forename type="middle">S</forename><surname>Gunawi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 14th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reducing File System Tail Latencies with Chopper</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Performance Impact and Interplay of SSD Parallelism through Advanced Commands, Allocation Strategy and Data Granularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Supercomputing (ICS)</title>
		<meeting>the 25th International Conference on Supercomputing (ICS)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flash-Aware RAID Techniques for Dependable and High-Performance Flash Memory SSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soojun</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkun</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TOC)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Characterization of Storage Workload Traces from Production Windows Servers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swaroop</forename><surname>Kavalanekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Worthington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Sharda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Workload Characterization (IISWC)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BPLRU: A Buffer Management Scheme for Improving Random Writes in Flash Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyojun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjun</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 6th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards SLO Complying SSDs Through OPS Isolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving SSD Reliability with RAID via Elastic Striping and Anywhere Parity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaeho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Noh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Dependable Systems and Networks (DSN)</title>
		<meeting>the International Conference on Dependable Systems and Networks (DSN)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Harmonia: A Globally Coordinated Garbage Collector for Arrays of Solid-State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarp</forename><surname>Oral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><forename type="middle">M</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Dillow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th IEEE Symposium on Massive Storage Systems and Technologies (MSST)</title>
		<meeting>the 27th IEEE Symposium on Massive Storage Systems and Technologies (MSST)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">F2FS: A New File System for Flash Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changman</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongho</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joo-Young</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangyeun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Semi-Preemptive Garbage Collector for Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junghee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><forename type="middle">M</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarp</forename><surname>Oral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongman</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Lifespan-aware Reliability Scheme for RAID-based Flash Storage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sehwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bitna</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kern</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyokyung</forename><surname>Bahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM Symposium on Applied Computing (SAC)</title>
		<meeting>the 2011 ACM Symposium on Applied Computing (SAC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FRA: A Flash-aware Redundancy Array of Flash Storage Devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangsup</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Ho</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th IEEE/ACM International Conference on Hardware/Software Codesign and System (CODES+ISSS)</title>
		<meeting>the 7th IEEE/ACM International Conference on Hardware/Software Codesign and System (CODES+ISSS)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sang-Won Lee, and Young Ik Eom. SFS: Random Write Considered Harmful in Solid State Drives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangnyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjin</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 10th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SDF: Software-Defined Flash for Web-Scale Internet Storage System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiding</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the 18th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Flash Reliability in Production: The Expected and the Unexpected</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Lagisetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arif</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 14th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flash on Rails: Consistent Flash Performance through Redundancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Skourtis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Maltzahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 USENIX Annual Technical Conference (ATC)</title>
		<meeting>the 2014 USENIX Annual Technical Conference (ATC)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reducing SSD Read Latency via NAND Flash Program and Erase Suspension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 10th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Write Once, Get 50% Free: Saving SSD Erase Costs Using WOM Codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gala</forename><surname>Yadgar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eitan</forename><surname>Yaakobi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Symposium on File and Storage Technologies (FAST)</title>
		<meeting>the 13th USENIX Symposium on File and Storage Technologies (FAST)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Split-Level I/O Scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Harter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salini</forename><surname>Selvaraj Kowsalya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samer</forename><surname>Al-Kiswany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rini</forename><forename type="middle">T</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">C</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remzi</forename><forename type="middle">H</forename><surname>Arpaci-Dusseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM Symposium on Operating Systems Principles (SOSP)</title>
		<meeting>the 25th ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">VSSIM: Virtual Machine based SSD Simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjip</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongwoo</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sooyong</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyuk</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th IEEE Symposium on Massive Storage Systems and Technologies (MSST)</title>
		<meeting>the 29th IEEE Symposium on Massive Storage Systems and Technologies (MSST)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
